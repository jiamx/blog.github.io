<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记录朴实无华且枯燥的生活"><title>Flink集成Hive之快速入门--以Flink1.12为例 | Jmx's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '912b6cfc43243cd27aeb428f7dbf7823';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Flink集成Hive之快速入门--以Flink1.12为例</h1><a id="logo" href="/.">Jmx's Blog</a><p class="description">Keep it Simple and Stupid!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-pied-piper-alt"> 关于</i></a><a href="/tags"><i class="fa fa-tags"> 标签</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Flink集成Hive之快速入门--以Flink1.12为例</h1><div class="post-meta">Dec 18, 2020<span> | </span><span class="category"><a href="/categories/Flink/">Flink</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2.8k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 12</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>使用Hive构建数据仓库已经成为了比较普遍的一种解决方案。目前，一些比较常见的大数据处理引擎，都无一例外兼容Hive。Flink从1.9开始支持集成Hive，不过1.9版本为beta版，不推荐在生产环境中使用。在Flink1.10版本中，标志着对 Blink的整合宣告完成，对 Hive 的集成也达到了生产级别的要求。值得注意的是，不同版本的Flink对于Hive的集成有所差异，本文将以最新的Flink1.12版本为例，阐述Flink集成Hive的简单步骤，以下是全文，希望对你有所帮助。</p>
<h2 id="Flink集成Hive的基本方式"><a href="#Flink集成Hive的基本方式" class="headerlink" title="Flink集成Hive的基本方式"></a>Flink集成Hive的基本方式</h2><p>Flink 与 Hive 的集成主要体现在以下两个方面:</p>
<ul>
<li>持久化元数据</li>
</ul>
<p>Flink利用 Hive 的 MetaStore 作为持久化的 Catalog，我们可通过<code>HiveCatalog</code>将不同会话中的 Flink 元数据存储到 Hive Metastore 中。 例如，我们可以使用<code>HiveCatalog</code>将其 Kafka的数据源表存储在 Hive Metastore 中，这样该表的元数据信息会被持久化到Hive的MetaStore对应的元数据库中，在后续的 SQL 查询中，我们可以重复使用它们。</p>
<ul>
<li>利用 Flink 来读写 Hive 的表。</li>
</ul>
<p>Flink打通了与Hive的集成，如同使用SparkSQL或者Impala操作Hive中的数据一样，我们可以使用Flink直接读写Hive中的表。</p>
<p><code>HiveCatalog</code>的设计提供了与 Hive 良好的兼容性，用户可以”开箱即用”的访问其已有的 Hive表。 不需要修改现有的 Hive Metastore，也不需要更改表的数据位置或分区。</p>
<h2 id="Flink集成Hive的步骤"><a href="#Flink集成Hive的步骤" class="headerlink" title="Flink集成Hive的步骤"></a>Flink集成Hive的步骤</h2><h3 id="Flink支持的Hive版本"><a href="#Flink支持的Hive版本" class="headerlink" title="Flink支持的Hive版本"></a>Flink支持的Hive版本</h3><table>
<thead>
<tr>
<th>大版本</th>
<th>V1</th>
<th>V2</th>
<th>V3</th>
<th>V4</th>
<th>V5</th>
<th>V6</th>
<th>V7</th>
</tr>
</thead>
<tbody><tr>
<td>1.0</td>
<td>1.0.0</td>
<td>1.0.1</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1.1</td>
<td>1.1.0</td>
<td>1.1.1</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1.2</td>
<td>1.2.0</td>
<td>1.2.1</td>
<td>1.2.2</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2.0</td>
<td>2.0.0</td>
<td>2.0.1</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2.1</td>
<td>2.1.0</td>
<td>2.1.1</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2.2</td>
<td>2.2.0</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2.3</td>
<td>2.3.0</td>
<td>2.3.1</td>
<td>2.3.2</td>
<td>2.3.3</td>
<td>2.3.4</td>
<td>2.3.5</td>
<td>2.3.6</td>
</tr>
<tr>
<td>3.1</td>
<td>3.1.0</td>
<td>3.1.1</td>
<td>3.1.2</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>值得注意的是，对于不同的Hive版本，可能在功能方面有所差异，这些差异取决于你使用的Hive版本，而不取决于Flink，一些版本的功能差异如下：</p>
<ul>
<li>Hive 内置函数在使用 Hive-1.2.0 及更高版本时支持。</li>
<li>列约束，也就是 PRIMARY KEY 和 NOT NULL，在使用 Hive-3.1.0 及更高版本时支持。</li>
<li>更改表的统计信息，在使用 Hive-1.2.0 及更高版本时支持。</li>
<li><code>DATE</code>列统计信息，在使用 Hive-1.2.0 及更高版时支持。</li>
<li>使用 Hive-2.0.x 版本时不支持写入 ORC 表。</li>
</ul>
<h3 id="依赖项"><a href="#依赖项" class="headerlink" title="依赖项"></a>依赖项</h3><p>本文以Flink1.12为例，集成的Hive版本为Hive2.3.4。集成Hive需要额外添加一些依赖jar包，并将其放置在Flink安装目录下的lib文件夹下，这样我们才能通过 Table API 或 SQL Client 与 Hive 进行交互。</p>
<p>另外，Apache Hive 是基于 Hadoop 之上构建的, 所以还需要 Hadoop 的依赖，配置好HADOOP_CLASSPATH即可。这一点非常重要，否则在使用FlinkSQL Cli查询Hive中的表时，会报如下错误：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">java.lang.ClassNotFoundException: org.apache.hadoop.mapred.JobConf</span><br></pre></td></tr></table></figure>

<p><strong>配置HADOOP_CLASSPATH，需要在/etc/profile文件中配置如下的环境变量</strong>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_CLASSPATH=`hadoop classpath`</span><br></pre></td></tr></table></figure>

<p>Flink官网提供了两种方式添加Hive的依赖项。第一种是使用 Flink 提供的 Hive Jar包(根据使用的 Metastore 的版本来选择对应的 Hive jar)，建议优先使用Flink提供的Hive jar包，这种方式比较简单方便。本文使用的就是此种方式。当然，如果你使用的Hive版本与Flink提供的Hive jar包兼容的版本不一致，你可以选择第二种方式，即别添加每个所需的 jar 包。</p>
<p>下面列举了可用的jar包及其适用的Hive版本，我们可以根据使用的Hive版本，下载对应的jar包即可。比如本文使用的Hive版本为Hive2.3.4，所以只需要下载<strong>flink-sql-connector-hive-2.3.6</strong>即可，并将其放置在Flink安装目录的lib文件夹下。</p>
<table>
<thead>
<tr>
<th align="left">Metastore version</th>
<th align="left">Maven dependency</th>
<th align="left">SQL Client JAR</th>
</tr>
</thead>
<tbody><tr>
<td align="left">1.0.0 ~ 1.2.2</td>
<td align="left"><code>flink-sql-connector-hive-1.2.2</code></td>
<td align="left"><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-hive-1.2.2_2.11/1.12.0/flink-sql-connector-hive-1.2.2_2.11-1.12.0.jar" target="_blank" rel="noopener">Download</a></td>
</tr>
<tr>
<td align="left">2.0.0 ~2.2.0</td>
<td align="left"><code>flink-sql-connector-hive-2.2.0</code></td>
<td align="left"><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-hive-2.2.0_2.11/1.12.0/flink-sql-connector-hive-2.2.0_2.11-1.12.0.jar" target="_blank" rel="noopener">Download</a></td>
</tr>
<tr>
<td align="left">2.3.0 ~2.3.6</td>
<td align="left"><code>flink-sql-connector-hive-2.3.6</code></td>
<td align="left"><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-hive-2.3.6_2.11/1.12.0/flink-sql-connector-hive-2.3.6_2.11-1.12.0.jar" target="_blank" rel="noopener">Download</a></td>
</tr>
<tr>
<td align="left">3.0.0 ~ 3.1.2</td>
<td align="left"><code>flink-sql-connector-hive-3.1.2</code></td>
<td align="left"><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-hive-3.1.2_2.11/1.12.0/flink-sql-connector-hive-3.1.2_2.11-1.12.0.jar" target="_blank" rel="noopener">Download</a></td>
</tr>
</tbody></table>
<p>上面列举的jar包，是我们在使用Flink SQL Cli所需要的jar包，除此之外，根据不同的Hive版本，还需要添加如下jar包。以Hive2.3.4为例，除了上面的一个jar包之外，还需要添加下面两个jar包：</p>
<p><strong>flink-connector-hive_2.11-1.12.0.jar</strong>和<strong>hive-exec-2.3.4.jar</strong>。其中<strong>hive-exec-2.3.4.jar</strong>包存在于Hive安装路径下的lib文件夹。<strong>flink-connector-hive_2.11-1.12.0.jar</strong>的下载地址为：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">https://repo1.maven.org/maven2/org/apache/flink/flink-connector-hive_2.11/1.12.0/</span><br></pre></td></tr></table></figure>

<blockquote>
<p>NOTE:black_nib::Flink1.12集成Hive只需要添加如下三个jar包，以Hive2.3.4为例，分别为：</p>
<p><strong>flink-sql-connector-hive-2.3.6</strong></p>
<p><strong>flink-connector-hive_2.11-1.12.0.jar</strong></p>
<p><strong>hive-exec-2.3.4.jar</strong></p>
</blockquote>
<h2 id="Flink-SQL-Cli集成Hive"><a href="#Flink-SQL-Cli集成Hive" class="headerlink" title="Flink SQL Cli集成Hive"></a>Flink SQL Cli集成Hive</h2><p>将上面的三个jar包添加至Flink的lib目录下之后，就可以使用Flink操作Hive的数据表了。以FlinkSQL Cli为例：</p>
<h3 id="配置sql-client-defaults-yaml"><a href="#配置sql-client-defaults-yaml" class="headerlink" title="配置sql-client-defaults.yaml"></a>配置sql-client-defaults.yaml</h3><p>该文件时Flink SQL Cli启动时使用的配置文件，该文件位于Flink安装目录的conf/文件夹下，具体的配置如下，主要是配置catalog：</p>
<img src="//jiamaoxiang.top/2020/12/18/Flink集成Hive之快速入门-以Flink1-12为例/yarm.png" style="zoom: 67%;">

<p>除了上面的一些配置参数，Flink还提供了下面的一些其他配置参数：</p>
<table>
<thead>
<tr>
<th align="left">参数</th>
<th align="center">必选</th>
<th align="center">默认值</th>
<th align="center">类型</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">type</td>
<td align="center">是</td>
<td align="center">(无)</td>
<td align="center">String</td>
<td align="center">Catalog 的类型。 创建 HiveCatalog 时，该参数必须设置为<code>&#39;hive&#39;</code>。</td>
</tr>
<tr>
<td align="left">name</td>
<td align="center">是</td>
<td align="center">(无)</td>
<td align="center">String</td>
<td align="center">Catalog 的名字。仅在使用 YAML file 时需要指定。</td>
</tr>
<tr>
<td align="left">hive-conf-dir</td>
<td align="center">否</td>
<td align="center">(无)</td>
<td align="center">String</td>
<td align="center">指向包含 hive-site.xml 目录的 URI。 该 URI 必须是 Hadoop 文件系统所支持的类型。 如果指定一个相对 URI，即不包含 scheme，则默认为本地文件系统。如果该参数没有指定，我们会在 class path 下查找hive-site.xml。</td>
</tr>
<tr>
<td align="left">default-database</td>
<td align="center">否</td>
<td align="center">default</td>
<td align="center">String</td>
<td align="center">当一个catalog被设为当前catalog时，所使用的默认当前database。</td>
</tr>
<tr>
<td align="left">hive-version</td>
<td align="center">否</td>
<td align="center">(无)</td>
<td align="center">String</td>
<td align="center">HiveCatalog 能够自动检测使用的 Hive 版本。我们建议<strong>不要</strong>手动设置 Hive 版本，除非自动检测机制失败。</td>
</tr>
<tr>
<td align="left">hadoop-conf-dir</td>
<td align="center">否</td>
<td align="center">(无)</td>
<td align="center">String</td>
<td align="center">Hadoop 配置文件目录的路径。目前仅支持本地文件系统路径。我们推荐使用 <strong>HADOOP_CONF_DIR</strong> 环境变量来指定 Hadoop 配置。因此仅在环境变量不满足您的需求时再考虑使用该参数，例如当您希望为每个 HiveCatalog 单独设置 Hadoop 配置时。</td>
</tr>
</tbody></table>
<h3 id="操作Hive中的表"><a href="#操作Hive中的表" class="headerlink" title="操作Hive中的表"></a>操作Hive中的表</h3><p>首先启动FlinkSQL Cli，命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./bin/sql-client.sh embedded</span><br></pre></td></tr></table></figure>

<p>接下来，我们可以查看注册的catalog</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Flink SQL&gt; show catalogs;</span><br><span class="line">default_catalog</span><br><span class="line">myhive</span><br></pre></td></tr></table></figure>

<p>使用注册的myhive catalog</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Flink SQL&gt; use catalog myhive;</span><br></pre></td></tr></table></figure>

<p>假设Hive中有一张users表，在Hive中查询该表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from users;</span><br><span class="line">OK</span><br><span class="line">users.id        users.mame</span><br><span class="line">1       jack</span><br><span class="line">2       tom</span><br><span class="line">3       robin</span><br><span class="line">4       haha</span><br><span class="line">5       haha</span><br></pre></td></tr></table></figure>

<p>查看对应的数据库表，我们可以看到Hive中已经存在的表，这样就可以使用FlinkSQL操作Hive中的表，比如查询，写入数据。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Flink SQL&gt; show tables;</span><br><span class="line">Flink SQL&gt; select * from users;</span><br></pre></td></tr></table></figure>

<p><img src="//jiamaoxiang.top/2020/12/18/Flink集成Hive之快速入门-以Flink1-12为例/3.png" alt></p>
<p>向Hive表users中插入一条数据：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">Flink SQL&gt; insert into users select 6,'bob';</span><br></pre></td></tr></table></figure>

<p>再次使用Hive客户端去查询该表的数据，会发现写入了一条数据。</p>
<p>接下来，我们再在FlinkSQL Cli中创建一张kafka的数据源表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_behavior ( </span><br><span class="line">    <span class="string">`user_id`</span> <span class="built_in">BIGINT</span>, <span class="comment">-- 用户id</span></span><br><span class="line">    <span class="string">`item_id`</span> <span class="built_in">BIGINT</span>, <span class="comment">-- 商品id</span></span><br><span class="line">    <span class="string">`cat_id`</span> <span class="built_in">BIGINT</span>, <span class="comment">-- 品类id</span></span><br><span class="line">    <span class="string">`action`</span> <span class="keyword">STRING</span>, <span class="comment">-- 用户行为</span></span><br><span class="line">    <span class="string">`province`</span> <span class="built_in">INT</span>, <span class="comment">-- 用户所在的省份</span></span><br><span class="line">    <span class="string">`ts`</span> <span class="built_in">BIGINT</span>, <span class="comment">-- 用户行为发生的时间戳</span></span><br><span class="line">    <span class="string">`proctime`</span> <span class="keyword">AS</span> PROCTIME(), <span class="comment">-- 通过计算列产生一个处理时间列</span></span><br><span class="line">    <span class="string">`eventTime`</span> <span class="keyword">AS</span> TO_TIMESTAMP(FROM_UNIXTIME(ts, <span class="string">'yyyy-MM-dd HH:mm:ss'</span>)), <span class="comment">-- 事件时间</span></span><br><span class="line">     WATERMARK <span class="keyword">FOR</span> eventTime <span class="keyword">AS</span> eventTime - <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span>  <span class="comment">-- 定义watermark</span></span><br><span class="line"> ) <span class="keyword">WITH</span> ( </span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'kafka'</span>, <span class="comment">-- 使用 kafka connector</span></span><br><span class="line">    <span class="string">'topic'</span> = <span class="string">'user_behavior'</span>, <span class="comment">-- kafka主题</span></span><br><span class="line">    <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span>, <span class="comment">-- 偏移量</span></span><br><span class="line">    <span class="string">'properties.group.id'</span> = <span class="string">'group1'</span>, <span class="comment">-- 消费者组</span></span><br><span class="line">    <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-2:9092,kms-3:9092,kms-4:9092'</span>, </span><br><span class="line">    <span class="string">'format'</span> = <span class="string">'json'</span>, <span class="comment">-- 数据源格式为json</span></span><br><span class="line">    <span class="string">'json.fail-on-missing-field'</span> = <span class="string">'true'</span>,</span><br><span class="line">    <span class="string">'json.ignore-parse-errors'</span> = <span class="string">'false'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>查看表结构</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">Flink SQL&gt; DESCRIBE user_behavior;</span><br></pre></td></tr></table></figure>

<p><img src="//jiamaoxiang.top/2020/12/18/Flink集成Hive之快速入门-以Flink1-12为例/table.png" alt></p>
<p>我们可以在Hive的客户端中执行下面命令查看刚刚在Flink SQLCli中创建的表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted  user_behavior;</span><br><span class="line"><span class="comment"># Detailed Table Information             </span></span><br><span class="line">Database:               default                  </span><br><span class="line">Owner:                  null                     </span><br><span class="line">CreateTime:             Sun Dec 20 16:04:59 CST 2020     </span><br><span class="line">LastAccessTime:         UNKNOWN                  </span><br><span class="line">Retention:              0                        </span><br><span class="line">Location:               hdfs://kms-1.apache.com:8020/user/hive/warehouse/user_behavior   </span><br><span class="line">Table Type:             MANAGED_TABLE            </span><br><span class="line">Table Parameters:                </span><br><span class="line">        flink.connector         kafka               </span><br><span class="line">        flink.format            json                </span><br><span class="line">        flink.json.fail-on-missing-field        true                </span><br><span class="line">        flink.json.ignore-parse-errors  false               </span><br><span class="line">        flink.properties.bootstrap.servers      kms-2:9092,kms-3:9092,kms-4:9092</span><br><span class="line">        flink.properties.group.id       group1              </span><br><span class="line">        flink.scan.startup.mode earliest-offset     </span><br><span class="line">        flink.schema.0.data-type        BIGINT              </span><br><span class="line">        flink.schema.0.name     user_id             </span><br><span class="line">        flink.schema.1.data-type        BIGINT              </span><br><span class="line">        flink.schema.1.name     item_id             </span><br><span class="line">        flink.schema.2.data-type        BIGINT              </span><br><span class="line">        flink.schema.2.name     cat_id              </span><br><span class="line">        flink.schema.3.data-type        VARCHAR(2147483647) </span><br><span class="line">        flink.schema.3.name     action              </span><br><span class="line">        flink.schema.4.data-type        INT                 </span><br><span class="line">        flink.schema.4.name     province            </span><br><span class="line">        flink.schema.5.data-type        BIGINT              </span><br><span class="line">        flink.schema.5.name     ts                  </span><br><span class="line">        flink.schema.6.data-type        TIMESTAMP(3) NOT NULL</span><br><span class="line">        flink.schema.6.expr     PROCTIME()          </span><br><span class="line">        flink.schema.6.name     proctime            </span><br><span class="line">        flink.schema.7.data-type        TIMESTAMP(3)        </span><br><span class="line">        flink.schema.7.expr     TO_TIMESTAMP(FROM_UNIXTIME(`ts`, 'yyyy-MM-dd HH:mm:ss'))</span><br><span class="line">        flink.schema.7.name     eventTime           </span><br><span class="line">        flink.schema.watermark.0.rowtime        eventTime           </span><br><span class="line">        flink.schema.watermark.0.strategy.data-type     TIMESTAMP(3)        </span><br><span class="line">        flink.schema.watermark.0.strategy.expr  `eventTime` - INTERVAL '5' SECOND</span><br><span class="line">        flink.topic             user_behavior       </span><br><span class="line">        is_generic              true                </span><br><span class="line">        transient_lastDdlTime   1608451499          </span><br><span class="line">                 </span><br><span class="line"><span class="comment"># Storage Information            </span></span><br><span class="line">SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       </span><br><span class="line">InputFormat:            org.apache.hadoop.mapred.TextInputFormat         </span><br><span class="line">OutputFormat:           org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat   </span><br><span class="line">Compressed:             No                       </span><br><span class="line">Num Buckets:            -1                       </span><br><span class="line">Bucket Columns:         []                       </span><br><span class="line">Sort Columns:           []                       </span><br><span class="line">Storage Desc Params:             </span><br><span class="line">        serialization.format    1</span><br></pre></td></tr></table></figure>

<blockquote>
<p>NOTE:black_flag::在Flink中创建一张表，会把该表的元数据信息持久化到Hive的metastore中，我们可以在Hive的metastore中查看该表的元数据信息</p>
</blockquote>
<p>进入Hive的元数据信息库，本文使用的是MySQL。执行下面的命令：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    a.tbl_id, <span class="comment">-- 表id</span></span><br><span class="line">    from_unixtime(create_time) <span class="keyword">AS</span> create_time, <span class="comment">-- 创建时间</span></span><br><span class="line">    a.db_id, <span class="comment">-- 数据库id</span></span><br><span class="line">    b.name <span class="keyword">AS</span> db_name, <span class="comment">-- 数据库名称</span></span><br><span class="line">    a.tbl_name <span class="comment">-- 表名称</span></span><br><span class="line"><span class="keyword">FROM</span> TBLS <span class="keyword">AS</span> a</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> DBS <span class="keyword">AS</span> b <span class="keyword">ON</span> a.db_id =b.db_id</span><br><span class="line"><span class="keyword">WHERE</span> a.tbl_name = <span class="string">"user_behavior"</span>;</span><br></pre></td></tr></table></figure>

<img src="//jiamaoxiang.top/2020/12/18/Flink集成Hive之快速入门-以Flink1-12为例/4.png" style="zoom:67%;">

<h2 id="使用代码连接到-Hive"><a href="#使用代码连接到-Hive" class="headerlink" title="使用代码连接到 Hive"></a>使用代码连接到 Hive</h2><h3 id="maven依赖"><a href="#maven依赖" class="headerlink" title="maven依赖"></a>maven依赖</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Flink Dependency --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.12.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-java-bridge_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.12.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Hive Dependency --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HiveIntegrationDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().build();</span><br><span class="line">        TableEnvironment tableEnv = TableEnvironment.create(settings);</span><br><span class="line"></span><br><span class="line">        String name            = <span class="string">"myhive"</span>;</span><br><span class="line">        String defaultDatabase = <span class="string">"default"</span>;</span><br><span class="line">        String hiveConfDir = <span class="string">"/opt/modules/apache-hive-2.3.4-bin/conf"</span>;</span><br><span class="line">        </span><br><span class="line">        HiveCatalog hive = <span class="keyword">new</span> HiveCatalog(name, defaultDatabase, hiveConfDir);</span><br><span class="line">        tableEnv.registerCatalog(<span class="string">"myhive"</span>, hive);</span><br><span class="line">        <span class="comment">// 使用注册的catalog</span></span><br><span class="line">        tableEnv.useCatalog(<span class="string">"myhive"</span>);</span><br><span class="line">        <span class="comment">// 向Hive表中写入一条数据 </span></span><br><span class="line">        String insertSQL = <span class="string">"insert into users select 10,'lihua'"</span>;</span><br><span class="line"></span><br><span class="line">        TableResult result2 = tableEnv.executeSql(insertSQL);</span><br><span class="line">        System.out.println(result2.getJobClient().get().getJobStatus());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>提交程序，观察Hive表的变化：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/flink run -m kms-1:8081 \</span><br><span class="line">-c com.flink.sql.hiveintegration.HiveIntegrationDemo \</span><br><span class="line">./original-study-flink-sql-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文以最新的Flink1.12为例，阐述了Flink集成Hive的基本步骤，并对其注意事项进行了说明。文中也给出了如何通过FlinkSQL Cli和代码去操作Hive表的步骤。下一篇，将介绍Hive Catalog与Hive Dialect。</p>
</div><div class="recommended_posts"><h3>相关推荐 ☟</h3><li><a href="https://jiamaoxiang.top/2020/12/15/使用自定义分区器解决Spark-DataSet数据分区不均匀的问题/" target="_blank">使用自定义分区器解决Spark DataSet数据分区不均匀的问题</a></li><li><a href="https://jiamaoxiang.top/2020/12/11/秒懂推荐系统-Spark平台下基于物品的协同过滤推荐系统构建/" target="_blank">秒懂推荐系统-Spark平台下基于物品的协同过滤推荐系统构建</a></li><li><a href="https://jiamaoxiang.top/2020/12/06/如何管理Spark的分区/" target="_blank">如何管理Spark的分区</a></li><li><a href="https://jiamaoxiang.top/2020/11/26/数仓开发需要了解的5大SQL分析函数/" target="_blank">数仓开发需要了解的5大SQL分析函数</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Jia MaoXiang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2020/12/18/Flink集成Hive之快速入门-以Flink1-12为例/">https://jiamaoxiang.top/2020/12/18/Flink集成Hive之快速入门-以Flink1-12为例/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本文为博主原创文章，遵循CC BY-SA 4.0版权协议，转载请附上原文出处链接和本声明</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://jiamaoxiang.top/2020/12/18/Flink集成Hive之快速入门-以Flink1-12为例/" data-id="ckix547xx0035wo7qu4qpllyj" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPYAAAD2CAAAAADAeSUUAAADO0lEQVR42u3awVLjQAwEUP7/p7NXqlhCt+RQ8fB8SoXg8ZscOpLm4yO+Hl+uz+9//cx3//X5r8/v337+4xUXNjY29k3YyZJfb5081vPPz5DP1yq2DxsbG/s4dnK75PUMkL+ThGjyNWBjY2P/ZfYrHjeJtPY1NjY2NvbsJ37yELPS5XlJg42NjY2dhEcSQjlsP674pV4aNjY29tuz98u8z+uXzLexsbGx35j9KK99hMwYm+M+/1kdGxsb+yB2W4okS7ajgufx07aQioNE2NjY2Eew84XzAzpto79da7at3+Y2NjY29hHs/PDN7PhOPnhoC4/ZF4ONjY19EruNmau2ZhZ+yRigLU6wsbGxz2Dny28aPfnd2s3KW13f9tKwsbGxb85OAqwtP2alyKwhlRc5dW5jY2Nj34p9VTmRlyj5WOKqLcbGxsY+jz1r6LQlwaxF1Ta58mNG2NjY2CexZ22dfaO/DbCrtuCHXho2Njb2DdlX3Xo/HsijsYioV+8oNjY29puxZ6Eya8RvRrZ58yhqKmFjY2MfwZ7FSRJg7cPl5Uobb8UZJWxsbOxbsTdj1Dao2phsQ2s4fsbGxsY+gp0czckjbTM82DSPklYUNjY29tnsPB42o9x8AJAPbvPyBhsbG/tU9uvwG97szsMKDBsbG/vm7GRk28bVJsbytlHyzNGUGxsbG/u27Dxg2nbSpjG0aSRFo2JsbGzs49iz4Ws7lG3DbPPJ6PAlNjY29hHsWWNo8861zalZxGJjY2OfxE5Cq23654cv2yCcHfeJogsbGxv7tuxZy6b4uV+OCvKSI4+ueiOwsbGxb8ueDXdbfB5FV5U3xZQbGxsb+7bsTVN+U67MRsJ5oVKMB7CxsbGPYCf/sGnQX/voq7YRNjY29kHsR3m1BcasnTQLvKJ0wcbGxj6IvQm550tuhr7Xbu4q9rCxsbFvws4PyrQN/aQKaA8J5YPhHzYXGxsb+zh28tN/1sRvYybZxDyBi8M62NjY2H+e3SLzO+Sj5WJFbGxs7D/MbgcAbfC0Y+Z8nICNjY19NrttKuWN/tnotz3umX9V2NjY2Oex29ZMe7vNYZqk/MiH0MP5NjY2Nvb7sv8B+7y6llyZog0AAAAASUVORK5CYII=">分享</a><div class="tags"><a href="/tags/Flink/">Flink</a></div><div class="post-nav"><a class="next" href="/2020/12/15/使用自定义分区器解决Spark-DataSet数据分区不均匀的问题/">使用自定义分区器解决Spark DataSet数据分区不均匀的问题</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar-toc"><div class="stoc-article" id="sidebar-stoc"><strong class="stoc-title"><i class="fa fa-list-ul"> 目录</i></strong><div class="toc-nav" id="stoc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Flink集成Hive的基本方式"><span class="toc-number">1.</span> <span class="toc-text">Flink集成Hive的基本方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Flink集成Hive的步骤"><span class="toc-number">2.</span> <span class="toc-text">Flink集成Hive的步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Flink支持的Hive版本"><span class="toc-number">2.1.</span> <span class="toc-text">Flink支持的Hive版本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#依赖项"><span class="toc-number">2.2.</span> <span class="toc-text">依赖项</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Flink-SQL-Cli集成Hive"><span class="toc-number">3.</span> <span class="toc-text">Flink SQL Cli集成Hive</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#配置sql-client-defaults-yaml"><span class="toc-number">3.1.</span> <span class="toc-text">配置sql-client-defaults.yaml</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#操作Hive中的表"><span class="toc-number">3.2.</span> <span class="toc-text">操作Hive中的表</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用代码连接到-Hive"><span class="toc-number">4.</span> <span class="toc-text">使用代码连接到 Hive</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#maven依赖"><span class="toc-number">4.1.</span> <span class="toc-text">maven依赖</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#代码"><span class="toc-number">4.2.</span> <span class="toc-text">代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">5.</span> <span class="toc-text">总结</span></a></li></ol></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 - 2020 <a href="/." rel="nofollow">Jmx's Blog.</a>All rights reserved.<br>Thoughts on technology, life and everything else.</div></div></div><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" color="100,99,98" opacity="0.5" zindex="0.7" count="150" src="//lib.baomitu.com/canvas-nest.js/2.0.4/canvas-nest.umd.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>