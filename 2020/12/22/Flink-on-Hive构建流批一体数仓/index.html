<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记录朴实无华且枯燥的生活"><title>Flink on Hive构建流批一体数仓 | Jmx's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '912b6cfc43243cd27aeb428f7dbf7823';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Flink on Hive构建流批一体数仓</h1><a id="logo" href="/.">Jmx's Blog</a><p class="description">Keep it Simple and Stupid!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-pied-piper-alt"> 关于</i></a><a href="/tags"><i class="fa fa-tags"> 标签</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Flink on Hive构建流批一体数仓</h1><div class="post-meta">Dec 22, 2020<span> | </span><span class="category"><a href="/categories/Flink/">Flink</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3.8k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 16</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>Flink使用HiveCatalog可以通过<strong>批</strong>或者<strong>流</strong>的方式来处理Hive中的表。这就意味着Flink既可以作为Hive的一个批处理引擎，也可以通过流处理的方式来读写Hive中的表，从而为实时数仓的应用和流批一体的落地实践奠定了坚实的基础。本文将以Flink1.12为例，介绍Flink集成Hive的另外一个非常重要的方面——<strong>Hive维表JOIN(Temporal Table Join)与Flink读写Hive表的方式</strong>。以下是全文，希望本文对你有所帮助。</p>
<h2 id="Flink写入Hive表"><a href="#Flink写入Hive表" class="headerlink" title="Flink写入Hive表"></a>Flink写入Hive表</h2><p>Flink支持以<strong>批处理(Batch)和流处理(Streaming)</strong>的方式写入Hive表。当以批处理的方式写入Hive表时，只有当写入作业结束时，才可以看到写入的数据。<strong>批处理的方式写入支持append模式和overwrite模式</strong>。</p>
<h3 id="批处理模式写入"><a href="#批处理模式写入" class="headerlink" title="批处理模式写入"></a>批处理模式写入</h3><ul>
<li>向非分区表写入数据</li>
</ul>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">Flink SQL&gt; use catalog myhive; -- 使用catalog</span><br><span class="line">Flink SQL&gt; INSERT INTO users SELECT 2,'tom';</span><br><span class="line">Flink SQL&gt; set execution.type=batch; -- 使用批处理模式</span><br><span class="line">Flink SQL&gt; INSERT OVERWRITE users SELECT 2,'tom';</span><br></pre></td></tr></table></figure>

<ul>
<li>向分区表写入数据</li>
</ul>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 向静态分区表写入数据</span></span><br><span class="line">Flink SQL&gt; INSERT OVERWRITE myparttable PARTITION (my_type='type_1', my_date='2019-08-08') SELECT 'Tom', 25;</span><br><span class="line"><span class="comment">-- 向动态分区表写入数据</span></span><br><span class="line">Flink SQL&gt; INSERT OVERWRITE myparttable SELECT 'Tom', 25, 'type_1', '2019-08-08';</span><br></pre></td></tr></table></figure>

<h3 id="流处理模式写入"><a href="#流处理模式写入" class="headerlink" title="流处理模式写入"></a>流处理模式写入</h3><p>流式写入Hive表，不支持*<em>Insert overwrite *</em>方式，否则报如下错误：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[ERROR] Could not execute SQL statement. Reason:</span><br><span class="line">java.lang.IllegalStateException: Streaming mode not support overwrite.</span><br></pre></td></tr></table></figure>

<p>下面的示例是将kafka的数据流式写入Hive的分区表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 使用流处理模式</span></span><br><span class="line">Flink SQL&gt; set execution.type=streaming;</span><br><span class="line"><span class="comment">-- 使用Hive方言</span></span><br><span class="line">Flink SQL&gt; SET table.sql-dialect=hive; </span><br><span class="line"><span class="comment">-- 创建一张Hive分区表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_behavior_hive_tbl (</span><br><span class="line">   <span class="string">`user_id`</span> <span class="built_in">BIGINT</span>, <span class="comment">-- 用户id</span></span><br><span class="line">    <span class="string">`item_id`</span> <span class="built_in">BIGINT</span>, <span class="comment">-- 商品id</span></span><br><span class="line">    <span class="string">`cat_id`</span> <span class="built_in">BIGINT</span>, <span class="comment">-- 品类id</span></span><br><span class="line">    <span class="string">`action`</span> <span class="keyword">STRING</span>, <span class="comment">-- 用户行为</span></span><br><span class="line">    <span class="string">`province`</span> <span class="built_in">INT</span>, <span class="comment">-- 用户所在的省份</span></span><br><span class="line">    <span class="string">`ts`</span> <span class="built_in">BIGINT</span> <span class="comment">-- 用户行为发生的时间戳</span></span><br><span class="line">) PARTITIONED <span class="keyword">BY</span> (dt <span class="keyword">STRING</span>,hr <span class="keyword">STRING</span>,mi <span class="keyword">STRING</span>) <span class="keyword">STORED</span> <span class="keyword">AS</span> parquet  TBLPROPERTIES (</span><br><span class="line">  <span class="string">'partition.time-extractor.timestamp-pattern'</span>=<span class="string">'$dt $hr:$mi:00'</span>,</span><br><span class="line">  <span class="string">'sink.partition-commit.trigger'</span>=<span class="string">'partition-time'</span>,</span><br><span class="line">  <span class="string">'sink.partition-commit.delay'</span>=<span class="string">'0S'</span>,</span><br><span class="line">  <span class="string">'sink.partition-commit.policy.kind'</span>=<span class="string">'metastore,success-file'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 使用默认SQL方言</span></span><br><span class="line">Flink SQL&gt; SET table.sql-dialect=default; </span><br><span class="line"><span class="comment">-- 创建一张kafka数据源表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_behavior ( </span><br><span class="line">    <span class="string">`user_id`</span> <span class="built_in">BIGINT</span>, <span class="comment">-- 用户id</span></span><br><span class="line">    <span class="string">`item_id`</span> <span class="built_in">BIGINT</span>, <span class="comment">-- 商品id</span></span><br><span class="line">    <span class="string">`cat_id`</span> <span class="built_in">BIGINT</span>, <span class="comment">-- 品类id</span></span><br><span class="line">    <span class="string">`action`</span> <span class="keyword">STRING</span>, <span class="comment">-- 用户行为</span></span><br><span class="line">    <span class="string">`province`</span> <span class="built_in">INT</span>, <span class="comment">-- 用户所在的省份</span></span><br><span class="line">    <span class="string">`ts`</span> <span class="built_in">BIGINT</span>, <span class="comment">-- 用户行为发生的时间戳</span></span><br><span class="line">    <span class="string">`proctime`</span> <span class="keyword">AS</span> PROCTIME(), <span class="comment">-- 通过计算列产生一个处理时间列</span></span><br><span class="line">    <span class="string">`eventTime`</span> <span class="keyword">AS</span> TO_TIMESTAMP(FROM_UNIXTIME(ts, <span class="string">'yyyy-MM-dd HH:mm:ss'</span>)), <span class="comment">-- 事件时间</span></span><br><span class="line">     WATERMARK <span class="keyword">FOR</span> eventTime <span class="keyword">AS</span> eventTime - <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span>  <span class="comment">-- 定义watermark</span></span><br><span class="line"> ) <span class="keyword">WITH</span> ( </span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'kafka'</span>, <span class="comment">-- 使用 kafka connector</span></span><br><span class="line">    <span class="string">'topic'</span> = <span class="string">'user_behaviors'</span>, <span class="comment">-- kafka主题</span></span><br><span class="line">    <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span>, <span class="comment">-- 偏移量</span></span><br><span class="line">    <span class="string">'properties.group.id'</span> = <span class="string">'group1'</span>, <span class="comment">-- 消费者组</span></span><br><span class="line">    <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-2:9092,kms-3:9092,kms-4:9092'</span>, </span><br><span class="line">    <span class="string">'format'</span> = <span class="string">'json'</span>, <span class="comment">-- 数据源格式为json</span></span><br><span class="line">    <span class="string">'json.fail-on-missing-field'</span> = <span class="string">'true'</span>,</span><br><span class="line">    <span class="string">'json.ignore-parse-errors'</span> = <span class="string">'false'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>关于Hive表的一些属性解释：</p>
<ul>
<li><p><strong>partition.time-extractor.timestamp-pattern</strong></p>
<ul>
<li>默认值：(none)</li>
<li>解释：分区时间抽取器，与 DDL 中的分区字段保持一致,如果是按天分区，则可以是<strong>$dt</strong>，如果是按年(year)月(month)日(day)时(hour)进行分区，则该属性值为：<code>$year-$month-$day $hour:00:00</code>，如果是按天时进行分区，则该属性值为：<code>$dt $hour:00:00</code>;</li>
</ul>
</li>
<li><p><strong>sink.partition-commit.trigger</strong></p>
<ul>
<li>默认值：<strong>process-time</strong></li>
<li>解释：分区触发器类型，可选 <strong>process-time 或partition-time</strong>。<ul>
<li><strong>process-time</strong>：不需要时间提取器和水位线，当当前时间大于分区创建时间 + sink.partition-commit.delay 中定义的时间，提交分区；</li>
<li><strong>partition-time</strong>：需要 Source 表中定义 watermark，当 watermark &gt; 提取到的分区时间 +sink.partition-commit.delay 中定义的时间，提交分区；</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>sink.partition-commit.delay</strong></p>
<ul>
<li>默认值：0S</li>
<li>解释：分区提交的延时时间，如果是按天分区，则该属性的值为：<strong>1d</strong>，如果是按小时分区，则该属性值为<strong>1h</strong>;</li>
</ul>
</li>
<li><p><strong>sink.partition-commit.policy.kind</strong></p>
<ul>
<li><p>默认值：(none)</p>
</li>
<li><p>解释：提交分区的策略，用于通知下游的应用该分区已经完成了写入，也就是说该分区的数据可以被访问读取。可选的值如下：</p>
<ul>
<li><strong>metastore</strong>：添加分区的元数据信息，仅Hive表支持该值配置</li>
<li><strong>success-file</strong>：在表的存储路径下添加一个<code>_SUCCESS</code>文件</li>
</ul>
<p>可以同时配置上面的两个值，比如<strong>metastore,success-file</strong></p>
</li>
</ul>
</li>
</ul>
<p>执行流式写入Hive表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- streaming sql,将数据写入Hive表</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> user_behavior_hive_tbl </span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    user_id,</span><br><span class="line">    item_id,</span><br><span class="line">    cat_id,</span><br><span class="line">    <span class="keyword">action</span>,</span><br><span class="line">    province,</span><br><span class="line">    ts,</span><br><span class="line">    FROM_UNIXTIME(ts, <span class="string">'yyyy-MM-dd'</span>),</span><br><span class="line">    FROM_UNIXTIME(ts, <span class="string">'HH'</span>),</span><br><span class="line">    FROM_UNIXTIME(ts, <span class="string">'mm'</span>)</span><br><span class="line"><span class="keyword">FROM</span> user_behavior;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- batch sql,查询Hive表的分区数据</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> user_behavior_hive_tbl <span class="keyword">WHERE</span> dt=<span class="string">'2021-01-04'</span> <span class="keyword">AND</span>  hr=<span class="string">'16'</span> <span class="keyword">AND</span> mi = <span class="string">'46'</span>;</span><br></pre></td></tr></table></figure>

<p>同时查看Hive表的分区数据：</p>
<p><img src="//jiamaoxiang.top/2020/12/22/Flink-on-Hive构建流批一体数仓/%E5%88%86%E5%8C%BA.png" alt></p>
<blockquote>
<p>尖叫提示：</p>
<p>1.Flink读取Hive表默认使用的是batch模式，如果要使用流式读取Hive表，需要而外指定一些参数，见下文。</p>
<p>2.只有在完成 Checkpoint 之后，文件才会从 In-progress 状态变成 Finish 状态，同时生成<code>_SUCCESS</code>文件，所以，Flink流式写入Hive表需要开启并配置 Checkpoint。对于Flink SQL Client而言，需要在flink-conf.yaml中开启CheckPoint，配置内容为：</p>
<p>state.backend: filesystem<br>execution.checkpointing.externalized-checkpoint-retention:RETAIN_ON_CANCELLATION<br>execution.checkpointing.interval: 60s<br>execution.checkpointing.mode: EXACTLY_ONCE<br>state.savepoints.dir: hdfs://kms-1:8020/flink-savepoints</p>
</blockquote>
<h2 id="Flink读取Hive表"><a href="#Flink读取Hive表" class="headerlink" title="Flink读取Hive表"></a>Flink读取Hive表</h2><p>Flink支持以<strong>批处理(Batch)和流处理(Streaming)</strong>的方式读取Hive中的表。批处理的方式与Hive的本身查询类似，即只在提交查询的时刻查询一次Hive表。流处理的方式将会持续地监控Hive表，并且会增量地提取新的数据。<strong>默认情况下，Flink是以批处理的方式读取Hive表</strong>。</p>
<p>关于流式读取Hive表，Flink既支持分区表又支持非分区表。对于分区表而言，Flink将会监控新产生的分区数据，并以增量的方式读取这些数据。对于非分区表，Flink会监控Hive表存储路径文件夹里面的新文件，并以增量的方式读取新的数据。</p>
<p>Flink读取Hive表可以配置一下参数：</p>
<ul>
<li><p><strong>streaming-source.enable</strong></p>
<ul>
<li>默认值：<strong>false</strong></li>
<li>解释：是否开启流式读取 Hive 表，默认不开启。</li>
</ul>
</li>
<li><p><strong>streaming-source.partition.include</strong></p>
<ul>
<li>默认值：<strong>all</strong></li>
<li>解释：配置读取Hive的分区，包括两种方式：<strong>all和latest</strong>。all意味着读取所有分区的数据，latest表示只读取最新的分区数据。值得注意的是，latest方式只能用于开启了流式读取Hive表，并用于维表JOIN的场景。</li>
</ul>
</li>
<li><p><strong>streaming-source.monitor-interval</strong></p>
<ul>
<li>默认值：<strong>None</strong></li>
<li>解释：持续监控Hive表分区或者文件的时间间隔。值得注意的是，当以流的方式读取Hive表时，该参数的默认值是<strong>1m</strong>，即1分钟。当temporal join时，默认的值是<strong>60m</strong>，即1小时。另外，该参数配置不宜过短 ，最短是1 个小时，因为目前的实现是每个 task 都会查询 metastore，高频的查可能会对metastore 产生过大的压力。</li>
</ul>
</li>
<li><p><strong>streaming-source.partition-order</strong></p>
<ul>
<li>默认值：<strong>partition-name</strong></li>
<li>解释：streaming source的分区顺序。默认的是<strong>partition-name</strong>，表示使用默认分区名称顺序加载最新分区，也是推荐使用的方式。除此之外还有两种方式，分别为：<strong>create-time和partition-time</strong>。其中create-time表示使用分区文件创建时间顺序。partition-time表示使用分区时间顺序。指的注意的是，对于非分区表，该参数的默认值为：<strong>create-time</strong>。</li>
</ul>
</li>
<li><p><strong>streaming-source.consume-start-offset</strong></p>
<ul>
<li>默认值：<strong>None</strong></li>
<li>解释：流式读取Hive表的起始偏移量。</li>
</ul>
</li>
<li><h5 id="partition-time-extractor-kind"><a href="#partition-time-extractor-kind" class="headerlink" title="partition.time-extractor.kind"></a>partition.time-extractor.kind</h5><ul>
<li>默认值：default</li>
<li>分区时间提取器类型。用于从分区中提取时间，支持default和自定义。如果使用default，则需要通过参数<code>partition.time-extractor.timestamp-pattern</code>配置时间戳提取的正则表达式。</li>
</ul>
<p>在 SQL Client 中需要显示地开启 SQL Hint 功能</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink SQL&gt; set table.dynamic-table-options.enabled= true;</span><br></pre></td></tr></table></figure>

<p>使用SQLHint流式查询Hive表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> user_behavior_hive_tbl <span class="comment">/*+ OPTIONS('streaming-source.enable'='true', 'streaming-source.consume-start-offset'='2021-01-03') */</span>;</span><br></pre></td></tr></table></figure>

<h2 id="Hive维表JOIN"><a href="#Hive维表JOIN" class="headerlink" title="Hive维表JOIN"></a>Hive维表JOIN</h2><p>Flink 1.12 支持了 Hive 最新的分区作为时态表的功能，可以通过 SQL 的方式直接关联 Hive 分区表的最新分区，并且会自动监听最新的 Hive 分区，当监控到新的分区后，会自动地做维表数据的全量替换。</p>
<p>Flink支持的是processing-time的temporal join，也就是说总是与最新版本的时态表进行JOIN。另外，Flink既支持非分区表的temporal join，又支持分区表的temporal join。对于分区表而言，Flink会监听Hive表的最新分区数据。值得注意的是，Flink尚不支持 event-time temporal join。</p>
<h3 id="Temporal-Join最新分区"><a href="#Temporal-Join最新分区" class="headerlink" title="Temporal Join最新分区"></a>Temporal Join最新分区</h3><p>对于一张随着时间变化的Hive分区表，Flink可以读取该表的数据作为一个无界流。<strong>如果Hive分区表的每个分区都包含全量的数据，那么每个分区将做为一个时态表的版本数据</strong>，即将最新的分区数据作为一个全量维表数据。值得注意的是，该功能特点仅支持Flink的STREAMING模式。</p>
<p>使用 Hive 最新分区作为 Tempmoral table 之前，需要设置必要的两个参数：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="string">'streaming-source.enable'</span> = <span class="string">'true'</span>,  </span><br><span class="line"><span class="string">'streaming-source.partition.include'</span> = <span class="string">'latest'</span></span><br></pre></td></tr></table></figure>

<p>除此之外还有一些其他的参数，关于参数的解释见上面的分析。<strong>我们在使用Hive维表的时候，既可以在创建Hive表时指定具体的参数，也可以使用SQL Hint的方式动态指定参数</strong>。一个Hive维表的创建模板如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 使用Hive的sql方言</span></span><br><span class="line"><span class="keyword">SET</span> table.sql-dialect=hive;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> dimension_table (</span><br><span class="line">  product_id <span class="keyword">STRING</span>,</span><br><span class="line">  product_name <span class="keyword">STRING</span>,</span><br><span class="line">  unit_price <span class="built_in">DECIMAL</span>(<span class="number">10</span>, <span class="number">4</span>),</span><br><span class="line">  pv_count <span class="built_in">BIGINT</span>,</span><br><span class="line">  like_count <span class="built_in">BIGINT</span>,</span><br><span class="line">  comment_count <span class="built_in">BIGINT</span>,</span><br><span class="line">  update_time <span class="built_in">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  update_user <span class="keyword">STRING</span>,</span><br><span class="line">  ...</span><br><span class="line">) PARTITIONED <span class="keyword">BY</span> (pt_year <span class="keyword">STRING</span>, pt_month <span class="keyword">STRING</span>, pt_day <span class="keyword">STRING</span>) TBLPROPERTIES (</span><br><span class="line">  <span class="comment">-- 方式1：按照分区名排序来识别最新分区(推荐使用该种方式)</span></span><br><span class="line">  <span class="string">'streaming-source.enable'</span> = <span class="string">'true'</span>, <span class="comment">-- 开启Streaming source</span></span><br><span class="line">  <span class="string">'streaming-source.partition.include'</span> = <span class="string">'latest'</span>,<span class="comment">-- 选择最新分区</span></span><br><span class="line">  <span class="string">'streaming-source.monitor-interval'</span> = <span class="string">'12 h'</span>,<span class="comment">-- 每12小时加载一次最新分区数据</span></span><br><span class="line">  <span class="string">'streaming-source.partition-order'</span> = <span class="string">'partition-name'</span>,  <span class="comment">-- 按照分区名排序</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">-- 方式2:分区文件的创建时间排序来识别最新分区</span></span><br><span class="line">  <span class="string">'streaming-source.enable'</span> = <span class="string">'true'</span>,</span><br><span class="line">  <span class="string">'streaming-source.partition.include'</span> = <span class="string">'latest'</span>,</span><br><span class="line">  <span class="string">'streaming-source.partition-order'</span> = <span class="string">'create-time'</span>,<span class="comment">-- 分区文件的创建时间排序</span></span><br><span class="line">  <span class="string">'streaming-source.monitor-interval'</span> = <span class="string">'12 h'</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">-- 方式3:按照分区时间排序来识别最新分区</span></span><br><span class="line">  <span class="string">'streaming-source.enable'</span> = <span class="string">'true'</span>,</span><br><span class="line">  <span class="string">'streaming-source.partition.include'</span> = <span class="string">'latest'</span>,</span><br><span class="line">  <span class="string">'streaming-source.monitor-interval'</span> = <span class="string">'12 h'</span>,</span><br><span class="line">  <span class="string">'streaming-source.partition-order'</span> = <span class="string">'partition-time'</span>, <span class="comment">-- 按照分区时间排序</span></span><br><span class="line">  <span class="string">'partition.time-extractor.kind'</span> = <span class="string">'default'</span>,</span><br><span class="line">  <span class="string">'partition.time-extractor.timestamp-pattern'</span> = <span class="string">'$pt_year-$pt_month-$pt_day 00:00:00'</span> </span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>有了上面的Hive维表，我们就可以使用该维表与Kafka的实时流数据进行JOIN，得到相应的宽表数据。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 使用default sql方言</span></span><br><span class="line"><span class="keyword">SET</span> table.sql-dialect=<span class="keyword">default</span>;</span><br><span class="line"><span class="comment">-- kafka实时流数据表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> orders_table (</span><br><span class="line">  order_id <span class="keyword">STRING</span>,</span><br><span class="line">  order_amount <span class="keyword">DOUBLE</span>,</span><br><span class="line">  product_id <span class="keyword">STRING</span>,</span><br><span class="line">  log_ts <span class="built_in">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  proctime <span class="keyword">as</span> PROCTIME()</span><br><span class="line">) <span class="keyword">WITH</span> (...);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 将流表与hive最新分区数据关联 </span></span><br><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> orders_table <span class="keyword">AS</span> orders</span><br><span class="line"><span class="keyword">JOIN</span> dimension_table <span class="keyword">FOR</span> SYSTEM_TIME <span class="keyword">AS</span> <span class="keyword">OF</span> orders.proctime <span class="keyword">AS</span> dim </span><br><span class="line"><span class="keyword">ON</span> orders.product_id = dim.product_id;</span><br></pre></td></tr></table></figure>

<p>除了在定义Hive维表时指定相关的参数，我们还可以通过SQL Hint的方式动态指定相关的参数，具体方式如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> orders_table <span class="keyword">AS</span> orders</span><br><span class="line"><span class="keyword">JOIN</span> dimension_table</span><br><span class="line"><span class="comment">/*+ OPTIONS('streaming-source.enable'='true',             </span></span><br><span class="line"><span class="comment">    'streaming-source.partition.include' = 'latest',</span></span><br><span class="line"><span class="comment">    'streaming-source.monitor-interval' = '1 h',</span></span><br><span class="line"><span class="comment">    'streaming-source.partition-order' = 'partition-name') */</span></span><br><span class="line"><span class="keyword">FOR</span> SYSTEM_TIME <span class="keyword">AS</span> <span class="keyword">OF</span> orders.proctime <span class="keyword">AS</span> dim <span class="comment">-- 时态表(维表)</span></span><br><span class="line"><span class="keyword">ON</span> orders.product_id = dim.product_id;</span><br></pre></td></tr></table></figure>

<h3 id="Temporal-Join最新表"><a href="#Temporal-Join最新表" class="headerlink" title="Temporal Join最新表"></a>Temporal Join最新表</h3><p>对于Hive的非分区表，当使用temporal join时，整个Hive表会被缓存到Slot内存中，然后根据流中的数据对应的key与其进行匹配。使用最新的Hive表进行temporal join不需要进行额外的配置，我们只需要配置一个Hive表缓存的TTL时间，该时间的作用是：当缓存过期时，就会重新扫描Hive表并加载最新的数据。</p>
<ul>
<li><p><strong>lookup.join.cache.ttl</strong></p>
<ul>
<li>默认值：60min</li>
<li>解释：表示缓存时间。由于 Hive 维表会把维表所有数据缓存在 TM 的内存中，当维表数据量很大时，很容易造成 OOM。当然TTL的时间也不能太短，因为会频繁地加载数据，从而影响性能。</li>
</ul>
<blockquote>
<p>尖叫提示：</p>
<p>当使用此种方式时，Hive表必须是有界的lookup表，即非Streaming Source的时态表，换句话说，该表的属性<strong>streaming-source.enable = false</strong>。</p>
<p>如果要使用Streaming Source的时态表，记得配置<strong>streaming-source.monitor-interval</strong>的值，即数据更新的时间间隔。</p>
</blockquote>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- Hive维表数据使用批处理的方式按天装载</span></span><br><span class="line"><span class="keyword">SET</span> table.sql-dialect=hive;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> dimension_table (</span><br><span class="line">  product_id <span class="keyword">STRING</span>,</span><br><span class="line">  product_name <span class="keyword">STRING</span>,</span><br><span class="line">  unit_price <span class="built_in">DECIMAL</span>(<span class="number">10</span>, <span class="number">4</span>),</span><br><span class="line">  pv_count <span class="built_in">BIGINT</span>,</span><br><span class="line">  like_count <span class="built_in">BIGINT</span>,</span><br><span class="line">  comment_count <span class="built_in">BIGINT</span>,</span><br><span class="line">  update_time <span class="built_in">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  update_user <span class="keyword">STRING</span>,</span><br><span class="line">  ...</span><br><span class="line">) TBLPROPERTIES (</span><br><span class="line">  <span class="string">'streaming-source.enable'</span> = <span class="string">'false'</span>, <span class="comment">-- 关闭streaming source</span></span><br><span class="line">  <span class="string">'streaming-source.partition.include'</span> = <span class="string">'all'</span>,  <span class="comment">-- 读取所有数据</span></span><br><span class="line">  <span class="string">'lookup.join.cache.ttl'</span> = <span class="string">'12 h'</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">-- kafka事实表</span></span><br><span class="line"><span class="keyword">SET</span> table.sql-dialect=<span class="keyword">default</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> orders_table (</span><br><span class="line">  order_id <span class="keyword">STRING</span>,</span><br><span class="line">  order_amount <span class="keyword">DOUBLE</span>,</span><br><span class="line">  product_id <span class="keyword">STRING</span>,</span><br><span class="line">  log_ts <span class="built_in">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  proctime <span class="keyword">as</span> PROCTIME()</span><br><span class="line">) <span class="keyword">WITH</span> (...);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Hive维表join，Flink会加载该维表的所有数据到内存中</span></span><br><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> orders_table <span class="keyword">AS</span> orders</span><br><span class="line"><span class="keyword">JOIN</span> dimension_table <span class="keyword">FOR</span> SYSTEM_TIME <span class="keyword">AS</span> <span class="keyword">OF</span> orders.proctime <span class="keyword">AS</span> dim</span><br><span class="line"><span class="keyword">ON</span> orders.product_id = dim.product_id;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>尖叫提示：</p>
<p>1.每一个子任务都需要缓存一份维表的全量数据，一定要确保TM的task Slot 大小能够容纳维表的数据量；</p>
<p>2.推荐将streaming-source.monitor-interval和lookup.join.cache.ttl的值设为一个较大的数，因为频繁的更新和加载数据会影响性能。</p>
<p>3.当缓存的维表数据需要重新刷新时，目前的做法是将整个表进行加载，因此不能够将新数据与旧数据区分开来。</p>
</blockquote>
<h3 id="Hive维表JOIN示例"><a href="#Hive维表JOIN示例" class="headerlink" title="Hive维表JOIN示例"></a>Hive维表JOIN示例</h3><p>假设维表的数据是通过批处理的方式(比如每天)装载至Hive中，而Kafka中的事实流数据需要与该维表进行JOIN，从而构建一个宽表数据，这个时候就可以使用Hive的维表JOIN。</p>
<ul>
<li>创建一张kafka数据源表,实时流</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> table.sql-dialect=<span class="keyword">default</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> fact_user_behavior ( </span><br><span class="line">    <span class="string">`user_id`</span> <span class="built_in">BIGINT</span>, <span class="comment">-- 用户id</span></span><br><span class="line">    <span class="string">`item_id`</span> <span class="built_in">BIGINT</span>, <span class="comment">-- 商品id</span></span><br><span class="line">    <span class="string">`action`</span> <span class="keyword">STRING</span>, <span class="comment">-- 用户行为</span></span><br><span class="line">    <span class="string">`province`</span> <span class="built_in">INT</span>, <span class="comment">-- 用户所在的省份</span></span><br><span class="line">    <span class="string">`ts`</span> <span class="built_in">BIGINT</span>, <span class="comment">-- 用户行为发生的时间戳</span></span><br><span class="line">    <span class="string">`proctime`</span> <span class="keyword">AS</span> PROCTIME(), <span class="comment">-- 通过计算列产生一个处理时间列</span></span><br><span class="line">    <span class="string">`eventTime`</span> <span class="keyword">AS</span> TO_TIMESTAMP(FROM_UNIXTIME(ts, <span class="string">'yyyy-MM-dd HH:mm:ss'</span>)), <span class="comment">-- 事件时间</span></span><br><span class="line">     WATERMARK <span class="keyword">FOR</span> eventTime <span class="keyword">AS</span> eventTime - <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span>  <span class="comment">-- 定义watermark</span></span><br><span class="line"> ) <span class="keyword">WITH</span> ( </span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'kafka'</span>, <span class="comment">-- 使用 kafka connector</span></span><br><span class="line">    <span class="string">'topic'</span> = <span class="string">'user_behaviors'</span>, <span class="comment">-- kafka主题</span></span><br><span class="line">    <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span>, <span class="comment">-- 偏移量</span></span><br><span class="line">    <span class="string">'properties.group.id'</span> = <span class="string">'group1'</span>, <span class="comment">-- 消费者组</span></span><br><span class="line">    <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-2:9092,kms-3:9092,kms-4:9092'</span>, </span><br><span class="line">    <span class="string">'format'</span> = <span class="string">'json'</span>, <span class="comment">-- 数据源格式为json</span></span><br><span class="line">    <span class="string">'json.fail-on-missing-field'</span> = <span class="string">'true'</span>,</span><br><span class="line">    <span class="string">'json.ignore-parse-errors'</span> = <span class="string">'false'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<ul>
<li>创建一张Hive维表</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> table.sql-dialect=hive;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> dim_item (</span><br><span class="line">  item_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  item_name <span class="keyword">STRING</span>,</span><br><span class="line">  unit_price <span class="built_in">DECIMAL</span>(<span class="number">10</span>, <span class="number">4</span>)</span><br><span class="line">) PARTITIONED <span class="keyword">BY</span> (dt <span class="keyword">STRING</span>) TBLPROPERTIES (</span><br><span class="line">  <span class="string">'streaming-source.enable'</span> = <span class="string">'true'</span>,</span><br><span class="line">  <span class="string">'streaming-source.partition.include'</span> = <span class="string">'latest'</span>,</span><br><span class="line">  <span class="string">'streaming-source.monitor-interval'</span> = <span class="string">'12 h'</span>,</span><br><span class="line">  <span class="string">'streaming-source.partition-order'</span> = <span class="string">'partition-name'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<ul>
<li>关联Hive维表的最新数据</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    fact.item_id,</span><br><span class="line">    dim.item_name,</span><br><span class="line">    <span class="keyword">count</span>(*) <span class="keyword">AS</span> buy_cnt</span><br><span class="line"><span class="keyword">FROM</span> fact_user_behavior <span class="keyword">AS</span> fact</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> dim_item <span class="keyword">FOR</span> SYSTEM_TIME <span class="keyword">AS</span> <span class="keyword">OF</span> fact.proctime <span class="keyword">AS</span> dim</span><br><span class="line"><span class="keyword">ON</span> fact.item_id = dim.item_id</span><br><span class="line"><span class="keyword">WHERE</span> fact.action = <span class="string">'buy'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> fact.item_id,dim.item_name;</span><br></pre></td></tr></table></figure>

<p>使用SQL Hint方式，关联非分区的Hive维表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> table.dynamic-<span class="keyword">table</span>-options.enabled= <span class="literal">true</span>; </span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    fact.item_id,</span><br><span class="line">    dim.item_name,</span><br><span class="line">    <span class="keyword">count</span>(*) <span class="keyword">AS</span> buy_cnt</span><br><span class="line"><span class="keyword">FROM</span> fact_user_behavior <span class="keyword">AS</span> fact</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> dim_item1</span><br><span class="line"><span class="comment">/*+ OPTIONS('streaming-source.enable'='false',             </span></span><br><span class="line"><span class="comment">    'streaming-source.partition.include' = 'all',</span></span><br><span class="line"><span class="comment">    'lookup.join.cache.ttl' = '12 h') */</span></span><br><span class="line"><span class="keyword">FOR</span> SYSTEM_TIME <span class="keyword">AS</span> <span class="keyword">OF</span> fact.proctime <span class="keyword">AS</span> dim</span><br><span class="line"><span class="keyword">ON</span> fact.item_id = dim.item_id</span><br><span class="line"><span class="keyword">WHERE</span> fact.action = <span class="string">'buy'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> fact.item_id,dim.item_name;</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文以最新版本的Flink1.12为例，介绍了Flink读写Hive的不同方式，并对每种方式给出了相应的使用示例。在实际应用中，通常有将实时数据流与 Hive 维表 join 来构造宽表的需求，Flink提供了Hive维表JOIN，可以简化用户使用的复杂度。本文在最后详细说明了Flink进行Hive维表JOIN的基本步骤以及使用示例，希望对你有所帮助。</p>
<blockquote>
<p>公众号『大数据技术与数仓』，回复『资料』领取大数据资料包</p>
</blockquote>
</div><div class="recommended_posts"><h3>相关推荐 ☟</h3><li><a href="https://jiamaoxiang.top/2021/01/16/Greenplum5-9生产环境集群部署/" target="_blank">Greenplum5.9生产环境集群部署</a></li><li><a href="https://jiamaoxiang.top/2021/01/08/实时数仓-以upsert的方式读写Kafka数据——以Flink1-12为例/" target="_blank">实时数仓|以upsert的方式读写Kafka数据——以Flink1.12为例</a></li><li><a href="https://jiamaoxiang.top/2020/12/21/Flink集成Hive之Hive-Catalog与Hive-Dialect-以Flink1-12为例/" target="_blank">Flink集成Hive之Hive Catalog与Hive Dialect--以Flink1.12为例</a></li><li><a href="https://jiamaoxiang.top/2020/12/18/Flink集成Hive之快速入门-以Flink1-12为例/" target="_blank">Flink集成Hive之快速入门--以Flink1.12为例</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Jia MaoXiang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2020/12/22/Flink-on-Hive构建流批一体数仓/">https://jiamaoxiang.top/2020/12/22/Flink-on-Hive构建流批一体数仓/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本文为博主原创文章，遵循CC BY-SA 4.0版权协议，转载请附上原文出处链接和本声明</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://jiamaoxiang.top/2020/12/22/Flink-on-Hive构建流批一体数仓/" data-id="ckjzju0ko005vrc7qtki2vg4p" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACuklEQVR42u3aQU7DQAwF0N7/0rBhgVTSfNtjKNLLqqKQ5M1ixnz78Yivj4vr+dur3//+86v7X/3k+T6HLzw8PLxDr548IAe8fuLkfV7fDQ8PD2+P93qzzj9Hj4wPgOTzjQUPDw/vDXjVUrhXpveOHzw8PLz/xcsDiGrZ/RYHAx4eHl7rn/8kUMjj3bMx8YGsBQ8PD6+TDDzyyOBvP6/09/Dw8PDGXfXqhpts3MnxMH/Pr/vg4eHhLfDm0UCvFE5GqSZLjIeHh7fHy4PRfNPvDVrNl+OHAwYPDw9vmZc3qHpbeVJG53eIhgnw8PDwlnnJdjx58GSwoHdnPDw8vA3eTUoxKHZ7i9JryJWbYXh4eHgD3tkGWB4ZzIPjwrmHh4eHd5SXb+JJfFCNg3PG6KjAw8PDW+Plm+/kJXotsdFRhIeHh/crvEnIW12y/NjID7BmVo2Hh4cX8yYpaN64yn8nX9D1kBcPDw/v6W+r7fzy2FM8XJWMEZSLezw8PLw1XnWDnke3kxC5EGrg4eHhrfGqL/G65K2WxdX7FApuPDw8vDVevtVO4tckC0lK9vxbPDw8vD1eHtH22vzz5UgK9KikxsPDwzvKy4eu8kZXYRNvDS7cPAsPDw/vV3jVDX0y5FoNOKoFPR4eHt4GLw8O8rGAcpuquKzlowsPDw9vjVel5s2tXqTba5U1p73w8PDwBrxJQZy09ntNr9Fhg4eHh7fMy6OB+VY+OTDy4TA8PDy8s7yP4pVv/fNCubcceHh4eNu83oabF77V+HU+WNC88PDw8Iq85DBIQtvkYEhI1eW4AePh4eGt8Sb5Z7I0E1ivw4WHh4f3Prz81snxMBliiGIIPDw8vDfg9ULbXkYyGefCw8PD2+ZVC998o8/HCOZ3uzwY8PDw8I7yqv/wzwvfJKQol87z/h4eHh7ePe8TV27QdUMuZ/4AAAAASUVORK5CYII=">分享</a><div class="tags"><a href="/tags/Flink/">Flink</a></div><div class="post-nav"><a class="pre" href="/2021/01/08/实时数仓-以upsert的方式读写Kafka数据——以Flink1-12为例/">实时数仓|以upsert的方式读写Kafka数据——以Flink1.12为例</a><a class="next" href="/2020/12/21/Flink集成Hive之Hive-Catalog与Hive-Dialect-以Flink1-12为例/">Flink集成Hive之Hive Catalog与Hive Dialect--以Flink1.12为例</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar-toc"><div class="stoc-article" id="sidebar-stoc"><strong class="stoc-title"><i class="fa fa-list-ul"> 目录</i></strong><div class="toc-nav" id="stoc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Flink写入Hive表"><span class="toc-number">1.</span> <span class="toc-text">Flink写入Hive表</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#批处理模式写入"><span class="toc-number">1.1.</span> <span class="toc-text">批处理模式写入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#流处理模式写入"><span class="toc-number">1.2.</span> <span class="toc-text">流处理模式写入</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Flink读取Hive表"><span class="toc-number">2.</span> <span class="toc-text">Flink读取Hive表</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#partition-time-extractor-kind"><span class="toc-number">2.0.0.1.</span> <span class="toc-text">partition.time-extractor.kind</span></a></li></ol></li></ol><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive维表JOIN"><span class="toc-number">3.</span> <span class="toc-text">Hive维表JOIN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Temporal-Join最新分区"><span class="toc-number">3.1.</span> <span class="toc-text">Temporal Join最新分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Temporal-Join最新表"><span class="toc-number">3.2.</span> <span class="toc-text">Temporal Join最新表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive维表JOIN示例"><span class="toc-number">3.3.</span> <span class="toc-text">Hive维表JOIN示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">4.</span> <span class="toc-text">总结</span></a></li></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 - 2021 <a href="/." rel="nofollow">Jmx's Blog.</a>All rights reserved.<br>Thoughts on technology, life and everything else.</div></div></div><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" color="100,99,98" opacity="0.5" zindex="0.7" count="150" src="//lib.baomitu.com/canvas-nest.js/2.0.4/canvas-nest.umd.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>