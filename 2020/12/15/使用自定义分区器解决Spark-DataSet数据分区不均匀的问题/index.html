<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记录朴实无华且枯燥的生活"><title>使用自定义分区器解决Spark DataSet数据分区不均匀的问题 | Jmx's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '912b6cfc43243cd27aeb428f7dbf7823';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">使用自定义分区器解决Spark DataSet数据分区不均匀的问题</h1><a id="logo" href="/.">Jmx's Blog</a><p class="description">Keep it Simple and Stupid!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-pied-piper-alt"> 关于</i></a><a href="/tags"><i class="fa fa-tags"> 标签</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">使用自定义分区器解决Spark DataSet数据分区不均匀的问题</h1><div class="post-meta">Dec 15, 2020<span> | </span><span class="category"><a href="/categories/Spark/">Spark</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.5k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 6</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>在<a href="https://mp.weixin.qq.com/s/o30K2Rdjx8fqu9Vapcvb1Q" target="_blank" rel="noopener">如何管理Spark的分区</a>一文中，介绍了Spark是如何管理分区的，分别解释了Spark提供的两种分区方法，并给出了相应的使用示例和分析，感兴趣的可以参考之前的分享。我们知道，Apache Spark通常用于以分布式方式处理大规模数据集，既然是分布式，就会面临一个问题：<strong>数据是否均匀地分布</strong>。当数据分布不均匀时，数据量较少的分区将会很快的被执行完成，而数据量较大的分区将需要很长时间才能够执行完毕，这就是我们经常所说的<strong>数据倾斜</strong>， 这可能会导致Spark作业的性能降低。那么，该如何解决类似的问题呢？我们可以使用 Spark提供的自定义分区器在RDD上应用数据分区的逻辑。以下是正文，希望对你有所帮助。</p>
<blockquote>
<p>公众号『大数据技术与数仓』，回复『资料』领取大数据资料包</p>
</blockquote>
<h2 id="Spark默认的分区器"><a href="#Spark默认的分区器" class="headerlink" title="Spark默认的分区器"></a>Spark默认的分区器</h2><p>Spark在处理大规模数据集时，会将数据分为不同的分区，并以并行方式处理数据。 默认情况下，它使用哈希分区程序将数据分散到不同的分区中。 哈希分区程序使用的是hashcode函数， 其原理是相等的对象具有相同的哈希码，这样就可以根据key的哈希值，将其分布到各自的分区中。</p>
<h3 id="哈希分区源码"><a href="#哈希分区源码" class="headerlink" title="哈希分区源码"></a><strong>哈希分区源码</strong></h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>(<span class="params">partitions: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">  require(partitions &gt;= <span class="number">0</span>, <span class="string">s"Number of partitions (<span class="subst">$partitions</span>) cannot be negative."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = partitions</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = key <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="literal">null</span> =&gt; <span class="number">0</span></span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="type">Utils</span>.nonNegativeMod(key.hashCode, numPartitions)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">equals</span></span>(other: <span class="type">Any</span>): <span class="type">Boolean</span> = other <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> h: <span class="type">HashPartitioner</span> =&gt;</span><br><span class="line">      h.numPartitions == numPartitions</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hashCode</span></span>: <span class="type">Int</span> = numPartitions</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a><strong>示例</strong></h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建spark session</span></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder</span><br><span class="line">      .master(<span class="string">"local[4]"</span>)</span><br><span class="line">      .getOrCreate</span><br><span class="line">    <span class="keyword">val</span> bigData = util.<span class="type">Arrays</span>.asList(<span class="string">"Hadoop"</span>, <span class="string">"Spark"</span>, <span class="string">"Flink"</span>, <span class="string">"Hive"</span>, <span class="string">"Impala"</span>, <span class="string">"Hbase"</span>, <span class="string">"Kafka"</span>, <span class="string">"ClickHouse"</span>, <span class="string">"KUDU"</span>, <span class="string">"zookeeper"</span>)</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> stringDataset = spark.createDataset(bigData)</span><br><span class="line">    println(<span class="string">"当前rdd的分区数为："</span> + stringDataset.rdd.partitions.length) <span class="comment">// 当前rdd的分区数为：4</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 每个分区的数据会写入一个对应的分区文件中，</span></span><br><span class="line"><span class="comment">      * 每个分区文件对应的数据如下：</span></span><br><span class="line"><span class="comment">      * Partiiton 0: Hadoop Spark</span></span><br><span class="line"><span class="comment">      * Partition 1: Flink Hive Impala</span></span><br><span class="line"><span class="comment">      * Partition 2: Hbase Kafka</span></span><br><span class="line"><span class="comment">      * Partition 3: ClickHouse KUDU zookeeper</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    stringDataset.write.csv(<span class="string">"E://testPartition"</span>)</span><br></pre></td></tr></table></figure>

<p>通常情况下，我们需要加大分区的数量，从而保证每个分区的数据量尽量少，进而可以提升处理的并行度。此时，就需要使用<em>repartition()</em> 方法对数据进行重分区。</p>
<p><em>repartition()</em> 方法既可以用于增加分区，也可以用于减少分区。比如，对上面的示例进行增加分区，如下代码所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> reparationDS = stringDataset.repartition(<span class="number">8</span>)</span><br><span class="line">    println(<span class="string">"重分区之后的分区为："</span> + reparationDS.rdd.partitions.length) <span class="comment">// 重分区之后的分区为：8</span></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      *</span></span><br><span class="line"><span class="comment">      * Partition 0: FlinK ClickHouse</span></span><br><span class="line"><span class="comment">      * Partition 1:</span></span><br><span class="line"><span class="comment">      * Partition 2:</span></span><br><span class="line"><span class="comment">      * Partition 3:</span></span><br><span class="line"><span class="comment">      * Partition 4:</span></span><br><span class="line"><span class="comment">      * Partition 5:</span></span><br><span class="line"><span class="comment">      * Partition 6: Spark Impala Hbase zookeeper</span></span><br><span class="line"><span class="comment">      * Partition 7: Hadoop Hive Kafka KUDU</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    reparationDS.write.csv(<span class="string">"E://repartition"</span>)</span><br></pre></td></tr></table></figure>

<p>上面的数据没有在所有分区中平均分配。 尽管通过应用repartition()方法增加了分区的数量，但是数据并不是均匀分布的。 上面提到，Spark使用的是哈希分区，所以有时，应用repartition()也可能无法解决问题(可能会存在部分分区无数据，而个别分区数据比较多的情况)。</p>
<p>为了解决这个问题，Spark提供了自定义分区器，用户可以根据处理数据的特点，进行自定义分区器。</p>
<h2 id="如何自定义Spark的分区器"><a href="#如何自定义Spark的分区器" class="headerlink" title="如何自定义Spark的分区器"></a>如何自定义Spark的分区器</h2><p>需要注意的是，自定义分区器只能应用于<strong>key-value形式的 pair RDD</strong>。所以在使用自定义分区器的时候，需要从原始的RDD中创建出PairedRDD，然后再使用自定义分区器。</p>
<p>实现一个自定义的分区器非常简单，只需要继承一个<code>org.apache.spark.Partitioner</code>类，然后重写下面的方法即可：</p>
<ul>
<li><p><strong>numPartitions</strong>：此方法返回要为RDD创建的分区数</p>
</li>
<li><p><strong>def getPartition(key: Any)</strong>：此方法返回key对应的分区号（范围从0到numPartitions - 1）</p>
</li>
</ul>
<h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * An object that defines how the elements in a key-value pair RDD are partitioned by key.</span></span><br><span class="line"><span class="comment"> * Maps each key to a partition ID, from 0 to `numPartitions - 1`.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Partitioner</span> <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="自定义分区器"><a href="#自定义分区器" class="headerlink" title="自定义分区器"></a>自定义分区器</h3><ul>
<li>自定义分区类</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomerPartition</span>(<span class="params">partitions: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = partitions</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line"></span><br><span class="line">    (key.toString.charAt(<span class="number">0</span>) + scala.util.<span class="type">Random</span>.nextInt(<span class="number">10</span>)) % numPartitions</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>使用示例</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DefaultPartition</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建spark session</span></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder</span><br><span class="line">      .master(<span class="string">"local[4]"</span>)</span><br><span class="line">      .getOrCreate</span><br><span class="line">    <span class="keyword">val</span> bigData = util.<span class="type">Arrays</span>.asList(<span class="string">"Hadoop"</span>, <span class="string">"Spark"</span>, <span class="string">"Flink"</span>, <span class="string">"Hive"</span>, <span class="string">"Impala"</span>, <span class="string">"Hbase"</span>, <span class="string">"Kafka"</span>, <span class="string">"ClickHouse"</span>, <span class="string">"KUDU"</span>, <span class="string">"zookeeper"</span>)</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> stringDataset = spark.createDataset(bigData)</span><br><span class="line">    println(<span class="string">"当前rdd的分区数为："</span> + stringDataset.rdd.partitions.length) <span class="comment">// 当前rdd的分区数为：4</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 每个分区的数据会写入一个对应的分区文件中，</span></span><br><span class="line"><span class="comment">      * 每个分区文件对应的数据如下：</span></span><br><span class="line"><span class="comment">      * Partiiton 0: Hadoop Spark</span></span><br><span class="line"><span class="comment">      * Partition 1: Flink Hive Impala</span></span><br><span class="line"><span class="comment">      * Partition 2: Hbase Kafka</span></span><br><span class="line"><span class="comment">      * Partition 3: ClickHouse KUDU zookeeper</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="comment">//stringDataset.write.csv("E://testPartition")</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> reparationDS = stringDataset.repartition(<span class="number">8</span>)</span><br><span class="line">    println(<span class="string">"重分区之后的分区为："</span> + reparationDS.rdd.partitions.length) <span class="comment">// 重分区之后的分区为：8</span></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      *</span></span><br><span class="line"><span class="comment">      * Partition 0: FlinK ClickHouse</span></span><br><span class="line"><span class="comment">      * Partition 1:</span></span><br><span class="line"><span class="comment">      * Partition 2:</span></span><br><span class="line"><span class="comment">      * Partition 3:</span></span><br><span class="line"><span class="comment">      * Partition 4:</span></span><br><span class="line"><span class="comment">      * Partition 5:</span></span><br><span class="line"><span class="comment">      * Partition 6: Spark Impala Hbase zookeeper</span></span><br><span class="line"><span class="comment">      * Partition 7: Hadoop Hive Kafka KUDU</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="comment">// reparationDS.write.csv("E://repartition")</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stringRDD = stringDataset.rdd</span><br><span class="line">    <span class="keyword">val</span> pairRDD = stringRDD.map(word =&gt; (word, word.length))</span><br><span class="line">    <span class="comment">// 使用自定义分区器</span></span><br><span class="line">    <span class="keyword">val</span> resultRDD = pairRDD.partitionBy(<span class="keyword">new</span> <span class="type">CustomerPartition</span>(<span class="number">8</span>)) <span class="comment">// 自定义分区的数量：8</span></span><br><span class="line">    println(<span class="string">"自定义分区的数量："</span> + resultRDD.getNumPartitions)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 数据写入CSV文件</span></span><br><span class="line">    <span class="keyword">val</span> outputRDD = resultRDD.map(_._1)</span><br><span class="line">    <span class="keyword">val</span> outputDS = spark.createDataset(outputRDD)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * Partition 0: Hive</span></span><br><span class="line"><span class="comment">      * Partition 1: Impala</span></span><br><span class="line"><span class="comment">      * Partition 2: ClickHouse</span></span><br><span class="line"><span class="comment">      * Partition 3: Spark Hbase</span></span><br><span class="line"><span class="comment">      * Partition 4: KUDU</span></span><br><span class="line"><span class="comment">      * Partition 5: Hadoop</span></span><br><span class="line"><span class="comment">      * Partition 6: Flink</span></span><br><span class="line"><span class="comment">      * Partition 7: Kafka zookeeper</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    outputDS.write.csv(<span class="string">"E:/customerpartition/"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从上面的结果可以看出，数据均匀地分布在了每个分区上，这样就会缓解数据倾斜造成的性能瓶颈。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要分享了如何使用Spark提供的Partitioner类进行自定义一个分区器，并给出了具体的示例。通过自定义分区器，就可以有效地分布数据，从而缓解数据倾斜的性能瓶颈。如果本文对你所有帮助，请分享转发。</p>
<blockquote>
<p>公众号『大数据技术与数仓』，回复『资料』领取大数据资料包</p>
</blockquote>
</div><div class="recommended_posts"><h3>相关推荐 ☟</h3><li><a href="https://jiamaoxiang.top/2020/12/21/Flink集成Hive之Hive-Catalog与Hive-Dialect-以Flink1-12为例/" target="_blank">Flink集成Hive之Hive Catalog与Hive Dialect--以Flink1.12为例</a></li><li><a href="https://jiamaoxiang.top/2020/12/18/Flink集成Hive之快速入门-以Flink1-12为例/" target="_blank">Flink集成Hive之快速入门--以Flink1.12为例</a></li><li><a href="https://jiamaoxiang.top/2020/12/11/秒懂推荐系统-Spark平台下基于物品的协同过滤推荐系统构建/" target="_blank">秒懂推荐系统-Spark平台下基于物品的协同过滤推荐系统构建</a></li><li><a href="https://jiamaoxiang.top/2020/12/06/如何管理Spark的分区/" target="_blank">如何管理Spark的分区</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Jia MaoXiang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2020/12/15/使用自定义分区器解决Spark-DataSet数据分区不均匀的问题/">https://jiamaoxiang.top/2020/12/15/使用自定义分区器解决Spark-DataSet数据分区不均匀的问题/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本文为博主原创文章，遵循CC BY-SA 4.0版权协议，转载请附上原文出处链接和本声明</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://jiamaoxiang.top/2020/12/15/使用自定义分区器解决Spark-DataSet数据分区不均匀的问题/" data-id="ckiye93ki001zbo7qbgvmye6a" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAQ4AAAEOCAAAAABd2qZ5AAAD8ElEQVR42u3aS27jQAwFQN//0hnMOrH0HtlysiitDERutaoD0Py8XvH19e26vvP6W/nn2bO+//XwhQMHDhw4yldNls5fMnmN62/l+7m+//odceDAgQPHWY42ZOaB7Su48iCdH0a+Nxw4cODA8bsceci8TsPa7earzdbHgQMHDhyf52hLflHKFKR5+ZHgwIEDB46/w5EkV6dKh3mStu8WPVgrxYEDBw4cXaSrG0V/+fMj8x04cODAgWPUENqE2Lx11A5D5IN0xRvhwIEDB441Rxu09q+XBL82tZsd4dtd4cCBAweOxzg2TaZZKN0U+JK9RceDAwcOHDgWHG3ilD+sDm/rg8l3OPy/wIEDBw4cMcdsNCEp0rWFv1PDE22I/aFWigMHDhw41hx5oJ2F3k3hLzmY5MfBzU5w4MCBA8chjjzs5e2oNky2QXQ26ndz/Dhw4MCB4yjHvsS2SeSeaFC1R44DBw4cOD7DkYe6NglsxxTyllib5uHAgQMHjic4kk23xb4N64YmLxq+DbQ4cODAgWPBMRuS2xT7ZsXB/XUTenHgwIEDxyGOPAHbDBDM2lSzAmIbpN9mtDhw4MCBY8SRjK/loStPnPKXPMV0syscOHDgwPEAxyu48kLhbP12cGG2Mg4cOHDgeIIjCVSbQYdNO6pOwMoA/HZNHDhw4MBxlKPtwiTffXfnZv08DOefceDAgQPHnmMW6q6DVjsekQfC2fjFcL4DBw4cOHCsOdpWTR4m85XbOzfXDyg4cODAgWPNkbeLnhh6yP96fQztt27QceDAgQPHIY481LUbbV+mLSPmqVoUwnHgwIEDxyGOtlGU07R/3SR1baCN8locOHDgwDHiONsKaoNoHkpPtcRuxhpw4MCBA8eCI8ll2oRq1oLaN73a48GBAwcOHM9x5I9pS343KVOZhm1G+vJmFQ4cOHDgmHHkAwHt4MIsZcpTxxyr+BGAAwcOHDgOccwaP7Ni32x4YhaY68CPAwcOHDjWHLMJsc1GN8XBr/jKU7tXew44cODAgePyKZve1Cw9m6WFbSFyOCqBAwcOHDgOcbSB7WwCNsOdDWHc7B8HDhw4cKw56lvLRtF+zTxha9cf5rI4cODAgWNUDZuNLOQBry3e5enlquGEAwcOHDgO9e5nqdFmrG1TXsxfYPMsHDhw4MCxedbmVWdheDZwcKopdZMK4sCBAweOQxybEYR2NKENk6vfCG1LDAcOHDhwfJDjeov7YbhZ4N+vhgMHDhw4fotjVpJL1k++m6eXq5CMAwcOHDgOcbTFwTy8tYlZW5rM3yg6bBw4cODAsebYlOpmKVyyZtJe2ozZHf5tggMHDhw4/l//AH4Z96q4H1VoAAAAAElFTkSuQmCC">分享</a><div class="tags"><a href="/tags/Spark/">Spark</a></div><div class="post-nav"><a class="pre" href="/2020/12/18/Flink集成Hive之快速入门-以Flink1-12为例/">Flink集成Hive之快速入门--以Flink1.12为例</a><a class="next" href="/2020/12/11/秒懂推荐系统-Spark平台下基于物品的协同过滤推荐系统构建/">秒懂推荐系统-Spark平台下基于物品的协同过滤推荐系统构建</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar-toc"><div class="stoc-article" id="sidebar-stoc"><strong class="stoc-title"><i class="fa fa-list-ul"> 目录</i></strong><div class="toc-nav" id="stoc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark默认的分区器"><span class="toc-number">1.</span> <span class="toc-text">Spark默认的分区器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#哈希分区源码"><span class="toc-number">1.1.</span> <span class="toc-text">哈希分区源码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#示例"><span class="toc-number">1.2.</span> <span class="toc-text">示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#如何自定义Spark的分区器"><span class="toc-number">2.</span> <span class="toc-text">如何自定义Spark的分区器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#源码"><span class="toc-number">2.1.</span> <span class="toc-text">源码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#自定义分区器"><span class="toc-number">2.2.</span> <span class="toc-text">自定义分区器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">3.</span> <span class="toc-text">总结</span></a></li></ol></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 - 2020 <a href="/." rel="nofollow">Jmx's Blog.</a>All rights reserved.<br>Thoughts on technology, life and everything else.</div></div></div><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" color="100,99,98" opacity="0.5" zindex="0.7" count="150" src="//lib.baomitu.com/canvas-nest.js/2.0.4/canvas-nest.umd.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>