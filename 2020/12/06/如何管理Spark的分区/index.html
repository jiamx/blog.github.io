<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记录朴实无华且枯燥的生活"><title>如何管理Spark的分区 | Jmx's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '912b6cfc43243cd27aeb428f7dbf7823';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">如何管理Spark的分区</h1><a id="logo" href="/.">Jmx's Blog</a><p class="description">Keep it Simple and Stupid!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-pied-piper-alt"> 关于</i></a><a href="/tags"><i class="fa fa-tags"> 标签</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">如何管理Spark的分区</h1><div class="post-meta">Dec 6, 2020<span> | </span><span class="category"><a href="/categories/Spark/">Spark</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2.4k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 9</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>当我们使用Spark加载数据源并进行一些列转换时，Spark会将数据拆分为多个分区Partition，并在分区上并行执行计算。所以理解Spark是如何对数据进行分区的以及何时需要手动调整Spark的分区，可以帮助我们提升Spark程序的运行效率。</p>
<a id="more"></a>

<blockquote>
<p>公众号『大数据技术与数仓』，回复『资料』领取大数据资料包</p>
</blockquote>
<h2 id="什么是分区"><a href="#什么是分区" class="headerlink" title="什么是分区"></a>什么是分区</h2><p>关于什么是分区，其实没有什么神秘的。我们可以通过创建一个DataFrame来说明如何对数据进行分区：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> x = (<span class="number">1</span> to <span class="number">10</span>).toList</span><br><span class="line">x: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> numsDF = x.toDF(<span class="string">"num"</span>)</span><br><span class="line">numsDF: org.apache.spark.sql.<span class="type">DataFrame</span> = [num: int]</span><br></pre></td></tr></table></figure>

<p>创建好DataFrame之后，我们再来看一下该DataFame的分区，可以看出分区数为4：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; numsDF.rdd.partitions.size</span><br><span class="line">res0: <span class="type">Int</span> = <span class="number">4</span></span><br></pre></td></tr></table></figure>

<p>当我们将DataFrame写入磁盘文件时，再来观察一下文件的个数，</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; numsDF.write.csv(<span class="string">"file:///opt/modules/data/numsDF"</span>)</span><br></pre></td></tr></table></figure>

<p>可以发现，上述的写入操作会生成4个文件</p>
<p><img src="//jiamaoxiang.top/2020/12/06/如何管理Spark的分区/1.png" alt></p>
<p>每个分区的数据如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">part-00000: 1, 2</span><br><span class="line">part-00001: 3, 4, 5</span><br><span class="line">part-00002: 6, 7</span><br><span class="line">part-00003: 8, 9, 10</span><br></pre></td></tr></table></figure>

<h2 id="coalesce操作"><a href="#coalesce操作" class="headerlink" title="coalesce操作"></a>coalesce操作</h2><h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coalesce</span></span>(numPartitions: <span class="type">Int</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</span><br><span class="line">    <span class="type">Repartition</span>(numPartitions, shuffle = <span class="literal">false</span>, planWithBarrier)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h3><p>在减少分区时，返回一个新的分区数为指定<code>numPartitions</code>的DataSet，在增大分区时，则分区数保持不变。值得注意的是，该操作生成的是窄依赖，<strong>所以不会发生shuffle</strong>。然而，如果是极端的操作，比如numPartitions = 1，这样会导致只在一个节点进行计算。为了避免这种情况发生，可以使用repartition方法，<strong>该方法会发生shuffle操作</strong>，这就意味着当前的上游分区可以并行执行</p>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><h4 id="减少分区操作"><a href="#减少分区操作" class="headerlink" title="减少分区操作"></a>减少分区操作</h4><p>coalesce方法可以用来减少DataFrame的分区数。以下操作是将数据合并到两个分区：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> numsDF2 = numsDF.coalesce(<span class="number">2</span>)</span><br><span class="line">numsDF2: org.apache.spark.sql.<span class="type">Dataset</span>[org.apache.spark.sql.<span class="type">Row</span>] = [num: int]</span><br></pre></td></tr></table></figure>

<p>我们可以验证上述操作是否创建了只有两个分区的新DataFrame：可以看出，分区数变为了2</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; numsDF2.rdd.partitions.size </span><br><span class="line">res13: <span class="type">Int</span> = <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>将numsDF2写入文件存储，观察文件数量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">numsDF2.write.csv(&quot;file:///opt/modules/data/numsDF2&quot;)</span><br></pre></td></tr></table></figure>

<p>可以发现，上述的写入操作会生成2个文件</p>
<p><img src="//jiamaoxiang.top/2020/12/06/如何管理Spark的分区/2.png" alt></p>
<p>上述每个分区的数据如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">part-00000: 1, 2, 3, 4, 5</span><br><span class="line">part-00001: 6, 7, 8, 9, 10</span><br></pre></td></tr></table></figure>

<p>对比减少分区之前的数据存储，可以看出：在减少分区时，并没有对所有数据进行了移动，仅仅是在原来分区的基础之上进行了合并而已，这样的操作可以减少数据的移动，所以效率较高。</p>
<h4 id="增加分区操作"><a href="#增加分区操作" class="headerlink" title="增加分区操作"></a>增加分区操作</h4><p>从上面的源码可以看出，如果使用coalesce方法进行增加分区，将不会生效。我们可以尝试通过coalesce来增加分区的数量，观察一下具体结果：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> numsDF3 = numsDF.coalesce(<span class="number">6</span>)</span><br><span class="line">numsDF3: org.apache.spark.sql.<span class="type">Dataset</span>[org.apache.spark.sql.<span class="type">Row</span>] = [num: int]</span><br><span class="line"></span><br><span class="line">scala&gt; numsDF3.rdd.partitions.size</span><br><span class="line">res16: <span class="type">Int</span> = <span class="number">4</span></span><br></pre></td></tr></table></figure>

<p>可以看出，即使我们尝试使用coalesce(6)来创建6个分区，numsDF3的分区数依然是4，并没有发生变化。**coalesce算法通过将数据从某些分区移动到现有分区来更改节点数，该方法显然用户增加分区数。</p>
<h2 id="repartition操作"><a href="#repartition操作" class="headerlink" title="repartition操作"></a>repartition操作</h2><h3 id="源码-1"><a href="#源码-1" class="headerlink" title="源码"></a>源码</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 返回一个分区数为`numPartitions`的新的DataSet</span></span><br><span class="line"><span class="comment"> * @group typedrel</span></span><br><span class="line"><span class="comment"> * @since 1.6.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(numPartitions: <span class="type">Int</span>): <span class="type">Dataset</span>[<span class="type">T</span>] = withTypedPlan &#123;</span><br><span class="line">  <span class="type">Repartition</span>(numPartitions, shuffle = <span class="literal">true</span>, planWithBarrier)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从源码中可以看出，该方法可以用于减少或者增加分区的数量，并且会发生Shuffle操作。</p>
<h3 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h3><h4 id="减少分区操作-1"><a href="#减少分区操作-1" class="headerlink" title="减少分区操作"></a>减少分区操作</h4><p>已知numsDF有4个分区，现在将其分区置为2，观察结果</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> numsDF4 = numsDF.repartition(<span class="number">2</span>)</span><br><span class="line">numsDF4: org.apache.spark.sql.<span class="type">Dataset</span>[org.apache.spark.sql.<span class="type">Row</span>] = [num: int]</span><br><span class="line">scala&gt; numsDF4.rdd.partitions.size</span><br><span class="line">res19: <span class="type">Int</span> = <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>可以看出，分区确实减少了，我们在来看一下每个分区的数据：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">numsDF4.write.csv(<span class="string">"file:///opt/modules/data/numsDF4"</span>)</span><br></pre></td></tr></table></figure>

<p>上面的操作会产生两个文件，每个分区文件的数据为：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">part-00000:2,3,4,7,10,9</span><br><span class="line">part-00000:1,5,6,8</span><br></pre></td></tr></table></figure>

<p>从上面的数据分布可以看出，数据被Shuffle了。这也印证了源码中说的，repartition操作会将所有数据进行Shuffle，并且将数据均匀地分布在不同的分区上，并不是像coalesce方法一样，会尽量减少数据的移动。</p>
<h4 id="增加分区操作-1"><a href="#增加分区操作-1" class="headerlink" title="增加分区操作"></a>增加分区操作</h4><p>repartition操作方法不仅可以用于减少分区操作，也可以用于增加分区数量。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> numsDF5 = numsDF.repartition(<span class="number">6</span>)</span><br><span class="line">numsDF5: org.apache.spark.sql.<span class="type">Dataset</span>[org.apache.spark.sql.<span class="type">Row</span>] = [num: int]</span><br><span class="line">scala&gt; numsDF5.rdd.partitions.size</span><br><span class="line">res22: <span class="type">Int</span> = <span class="number">6</span></span><br></pre></td></tr></table></figure>

<h3 id="coalesce-与repartition之间的区别"><a href="#coalesce-与repartition之间的区别" class="headerlink" title="coalesce 与repartition之间的区别"></a>coalesce 与repartition之间的区别</h3><p>repartition算法对数据进行了Shuffle操作，并创建了大小相等的数据分区。coalesce操作合并现有分区以避免Shuffle。除此之外，coalesce操作仅能用于减少分区，不能用于增加分区操作。</p>
<h3 id="按照列字段进行repartition"><a href="#按照列字段进行repartition" class="headerlink" title="按照列字段进行repartition"></a>按照列字段进行repartition</h3><ul>
<li>源码</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(partitionExprs: <span class="type">Column</span>*): <span class="type">Dataset</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  repartition(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>解释</li>
</ul>
<p>返回一个按照指定分区列的新的DataSet，<strong>具体的分区数量有参数<code>spark.sql.shuffle.partitions</code>默认指定，该默认值为200</strong>，该操作与HiveSQL的DISTRIBUTE BY操作类似。</p>
<p>repartition除了可以指定具体的分区数之外，还可以指定具体的分区字段。我们可以使用下面的示例来探究如何使用特定的列对DataFrame进行重新分区。</p>
<p>首先创建DataFrame：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> people = <span class="type">List</span>(</span><br><span class="line">(<span class="string">"jack"</span>,<span class="string">"male"</span>),</span><br><span class="line">(<span class="string">"Alice"</span>,<span class="string">"female"</span>),</span><br><span class="line">(<span class="string">"tom"</span>,<span class="string">"male"</span>),</span><br><span class="line">(<span class="string">"Angela"</span>,<span class="string">"female"</span>),</span><br><span class="line">(<span class="string">"tony"</span>,<span class="string">"male"</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> peopleDF = people.toDF(<span class="string">"name"</span>,<span class="string">"gender"</span>)</span><br></pre></td></tr></table></figure>

<p>让我们按gender列对DataFrame进行分区：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> genderDF = peopleDF.repartition($<span class="string">"gender"</span>)</span><br><span class="line">genderDF: org.apache.spark.sql.<span class="type">Dataset</span>[org.apache.spark.sql.<span class="type">Row</span>] = [name: string, gender: string]</span><br></pre></td></tr></table></figure>

<p>按列进行分区时，Spark默认会创建200个分区。此示例将有两个带有数据的分区,其他分区将没有数据。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; genderDF.rdd.partitions.size</span><br><span class="line">res23: <span class="type">Int</span> = <span class="number">200</span></span><br></pre></td></tr></table></figure>

<h2 id="一些注意点"><a href="#一些注意点" class="headerlink" title="一些注意点"></a>一些注意点</h2><h3 id="该如何设置分区数量"><a href="#该如何设置分区数量" class="headerlink" title="该如何设置分区数量"></a>该如何设置分区数量</h3><p>假设我们要对一个大数据集进行操作，该数据集的分区数也比较大，那么当我们进行一些操作之后，比如filter过滤操作、sample采样操作，这些操作可能会使结果数据集的数据量大量减少。但是Spark却不会对其分区进行调整，由此会造成大量的分区没有数据，并且向HDFS读取和写入大量的空文件，效率会很低，这种情况就需要我们重新调整分数数量，以此来提升效率。</p>
<p>通常情况下，结果集的数据量减少时，其对应的分区数也应当相应地减少。那么该如何确定具体的分区数呢？</p>
<ul>
<li><strong>分区过少</strong>：将无法充分利用群集中的所有可用的CPU core</li>
<li><strong>分区过多</strong>：产生非常多的小任务，从而会产生过多的开销</li>
</ul>
<p>在这两者之间，第一个对性能的影响相对比较大。对于小于1000个分区数的情况而言，调度太多的小任务所产生的影响相对较小。但是，如果有成千上万个分区，那么Spark会变得非常慢。</p>
<p>spark中的shuffle分区数是静态的。它不会随着不同的数据大小而变化。上文提到：默认情况下，控制shuffle分区数的参数<strong>spark.sql.shuffle.partitions</strong>值为<strong>200</strong>，这将导致以下问题</p>
<ul>
<li>对于较小的数据，200是一个过大的选择，由于调度开销，通常会导致处理速度变慢。</li>
<li>对于大数据，200很小，无法有效使用群集中的所有资源</li>
</ul>
<p>一般情况下，我们可以通过将集群中的CPU数量乘以2、3或4来确定分区的数量。如果要将数据写出到文件系统中，则可以选择一个分区大小，以创建合理大小的文件。</p>
<p><strong>该使用哪种方法进行重分区呢？</strong></p>
<p>对于大型数据集，进行Shuffle操作是很消耗性能的，但是当我们的数据集比较小的时候，可以使用repartition方法进行重分区，这样可以尽量保证每个分区的数据分布比较均匀(使用coalesce可能会造成数据倾斜)，对于下游使用者来说效率更高。</p>
<h3 id="如何将数据写入到单个文件"><a href="#如何将数据写入到单个文件" class="headerlink" title="如何将数据写入到单个文件"></a>如何将数据写入到单个文件</h3><p>通过使用repartition(1)和coalesce(1))可用于将DataFrame写入到单个文件中。通常情况下，不会只将数据写入到单个文件中，因为这样效率很低，写入速度很慢，在数据量比较大的情况，很可能会出现写入错误的情况。所以，只有当DataFrame很小时，我们才会考虑将其写入到单个文件中。</p>
<h3 id="何时考虑重分区"><a href="#何时考虑重分区" class="headerlink" title="何时考虑重分区"></a>何时考虑重分区</h3><p>一般对于在对比较大的数据集进行过滤操作之后，产生的较小数据集，通常需要对其考虑进行重分区，从而提升任务执行的效率。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要介绍了Spark是如何管理分区的，分别解释了Spark提供的两种分区方法，并给出了相应的使用示例和分析。最后对分区情况及其影响进行了讨论，并给出了一些实践的建议。希望本文对你有所帮助。</p>
<blockquote>
<p> 公众号『大数据技术与数仓』，回复『资料』领取大数据资料包</p>
</blockquote>
</div><div class="recommended_posts"><h3>相关推荐 ☟</h3><li><a href="https://jiamaoxiang.top/2020/11/26/数仓开发需要了解的5大SQL分析函数/" target="_blank">数仓开发需要了解的5大SQL分析函数</a></li><li><a href="https://jiamaoxiang.top/2020/11/25/数仓-几种SQL隐藏的错误，你遇到过吗？/" target="_blank">数仓|几种SQL隐藏的错误，你遇到过吗？</a></li><li><a href="https://jiamaoxiang.top/2020/11/20/第十一篇-基于SparkSQL的电影分析项目实战/" target="_blank">第十一篇|基于SparkSQL的电影分析项目实战</a></li><li><a href="https://jiamaoxiang.top/2020/11/17/第五篇-ClickHouse数据导入-Flink、Spark、Kafka、MySQL/" target="_blank">篇五|ClickHouse数据导入(Flink、Spark、Kafka、MySQL、Hive)</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Jia MaoXiang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2020/12/06/如何管理Spark的分区/">https://jiamaoxiang.top/2020/12/06/如何管理Spark的分区/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本文为博主原创文章，遵循CC BY-SA 4.0版权协议，转载请附上原文出处链接和本声明</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://jiamaoxiang.top/2020/12/06/如何管理Spark的分区/" data-id="ckicw6clf0046ao7qy15nklqr" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACKklEQVR42u3aS3LDMAwEUd//0soBXKJ6ADlVBFurlKwPnxYTkODng4/r6/j+lV9/dy+5vnXIkCFjW8a1PNIX3F3Dn78+fzsSGTJkHMAgUcjx6+vXz1+/9/a8DBkyZDTKO/KrDBkyZLzL4KUbiWweyjJkyJCRDqL/BM54eS4uQ4aMDRl81f3///5Jf0OGDBlbMa7wIIv467eQ8I1HJUOGjNEMHnBpwVcbem08MmTImM3gS/bkfG3h7OEbk3tlyJBxACNd9kqHlS6oxa1QGTJkjGas8yot4NKgjKeppMKVIUPGaEbaGOARyaevNbwMGTJmM9JJbG2IQWiS7V/9ubgMGTK2ZfDJZ9rO7E+GH+6VIUPGaAbf6NBpFfSblyjuZciQMZrRj8L+po1O6MuQIeMcBg/ENJTTwI03W8iQIeMYBl+sTyeofKLLI/ihNJQhQ8Y4xltLY+mwXv5fIUOGjNEMXtKlr0/bmWncy5Ah4xwG3zDR32DBIzXeoiFDhowjGeR8rc3Z+QRos4UMGTKGMkjLkBSOtVZB7TN90m8vQ4aMzRm1JbB0cwZpEqTTZhkyZMxmXOHx7paLWnH5UBTKkCFjHKPWGeTXpB+os7FDhgwZsxk8QEmY9iOSx3RQIMqQIWMEgy/Bd8q+dTS3kDJkyJAB2ga1grJTjLYCV4YMGQcw0tZjp8GJWhcyZMg4gJEu9Keh3OEhsAwZMkYzanVXrRzstAQ6U18ZMmRsy/gDdjoi4Bj2yM4AAAAASUVORK5CYII=">分享</a><div class="tags"><a href="/tags/Spark/">Spark</a></div><div class="post-nav"><a class="next" href="/2020/11/26/数仓开发需要了解的5大SQL分析函数/">数仓开发需要了解的5大SQL分析函数</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar-toc"><div class="stoc-article" id="sidebar-stoc"><strong class="stoc-title"><i class="fa fa-list-ul"> 目录</i></strong><div class="toc-nav" id="stoc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是分区"><span class="toc-number">1.</span> <span class="toc-text">什么是分区</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#coalesce操作"><span class="toc-number">2.</span> <span class="toc-text">coalesce操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#源码"><span class="toc-number">2.1.</span> <span class="toc-text">源码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#解释"><span class="toc-number">2.2.</span> <span class="toc-text">解释</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#示例"><span class="toc-number">2.3.</span> <span class="toc-text">示例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#减少分区操作"><span class="toc-number">2.3.1.</span> <span class="toc-text">减少分区操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#增加分区操作"><span class="toc-number">2.3.2.</span> <span class="toc-text">增加分区操作</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#repartition操作"><span class="toc-number">3.</span> <span class="toc-text">repartition操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#源码-1"><span class="toc-number">3.1.</span> <span class="toc-text">源码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#示例-1"><span class="toc-number">3.2.</span> <span class="toc-text">示例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#减少分区操作-1"><span class="toc-number">3.2.1.</span> <span class="toc-text">减少分区操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#增加分区操作-1"><span class="toc-number">3.2.2.</span> <span class="toc-text">增加分区操作</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#coalesce-与repartition之间的区别"><span class="toc-number">3.3.</span> <span class="toc-text">coalesce 与repartition之间的区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#按照列字段进行repartition"><span class="toc-number">3.4.</span> <span class="toc-text">按照列字段进行repartition</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#一些注意点"><span class="toc-number">4.</span> <span class="toc-text">一些注意点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#该如何设置分区数量"><span class="toc-number">4.1.</span> <span class="toc-text">该如何设置分区数量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#如何将数据写入到单个文件"><span class="toc-number">4.2.</span> <span class="toc-text">如何将数据写入到单个文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#何时考虑重分区"><span class="toc-number">4.3.</span> <span class="toc-text">何时考虑重分区</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">5.</span> <span class="toc-text">总结</span></a></li></ol></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 - 2020 <a href="/." rel="nofollow">Jmx's Blog.</a>All rights reserved.<br>Thoughts on technology, life and everything else.</div></div></div><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" color="100,99,98" opacity="0.5" zindex="0.7" count="150" src="//lib.baomitu.com/canvas-nest.js/2.0.4/canvas-nest.umd.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>