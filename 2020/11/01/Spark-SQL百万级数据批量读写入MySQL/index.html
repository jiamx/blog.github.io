<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记录朴实无华且枯燥的生活"><title>Spark SQL百万级数据批量读写入MySQL | Jmx's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '912b6cfc43243cd27aeb428f7dbf7823';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Spark SQL百万级数据批量读写入MySQL</h1><a id="logo" href="/.">Jmx's Blog</a><p class="description">Keep it Simple and Stupid!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-pied-piper-alt"> 关于</i></a><a href="/tags"><i class="fa fa-tags"> 标签</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Spark SQL百万级数据批量读写入MySQL</h1><div class="post-meta">Nov 1, 2020<span> | </span><span class="category"><a href="/categories/Spark/">Spark</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.7k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 7</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><a id="more"></a>

<h2 id="Spark-SQL读取MySQL的方式"><a href="#Spark-SQL读取MySQL的方式" class="headerlink" title="Spark SQL读取MySQL的方式"></a>Spark SQL读取MySQL的方式</h2><p>Spark SQL还包括一个可以使用JDBC从其他数据库读取数据的数据源。与使用JdbcRDD相比，应优先使用此功能。这是因为结果作为DataFrame返回，它们可以在Spark SQL中轻松处理或与其他数据源连接。JDBC数据源也更易于使用Java或Python，因为它不需要用户提供ClassTag。</p>
<p> 可以使用Data Sources API将远程数据库中的表加载为DataFrame或Spark SQL临时视图。用户可以在数据源选项中指定JDBC连接属性。 user和password通常作为用于登录数据源的连接属性。除连接属性外，Spark还支持以下不区分大小写的选项：</p>
<table>
<thead>
<tr>
<th align="left">属性名称</th>
<th align="left">解释</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>url</code></td>
<td align="left">要连接的JDBC URL</td>
</tr>
<tr>
<td align="left"><code>dbtable</code></td>
<td align="left">读取或写入的JDBC表</td>
</tr>
<tr>
<td align="left"><code>query</code></td>
<td align="left">指定查询语句</td>
</tr>
<tr>
<td align="left"><code>driver</code></td>
<td align="left">用于连接到该URL的JDBC驱动类名</td>
</tr>
<tr>
<td align="left"><code>partitionColumn, lowerBound, upperBound</code></td>
<td align="left">如果指定了这些选项，则必须全部指定。另外， <code>numPartitions</code>必须指定</td>
</tr>
<tr>
<td align="left"><code>numPartitions</code></td>
<td align="left">表读写中可用于并行处理的最大分区数。这也确定了并发JDBC连接的最大数量。如果要写入的分区数超过此限制，我们可以通过<code>coalesce(numPartitions)</code>在写入之前进行调用将其降低到此限制</td>
</tr>
<tr>
<td align="left"><code>queryTimeout</code></td>
<td align="left">默认为<code>0</code>，查询超时时间</td>
</tr>
<tr>
<td align="left"><code>fetchsize</code></td>
<td align="left">JDBC的获取大小，它确定每次要获取多少行。这可以帮助提高JDBC驱动程序的性能</td>
</tr>
<tr>
<td align="left"><code>batchsize</code></td>
<td align="left">默认为1000，JDBC批处理大小，这可以帮助提高JDBC驱动程序的性能。</td>
</tr>
<tr>
<td align="left"><code>isolationLevel</code></td>
<td align="left">事务隔离级别，适用于当前连接。它可以是一个<code>NONE</code>，<code>READ_COMMITTED</code>，<code>READ_UNCOMMITTED</code>，<code>REPEATABLE_READ</code>，或<code>SERIALIZABLE</code>，对应于由JDBC的连接对象定义，缺省值为标准事务隔离级别<code>READ_UNCOMMITTED</code>。此选项仅适用于写作。</td>
</tr>
<tr>
<td align="left"><code>sessionInitStatement</code></td>
<td align="left">在向远程数据库打开每个数据库会话之后，在开始读取数据之前，此选项将执行自定义SQL语句，使用它来实现会话初始化代码。</td>
</tr>
<tr>
<td align="left"><code>truncate</code></td>
<td align="left">这是与JDBC writer相关的选项。当<code>SaveMode.Overwrite</code>启用时，就会清空目标表的内容，而不是删除和重建其现有的表。默认为<code>false</code></td>
</tr>
<tr>
<td align="left"><code>pushDownPredicate</code></td>
<td align="left">用于启用或禁用谓词下推到JDBC数据源的选项。默认值为true，在这种情况下，Spark将尽可能将过滤器下推到JDBC数据源。</td>
</tr>
</tbody></table>
<h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><ul>
<li><strong>SparkSession</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Returns a [[DataFrameReader]] that can be used to read non-streaming data in as a</span></span><br><span class="line"><span class="comment">   * `DataFrame`.</span></span><br><span class="line"><span class="comment">   * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment">   *   sparkSession.read.parquet("/path/to/file.parquet")</span></span><br><span class="line"><span class="comment">   *   sparkSession.read.schema(schema).json("/path/to/file.json")</span></span><br><span class="line"><span class="comment">   * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @since 2.0.0</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>: <span class="type">DataFrameReader</span> = <span class="keyword">new</span> <span class="type">DataFrameReader</span>(self)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>DataFrameReader</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">  <span class="comment">// ...省略代码...</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   *所有的数据由RDD的一个分区处理，如果你这个表很大，很可能会出现OOM</span></span><br><span class="line"><span class="comment">   *可以使用DataFrameDF.rdd.partitions.size方法查看</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">jdbc</span></span>(url: <span class="type">String</span>, table: <span class="type">String</span>, properties: <span class="type">Properties</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    assertNoSpecifiedSchema(<span class="string">"jdbc"</span>)</span><br><span class="line">    <span class="keyword">this</span>.extraOptions ++= properties.asScala</span><br><span class="line">    <span class="keyword">this</span>.extraOptions += (<span class="type">JDBCOptions</span>.<span class="type">JDBC_URL</span> -&gt; url, <span class="type">JDBCOptions</span>.<span class="type">JDBC_TABLE_NAME</span> -&gt; table)</span><br><span class="line">    format(<span class="string">"jdbc"</span>).load()</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * @param url 数据库url</span></span><br><span class="line"><span class="comment">   * @param table 表名</span></span><br><span class="line"><span class="comment">   * @param columnName 分区字段名</span></span><br><span class="line"><span class="comment">   * @param lowerBound  `columnName`的最小值,用于分区步长</span></span><br><span class="line"><span class="comment">   * @param upperBound  `columnName`的最大值,用于分区步长.</span></span><br><span class="line"><span class="comment">   * @param numPartitions 分区数量 </span></span><br><span class="line"><span class="comment">   * @param connectionProperties 其他参数</span></span><br><span class="line"><span class="comment">   * @since 1.4.0</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">jdbc</span></span>(</span><br><span class="line">      url: <span class="type">String</span>,</span><br><span class="line">      table: <span class="type">String</span>,</span><br><span class="line">      columnName: <span class="type">String</span>,</span><br><span class="line">      lowerBound: <span class="type">Long</span>,</span><br><span class="line">      upperBound: <span class="type">Long</span>,</span><br><span class="line">      numPartitions: <span class="type">Int</span>,</span><br><span class="line">      connectionProperties: <span class="type">Properties</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.extraOptions ++= <span class="type">Map</span>(</span><br><span class="line">      <span class="type">JDBCOptions</span>.<span class="type">JDBC_PARTITION_COLUMN</span> -&gt; columnName,</span><br><span class="line">      <span class="type">JDBCOptions</span>.<span class="type">JDBC_LOWER_BOUND</span> -&gt; lowerBound.toString,</span><br><span class="line">      <span class="type">JDBCOptions</span>.<span class="type">JDBC_UPPER_BOUND</span> -&gt; upperBound.toString,</span><br><span class="line">      <span class="type">JDBCOptions</span>.<span class="type">JDBC_NUM_PARTITIONS</span> -&gt; numPartitions.toString)</span><br><span class="line">    jdbc(url, table, connectionProperties)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * @param predicates 每个分区的where条件</span></span><br><span class="line"><span class="comment">   * 比如："id &lt;= 1000", "score &gt; 1000 and score &lt;= 2000"</span></span><br><span class="line"><span class="comment">   * 将会分成两个分区</span></span><br><span class="line"><span class="comment">   * @since 1.4.0</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">jdbc</span></span>(</span><br><span class="line">      url: <span class="type">String</span>,</span><br><span class="line">      table: <span class="type">String</span>,</span><br><span class="line">      predicates: <span class="type">Array</span>[<span class="type">String</span>],</span><br><span class="line">      connectionProperties: <span class="type">Properties</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    assertNoSpecifiedSchema(<span class="string">"jdbc"</span>)</span><br><span class="line">    <span class="keyword">val</span> params = extraOptions.toMap ++ connectionProperties.asScala.toMap</span><br><span class="line">    <span class="keyword">val</span> options = <span class="keyword">new</span> <span class="type">JDBCOptions</span>(url, table, params)</span><br><span class="line">    <span class="keyword">val</span> parts: <span class="type">Array</span>[<span class="type">Partition</span>] = predicates.zipWithIndex.map &#123; <span class="keyword">case</span> (part, i) =&gt;</span><br><span class="line">      <span class="type">JDBCPartition</span>(part, i) : <span class="type">Partition</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> relation = <span class="type">JDBCRelation</span>(parts, options)(sparkSession)</span><br><span class="line">    sparkSession.baseRelationToDataFrame(relation)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runJdbcDatasetExample</span></span>(spark: <span class="type">SparkSession</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">   </span><br><span class="line">   <span class="comment">// 从JDBC source加载数据(load)</span></span><br><span class="line">   <span class="keyword">val</span> jdbcDF = spark.read</span><br><span class="line">     .format(<span class="string">"jdbc"</span>)</span><br><span class="line">     .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://127.0.0.1:3306/test"</span>)</span><br><span class="line">     .option(<span class="string">"dbtable"</span>, <span class="string">"mytable"</span>)</span><br><span class="line">     .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">     .option(<span class="string">"password"</span>, <span class="string">"root"</span>)</span><br><span class="line">     .load()</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> connectionProperties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">   connectionProperties.put(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">   connectionProperties.put(<span class="string">"password"</span>, <span class="string">"root"</span>)</span><br><span class="line">   <span class="keyword">val</span> jdbcDF2 = spark.read</span><br><span class="line">     .jdbc(<span class="string">"jdbc:mysql://127.0.0.1:3306/test"</span>, <span class="string">"mytable"</span>, connectionProperties)</span><br><span class="line">   <span class="comment">// 指定读取schema的数据类型</span></span><br><span class="line">   connectionProperties.put(<span class="string">"customSchema"</span>, <span class="string">"id DECIMAL(38, 0), name STRING"</span>)</span><br><span class="line">   <span class="keyword">val</span> jdbcDF3 = spark.read</span><br><span class="line">     .jdbc(<span class="string">"jdbc:mysql://127.0.0.1:3306/test"</span>, <span class="string">"mytable"</span>, connectionProperties)</span><br><span class="line"></span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>值得注意的是，上面的方式如果不指定分区的话，Spark默认会使用一个分区读取数据，这样在数据量特别大的情况下，会出现OOM。在读取数据之后，调用DataFrameDF.rdd.partitions.size方法可以查看分区数。</p>
<h2 id="Spark-SQL批量写入MySQL"><a href="#Spark-SQL批量写入MySQL" class="headerlink" title="Spark SQL批量写入MySQL"></a>Spark SQL批量写入MySQL</h2><p>代码示例如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchInsertMySQL</span> </span>&#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class">  <span class="title">def</span> <span class="title">main</span>(<span class="params">args: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span>: <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建sparkSession对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">"BatchInsertMySQL"</span>)</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> =  <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .config(conf)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// MySQL连接参数</span></span><br><span class="line">    <span class="keyword">val</span> url = <span class="type">JDBCUtils</span>.url</span><br><span class="line">    <span class="keyword">val</span> user = <span class="type">JDBCUtils</span>.user</span><br><span class="line">    <span class="keyword">val</span> pwd = <span class="type">JDBCUtils</span>.password</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建Properties对象，设置连接mysql的用户名和密码</span></span><br><span class="line">    <span class="keyword">val</span> properties: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line"></span><br><span class="line">    properties.setProperty(<span class="string">"user"</span>, user) <span class="comment">// 用户名</span></span><br><span class="line">    properties.setProperty(<span class="string">"password"</span>, pwd) <span class="comment">// 密码</span></span><br><span class="line">    properties.setProperty(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">    properties.setProperty(<span class="string">"numPartitions"</span>,<span class="string">"10"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取mysql中的表数据</span></span><br><span class="line">    <span class="keyword">val</span> testDF: <span class="type">DataFrame</span> = spark.read.jdbc(url, <span class="string">"test"</span>, properties)</span><br><span class="line">     println(<span class="string">"testDF的分区数：  "</span> + testDF.rdd.partitions.size)</span><br><span class="line">   testDF.createOrReplaceTempView(<span class="string">"test"</span>)</span><br><span class="line">   testDF.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK</span>)</span><br><span class="line">   testDF.printSchema()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result =</span><br><span class="line">      <span class="string">s""</span><span class="string">"-- SQL代码</span></span><br><span class="line"><span class="string">               "</span><span class="string">""</span>.stripMargin</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> resultBatch = spark.sql(result).as[<span class="type">Person</span>]</span><br><span class="line">    println(<span class="string">"resultBatch的分区数： "</span> + resultBatch.rdd.partitions.size)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 批量写入MySQL</span></span><br><span class="line">    <span class="comment">// 此处最好对处理的结果进行一次重分区</span></span><br><span class="line">    <span class="comment">// 由于数据量特别大，会造成每个分区数据特别多</span></span><br><span class="line">    resultBatch.repartition(<span class="number">500</span>).foreachPartition(record =&gt; &#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> list = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">Person</span>]</span><br><span class="line">      record.foreach(person =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> name = <span class="type">Person</span>.name</span><br><span class="line">        <span class="keyword">val</span> age = <span class="type">Person</span>.age</span><br><span class="line">        list.append(<span class="type">Person</span>(name,age))</span><br><span class="line">      &#125;)</span><br><span class="line">      upsertDateMatch(list) <span class="comment">//执行批量插入数据</span></span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">// 批量插入MySQL的方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">upsertPerson</span></span>(list: <span class="type">ListBuffer</span>[<span class="type">Person</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">var</span> connect: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line">      <span class="keyword">var</span> pstmt: <span class="type">PreparedStatement</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        connect = <span class="type">JDBCUtils</span>.getConnection()</span><br><span class="line">        <span class="comment">// 禁用自动提交</span></span><br><span class="line">        connect.setAutoCommit(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sql = <span class="string">"REPLACE INTO `person`(name, age)"</span> +</span><br><span class="line">          <span class="string">" VALUES(?, ?)"</span></span><br><span class="line"></span><br><span class="line">        pstmt = connect.prepareStatement(sql)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">var</span> batchIndex = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> (person &lt;- list) &#123;</span><br><span class="line">          pstmt.setString(<span class="number">1</span>, person.name)</span><br><span class="line">          pstmt.setString(<span class="number">2</span>, person.age)</span><br><span class="line">          <span class="comment">// 加入批次</span></span><br><span class="line">          pstmt.addBatch()</span><br><span class="line">          batchIndex +=<span class="number">1</span></span><br><span class="line">          <span class="comment">// 控制提交的数量,</span></span><br><span class="line">          <span class="comment">// MySQL的批量写入尽量限制提交批次的数据量，否则会把MySQL写挂！！！</span></span><br><span class="line">          <span class="keyword">if</span>(batchIndex % <span class="number">1000</span> == <span class="number">0</span> &amp;&amp; batchIndex !=<span class="number">0</span>)&#123;</span><br><span class="line">            pstmt.executeBatch()</span><br><span class="line">            pstmt.clearBatch()</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 提交批次</span></span><br><span class="line">        pstmt.executeBatch()</span><br><span class="line">        connect.commit()</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">          e.printStackTrace()</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="type">JDBCUtils</span>.closeConnection(connect, pstmt)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>JDBC连接工具类：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JDBCUtils</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> user = <span class="string">"root"</span></span><br><span class="line">  <span class="keyword">val</span> password = <span class="string">"root"</span></span><br><span class="line">  <span class="keyword">val</span> url = <span class="string">"jdbc:mysql://localhost:3306/mydb"</span></span><br><span class="line">  <span class="type">Class</span>.forName(<span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">  <span class="comment">// 获取连接</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getConnection</span></span>() = &#123;</span><br><span class="line">    <span class="type">DriverManager</span>.getConnection(url,user,password)</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">// 释放连接</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">closeConnection</span></span>(connection: <span class="type">Connection</span>, pstmt: <span class="type">PreparedStatement</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (pstmt != <span class="literal">null</span>) &#123;</span><br><span class="line">        pstmt.close()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; e.printStackTrace()</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (connection != <span class="literal">null</span>) &#123;</span><br><span class="line">        connection.close()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Spark写入大量数据到MySQL时，在写入之前尽量对写入的DF进行重分区处理，避免分区内数据过多。在写入时，要注意使用<strong>foreachPartition</strong>来进行写入，这样可以为每一个分区获取一个连接，在分区内部设定批次提交，提交的批次不易过大，以免将数据库写挂。</p>
<blockquote>
<p>公众号『大数据技术与数仓』，回复『资料』领取大数据资料包</p>
</blockquote>
</div><div class="recommended_posts"><h3>相关推荐 ☟</h3><li><a href="https://jiamaoxiang.top/2020/11/11/第十篇-SparkStreaming手动维护Kafka-Offset的几种方式/" target="_blank">第十篇|SparkStreaming手动维护Kafka Offset的几种方式</a></li><li><a href="https://jiamaoxiang.top/2020/11/01/Spark的五种JOIN方式解析/" target="_blank">Spark的五种JOIN策略解析</a></li><li><a href="https://jiamaoxiang.top/2020/10/26/篇四-ClickHouse的可视化界面与集群状态监控/" target="_blank">篇四|ClickHouse的可视化界面与集群状态监控</a></li><li><a href="https://jiamaoxiang.top/2020/10/24/Kafka-producer的几个重要配置参数/" target="_blank">Kafka producer的几个重要配置参数</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Jia MaoXiang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2020/11/01/Spark-SQL百万级数据批量读写入MySQL/">https://jiamaoxiang.top/2020/11/01/Spark-SQL百万级数据批量读写入MySQL/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本文为博主原创文章，遵循CC BY-SA 4.0版权协议，转载请附上原文出处链接和本声明</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://jiamaoxiang.top/2020/11/01/Spark-SQL百万级数据批量读写入MySQL/" data-id="ckk9d31zh003zxo7qtn8ifjhi" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACpUlEQVR42u3aQY7bMAwF0N7/0lOg28Lu/6SUZvG8CuIZhU8BTIbUr1/x9fPnyu++v/P36/e7SQyrCw8PD28U+tO137mWmqyTxIyHh4d3m5csl3/k0/blqx1bAQ8PD++/8toUMmO06QQPDw/vm3lJIf6eJPaJBw8PD+97eG24s4f4O3u2zrFeCx4eHl7Ma1ux3/D64nwPDw8PbzFVb5sO7+kkCS5ZoYgTDw8P7wIvbxy8B9EW6MnALGmCRAkDDw8P7ygv+dO2HZCvc6p9/LgCHh4e3jVeMgxLlm5TRd4gnt3Fw8PDu8HbBD2rXWfj/7YJgoeHh/dJ3uygwNkSPD/CVScGPDw8vDVv//jej8c2JwH+cRcPDw/vAq9ts+bUfLTWluP5mnh4eHg3eC0pL7I3BXS7fY9fAB4eHt5lXpsANu2MtmnbNjUefzHg4eHhrXmzpm3+AbNR1tkkhIeHh3eDlyeAZNHNMYIk0Paz8PDw8M7yfsqrLaxzTL4RRdrAw8PDu8aLWqLB0m1A+f+2RwoeB2B4eHh4H+HNMHm7Nm/15uO04feAh4eHV/KSXkU+iGpDTDarTjZ4eHh4H+S1h6XyInjWyGiby3h4eHj3eElhvSmUZwV3smZR1uPh4eFd4OVDqXacvz9KdWpghoeHh3eP1z6+2+3IGxZ5Sojix8PDwzvKywvWNtC2hZGklmQj8PDw8D7Da3/e54VyXnBfufDw8PAuwPZBbMZa+YN+eOgKDw8P7yhvkzzau7MgZukHDw8P7zZvNrg6u69tA6JoFuPh4eFd481asUl6aA9mzQ5+1YcG8PDw8L6Atwl9Qxr+GsDDw8P7CK9tsyZb0B4FaN/Hw8PDu8dLHuj7IwV7QJuo8PDw8G7wZgOwTVu2TSrtoS48PDy8a7zfLXENASuhEXsAAAAASUVORK5CYII=">分享</a><div class="tags"><a href="/tags/Spark/">Spark</a></div><div class="post-nav"><a class="pre" href="/2020/11/01/Spark的五种JOIN方式解析/">Spark的五种JOIN策略解析</a><a class="next" href="/2020/10/26/篇四-ClickHouse的可视化界面与集群状态监控/">篇四|ClickHouse的可视化界面与集群状态监控</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar-toc"><div class="stoc-article" id="sidebar-stoc"><strong class="stoc-title"><i class="fa fa-list-ul"> 目录</i></strong><div class="toc-nav" id="stoc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-SQL读取MySQL的方式"><span class="toc-number">1.</span> <span class="toc-text">Spark SQL读取MySQL的方式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#源码"><span class="toc-number">1.1.</span> <span class="toc-text">源码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#示例"><span class="toc-number">1.2.</span> <span class="toc-text">示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-SQL批量写入MySQL"><span class="toc-number">2.</span> <span class="toc-text">Spark SQL批量写入MySQL</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">3.</span> <span class="toc-text">总结</span></a></li></ol></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 - 2021 <a href="/." rel="nofollow">Jmx's Blog.</a>All rights reserved.<br>Thoughts on technology, life and everything else.</div></div></div><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" color="100,99,98" opacity="0.5" zindex="0.7" count="150" src="//lib.baomitu.com/canvas-nest.js/2.0.4/canvas-nest.umd.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>