<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记录朴实无华且枯燥的生活"><title>第十篇|SparkStreaming手动维护Kafka Offset的几种方式 | Jmx's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '912b6cfc43243cd27aeb428f7dbf7823';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">第十篇|SparkStreaming手动维护Kafka Offset的几种方式</h1><a id="logo" href="/.">Jmx's Blog</a><p class="description">Keep it Simple and Stupid!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-pied-piper-alt"> 关于</i></a><a href="/tags"><i class="fa fa-tags"> 标签</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">第十篇|SparkStreaming手动维护Kafka Offset的几种方式</h1><div class="post-meta">Nov 11, 2020<span> | </span><span class="category"><a href="/categories/Spark/">Spark</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2.7k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 13</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><a id="more"></a>

<p>Spark Streaming No Receivers 方式的createDirectStream 方法不使用接收器，而是创建输入流直接从Kafka 集群节点拉取消息。输入流保证每个消息从Kafka 集群拉取以后只完全转换一次，保证语义一致性。但是当作业发生故障或重启时，要保障从当前的消费位点去处理数据(即Exactly Once语义)，单纯的依靠SparkStreaming本身的机制是不太理想的，生产环境中通常借助手动管理offset的方式来维护kafka的消费位点。本文分享将介绍如何手动管理Kafka的Offset，希望对你有所帮助。本文主要包括以下内容：</p>
<ul>
<li>如何使用MySQL管理Kafka的Offset</li>
<li>如何使用Redis管理Kafka的OffSet</li>
</ul>
<blockquote>
<p>公众号『大数据技术与数仓』，回复『资料』领取大数据资料包</p>
</blockquote>
<h2 id="如何使用MySQL管理Kafka的Offset"><a href="#如何使用MySQL管理Kafka的Offset" class="headerlink" title="如何使用MySQL管理Kafka的Offset"></a>如何使用MySQL管理Kafka的Offset</h2><p>我们可以从Spark Streaming 应用程序中编写代码来手动管理Kafka偏移量，偏移量可以从每一批流处理中生成的RDDS偏移量来获取，获取方式为：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">KafkaUtils</span>.createDirectStream(...).foreachRDD &#123; rdd =&gt;</span><br><span class="line"><span class="comment">// 获取偏移量</span></span><br><span class="line"><span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line"> ...</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>当获取到偏移量之后，可以将将其保存到外部存储设备中(MySQL、Redis、Zookeeper、HBase等)。</p>
<h2 id="使用案例代码"><a href="#使用案例代码" class="headerlink" title="使用案例代码"></a>使用案例代码</h2><ul>
<li>MySQL中用于保存偏移量的表</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`topic_par_group_offset`</span> (</span><br><span class="line">  <span class="string">`topic`</span> <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`partition`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`groupid`</span> <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`offset`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`topic`</span>,<span class="string">`partition`</span>,<span class="string">`groupid`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 ;</span><br></pre></td></tr></table></figure>

<ul>
<li>常量配置类:<strong>ConfigConstants</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ConfigConstants</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Kafka配置</span></span><br><span class="line">  <span class="keyword">val</span> kafkaBrokers = <span class="string">"kms-2:9092,kms-3:9092,kms-4:9092"</span></span><br><span class="line">  <span class="keyword">val</span> groupId = <span class="string">"group_test"</span></span><br><span class="line">  <span class="keyword">val</span> kafkaTopics = <span class="string">"test"</span></span><br><span class="line">  <span class="keyword">val</span> batchInterval = <span class="type">Seconds</span>(<span class="number">5</span>)</span><br><span class="line">  <span class="keyword">val</span> streamingStorageLevel = <span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER_2</span></span><br><span class="line">  <span class="keyword">val</span> kafkaKeySer = <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span></span><br><span class="line">  <span class="keyword">val</span> kafkaValueSer = <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span></span><br><span class="line">  <span class="keyword">val</span> sparkSerializer = <span class="string">"org.apache.spark.serializer.KryoSerializer"</span></span><br><span class="line">  <span class="keyword">val</span> batchSize = <span class="number">16384</span></span><br><span class="line">  <span class="keyword">val</span> lingerMs = <span class="number">1</span></span><br><span class="line">  <span class="keyword">val</span> bufferMemory = <span class="number">33554432</span></span><br><span class="line">  <span class="comment">// MySQL配置</span></span><br><span class="line">  <span class="keyword">val</span> user = <span class="string">"root"</span></span><br><span class="line">  <span class="keyword">val</span> password = <span class="string">"123qwe"</span></span><br><span class="line">  <span class="keyword">val</span> url = <span class="string">"jdbc:mysql://localhost:3306/kafka_offset"</span></span><br><span class="line">  <span class="keyword">val</span> driver = <span class="string">"com.mysql.jdbc.Driver"</span></span><br><span class="line">  <span class="comment">// 检查点配置</span></span><br><span class="line">  <span class="keyword">val</span> checkpointDir = <span class="string">"file:///e:/checkpoint"</span></span><br><span class="line">  <span class="keyword">val</span> checkpointInterval = <span class="type">Seconds</span>(<span class="number">10</span>)</span><br><span class="line">  <span class="comment">// Redis配置</span></span><br><span class="line">  <span class="keyword">val</span> redisAddress = <span class="string">"192.168.10.203"</span></span><br><span class="line">  <span class="keyword">val</span> redisPort = <span class="number">6379</span></span><br><span class="line">  <span class="keyword">val</span> redisAuth = <span class="string">"123qwe"</span></span><br><span class="line">  <span class="keyword">val</span> redisTimeout = <span class="number">3000</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>JDBC连接工具类:<strong>JDBCConnPool</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JDBCConnPool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> log: <span class="type">Logger</span> = <span class="type">Logger</span>.getLogger(<span class="type">JDBCConnPool</span>.getClass)</span><br><span class="line">  <span class="keyword">var</span> dataSource: <span class="type">BasicDataSource</span> = <span class="literal">null</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 创建数据源</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getDataSource</span></span>(): <span class="type">BasicDataSource</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (dataSource == <span class="literal">null</span>) &#123;</span><br><span class="line">      dataSource = <span class="keyword">new</span> <span class="type">BasicDataSource</span>()</span><br><span class="line">      dataSource.setDriverClassName(<span class="type">ConfigConstants</span>.driver)</span><br><span class="line">      dataSource.setUrl(<span class="type">ConfigConstants</span>.url)</span><br><span class="line">      dataSource.setUsername(<span class="type">ConfigConstants</span>.user)</span><br><span class="line">      dataSource.setPassword(<span class="type">ConfigConstants</span>.password)</span><br><span class="line">      dataSource.setMaxTotal(<span class="number">50</span>)</span><br><span class="line">      dataSource.setInitialSize(<span class="number">3</span>)</span><br><span class="line">      dataSource.setMinIdle(<span class="number">3</span>)</span><br><span class="line">      dataSource.setMaxIdle(<span class="number">10</span>)</span><br><span class="line">      dataSource.setMaxWaitMillis(<span class="number">2</span> * <span class="number">10000</span>)</span><br><span class="line">      dataSource.setRemoveAbandonedTimeout(<span class="number">180</span>)</span><br><span class="line">      dataSource.setRemoveAbandonedOnBorrow(<span class="literal">true</span>)</span><br><span class="line">      dataSource.setRemoveAbandonedOnMaintenance(<span class="literal">true</span>)</span><br><span class="line">      dataSource.setTestOnReturn(<span class="literal">true</span>)</span><br><span class="line">      dataSource.setTestOnBorrow(<span class="literal">true</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dataSource</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 释放数据源</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">closeDataSource</span></span>() = &#123;</span><br><span class="line">    <span class="keyword">if</span> (dataSource != <span class="literal">null</span>) &#123;</span><br><span class="line">      dataSource.close()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 获取数据库连接</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getConnection</span></span>(): <span class="type">Connection</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> conn: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (dataSource != <span class="literal">null</span>) &#123;</span><br><span class="line">        conn = dataSource.getConnection()</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        conn = getDataSource().getConnection()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">        log.error(e.getMessage(), e)</span><br><span class="line">    &#125;</span><br><span class="line">    conn</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 关闭连接</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">closeConnection</span> </span>(ps:<span class="type">PreparedStatement</span> , conn:<span class="type">Connection</span> ) &#123;</span><br><span class="line">    <span class="keyword">if</span> (ps != <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        ps.close();</span><br><span class="line">      &#125; <span class="keyword">catch</span>  &#123;</span><br><span class="line">        <span class="keyword">case</span> e:<span class="type">Exception</span> =&gt;</span><br><span class="line">          log.error(<span class="string">"预编译SQL语句对象PreparedStatement关闭异常！"</span> + e.getMessage(), e);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (conn != <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        conn.close();</span><br><span class="line">      &#125; <span class="keyword">catch</span>  &#123;</span><br><span class="line">        <span class="keyword">case</span> e:<span class="type">Exception</span> =&gt;</span><br><span class="line">        log.error(<span class="string">"关闭连接对象Connection异常！"</span> + e.getMessage(), e);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>Kafka生产者：<strong>KafkaProducerTest</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaProducerTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span>  props : <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, <span class="type">ConfigConstants</span>.kafkaBrokers)</span><br><span class="line">    props.put(<span class="string">"batch.size"</span>, <span class="type">ConfigConstants</span>.batchSize.asInstanceOf[<span class="type">Integer</span>])</span><br><span class="line">    props.put(<span class="string">"linger.ms"</span>, <span class="type">ConfigConstants</span>.lingerMs.asInstanceOf[<span class="type">Integer</span>])</span><br><span class="line">    props.put(<span class="string">"buffer.memory"</span>, <span class="type">ConfigConstants</span>.bufferMemory.asInstanceOf[<span class="type">Integer</span>])</span><br><span class="line">    props.put(<span class="string">"key.serializer"</span>,<span class="type">ConfigConstants</span>.kafkaKeySer)</span><br><span class="line">    props.put(<span class="string">"value.serializer"</span>, <span class="type">ConfigConstants</span>.kafkaValueSer)</span><br><span class="line">   <span class="keyword">val</span>  producer :  <span class="type">Producer</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="keyword">new</span> <span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>](props)</span><br><span class="line">    <span class="keyword">val</span> startTime : <span class="type">Long</span>  = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">    <span class="keyword">for</span> ( i &lt;- <span class="number">1</span> to <span class="number">100</span>) &#123;</span><br><span class="line">      producer.send(<span class="keyword">new</span> <span class="type">ProducerRecord</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">ConfigConstants</span>.kafkaTopics, <span class="string">"Spark"</span>, <span class="type">Integer</span>.toString(i)))</span><br><span class="line">    &#125;</span><br><span class="line">  println(<span class="string">"消耗时间："</span> + (<span class="type">System</span>.currentTimeMillis() - startTime))</span><br><span class="line">    producer.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>读取和保存Offset：</li>
</ul>
<p>该对象的作用是从外部设备中读取和写入Offset，包括MySQL和Redis</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">OffsetReadAndSave</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 从MySQL中获取偏移量</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param groupid</span></span><br><span class="line"><span class="comment">    * @param topic</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getOffsetMap</span></span>(groupid: <span class="type">String</span>, topic: <span class="type">String</span>): mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conn = <span class="type">JDBCConnPool</span>.getConnection()</span><br><span class="line">    <span class="keyword">val</span> selectSql = <span class="string">"select * from topic_par_group_offset where groupid = ? and topic = ?"</span></span><br><span class="line">    <span class="keyword">val</span> ppst = conn.prepareStatement(selectSql)</span><br><span class="line">    ppst.setString(<span class="number">1</span>, groupid)</span><br><span class="line">    ppst.setString(<span class="number">2</span>, topic)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result: <span class="type">ResultSet</span> = ppst.executeQuery()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 主题分区偏移量</span></span><br><span class="line">    <span class="keyword">val</span> topicPartitionOffset = mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>]()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (result.next()) &#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> topicPartition: <span class="type">TopicPartition</span> = <span class="keyword">new</span> <span class="type">TopicPartition</span>(result.getString(<span class="string">"topic"</span>), result.getInt(<span class="string">"partition"</span>))</span><br><span class="line"></span><br><span class="line">      topicPartitionOffset += (topicPartition -&gt; result.getLong(<span class="string">"offset"</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">JDBCConnPool</span>.closeConnection(ppst, conn)</span><br><span class="line">    topicPartitionOffset</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 从Redis中获取偏移量</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param groupid</span></span><br><span class="line"><span class="comment">    * @param topic</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getOffsetFromRedis</span></span>(groupid: <span class="type">String</span>, topic: <span class="type">String</span>): <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> jedis: <span class="type">Jedis</span> = <span class="type">JedisConnPool</span>.getConnection()</span><br><span class="line">    <span class="keyword">var</span> offsets = mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>]()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> key = <span class="string">s"<span class="subst">$&#123;topic&#125;</span>_<span class="subst">$&#123;groupid&#125;</span>"</span></span><br><span class="line">    <span class="keyword">val</span> fields : java.util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = jedis.hgetAll(key)</span><br><span class="line">    <span class="keyword">for</span> (partition &lt;- <span class="type">JavaConversions</span>.mapAsScalaMap(fields)) &#123;</span><br><span class="line"></span><br><span class="line">      offsets.put(<span class="keyword">new</span> <span class="type">TopicPartition</span>(topic, partition._1.toInt), partition._2.toLong)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    offsets.toMap</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 将偏移量写入MySQL</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param groupid     消费者组ID</span></span><br><span class="line"><span class="comment">    * @param offsetRange 消息偏移量范围</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">saveOffsetRanges</span></span>(groupid: <span class="type">String</span>, offsetRange: <span class="type">Array</span>[<span class="type">OffsetRange</span>]) = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conn = <span class="type">JDBCConnPool</span>.getConnection()</span><br><span class="line">    <span class="keyword">val</span> insertSql = <span class="string">"replace into topic_par_group_offset(`topic`, `partition`, `groupid`, `offset`) values(?,?,?,?)"</span></span><br><span class="line">    <span class="keyword">val</span> ppst = conn.prepareStatement(insertSql)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (offset &lt;- offsetRange) &#123;</span><br><span class="line"></span><br><span class="line">      ppst.setString(<span class="number">1</span>, offset.topic)</span><br><span class="line">      ppst.setInt(<span class="number">2</span>, offset.partition)</span><br><span class="line">      ppst.setString(<span class="number">3</span>, groupid)</span><br><span class="line">      ppst.setLong(<span class="number">4</span>, offset.untilOffset)</span><br><span class="line">      ppst.executeUpdate()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">JDBCConnPool</span>.closeConnection(ppst, conn)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 将偏移量保存到Redis中</span></span><br><span class="line"><span class="comment">    * @param groupid</span></span><br><span class="line"><span class="comment">    * @param offsetRange</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">saveOffsetToRedis</span></span>(groupid: <span class="type">String</span>, offsetRange: <span class="type">Array</span>[<span class="type">OffsetRange</span>]) = &#123;</span><br><span class="line">    <span class="keyword">val</span> jedis :<span class="type">Jedis</span> = <span class="type">JedisConnPool</span>.getConnection()</span><br><span class="line">    <span class="keyword">for</span>(offsetRange&lt;-offsetRange)&#123;</span><br><span class="line">      <span class="keyword">val</span> topic=offsetRange.topic</span><br><span class="line">      <span class="keyword">val</span> partition=offsetRange.partition</span><br><span class="line">      <span class="keyword">val</span> offset=offsetRange.untilOffset</span><br><span class="line">      <span class="comment">// key为topic_groupid,field为partition，value为offset</span></span><br><span class="line">      jedis.hset(<span class="string">s"<span class="subst">$&#123;topic&#125;</span>_<span class="subst">$&#123;groupid&#125;</span>"</span>,partition.toString,offset.toString)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>业务处理类</li>
</ul>
<p>该对象是业务处理逻辑，主要是消费Kafka数据，再处理之后进行手动将偏移量保存到MySQL中。在启动程序时，会判断外部存储设备中是否存在偏移量，如果是首次启动则从最初的消费位点消费，如果存在Offset，则从当前的Offset去消费。</p>
<blockquote>
<p>观察现象：当首次启动时会从头消费数据，手动停止程序，然后再次启动，会发现会从当前提交的偏移量消费数据。</p>
</blockquote>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">object ManualCommitOffset &#123;</span><br><span class="line">  </span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val brokers = ConfigConstants.kafkaBrokers</span><br><span class="line">    val groupId = ConfigConstants.groupId</span><br><span class="line">    val topics = ConfigConstants.kafkaTopics</span><br><span class="line">    val batchInterval = ConfigConstants.batchInterval</span><br><span class="line"></span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">      .setAppName(ManualCommitOffset.getClass.getSimpleName)</span><br><span class="line">      .setMaster("local[1]")</span><br><span class="line">      .set("spark.serializer",ConfigConstants.sparkSerializer)</span><br><span class="line"></span><br><span class="line">    val ssc = new StreamingContext(conf, batchInterval)</span><br><span class="line">    // 必须开启checkpoint,否则会报错</span><br><span class="line">    ssc.checkpoint(ConfigConstants.checkpointDir)</span><br><span class="line"></span><br><span class="line">    ssc.sparkContext.setLogLevel("OFF")</span><br><span class="line">    //使用broker和topic创建direct kafka stream</span><br><span class="line">    val topicSet = topics.split(" ").toSet</span><br><span class="line"></span><br><span class="line">    // kafka连接参数</span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -&gt; brokers,</span><br><span class="line">      ConsumerConfig.GROUP_ID_CONFIG -&gt; groupId,</span><br><span class="line">      ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG -&gt; classOf[StringDeserializer],</span><br><span class="line">      ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG -&gt; classOf[StringDeserializer],</span><br><span class="line">      ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG -&gt; (false: java.lang.Boolean),</span><br><span class="line">      ConsumerConfig.AUTO_OFFSET_RESET_CONFIG -&gt; "earliest"</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // 从MySQL中读取该主题对应的消费者组的分区偏移量</span><br><span class="line">    val offsetMap = OffsetReadAndSave.getOffsetMap(groupId, topics)</span><br><span class="line">    var inputDStream: InputDStream[ConsumerRecord[String, String]] = null</span><br><span class="line"></span><br><span class="line">    //如果MySQL中已经存在了偏移量,则应该从该偏移量处开始消费</span><br><span class="line">    if (offsetMap.size &gt; 0) &#123;</span><br><span class="line">      println("存在偏移量，从该偏移量处进行消费！！")</span><br><span class="line"></span><br><span class="line">      inputDStream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">        ssc,</span><br><span class="line">        LocationStrategies.PreferConsistent,</span><br><span class="line">        ConsumerStrategies.Subscribe[String, String](topicSet, kafkaParams, offsetMap))</span><br><span class="line"></span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      //如果MySQL中没有存在了偏移量，从最早开始消费</span><br><span class="line">      inputDStream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">        ssc,</span><br><span class="line">        LocationStrategies.PreferConsistent,</span><br><span class="line">        ConsumerStrategies.Subscribe[String, String](topicSet, kafkaParams))</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    // checkpoint时间间隔，必须是batchInterval的整数倍</span><br><span class="line">    inputDStream.checkpoint(ConfigConstants.checkpointInterval)</span><br><span class="line"></span><br><span class="line">    // 保存batch的offset</span><br><span class="line">    var offsetRanges = Array[OffsetRange]()</span><br><span class="line">    // 获取当前DS的消息偏移量</span><br><span class="line">    val transformDS = inputDStream.transform &#123; rdd =&gt;</span><br><span class="line">      // 获取offset</span><br><span class="line">      offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line">      rdd</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 状态更新函数</span></span><br><span class="line"><span class="comment">      * @param newValues:新的value值</span></span><br><span class="line"><span class="comment">      * @param stateValue：状态值</span></span><br><span class="line"><span class="comment">      * @return</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    def updateFunc(newValues: Seq[Int], stateValue: Option[Int]): Option[Int] = &#123;</span><br><span class="line">      var oldvalue = stateValue.getOrElse(0) // 获取状态值</span><br><span class="line">      // 遍历当前数据，并更新状态</span><br><span class="line">      for (newValue &lt;- newValues) &#123;</span><br><span class="line">        oldvalue += newValue</span><br><span class="line">      &#125;</span><br><span class="line">      // 返回最新的状态</span><br><span class="line">      Option(oldvalue)</span><br><span class="line">    &#125;</span><br><span class="line">    // 业务逻辑处理</span><br><span class="line">    // 该示例统计消息key的个数，用于查看是否是从已经提交的偏移量消费数据</span><br><span class="line">    transformDS.map(meg =&gt; ("spark", meg.value().toInt)).updateStateByKey(updateFunc).print()</span><br><span class="line"></span><br><span class="line">    // 打印偏移量和数据信息，观察输出的结果</span><br><span class="line">    transformDS.foreachRDD &#123; (rdd, time) =&gt;</span><br><span class="line">      // 遍历打印该RDD数据</span><br><span class="line">      rdd.foreach &#123; record =&gt;</span><br><span class="line">        println(s"key=$&#123;record.key()&#125;,value=$&#123;record.value()&#125;,partition=$&#123;record.partition()&#125;,offset=$&#123;record.offset()&#125;")</span><br><span class="line">      &#125;</span><br><span class="line">      // 打印消费偏移量信息</span><br><span class="line">      for (o &lt;- offsetRanges) &#123;</span><br><span class="line">        println(s"topic=$&#123;o.topic&#125;,partition=$&#123;o.partition&#125;,fromOffset=$&#123;o.fromOffset&#125;,untilOffset=$&#123;o.untilOffset&#125;,time=$&#123;time&#125;")</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      //将偏移量保存到到MySQL中</span><br><span class="line">      OffsetReadAndSave.saveOffsetRanges(groupId, offsetRanges)</span><br><span class="line">    &#125;</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="如何使用Redis管理Kafka的OffSet"><a href="#如何使用Redis管理Kafka的OffSet" class="headerlink" title="如何使用Redis管理Kafka的OffSet"></a>如何使用Redis管理Kafka的OffSet</h2><ul>
<li>Redis连接类</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JedisConnPool</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> config = <span class="keyword">new</span> <span class="type">JedisPoolConfig</span></span><br><span class="line">  <span class="comment">//最大连接数</span></span><br><span class="line">  config.setMaxTotal(<span class="number">60</span>)</span><br><span class="line">  <span class="comment">//最大空闲连接数</span></span><br><span class="line">  config.setMaxIdle(<span class="number">10</span>)</span><br><span class="line">  config.setTestOnBorrow(<span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//服务器ip</span></span><br><span class="line">  <span class="keyword">val</span> redisAddress :<span class="type">String</span> = <span class="type">ConfigConstants</span>.redisAddress.toString</span><br><span class="line">  <span class="comment">// 端口号</span></span><br><span class="line">  <span class="keyword">val</span> redisPort:<span class="type">Int</span> = <span class="type">ConfigConstants</span>.redisPort.toInt</span><br><span class="line">  <span class="comment">//访问密码</span></span><br><span class="line">  <span class="keyword">val</span> redisAuth :<span class="type">String</span> = <span class="type">ConfigConstants</span>.redisAuth.toString</span><br><span class="line">  <span class="comment">//等待可用连接的最大时间</span></span><br><span class="line">  <span class="keyword">val</span> redisTimeout:<span class="type">Int</span> = <span class="type">ConfigConstants</span>.redisTimeout.toInt</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> pool = <span class="keyword">new</span> <span class="type">JedisPool</span>(config,redisAddress,redisPort,redisTimeout,redisAuth)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getConnection</span></span>():<span class="type">Jedis</span> = &#123;</span><br><span class="line">    pool.getResource</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>业务逻辑处理</li>
</ul>
<p>该对象与上面的基本类似，只不过使用的是Redis来进行存储Offset，存储到Redis的数据类型是Hash，基本格式为：[key field value]  -&gt; [ topic_groupid  partition offset]，即 key为topic_groupid,field为partition，value为offset。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ManualCommitOffsetToRedis</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> brokers = <span class="type">ConfigConstants</span>.kafkaBrokers</span><br><span class="line">    <span class="keyword">val</span> groupId = <span class="type">ConfigConstants</span>.groupId</span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">ConfigConstants</span>.kafkaTopics</span><br><span class="line">    <span class="keyword">val</span> batchInterval = <span class="type">ConfigConstants</span>.batchInterval</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="type">ManualCommitOffset</span>.getClass.getSimpleName)</span><br><span class="line">      .setMaster(<span class="string">"local[1]"</span>)</span><br><span class="line">      .set(<span class="string">"spark.serializer"</span>, <span class="type">ConfigConstants</span>.sparkSerializer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, batchInterval)</span><br><span class="line">    <span class="comment">// 必须开启checkpoint,否则会报错</span></span><br><span class="line">    ssc.checkpoint(<span class="type">ConfigConstants</span>.checkpointDir)</span><br><span class="line"></span><br><span class="line">    ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line">    <span class="comment">//使用broker和topic创建direct kafka stream</span></span><br><span class="line">    <span class="keyword">val</span> topicSet = topics.split(<span class="string">" "</span>).toSet</span><br><span class="line"></span><br><span class="line">    <span class="comment">// kafka连接参数</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; brokers,</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; groupId,</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">KEY_DESERIALIZER_CLASS_CONFIG</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">VALUE_DESERIALIZER_CLASS_CONFIG</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">ENABLE_AUTO_COMMIT_CONFIG</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>),</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">AUTO_OFFSET_RESET_CONFIG</span> -&gt; <span class="string">"earliest"</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从Redis中读取该主题对应的消费者组的分区偏移量</span></span><br><span class="line">    <span class="keyword">val</span> offsetMap = <span class="type">OffsetReadAndSave</span>.getOffsetFromRedis(groupId, topics)</span><br><span class="line">    <span class="keyword">var</span> inputDStream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//如果Redis中已经存在了偏移量,则应该从该偏移量处开始消费</span></span><br><span class="line">    <span class="keyword">if</span> (offsetMap.size &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      println(<span class="string">"存在偏移量，从该偏移量处进行消费！！"</span>)</span><br><span class="line"></span><br><span class="line">      inputDStream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">        ssc,</span><br><span class="line">        <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">        <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topicSet, kafkaParams, offsetMap))</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">//如果Redis中没有存在了偏移量，从最早开始消费</span></span><br><span class="line">      inputDStream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">        ssc,</span><br><span class="line">        <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">        <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topicSet, kafkaParams))</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// checkpoint时间间隔，必须是batchInterval的整数倍</span></span><br><span class="line">    inputDStream.checkpoint(<span class="type">ConfigConstants</span>.checkpointInterval)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 保存batch的offset</span></span><br><span class="line">    <span class="keyword">var</span> offsetRanges = <span class="type">Array</span>[<span class="type">OffsetRange</span>]()</span><br><span class="line">    <span class="comment">// 获取当前DS的消息偏移量</span></span><br><span class="line">    <span class="keyword">val</span> transformDS = inputDStream.transform &#123; rdd =&gt;</span><br><span class="line">      <span class="comment">// 获取offset</span></span><br><span class="line">      offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">      rdd</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 状态更新函数</span></span><br><span class="line"><span class="comment">      *</span></span><br><span class="line"><span class="comment">      * @param newValues  :新的value值</span></span><br><span class="line"><span class="comment">      * @param stateValue ：状态值</span></span><br><span class="line"><span class="comment">      * @return</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">updateFunc</span></span>(newValues: <span class="type">Seq</span>[<span class="type">Int</span>], stateValue: <span class="type">Option</span>[<span class="type">Int</span>]): <span class="type">Option</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">      <span class="keyword">var</span> oldvalue = stateValue.getOrElse(<span class="number">0</span>) <span class="comment">// 获取状态值</span></span><br><span class="line">      <span class="comment">// 遍历当前数据，并更新状态</span></span><br><span class="line">      <span class="keyword">for</span> (newValue &lt;- newValues) &#123;</span><br><span class="line">        oldvalue += newValue</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 返回最新的状态</span></span><br><span class="line">      <span class="type">Option</span>(oldvalue)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 业务逻辑处理</span></span><br><span class="line">    <span class="comment">// 该示例统计消息key的个数，用于查看是否是从已经提交的偏移量消费数据</span></span><br><span class="line">    transformDS.map(meg =&gt; (<span class="string">"spark"</span>, meg.value().toInt)).updateStateByKey(updateFunc).print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印偏移量和数据信息，观察输出的结果</span></span><br><span class="line">    transformDS.foreachRDD &#123; (rdd, time) =&gt;</span><br><span class="line">      <span class="comment">// 遍历打印该RDD数据</span></span><br><span class="line">      rdd.foreach &#123; record =&gt;</span><br><span class="line">        println(<span class="string">s"key=<span class="subst">$&#123;record.key()&#125;</span>,value=<span class="subst">$&#123;record.value()&#125;</span>,partition=<span class="subst">$&#123;record.partition()&#125;</span>,offset=<span class="subst">$&#123;record.offset()&#125;</span>"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 打印消费偏移量信息</span></span><br><span class="line">      <span class="keyword">for</span> (o &lt;- offsetRanges) &#123;</span><br><span class="line">        println(<span class="string">s"topic=<span class="subst">$&#123;o.topic&#125;</span>,partition=<span class="subst">$&#123;o.partition&#125;</span>,fromOffset=<span class="subst">$&#123;o.fromOffset&#125;</span>,untilOffset=<span class="subst">$&#123;o.untilOffset&#125;</span>,time=<span class="subst">$&#123;time&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">//将偏移量保存到到Redis中</span></span><br><span class="line">      <span class="type">OffsetReadAndSave</span>.saveOffsetToRedis(groupId, offsetRanges)</span><br><span class="line">    &#125;</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了如何使用外部存储设备来保存Kafka的消费位点，通过详细的代码示例说明了使用MySQL和Redis管理消费位点的方式。当然，外部存储设备很多，用户也可以使用其他的存储设备进行管理Offset，比如Zookeeper和HBase等，其基本处理思路都十分相似。</p>
<blockquote>
<p>公众号『大数据技术与数仓』，回复『资料』领取大数据资料包</p>
</blockquote>
</div><div class="recommended_posts"><h3>相关推荐 ☟</h3><li><a href="https://jiamaoxiang.top/2020/11/17/第五篇-ClickHouse数据导入-Flink、Spark、Kafka、MySQL/" target="_blank">篇五|ClickHouse数据导入(Flink、Spark、Kafka、MySQL、Hive)</a></li><li><a href="https://jiamaoxiang.top/2020/11/16/面试-不可不知的十大Hive调优技巧最佳实践/" target="_blank">面试|不可不知的十大Hive调优技巧最佳实践</a></li><li><a href="https://jiamaoxiang.top/2020/11/01/Spark的五种JOIN方式解析/" target="_blank">Spark的五种JOIN策略解析</a></li><li><a href="https://jiamaoxiang.top/2020/11/01/Spark-SQL百万级数据批量读写入MySQL/" target="_blank">Spark SQL百万级数据批量读写入MySQL</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Jia MaoXiang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2020/11/11/第十篇-SparkStreaming手动维护Kafka-Offset的几种方式/">https://jiamaoxiang.top/2020/11/11/第十篇-SparkStreaming手动维护Kafka-Offset的几种方式/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本文为博主原创文章，遵循CC BY-SA 4.0版权协议，转载请附上原文出处链接和本声明</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://jiamaoxiang.top/2020/11/11/第十篇-SparkStreaming手动维护Kafka-Offset的几种方式/" data-id="ckhramxit005ekk7q55d4mpuu" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPYAAAD2CAAAAADAeSUUAAADKUlEQVR42u3ayW7jQAwFQP//TyfA3AbOSI+kNLCY0knw1l1twDSX1yu+vv5c7/fvr3m/jj8nf+R43dcdFzY2NvZD2F+HV07NjyzZaIJMjumfK2JjY2OvY/e2mx/W8Xur4ar39WBjY2NjJ89OEonkvUkAw8bGxsauLlBNCSaBChsbGxt7UlSqFujz4lE1WOZh74JaGjY2NvbHsyeN3k+7/0/9bWxsbOyPYfcShrxtMGnlVod+CgpsbGzsRezkr/+kkTBp4lZjULXMhI2Njb2D3SskJZvrjfJUh356ARIbGxt7KzsPM5PWwrz03xsG+usRbGxs7NXs6u98b1ymehDVcaKTQ8TGxsZexG7+lY9Tjgs2Udxhb7QIGxsbewe7V1rKA14vtchHM0e9EWxsbOwV7PxvfZJIVEd2Jq/vJTPY2NjYm9jVkn2v3FMt6FcDZ746NjY29iZ2vqHJOE5e7p8U/QsJCTY2NvYidjX96BXu8wRm0rit3mNjY2PvYPe6wZOgNR+srJaQsLGxsX8DezIWkxebkm1NeIVVsLGxsZey7ygw5WWgpETVS2ywsbGxd7N7ZfdqQtIb7rmqqXDy5WFjY2M/nJ0XhvJkozesU01m8sbwD5+MjY2NvYh9XyCpphyTJCdvMGBjY2NvZecDOtV0JT/EPPAkRahC6MXGxsZ+ODv/0U9CQsLOC1LVMJknJNjY2Nhb2XlSUR24qb5mEiCj1bGxsbEXsatl/bylWk0zclKvbYCNjY29m31VapEcR6/FW20qRMEMGxsbewW7F6Imwax3WNURz5MdYmNjYy9izwtDVyGrIW2SqGBjY2NvYidh45jUayT0Uot8iLM8bYqNjY39cPa8ZF9lTJoKF7R7sbGxsdex8ysJUdXk4fhzrhrrwcbGxt7KnmwuaSH0Cvp3jAc1B4OwsbGxH8LOF+6VfqqrVMd98hVP3oaNjY39cHavNJ9ExV6SkLQBekkONjY29m9ml6eB4nSll1TkRzMKYNjY2NhL2XnQuiqwVYtQJ18YNjY29jr2fQX66hYnx5QXv7CxsbE3sasFmvyA8nJ/9Tiq4Q0bGxt7KfsbmRSqbpovYP4AAAAASUVORK5CYII=">分享</a><div class="tags"><a href="/tags/Spark/">Spark</a></div><div class="post-nav"><a class="pre" href="/2020/11/16/面试-不可不知的十大Hive调优技巧最佳实践/">面试|不可不知的十大Hive调优技巧最佳实践</a><a class="next" href="/2020/11/01/Spark的五种JOIN方式解析/">Spark的五种JOIN策略解析</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar-toc"><div class="stoc-article" id="sidebar-stoc"><strong class="stoc-title"><i class="fa fa-list-ul"> 目录</i></strong><div class="toc-nav" id="stoc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#如何使用MySQL管理Kafka的Offset"><span class="toc-number">1.</span> <span class="toc-text">如何使用MySQL管理Kafka的Offset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用案例代码"><span class="toc-number">2.</span> <span class="toc-text">使用案例代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#如何使用Redis管理Kafka的OffSet"><span class="toc-number">3.</span> <span class="toc-text">如何使用Redis管理Kafka的OffSet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">4.</span> <span class="toc-text">总结</span></a></li></ol></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 - 2020 <a href="/." rel="nofollow">Jmx's Blog.</a>All rights reserved.<br>Thoughts on technology, life and everything else.</div></div></div><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" color="100,99,98" opacity="0.5" zindex="0.7" count="150" src="//lib.baomitu.com/canvas-nest.js/2.0.4/canvas-nest.umd.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>