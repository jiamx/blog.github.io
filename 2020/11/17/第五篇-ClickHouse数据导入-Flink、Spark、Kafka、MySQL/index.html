<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记录朴实无华且枯燥的生活"><title>篇五|ClickHouse数据导入(Flink、Spark、Kafka、MySQL、Hive) | Jmx's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '912b6cfc43243cd27aeb428f7dbf7823';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">篇五|ClickHouse数据导入(Flink、Spark、Kafka、MySQL、Hive)</h1><a id="logo" href="/.">Jmx's Blog</a><p class="description">Keep it Simple and Stupid!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-pied-piper-alt"> 关于</i></a><a href="/tags"><i class="fa fa-tags"> 标签</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">篇五|ClickHouse数据导入(Flink、Spark、Kafka、MySQL、Hive)</h1><div class="post-meta">Nov 17, 2020<span> | </span><span class="category"><a href="/categories/ClickHouse/">ClickHouse</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2.3k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 10</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>本文分享主要是ClickHouse的数据导入方式，本文主要介绍如何使用Flink、Spark、Kafka、MySQL、Hive将数据导入ClickHouse，具体内容包括：</p>
<ul>
<li>使用Flink导入数据</li>
<li>使用Spark导入数据</li>
<li>从Kafka中导入数据</li>
<li>从MySQL中导入数据</li>
<li>从Hive中导入数据</li>
</ul>
<h2 id="使用Flink导入数据"><a href="#使用Flink导入数据" class="headerlink" title="使用Flink导入数据"></a>使用Flink导入数据</h2><p>本文介绍使用 flink-jdbc将数据导入ClickHouse，Maven依赖为：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-jdbc_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>本示例使用Kafka connector，通过Flink将Kafka数据实时导入到ClickHouse</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkSinkClickHouse</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String url = <span class="string">"jdbc:clickhouse://192.168.10.203:8123/default"</span>;</span><br><span class="line">        String user = <span class="string">"default"</span>;</span><br><span class="line">        String passwd = <span class="string">"hOn0d9HT"</span>;</span><br><span class="line">        String driver = <span class="string">"ru.yandex.clickhouse.ClickHouseDriver"</span>;</span><br><span class="line">        <span class="keyword">int</span> batchsize = <span class="number">500</span>; <span class="comment">// 设置batch size，测试的话可以设置小一点，这样可以立刻看到数据被写入</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建执行环境</span></span><br><span class="line">        EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">                .newInstance()</span><br><span class="line">                .useBlinkPlanner()</span><br><span class="line">                .inStreamingMode()</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings);</span><br><span class="line"></span><br><span class="line">        String kafkaSource11 = <span class="string">""</span> +</span><br><span class="line">                <span class="string">"CREATE TABLE user_behavior ( "</span> +</span><br><span class="line">                <span class="string">" `user_id` BIGINT, -- 用户id\n"</span> +</span><br><span class="line">                <span class="string">" `item_id` BIGINT, -- 商品id\n"</span> +</span><br><span class="line">                <span class="string">" `cat_id` BIGINT, -- 品类id\n"</span> +</span><br><span class="line">                <span class="string">" `action` STRING, -- 用户行为\n"</span> +</span><br><span class="line">                <span class="string">" `province` INT, -- 用户所在的省份\n"</span> +</span><br><span class="line">                <span class="string">" `ts` BIGINT, -- 用户行为发生的时间戳\n"</span> +</span><br><span class="line">                <span class="string">" `proctime` AS PROCTIME(), -- 通过计算列产生一个处理时间列\n"</span> +</span><br><span class="line">                <span class="string">" `eventTime` AS TO_TIMESTAMP(FROM_UNIXTIME(ts, 'yyyy-MM-dd HH:mm:ss')), -- 事件时间\n"</span> +</span><br><span class="line">                <span class="string">" WATERMARK FOR eventTime AS eventTime - INTERVAL '5' SECOND -- 在eventTime上定义watermark\n"</span> +</span><br><span class="line">                <span class="string">") WITH ( 'connector' = 'kafka', -- 使用 kafka connector\n"</span> +</span><br><span class="line">                <span class="string">" 'topic' = 'user_behavior', -- kafka主题\n"</span> +</span><br><span class="line">                <span class="string">" 'scan.startup.mode' = 'earliest-offset', -- 偏移量，从起始 offset 开始读取\n"</span> +</span><br><span class="line">                <span class="string">" 'properties.group.id' = 'group1', -- 消费者组\n"</span> +</span><br><span class="line">                <span class="string">" 'properties.bootstrap.servers' = 'kms-2:9092,kms-3:9092,kms-4:9092', -- kafka broker 地址\n"</span> +</span><br><span class="line">                <span class="string">" 'format' = 'json', -- 数据源格式为 json\n"</span> +</span><br><span class="line">                <span class="string">" 'json.fail-on-missing-field' = 'true',\n"</span> +</span><br><span class="line">                <span class="string">" 'json.ignore-parse-errors' = 'false'"</span> +</span><br><span class="line">                <span class="string">")"</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Kafka Source</span></span><br><span class="line">        tEnv.executeSql(kafkaSource11);</span><br><span class="line">        String query = <span class="string">"SELECT user_id,item_id,cat_id,action,province,ts FROM user_behavior"</span>;</span><br><span class="line">        Table table = tEnv.sqlQuery(query);</span><br><span class="line"></span><br><span class="line">        String insertIntoCkSql = <span class="string">"INSERT INTO behavior_mergetree(user_id,item_id,cat_id,action,province,ts)\n"</span> +</span><br><span class="line">                <span class="string">"VALUES(?,?,?,?,?,?)"</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将数据写入 ClickHouse Sink</span></span><br><span class="line">        JDBCAppendTableSink sink = JDBCAppendTableSink</span><br><span class="line">                .builder()</span><br><span class="line">                .setDrivername(driver)</span><br><span class="line">                .setDBUrl(url)</span><br><span class="line">                .setUsername(user)</span><br><span class="line">                .setPassword(passwd)</span><br><span class="line">                .setQuery(insertIntoCkSql)</span><br><span class="line">                .setBatchSize(batchsize)</span><br><span class="line">                .setParameterTypes(Types.LONG, Types.LONG,Types.LONG, Types.STRING,Types.INT,Types.LONG)</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        String[] arr = &#123;<span class="string">"user_id"</span>,<span class="string">"item_id"</span>,<span class="string">"cat_id"</span>,<span class="string">"action"</span>,<span class="string">"province"</span>,<span class="string">"ts"</span>&#125;;</span><br><span class="line">        TypeInformation[] type = &#123;Types.LONG, Types.LONG,Types.LONG, Types.STRING,Types.INT,Types.LONG&#125;;</span><br><span class="line"></span><br><span class="line">        tEnv.registerTableSink(</span><br><span class="line">                <span class="string">"sink"</span>,</span><br><span class="line">                arr,</span><br><span class="line">                type,</span><br><span class="line">                sink</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        tEnv.insertInto(table, <span class="string">"sink"</span>);</span><br><span class="line"></span><br><span class="line">        tEnv.execute(<span class="string">"Flink Table API to ClickHouse Example"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Note:</p>
<ul>
<li>由于 ClickHouse 单次插入的延迟比较高，我们需要设置 <code>BatchSize</code> 来批量插入数据，提高性能。</li>
<li>在 JDBCAppendTableSink 的实现中，若最后一批数据的数目不足 <code>BatchSize</code>，则不会插入剩余数据。</li>
</ul>
</blockquote>
<h2 id="使用Spark导入数据"><a href="#使用Spark导入数据" class="headerlink" title="使用Spark导入数据"></a>使用Spark导入数据</h2><p>本文主要介绍如何通过Spark程序写入数据到Clickhouse中。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ru.yandex.clickhouse<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>clickhouse-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.2.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 如果报错：Caused by: java.lang.ClassNotFoundException: com.google.common.escape.Escapers，则添加下面的依赖 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">version</span>&gt;</span>28.0-jre<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark2ClickHouseExample</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">  properties.put(<span class="string">"driver"</span>, <span class="string">"ru.yandex.clickhouse.ClickHouseDriver"</span>)</span><br><span class="line">  properties.put(<span class="string">"user"</span>, <span class="string">"default"</span>)</span><br><span class="line">  properties.put(<span class="string">"password"</span>, <span class="string">"hOn0d9HT"</span>)</span><br><span class="line">  properties.put(<span class="string">"batchsize"</span>, <span class="string">"1000"</span>)</span><br><span class="line">  properties.put(<span class="string">"socket_timeout"</span>, <span class="string">"300000"</span>)</span><br><span class="line">  properties.put(<span class="string">"numPartitions"</span>, <span class="string">"8"</span>)</span><br><span class="line">  properties.put(<span class="string">"rewriteBatchedStatements"</span>, <span class="string">"true"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">private</span> <span class="title">def</span> <span class="title">runDatasetCreationExample</span>(<span class="params">spark: <span class="type">SparkSession</span></span>)</span>: <span class="type">Dataset</span>[<span class="type">Person</span>] = &#123;</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// DataFrames转成DataSet</span></span><br><span class="line">    <span class="keyword">val</span> path = <span class="string">"file:///e:/people.json"</span></span><br><span class="line">    <span class="keyword">val</span> peopleDS = spark.read.json(path)</span><br><span class="line">    peopleDS.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line">    <span class="keyword">val</span> ds = spark.sql(<span class="string">"SELECT name,age FROM people"</span>).as[<span class="type">Person</span>]</span><br><span class="line">    ds.show()</span><br><span class="line">    ds</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> url = <span class="string">"jdbc:clickhouse://kms-1:8123/default"</span></span><br><span class="line">    <span class="keyword">val</span> table = <span class="string">"people"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"Spark  Example"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>) <span class="comment">//设置为本地运行</span></span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> ds = runDatasetCreationExample(spark)</span><br><span class="line"></span><br><span class="line">    ds.write.mode(<span class="type">SaveMode</span>.<span class="type">Append</span>).option(<span class="type">JDBCOptions</span>.<span class="type">JDBC_BATCH_INSERT_SIZE</span>, <span class="number">100000</span>).jdbc(url, table, properties)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="从Kafka中导入数据"><a href="#从Kafka中导入数据" class="headerlink" title="从Kafka中导入数据"></a>从Kafka中导入数据</h2><p>主要是使用ClickHouse的表引擎。</p>
<h4 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db.]table_name [<span class="keyword">ON</span> CLUSTER cluster]</span><br><span class="line">(</span><br><span class="line">    name1 [type1] [<span class="keyword">DEFAULT</span>|<span class="keyword">MATERIALIZED</span>|<span class="keyword">ALIAS</span> expr1],</span><br><span class="line">    name2 [type2] [<span class="keyword">DEFAULT</span>|<span class="keyword">MATERIALIZED</span>|<span class="keyword">ALIAS</span> expr2],</span><br><span class="line">    ...</span><br><span class="line">) <span class="keyword">ENGINE</span> = Kafka()</span><br><span class="line"><span class="keyword">SETTINGS</span></span><br><span class="line">    kafka_broker_list = <span class="string">'host:port'</span>,</span><br><span class="line">    kafka_topic_list = <span class="string">'topic1,topic2,...'</span>,</span><br><span class="line">    kafka_group_name = <span class="string">'group_name'</span>,</span><br><span class="line">    kafka_format = <span class="string">'data_format'</span>[,]</span><br><span class="line">    [kafka_row_delimiter = <span class="string">'delimiter_symbol'</span>,]</span><br><span class="line">    [kafka_schema = <span class="string">''</span>,]</span><br><span class="line">    [kafka_num_consumers = N,]</span><br><span class="line">    [kafka_max_block_size = <span class="number">0</span>,]</span><br><span class="line">    [kafka_skip_broken_messages = N,]</span><br><span class="line">    [kafka_commit_every_batch = <span class="number">0</span>,]</span><br><span class="line">    [kafka_thread_per_consumer = <span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<ul>
<li><code>kafka_broker_list</code> ：逗号分隔的brokers地址 (localhost:9092).</li>
<li><code>kafka_topic_list</code> ：Kafka 主题列表，多个主题用逗号分隔.</li>
<li><code>kafka_group_name</code> ：消费者组. </li>
<li><code>kafka_format</code> – Message format. 比如<code>JSONEachRow</code>、JSON、CSV等等</li>
</ul>
<h4 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h4><p>在kafka中创建user_behavior主题，并向该主题写入数据，数据示例为：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;<span class="attr">"user_id"</span>:<span class="number">63401</span>,<span class="attr">"item_id"</span>:<span class="number">6244</span>,<span class="attr">"cat_id"</span>:<span class="number">143</span>,<span class="attr">"action"</span>:<span class="string">"pv"</span>,<span class="attr">"province"</span>:<span class="number">3</span>,<span class="attr">"ts"</span>:<span class="number">1573445919</span>&#125;</span><br><span class="line">&#123;<span class="attr">"user_id"</span>:<span class="number">9164</span>,<span class="attr">"item_id"</span>:<span class="number">2817</span>,<span class="attr">"cat_id"</span>:<span class="number">611</span>,<span class="attr">"action"</span>:<span class="string">"fav"</span>,<span class="attr">"province"</span>:<span class="number">28</span>,<span class="attr">"ts"</span>:<span class="number">1573420486</span>&#125;</span><br><span class="line">&#123;<span class="attr">"user_id"</span>:<span class="number">63401</span>,<span class="attr">"item_id"</span>:<span class="number">6244</span>,<span class="attr">"cat_id"</span>:<span class="number">143</span>,<span class="attr">"action"</span>:<span class="string">"pv"</span>,<span class="attr">"province"</span>:<span class="number">3</span>,<span class="attr">"ts"</span>:<span class="number">1573445919</span>&#125;</span><br></pre></td></tr></table></figure>

<p>在ClickHouse中创建表，选择表引擎为Kafka()，如下:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"> <span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_user_behavior (</span><br><span class="line">    user_id UInt64 <span class="keyword">COMMENT</span> <span class="string">'用户id'</span>,</span><br><span class="line">    item_id UInt64 <span class="keyword">COMMENT</span> <span class="string">'商品id'</span>,</span><br><span class="line">    cat_id UInt16  <span class="keyword">COMMENT</span> <span class="string">'品类id'</span>,</span><br><span class="line">    <span class="keyword">action</span> <span class="keyword">String</span>  <span class="keyword">COMMENT</span> <span class="string">'行为'</span>,</span><br><span class="line">    province UInt8 <span class="keyword">COMMENT</span> <span class="string">'省份id'</span>,</span><br><span class="line">    ts UInt64      <span class="keyword">COMMENT</span> <span class="string">'时间戳'</span></span><br><span class="line">  ) <span class="keyword">ENGINE</span> = Kafka()</span><br><span class="line">    <span class="keyword">SETTINGS</span></span><br><span class="line">    kafka_broker_list = <span class="string">'cdh04:9092'</span>,</span><br><span class="line">    kafka_topic_list = <span class="string">'user_behavior'</span>,</span><br><span class="line">    kafka_group_name = <span class="string">'group1'</span>,</span><br><span class="line">    kafka_format = <span class="string">'JSONEachRow'</span></span><br><span class="line">;</span><br><span class="line"><span class="comment">-- 查询</span></span><br><span class="line">cdh04 :) <span class="keyword">select</span> * <span class="keyword">from</span> kafka_user_behavior ;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 再次查看数据，发现数据为空</span></span><br><span class="line">cdh04 :) <span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> kafka_user_behavior;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> kafka_user_behavior</span><br><span class="line"></span><br><span class="line">┌─<span class="keyword">count</span>()─┐</span><br><span class="line">│       <span class="number">0</span> │</span><br><span class="line">└─────────┘</span><br></pre></td></tr></table></figure>

<h4 id="通过物化视图将kafka数据导入ClickHouse"><a href="#通过物化视图将kafka数据导入ClickHouse" class="headerlink" title="通过物化视图将kafka数据导入ClickHouse"></a>通过物化视图将kafka数据导入ClickHouse</h4><p>当我们一旦查询完毕之后，ClickHouse会删除表内的数据，其实Kafka表引擎只是一个数据管道，我们可以通过物化视图的方式访问Kafka中的数据。</p>
<ul>
<li>首先创建一张Kafka表引擎的表，用于从Kafka中读取数据</li>
<li>然后再创建一张普通表引擎的表，比如MergeTree，面向终端用户使用</li>
<li>最后创建物化视图，用于将Kafka引擎表实时同步到终端用户所使用的表中</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--  创建Kafka引擎表</span></span><br><span class="line"> <span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_user_behavior_src (</span><br><span class="line">    user_id UInt64 <span class="keyword">COMMENT</span> <span class="string">'用户id'</span>,</span><br><span class="line">    item_id UInt64 <span class="keyword">COMMENT</span> <span class="string">'商品id'</span>,</span><br><span class="line">    cat_id UInt16  <span class="keyword">COMMENT</span> <span class="string">'品类id'</span>,</span><br><span class="line">    <span class="keyword">action</span> <span class="keyword">String</span>  <span class="keyword">COMMENT</span> <span class="string">'行为'</span>,</span><br><span class="line">    province UInt8 <span class="keyword">COMMENT</span> <span class="string">'省份id'</span>,</span><br><span class="line">    ts UInt64      <span class="keyword">COMMENT</span> <span class="string">'时间戳'</span></span><br><span class="line">  ) <span class="keyword">ENGINE</span> = Kafka()</span><br><span class="line">    <span class="keyword">SETTINGS</span></span><br><span class="line">    kafka_broker_list = <span class="string">'cdh04:9092'</span>,</span><br><span class="line">    kafka_topic_list = <span class="string">'user_behavior'</span>,</span><br><span class="line">    kafka_group_name = <span class="string">'group1'</span>,</span><br><span class="line">    kafka_format = <span class="string">'JSONEachRow'</span></span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建一张终端用户使用的表</span></span><br><span class="line"> <span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_user_behavior (</span><br><span class="line">    user_id UInt64 <span class="keyword">COMMENT</span> <span class="string">'用户id'</span>,</span><br><span class="line">    item_id UInt64 <span class="keyword">COMMENT</span> <span class="string">'商品id'</span>,</span><br><span class="line">    cat_id UInt16  <span class="keyword">COMMENT</span> <span class="string">'品类id'</span>,</span><br><span class="line">    <span class="keyword">action</span> <span class="keyword">String</span>  <span class="keyword">COMMENT</span> <span class="string">'行为'</span>,</span><br><span class="line">    province UInt8 <span class="keyword">COMMENT</span> <span class="string">'省份id'</span>,</span><br><span class="line">    ts UInt64      <span class="keyword">COMMENT</span> <span class="string">'时间戳'</span></span><br><span class="line">  ) <span class="keyword">ENGINE</span> = MergeTree()</span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> user_id</span><br><span class="line">;</span><br><span class="line"><span class="comment">-- 创建物化视图，同步数据</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">MATERIALIZED</span> <span class="keyword">VIEW</span> user_behavior_consumer <span class="keyword">TO</span> kafka_user_behavior</span><br><span class="line">    <span class="keyword">AS</span> <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> kafka_user_behavior_src ;</span><br><span class="line"><span class="comment">-- 查询，多次查询，已经被查询的数据依然会被输出</span></span><br><span class="line">cdh04 :) <span class="keyword">select</span> * <span class="keyword">from</span> kafka_user_behavior;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Note:</p>
<p> Kafka消费表不能直接作为结果表使用。Kafka消费表只是用来消费Kafka数据，没有真正的存储所有数据。</p>
</blockquote>
<h2 id="从MySQL中导入数据"><a href="#从MySQL中导入数据" class="headerlink" title="从MySQL中导入数据"></a>从MySQL中导入数据</h2><p>同kafka中导入数据类似，ClickHouse同样支持MySQL表引擎，即映射一张MySQL中的表到ClickHouse中。</p>
<h3 id="数据类型对应关系"><a href="#数据类型对应关系" class="headerlink" title="数据类型对应关系"></a>数据类型对应关系</h3><p>MySQL中数据类型与ClickHouse类型映射关系如下表。</p>
<table>
<thead>
<tr>
<th align="left">MySQL</th>
<th align="left">ClickHouse</th>
</tr>
</thead>
<tbody><tr>
<td align="left">UNSIGNED TINYINT</td>
<td align="left">UInt8</td>
</tr>
<tr>
<td align="left">TINYINT</td>
<td align="left">Int8</td>
</tr>
<tr>
<td align="left">UNSIGNED SMALLINT</td>
<td align="left">UInt16</td>
</tr>
<tr>
<td align="left">SMALLINT</td>
<td align="left">Int16</td>
</tr>
<tr>
<td align="left">UNSIGNED INT, UNSIGNED MEDIUMINT</td>
<td align="left">UInt32</td>
</tr>
<tr>
<td align="left">INT, MEDIUMINT</td>
<td align="left">Int32</td>
</tr>
<tr>
<td align="left">UNSIGNED BIGINT</td>
<td align="left">UInt64</td>
</tr>
<tr>
<td align="left">BIGINT</td>
<td align="left">Int64</td>
</tr>
<tr>
<td align="left">FLOAT</td>
<td align="left">Float32</td>
</tr>
<tr>
<td align="left">DOUBLE</td>
<td align="left">Float64</td>
</tr>
<tr>
<td align="left">DATE</td>
<td align="left">Date</td>
</tr>
<tr>
<td align="left">DATETIME, TIMESTAMP</td>
<td align="left">DateTime</td>
</tr>
<tr>
<td align="left">BINARY</td>
<td align="left">FixedString</td>
</tr>
</tbody></table>
<h4 id="使用方式-1"><a href="#使用方式-1" class="headerlink" title="使用方式"></a>使用方式</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db.]table_name [<span class="keyword">ON</span> CLUSTER cluster]</span><br><span class="line">(</span><br><span class="line">    name1 [type1] [<span class="keyword">DEFAULT</span>|<span class="keyword">MATERIALIZED</span>|<span class="keyword">ALIAS</span> expr1] [TTL expr1],</span><br><span class="line">    name2 [type2] [<span class="keyword">DEFAULT</span>|<span class="keyword">MATERIALIZED</span>|<span class="keyword">ALIAS</span> expr2] [TTL expr2],</span><br><span class="line">    ...</span><br><span class="line">) <span class="keyword">ENGINE</span> = MySQL(<span class="string">'host:port'</span>, <span class="string">'database'</span>, <span class="string">'table'</span>, <span class="string">'user'</span>, <span class="string">'password'</span>[, replace_query, <span class="string">'on_duplicate_clause'</span>]);</span><br></pre></td></tr></table></figure>

<h4 id="使用示例-1"><a href="#使用示例-1" class="headerlink" title="使用示例"></a>使用示例</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 连接MySQL中clickhouse数据库的test表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> mysql_users(</span><br><span class="line">    <span class="keyword">id</span> Int32,</span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">String</span></span><br><span class="line">) <span class="keyword">ENGINE</span> = MySQL(</span><br><span class="line"> <span class="string">'192.168.10.203:3306'</span>,</span><br><span class="line"> <span class="string">'clickhouse'</span>,</span><br><span class="line"> <span class="string">'users'</span>, </span><br><span class="line"> <span class="string">'root'</span>, </span><br><span class="line"> <span class="string">'123qwe'</span>);</span><br><span class="line"><span class="comment">-- 查询数据</span></span><br><span class="line">cdh04 :) <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> mysql_users;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> mysql_users</span><br><span class="line"></span><br><span class="line">┌─<span class="keyword">id</span>─┬─<span class="keyword">name</span>──┐</span><br><span class="line">│  <span class="number">1</span> │ tom   │</span><br><span class="line">│  <span class="number">2</span> │ jack  │</span><br><span class="line">│  <span class="number">3</span> │ lihua │</span><br><span class="line">└────┴───────┘</span><br><span class="line"><span class="comment">-- 插入数据，会将数据插入MySQL对应的表中</span></span><br><span class="line"><span class="comment">-- 所以当查询MySQL数据时，会发现新增了一条数据</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">users</span> <span class="keyword">VALUES</span>(<span class="number">4</span>,<span class="string">'robin'</span>);</span><br><span class="line"><span class="comment">-- 再次查询</span></span><br><span class="line">cdh04 :) <span class="keyword">select</span> * <span class="keyword">from</span> mysql_users;                </span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> mysql_users</span><br><span class="line"></span><br><span class="line">┌─<span class="keyword">id</span>─┬─<span class="keyword">name</span>──┐</span><br><span class="line">│  <span class="number">1</span> │ tom   │</span><br><span class="line">│  <span class="number">2</span> │ jack  │</span><br><span class="line">│  <span class="number">3</span> │ lihua │</span><br><span class="line">│  <span class="number">4</span> │ robin │</span><br><span class="line">└────┴───────┘</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>：对于MySQL表引擎，不支持UPDATE和DELETE操作，比如执行下面命令时，会报错：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 执行更新</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> mysql_users <span class="keyword">UPDATE</span> <span class="keyword">name</span> = <span class="string">'hanmeimei'</span> <span class="keyword">WHERE</span> <span class="keyword">id</span> = <span class="number">1</span>;</span><br><span class="line"><span class="comment">-- 执行删除</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> mysql_users <span class="keyword">DELETE</span> <span class="keyword">WHERE</span> <span class="keyword">id</span> = <span class="number">1</span>;</span><br><span class="line"><span class="comment">-- 报错</span></span><br><span class="line">DB::Exception: Mutations are not supported by storage MySQL.</span><br></pre></td></tr></table></figure>

<h2 id="从Hive中导入数据"><a href="#从Hive中导入数据" class="headerlink" title="从Hive中导入数据"></a>从Hive中导入数据</h2><p>本文使用Waterdrop进行数据导入，Waterdrop是一个非常易用，高性能，能够应对海量数据的实时数据处理产品，它构建在Spark之上。Waterdrop拥有着非常丰富的插件，支持从Kafka、HDFS、Kudu中读取数据，进行各种各样的数据处理，并将结果写入ClickHouse、Elasticsearch或者Kafka中。</p>
<p>我们仅需要编写一个Waterdrop Pipeline的配置文件即可完成数据的导入。配置文件包括四个部分，分别是Spark、Input、filter和Output。</p>
<p>关于Waterdrop的安装，十分简单，只需要下载ZIP文件，解压即可。使用Waterdrop需要安装Spark。</p>
<ul>
<li><p>在Waterdrop安装目录的config/文件夹下创建配置文件：<strong>hive_table_batch.conf</strong>，内容如下。主要包括四部分：Spark、Input、filter和Output。</p>
<ul>
<li><p>Spark部分是Spark的相关配置，主要配置Spark执行时所需的资源大小。</p>
</li>
<li><p>Input部分是定义数据源，其中<code>pre_sql</code>是从Hive中读取数据SQL，<code>table_name</code>是将读取后的数据，注册成为Spark中临时表的表名，可为任意字段。</p>
</li>
<li><p>filter部分配置一系列的转化，比如过滤字段</p>
</li>
<li><p>Output部分是将处理好的结构化数据写入ClickHouse，ClickHouse的连接配置。</p>
<p>需要注意的是，必须保证hive的metastore是在服务状态。</p>
</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">spark &#123;</span><br><span class="line">  spark.app.name = <span class="string">"Waterdrop_Hive2ClickHouse"</span></span><br><span class="line">  spark.executor.instances = 2</span><br><span class="line">  spark.executor.cores = 1</span><br><span class="line">  spark.executor.memory = <span class="string">"1g"</span></span><br><span class="line">  // 这个配置必需填写</span><br><span class="line">  spark.sql.catalogImplementation = <span class="string">"hive"</span></span><br><span class="line">&#125;</span><br><span class="line">input &#123;</span><br><span class="line">    hive &#123;</span><br><span class="line">        pre_sql = <span class="string">"select * from default.users"</span></span><br><span class="line">        table_name = <span class="string">"hive_users"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">filter &#123;&#125;</span><br><span class="line">output &#123;</span><br><span class="line">    clickhouse &#123;</span><br><span class="line">        host = <span class="string">"kms-1:8123"</span></span><br><span class="line">        database = <span class="string">"default"</span></span><br><span class="line">        table = <span class="string">"users"</span></span><br><span class="line">        fields = [<span class="string">"id"</span>, <span class="string">"name"</span>]</span><br><span class="line">        username = <span class="string">"default"</span></span><br><span class="line">        password = <span class="string">"hOn0d9HT"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>执行任务</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[kms@kms-1 waterdrop-1.5.1]$ bin/<span class="keyword">start</span>-waterdrop.sh  <span class="comment">--config config/hive_table_batch.conf --master yarn --deploy-mode cluster</span></span><br></pre></td></tr></table></figure>

<p>这样就会启动一个Spark作业执行数据的抽取，等执行完成之后，查看ClickHouse的数据。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要介绍了如何通过Flink、Spark、Kafka、MySQL以及Hive，将数据导入到ClickHouse，对每一种方式都出了详细的示例，希望对你有所帮助。</p>
<blockquote>
<p>公众号『大数据技术与数仓』，回复『资料』领取大数据资料包</p>
</blockquote>
</div><div class="recommended_posts"><h3>相关推荐 ☟</h3><li><a href="https://jiamaoxiang.top/2020/11/16/面试-不可不知的十大Hive调优技巧最佳实践/" target="_blank">面试|不可不知的十大Hive调优技巧最佳实践</a></li><li><a href="https://jiamaoxiang.top/2020/11/11/第十篇-SparkStreaming手动维护Kafka-Offset的几种方式/" target="_blank">第十篇|SparkStreaming手动维护Kafka Offset的几种方式</a></li><li><a href="https://jiamaoxiang.top/2020/11/01/Spark的五种JOIN方式解析/" target="_blank">Spark的五种JOIN策略解析</a></li><li><a href="https://jiamaoxiang.top/2020/11/01/Spark-SQL百万级数据批量读写入MySQL/" target="_blank">Spark SQL百万级数据批量读写入MySQL</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Jia MaoXiang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2020/11/17/第五篇-ClickHouse数据导入-Flink、Spark、Kafka、MySQL/">https://jiamaoxiang.top/2020/11/17/第五篇-ClickHouse数据导入-Flink、Spark、Kafka、MySQL/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本文为博主原创文章，遵循CC BY-SA 4.0版权协议，转载请附上原文出处链接和本声明</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://jiamaoxiang.top/2020/11/17/第五篇-ClickHouse数据导入-Flink、Spark、Kafka、MySQL/" data-id="ckhm1qwar004f1o7q6ck3hhw4" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPYAAAD2CAAAAADAeSUUAAADKklEQVR42u3aUY7iQAwEUO5/afYAu8lW2RkJel6+EGSgX2ekku1+veLrfXH9fc/VO/ffcP/+1Xru73ngwsbGxv4S9vv2uie1S88Zmy1IHhI2Njb2eew8JHLYLK7uA2m/rdjY2NjYmxIlQeZFCzY2Njb2T5QHyQbdI6NFjx4PNjY29tnsNjaSOEnY+ac5++FeGjY2NvbHs/Op6Oe//pH5NjY2NvYHs9/llTeG2jFA0pZqG1uXCmxsbOyD2LMjOy3yqTuTlRQlDTY2NvZB7HZxszZNMkjYtLQeay1hY2Njfy27LRWS4mS2oDbS2uE0NjY29nns9+LKIzBvHrWHhNqYxMbGxj6JnS8rX2geObPhQdta+sed2NjY2Aex20M57ZfO7mkP4uRF0fAsEjY2NvYHszcB1v5tW/y0TaX8V7CxsbHPZs+GAbOBbj5amBU8l7+LjY2NfRz7tbjaOEmKjaSZlW8fNjY29tnsNgY2r2cHg4quWDz6xcbGxv4N7P1BnGfLieRhbAYM2NjY2N/O3rRpZmXGLCYTTL5CbGxs7DPYe9jm2jSb8sImmopgY2NjH8FujzDOipYkour6qd0UbGxs7EPZ+1IhOUY5O6wzaypdPgBsbGzsg9hJ02e29CTA8jHDLA6LuMXGxsb+WvbsKMw9o31//221BRsbG/s4dhsqmxFv3oSa3R/FHjY2NvavYT9bhOTxM9uUfBSNjY2NfRJ7dvimXXS+KW2YteUTNjY29qnsZCnt6LctWjbsZBMv38fGxsY+gv1s8ORNn9nwIOn2v/JMw8bGxj6CvR/o5jG2KV02D2n1X4CNjY398ey8JMjHw/kgNo+6NsD+E7fY2NjYx7Hzlv1mENsWM200rqYi2NjY2F/I3ox424KkPZSThFNbaWFjY2Ofx54FwD6iZuVNvtGzAMbGxsb+XnYSWgl11iRKPm1jMipgsLGxsY9jP9XKaZs+bakza0XVJ5WwsbGxfwG7bQDlpUteP81CERsbGxt7Fm/tPe1ooW6EYWNjYx/HnjWVkrBp2/ezhlQ9AMbGxsY+iJ2HwaxBnw8V8kM8s8HzcL6NjY2N/bnsP6nuIUzVd/V4AAAAAElFTkSuQmCC">分享</a><div class="tags"><a href="/tags/ClickHouse/">ClickHouse</a></div><div class="post-nav"><a class="next" href="/2020/11/16/面试-不可不知的十大Hive调优技巧最佳实践/">面试|不可不知的十大Hive调优技巧最佳实践</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar-toc"><div class="stoc-article" id="sidebar-stoc"><strong class="stoc-title"><i class="fa fa-list-ul"> 目录</i></strong><div class="toc-nav" id="stoc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#使用Flink导入数据"><span class="toc-number">1.</span> <span class="toc-text">使用Flink导入数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#示例"><span class="toc-number">1.1.</span> <span class="toc-text">示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用Spark导入数据"><span class="toc-number">2.</span> <span class="toc-text">使用Spark导入数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#示例-1"><span class="toc-number">2.1.</span> <span class="toc-text">示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#从Kafka中导入数据"><span class="toc-number">3.</span> <span class="toc-text">从Kafka中导入数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#使用方式"><span class="toc-number">3.0.1.</span> <span class="toc-text">使用方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用示例"><span class="toc-number">3.0.2.</span> <span class="toc-text">使用示例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#通过物化视图将kafka数据导入ClickHouse"><span class="toc-number">3.0.3.</span> <span class="toc-text">通过物化视图将kafka数据导入ClickHouse</span></a></li></ol></li></ol><li class="toc-item toc-level-2"><a class="toc-link" href="#从MySQL中导入数据"><span class="toc-number">4.</span> <span class="toc-text">从MySQL中导入数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#数据类型对应关系"><span class="toc-number">4.1.</span> <span class="toc-text">数据类型对应关系</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#使用方式-1"><span class="toc-number">4.1.1.</span> <span class="toc-text">使用方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用示例-1"><span class="toc-number">4.1.2.</span> <span class="toc-text">使用示例</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#从Hive中导入数据"><span class="toc-number">5.</span> <span class="toc-text">从Hive中导入数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">6.</span> <span class="toc-text">总结</span></a></li></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 - 2020 <a href="/." rel="nofollow">Jmx's Blog.</a>All rights reserved.<br>Thoughts on technology, life and everything else.</div></div></div><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" color="100,99,98" opacity="0.5" zindex="0.7" count="150" src="//lib.baomitu.com/canvas-nest.js/2.0.4/canvas-nest.umd.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>