<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记录朴实无华且枯燥的生活"><title>Flink Table API &amp; SQL编程指南(1) | Jmx's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '912b6cfc43243cd27aeb428f7dbf7823';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Flink Table API &amp; SQL编程指南(1)</h1><a id="logo" href="/.">Jmx's Blog</a><p class="description">Keep it Simple and Stupid!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-pied-piper-alt"> 关于</i></a><a href="/tags"><i class="fa fa-tags"> 标签</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Flink Table API &amp; SQL编程指南(1)</h1><div class="post-meta">May 25, 2020<span> | </span><span class="category"><a href="/categories/Flink/">Flink</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 5.8k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 24</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>Apache Flink提供了两种顶层的关系型API，分别为Table API和SQL，Flink通过Table API&amp;SQL实现了批流统一。其中Table API是用于Scala和Java的语言集成查询API，它允许以非常直观的方式组合关系运算符（例如select，where和join）的查询。Flink SQL基于<a href="https://calcite.apache.org/" target="_blank" rel="noopener">Apache Calcite</a> 实现了标准的SQL，用户可以使用标准的SQL处理数据集。Table API和SQL与Flink的DataStream和DataSet API紧密集成在一起，用户可以实现相互转化，比如可以将DataStream或者DataSet注册为table进行操作数据。值得注意的是，<strong>Table API and SQL</strong>目前尚未完全完善，还在积极的开发中，所以并不是所有的算子操作都可以通过其实现。</p>
<a id="more"></a>

<h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><p>从Flink1.9开始，Flink为Table &amp; SQL API提供了两种planner,分别为Blink planner和old planner，其中old planner是在Flink1.9之前的版本使用。主要区别如下：</p>
<p><strong>尖叫提示</strong>：对于生产环境，目前推荐使用old planner.</p>
<ul>
<li><code>flink-table-common</code>: 通用模块，包含 Flink Planner 和 Blink Planner 一些共用的代码</li>
<li><code>flink-table-api-java</code>: java语言的Table &amp; SQL API，仅针对table(处于早期的开发阶段，不推荐使用) </li>
<li><code>flink-table-api-scala</code>: scala语言的Table &amp; SQL API，仅针对table(处于早期的开发阶段，不推荐使用) </li>
<li><code>flink-table-api-java-bridge</code>: java语言的Table &amp; SQL API，支持DataStream/DataSet API(推荐使用) </li>
<li><code>flink-table-api-scala-bridge</code>: scala语言的Table &amp; SQL API，支持DataStream/DataSet API(推荐使用) </li>
<li><code>flink-table-planner</code>:planner 和runtime. planner为Flink1,9之前的old planner(推荐使用) </li>
<li><code>flink-table-planner-blink</code>: 新的Blink planner.</li>
<li><code>flink-table-runtime-blink</code>: 新的Blink runtime.</li>
<li><code>flink-table-uber</code>: 将上述的API模块及old planner打成一个jar包，形如flink-table-*.jar，位与/lib目录下</li>
<li><code>flink-table-uber-blink</code>:将上述的API模块及Blink 模块打成一个jar包，形如fflink-table-blink-*.jar，位与/lib目录下</li>
</ul>
<h2 id="Blink-planner-amp-old-planner"><a href="#Blink-planner-amp-old-planner" class="headerlink" title="Blink planner &amp; old planner"></a>Blink planner &amp; old planner</h2><p>Blink planner和old planner有许多不同的特点，具体列举如下：</p>
<ul>
<li>Blink planner将批处理作业看做是流处理作业的特例。所以，不支持Table 与DataSet之间的转换，批处理的作业也不会被转成DataSet程序，而是被转为DataStream程序。</li>
<li>Blink planner不支持 <code>BatchTableSource</code>，使用的是有界的StreamTableSource。</li>
<li>Blink planner仅支持新的 <code>Catalog</code>，不支持<code>ExternalCatalog</code> (已过时)。</li>
<li>对于FilterableTableSource的实现，两种Planner是不同的。old planner会谓词下推到<code>PlannerExpression</code>(未来会被移除)，而Blink planner 会谓词下推到 <code>Expression</code>(表示一个产生计算结果的逻辑树)。</li>
<li>仅仅Blink planner支持key-value形式的配置，即通过Configuration进行参数设置。</li>
<li>关于PlannerConfig的实现，两种planner有所不同。</li>
<li>Blink planner 会将多个sink优化成一个DAG(仅支持TableEnvironment，StreamTableEnvironment不支持)，old planner总是将每一个sink优化成一个新的DAG，每一个DAG都是相互独立的。</li>
<li>old planner不支持catalog统计，Blink planner支持catalog统计。</li>
</ul>
<h2 id="Flink-Table-amp-SQL程序的pom依赖"><a href="#Flink-Table-amp-SQL程序的pom依赖" class="headerlink" title="Flink Table &amp; SQL程序的pom依赖"></a>Flink Table &amp; SQL程序的pom依赖</h2><p>根据使用的语言不同，可以选择下面的依赖，包括scala版和java版，如下：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;!-- java版 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-table-api-java-bridge_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;!-- scala版 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-table-api-scala-bridge_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>除此之外，如果需要在本地的IDE中运行Table API &amp; SQL的程序，则需要添加下面的pom依赖：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;!-- Flink <span class="number">1.9</span>之前的old planner --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-table-planner_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;!-- 新的Blink planner --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-table-planner-blink_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>另外，如果需要实现自定义的格式(比如和kafka交互)或者用户自定义函数，需要添加如下依赖：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<h2 id="Table-API-amp-SQL的编程模板"><a href="#Table-API-amp-SQL的编程模板" class="headerlink" title="Table API &amp; SQL的编程模板"></a>Table API &amp; SQL的编程模板</h2><p>所有的Table API&amp;SQL的程序(无论是批处理还是流处理)都有着相同的形式，下面将给出通用的编程结构形式：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建一个TableEnvironment对象，指定planner、处理模式(batch、streaming)</span></span><br><span class="line">TableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// 创建一个表</span></span><br><span class="line">tableEnv.connect(...).createTemporaryTable(<span class="string">"table1"</span>);</span><br><span class="line"><span class="comment">// 注册一个外部的表</span></span><br><span class="line">tableEnv.connect(...).createTemporaryTable(<span class="string">"outputTable"</span>);</span><br><span class="line"><span class="comment">// 通过Table API的查询创建一个Table 对象</span></span><br><span class="line">Table tapiResult = tableEnv.from(<span class="string">"table1"</span>).select(...);</span><br><span class="line"><span class="comment">// 通过SQL查询的查询创建一个Table 对象</span></span><br><span class="line">Table sqlResult  = tableEnv.sqlQuery(<span class="string">"SELECT ... FROM table1 ... "</span>);</span><br><span class="line"><span class="comment">// 将结果写入TableSink</span></span><br><span class="line">tapiResult.insertInto(<span class="string">"outputTable"</span>);</span><br><span class="line"><span class="comment">// 执行</span></span><br><span class="line">tableEnv.execute(<span class="string">"java_job"</span>);</span><br></pre></td></tr></table></figure>

<p>注意：Table API &amp; SQL的查询可以相互集成，另外还可以在DataStream或者DataSet中使用Table API &amp; SQL的API，实现DataStreams、 DataSet与Table之间的相互转换。</p>
<h2 id="创建TableEnvironment"><a href="#创建TableEnvironment" class="headerlink" title="创建TableEnvironment"></a>创建TableEnvironment</h2><p>TableEnvironment是Table API &amp; SQL程序的一个入口，主要包括如下的功能：</p>
<ul>
<li>在内部的catalog中注册Table</li>
<li>注册catalog</li>
<li>加载可插拔模块</li>
<li>执行SQL查询</li>
<li>注册用户定义函数</li>
<li><code>DataStream</code> 、<code>DataSet</code>与Table之间的相互转换</li>
<li>持有对<code>ExecutionEnvironment</code> 、<code>StreamExecutionEnvironment</code>的引用</li>
</ul>
<p>一个Table必定属于一个具体的TableEnvironment，不可以将不同TableEnvironment的表放在一起使用(比如join，union等操作)。</p>
<p>TableEnvironment是通过调用 <code>BatchTableEnvironment.create()</code> 或者StreamTableEnvironment.create()的静态方法进行创建的。另外，默认两个planner的jar包都存在与classpath下，所有需要明确指定使用的planner。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// **********************</span></span><br><span class="line"><span class="comment">// FLINK 流处理查询</span></span><br><span class="line"><span class="comment">// **********************</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line"></span><br><span class="line">EnvironmentSettings fsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build();</span><br><span class="line">StreamExecutionEnvironment fsEnv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment fsTableEnv = StreamTableEnvironment.create(fsEnv, fsSettings);</span><br><span class="line"><span class="comment">//或者TableEnvironment fsTableEnv = TableEnvironment.create(fsSettings);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line"><span class="comment">// FLINK 批处理查询</span></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.BatchTableEnvironment;</span><br><span class="line"></span><br><span class="line">ExecutionEnvironment fbEnv = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">BatchTableEnvironment fbTableEnv = BatchTableEnvironment.create(fbEnv);</span><br><span class="line"></span><br><span class="line"><span class="comment">// **********************</span></span><br><span class="line"><span class="comment">// BLINK 流处理查询</span></span><br><span class="line"><span class="comment">// **********************</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line"></span><br><span class="line">StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();</span><br><span class="line">StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);</span><br><span class="line"><span class="comment">// 或者 TableEnvironment bsTableEnv = TableEnvironment.create(bsSettings);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line"><span class="comment">// BLINK 批处理查询</span></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"></span><br><span class="line">EnvironmentSettings bbSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build();</span><br><span class="line">TableEnvironment bbTableEnv = TableEnvironment.create(bbSettings);</span><br></pre></td></tr></table></figure>

<h2 id="在catalog中创建表"><a href="#在catalog中创建表" class="headerlink" title="在catalog中创建表"></a>在catalog中创建表</h2><h3 id="临时表与永久表"><a href="#临时表与永久表" class="headerlink" title="临时表与永久表"></a>临时表与永久表</h3><p>表可以分为临时表和永久表两种，其中永久表需要一个catalog(比如Hive的Metastore)俩维护表的元数据信息，一旦永久表被创建，只要连接到该catalog就可以访问该表，只有显示删除永久表，该表才可以被删除。临时表的生命周期是Flink Session，这些表不能够被其他的Flink Session访问，这些表不属于任何的catalog或者数据库，如果与临时表相对应的数据库被删除了，该临时表也不会被删除。</p>
<h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><h4 id="虚表-Virtual-Tables"><a href="#虚表-Virtual-Tables" class="headerlink" title="虚表(Virtual Tables)"></a>虚表(Virtual Tables)</h4><p>一个Table对象相当于SQL中的视图(虚表)，它封装了一个逻辑执行计划，可以通过一个catalog创建，具体如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取一个TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// table对象，查询的结果集</span></span><br><span class="line">Table projTable = tableEnv.from(<span class="string">"X"</span>).select(...);</span><br><span class="line"><span class="comment">// 注册一个表，名称为 "projectedTable"</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"projectedTable"</span>, projTable);</span><br></pre></td></tr></table></figure>

<h4 id="外部数据源表-Connector-Tables"><a href="#外部数据源表-Connector-Tables" class="headerlink" title="外部数据源表(Connector Tables)"></a>外部数据源表(Connector Tables)</h4><p>可以把外部的数据源注册成表，比如可以读取MySQL数据库数据、Kafka数据等</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">tableEnvironment</span><br><span class="line">  .connect(...)</span><br><span class="line">  .withFormat(...)</span><br><span class="line">  .withSchema(...)</span><br><span class="line">  .inAppendMode()</span><br><span class="line">  .createTemporaryTable(<span class="string">"MyTable"</span>)</span><br></pre></td></tr></table></figure>

<h3 id="扩展创建表的标识属性"><a href="#扩展创建表的标识属性" class="headerlink" title="扩展创建表的标识属性"></a>扩展创建表的标识属性</h3><p>表的注册总是包含三部分标识属性：catalog、数据库、表名。用户可以在内部设置一个catalog和一个数据库作为当前的catalog和数据库，所以对于catalog和数据库这两个标识属性是可选的，即如果不指定，默认使用的是“current catalog”和 “current database”。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">TableEnvironment tEnv = ...;</span><br><span class="line">tEnv.useCatalog(<span class="string">"custom_catalog"</span>);<span class="comment">//设置catalog</span></span><br><span class="line">tEnv.useDatabase(<span class="string">"custom_database"</span>);<span class="comment">//设置数据库</span></span><br><span class="line">Table table = ...;</span><br><span class="line"><span class="comment">// 注册一个名为exampleView的视图，catalog名为custom_catalog</span></span><br><span class="line"><span class="comment">// 数据库的名为custom_database</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"exampleView"</span>, table);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个名为exampleView的视图，catalog的名为custom_catalog</span></span><br><span class="line"><span class="comment">// 数据库的名为other_database</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"other_database.exampleView"</span>, table);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 注册一个名为'View'的视图，catalog的名称为custom_catalog</span></span><br><span class="line"><span class="comment">// 数据库的名为custom_database，'View'是保留关键字，需要使用``(反引号)</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"`View`"</span>, table);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个名为example.View的视图，catalog的名为custom_catalog，</span></span><br><span class="line"><span class="comment">// 数据库名为custom_database</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"`example.View`"</span>, table);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个名为'exampleView'的视图， catalog的名为'other_catalog'</span></span><br><span class="line"><span class="comment">// 数据库名为other_database' </span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"other_catalog.other_database.exampleView"</span>, table);</span><br></pre></td></tr></table></figure>

<h2 id="查询表"><a href="#查询表" class="headerlink" title="查询表"></a>查询表</h2><h3 id="Table-API"><a href="#Table-API" class="headerlink" title="Table API"></a>Table API</h3><p>Table API是一个集成Scala与Java语言的查询API，与SQL相比，它的查询不是一个标准的SQL语句，而是由一步一步的操作组成的。如下展示了一个使用Table API实现一个简单的聚合查询。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"><span class="comment">//注册Orders表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 查询注册的表</span></span><br><span class="line">Table orders = tableEnv.from(<span class="string">"Orders"</span>);</span><br><span class="line"><span class="comment">// 计算操作</span></span><br><span class="line">Table revenue = orders</span><br><span class="line">  .filter(<span class="string">"cCountry === 'FRANCE'"</span>)</span><br><span class="line">  .groupBy(<span class="string">"cID, cName"</span>)</span><br><span class="line">  .select(<span class="string">"cID, cName, revenue.sum AS revSum"</span>);</span><br></pre></td></tr></table></figure>

<h3 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h3><p>Flink SQL依赖于<a href="https://calcite.apache.org/" target="_blank" rel="noopener">Apache Calcite</a>，其实现了标准的SQL语法，如下案例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//注册Orders表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算逻辑同上面的Table API</span></span><br><span class="line">Table revenue = tableEnv.sqlQuery(</span><br><span class="line">    <span class="string">"SELECT cID, cName, SUM(revenue) AS revSum "</span> +</span><br><span class="line">    <span class="string">"FROM Orders "</span> +</span><br><span class="line">    <span class="string">"WHERE cCountry = 'FRANCE' "</span> +</span><br><span class="line">    <span class="string">"GROUP BY cID, cName"</span></span><br><span class="line">  );</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册"RevenueFrance"外部输出表</span></span><br><span class="line"><span class="comment">// 计算结果插入"RevenueFrance"表</span></span><br><span class="line">tableEnv.sqlUpdate(</span><br><span class="line">    <span class="string">"INSERT INTO RevenueFrance "</span> +</span><br><span class="line">    <span class="string">"SELECT cID, cName, SUM(revenue) AS revSum "</span> +</span><br><span class="line">    <span class="string">"FROM Orders "</span> +</span><br><span class="line">    <span class="string">"WHERE cCountry = 'FRANCE' "</span> +</span><br><span class="line">    <span class="string">"GROUP BY cID, cName"</span></span><br><span class="line">  );</span><br></pre></td></tr></table></figure>

<h2 id="输出表"><a href="#输出表" class="headerlink" title="输出表"></a>输出表</h2><p>一个表通过将其写入到TableSink，然后进行输出。TableSink是一个通用的支持多种文件格式(CSV、Parquet, Avro)和多种外部存储系统(JDBC, Apache HBase, Apache Cassandra, Elasticsearch)以及多种消息对列(Apache Kafka, RabbitMQ)的接口。</p>
<p>批处理的表只能被写入到 <code>BatchTableSink</code>,流处理的表需要指明AppendStreamTableSink、RetractStreamTableSink或者 <code>UpsertStreamTableSink</code></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建输出表</span></span><br><span class="line"><span class="keyword">final</span> Schema schema = <span class="keyword">new</span> Schema()</span><br><span class="line">    .field(<span class="string">"a"</span>, DataTypes.INT())</span><br><span class="line">    .field(<span class="string">"b"</span>, DataTypes.STRING())</span><br><span class="line">    .field(<span class="string">"c"</span>, DataTypes.LONG());</span><br><span class="line"></span><br><span class="line">tableEnv.connect(<span class="keyword">new</span> FileSystem(<span class="string">"/path/to/file"</span>))</span><br><span class="line">    .withFormat(<span class="keyword">new</span> Csv().fieldDelimiter(<span class="string">'|'</span>).deriveSchema())</span><br><span class="line">    .withSchema(schema)</span><br><span class="line">    .createTemporaryTable(<span class="string">"CsvSinkTable"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算结果表</span></span><br><span class="line">Table result = ...</span><br><span class="line"><span class="comment">// 输出结果表到注册的TableSink</span></span><br><span class="line">result.insertInto(<span class="string">"CsvSinkTable"</span>);</span><br></pre></td></tr></table></figure>

<h2 id="Table-API-amp-SQL底层的转换与执行"><a href="#Table-API-amp-SQL底层的转换与执行" class="headerlink" title="Table API &amp; SQL底层的转换与执行"></a>Table API &amp; SQL底层的转换与执行</h2><p>上文提到了Flink提供了两种planner，分别为old planner和Blink planner，对于不同的planner而言，Table API &amp; SQL底层的执行与转换是有所不同的。</p>
<h4 id="Old-planner"><a href="#Old-planner" class="headerlink" title="Old planner"></a>Old planner</h4><p>根据是流处理作业还是批处理作业，Table API &amp;SQL会被转换成DataStream或者DataSet程序。一个查询在内部表示为一个逻辑查询计划，会被转换为两个阶段:</p>
<ul>
<li>1.逻辑查询计划优化</li>
<li>2.转换成DataStream或者DataSet程序</li>
</ul>
<p>上面的两个阶段只有下面的操作被执行时才会被执行：</p>
<ul>
<li>当一个表被输出到TableSink时，比如调用了Table.insertInto()方法</li>
<li>当执行更新查询时，比如调用TableEnvironment.sqlUpdate()方法</li>
<li>当一个表被转换为DataStream或者DataSet时</li>
</ul>
<p>一旦执行上述两个阶段，Table API &amp; SQL的操作会被看做是普通的DataStream或者DataSet程序，所以当<code>StreamExecutionEnvironment.execute()</code>或者<code>ExecutionEnvironment.execute()</code> 被调用时，会执行转换后的程序。</p>
<h4 id="Blink-planner"><a href="#Blink-planner" class="headerlink" title="Blink planner"></a>Blink planner</h4><p>无论是批处理作业还是流处理作业，如果使用的是Blink planner，底层都会被转换为DataStream程序。在一个查询在内部表示为一个逻辑查询计划，会被转换成两个阶段：</p>
<ul>
<li>1.逻辑查询计划优化</li>
<li>2.转换成DataStream程序</li>
</ul>
<p>对于<code>TableEnvironment</code> and <code>StreamTableEnvironment</code>而言，一个查询的转换是不同的</p>
<p>首先对于TableEnvironment，当TableEnvironment.execute()方法执行时，Table API &amp; SQL的查询才会被转换，因为TableEnvironment会将多个sink优化为一个DAG。</p>
<p>对于StreamTableEnvironment，转换发生的时间与old planner相同。</p>
<h2 id="与DataStream-amp-DataSet-API集成"><a href="#与DataStream-amp-DataSet-API集成" class="headerlink" title="与DataStream &amp; DataSet API集成"></a>与DataStream &amp; DataSet API集成</h2><p>对于Old planner与Blink planner而言，只要是流处理的操作，都可以与DataStream API集成，<strong>仅仅只有Old planner才可以与DataSet API集成</strong>，由于Blink planner的批处理作业会被转换成DataStream程序，所以不能够与DataSet API集成。值得注意的是，下面提到的table与DataSet之间的转换仅适用于Old planner。</p>
<p>Table API &amp; SQL的查询很容易与DataStream或者DataSet程序集成，并可以将Table API &amp; SQL的查询嵌入DataStream或者DataSet程序中。DataStream或者DataSet可以转换成表，反之，表也可以被转换成DataStream或者DataSet。</p>
<h3 id="从DataStream或者DataSet中注册临时表-视图"><a href="#从DataStream或者DataSet中注册临时表-视图" class="headerlink" title="从DataStream或者DataSet中注册临时表(视图)"></a>从DataStream或者DataSet中注册临时表(视图)</h3><p><strong>尖叫提示：</strong>只能将DataStream或者DataSet转换为临时表(视图)</p>
<p>下面演示DataStream的转换，对于DataSet的转换类似。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream注册为一个名为myTable的视图，其中字段分别为"f0", "f1"</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"myTable"</span>, stream);</span><br><span class="line"><span class="comment">// 将DataStream注册为一个名为myTable2的视图,其中字段分别为"myLong", "myString"</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"myTable2"</span>, stream, <span class="string">"myLong, myString"</span>);</span><br></pre></td></tr></table></figure>

<h3 id="将DataStream或者DataSet转化为Table对象"><a href="#将DataStream或者DataSet转化为Table对象" class="headerlink" title="将DataStream或者DataSet转化为Table对象"></a>将DataStream或者DataSet转化为Table对象</h3><p>可以直接将DataStream或者DataSet转换为Table对象，之后可以使用Table API进行查询操作。下面演示DataStream的转换，对于DataSet的转换类似。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转换为Table对象，默认的字段为"f0", "f1"</span></span><br><span class="line">Table table1 = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">// 将DataStream转换为Table对象，默认的字段为"myLong", "myString"</span></span><br><span class="line">Table table2 = tableEnv.fromDataStream(stream, <span class="string">"myLong, myString"</span>);</span><br></pre></td></tr></table></figure>

<h3 id="将表转换为DataStream或者DataSet"><a href="#将表转换为DataStream或者DataSet" class="headerlink" title="将表转换为DataStream或者DataSet"></a>将表转换为DataStream或者DataSet</h3><p>当将Table转为DataStream或者DataSet时，需要指定DataStream或者DataSet的数据类型。通常最方便的数据类型是row类型，Flink提供了很多的数据类型供用户选择，具体包括Row、POJO、样例类、Tuple和原子类型。</p>
<h4 id="将表转换为DataStream"><a href="#将表转换为DataStream" class="headerlink" title="将表转换为DataStream"></a>将表转换为DataStream</h4><p>一个流处理查询的结果是动态变化的，所以将表转为DataStream时需要指定一个更新模式，共有两种模式：<strong>Append Mode</strong>和<strong>Retract Mode</strong>。</p>
<ul>
<li><strong>Append Mode</strong></li>
</ul>
<p>如果动态表仅只有Insert操作，即之前输出的结果不会被更新，则使用该模式。如果更新或删除操作使用追加模式会失败报错</p>
<ul>
<li><strong>Retract Mode</strong></li>
</ul>
<p>始终可以使用此模式。返回值是boolean类型。它用true或false来标记数据的插入和撤回，返回true代表数据插入，false代表数据的撤回。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment. </span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// 包含两个字段的表(String name, Integer age)</span></span><br><span class="line">Table table = ...</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用Append Mode追加模式，数据类型为Row</span></span><br><span class="line">DataStream&lt;Row&gt; dsRow = tableEnv.toAppendStream(table, Row.class);</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用Append Mode追加模式，数据类型为定义好的TypeInformation</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(</span><br><span class="line">  Types.STRING(),</span><br><span class="line">  Types.INT());</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = </span><br><span class="line">  tableEnv.toAppendStream(table, tupleType);</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用的模式为Retract Mode撤回模式，类型为Row</span></span><br><span class="line"><span class="comment">// 对于转换后的DataStream&lt;Tuple2&lt;Boolean, X&gt;&gt;，X表示流的数据类型，</span></span><br><span class="line"><span class="comment">// boolean值表示数据改变的类型，其中INSERT返回true，DELETE返回的是false</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retractStream = </span><br><span class="line">  tableEnv.toRetractStream(table, Row.class);</span><br></pre></td></tr></table></figure>

<h4 id="将表转换为DataSet"><a href="#将表转换为DataSet" class="headerlink" title="将表转换为DataSet"></a>将表转换为DataSet</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取BatchTableEnvironment</span></span><br><span class="line">BatchTableEnvironment tableEnv = BatchTableEnvironment.create(env);</span><br><span class="line"><span class="comment">// 包含两个字段的表(String name, Integer age)</span></span><br><span class="line">Table table = ...</span><br><span class="line"><span class="comment">// 将表转为DataSet数据类型为Row</span></span><br><span class="line">DataSet&lt;Row&gt; dsRow = tableEnv.toDataSet(table, Row.class);</span><br><span class="line"><span class="comment">// 将表转为DataSet，通过TypeInformation定义Tuple2&lt;String, Integer&gt;数据类型</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(</span><br><span class="line">  Types.STRING(),</span><br><span class="line">  Types.INT());</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = </span><br><span class="line">  tableEnv.toDataSet(table, tupleType);</span><br></pre></td></tr></table></figure>

<h3 id="表的Schema与数据类型之间的映射"><a href="#表的Schema与数据类型之间的映射" class="headerlink" title="表的Schema与数据类型之间的映射"></a>表的Schema与数据类型之间的映射</h3><p>表的Schema与数据类型之间的映射有两种方式：分别是基于字段下标位置的映射和基于字段名称的映射。</p>
<h4 id="基于字段下标位置的映射"><a href="#基于字段下标位置的映射" class="headerlink" title="基于字段下标位置的映射"></a>基于字段下标位置的映射</h4><p>该方式是按照字段的顺序进行一一映射，使用方式如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Integer&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为"f0"和"f1"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">// 将DataStream转为表，选取tuple的第一个元素，指定一个名为"myLong"的字段名</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"myLong"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，为tuple的第一个元素指定名为"myLong"，为第二个元素指定myInt的字段名</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"myLong, myInt"</span>);</span><br></pre></td></tr></table></figure>

<h4 id="基于字段名称的映射"><a href="#基于字段名称的映射" class="headerlink" title="基于字段名称的映射"></a>基于字段名称的映射</h4><p>基于字段名称的映射方式支持任意的数据类型包括POJO类型，可以很灵活地定义表Schema映射，所有的字段被映射成一个具体的字段名称，同时也可以使用”as”为字段起一个别名。其中Tuple元素的第一个元素为f0,第二个元素为f1，以此类推。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Integer&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为"f0"和"f1"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">// 将DataStream转为表，选择tuple的第二个元素，指定一个名为"f1"的字段名</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"f1"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，交换字段的顺序</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"f1, f0"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，交换字段的顺序，并为f1起别名为"myInt"，为f0起别名为"myLong</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"f1 as myInt, f0 as myLong"</span>);</span><br></pre></td></tr></table></figure>

<h4 id="原子类型"><a href="#原子类型" class="headerlink" title="原子类型"></a>原子类型</h4><p>Flink将<code>Integer</code>, <code>Double</code>, <code>String</code>或者普通的类型称之为原子类型，一个数据类型为原子类型的DataStream或者DataSet可以被转成单个字段属性的表，这个字段的类型与DataStream或者DataSet的数据类型一致，这个字段的名称可以进行指定。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// 数据类型为原子类型Long</span></span><br><span class="line">DataStream&lt;Long&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为"f0"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">// 将DataStream转为表，指定字段名为myLong"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"myLong"</span>);</span><br></pre></td></tr></table></figure>

<h4 id="Tuple类型"><a href="#Tuple类型" class="headerlink" title="Tuple类型"></a>Tuple类型</h4><p>Tuple类型的DataStream或者DataSet都可以转为表，可以重新设定表的字段名(即根据tuple元素的位置进行一一映射，转为表之后，每个元素都有一个别名)，如果不为字段指定名称，则使用默认的名称(java语言默认的是f0,f1,scala默认的是_1),用户也可以重新排列字段的顺序，并为每个字段起一个别名。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">//Tuple2&lt;Long, String&gt;类型的DataStream</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为 "f0", "f1"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">// 将DataStream转为表，指定字段名为 "myLong", "myString"(按照Tuple元素的顺序位置)</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"myLong, myString"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，指定字段名为 "f0", "f1"，并且交换顺序</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"f1, f0"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，只选择Tuple的第二个元素，指定字段名为"f1"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"f1"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，为Tuple的第二个元素指定别名为myString，为第一个元素指定字段名为myLong</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"f1 as 'myString', f0 as 'myLong'"</span>);</span><br></pre></td></tr></table></figure>

<h4 id="POJO类型"><a href="#POJO类型" class="headerlink" title="POJO类型"></a>POJO类型</h4><p>当将POJO类型的DataStream或者DataSet转为表时，如果不指定表名，则默认使用的是POJO字段本身的名称，原始字段名称的映射需要指定原始字段的名称，可以为其起一个别名，也可以调换字段的顺序，也可以只选择部分的字段。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">//数据类型为Person的POJO类型，字段包括"name"和"age"</span></span><br><span class="line">DataStream&lt;Person&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名称为"age", "name"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">//  将DataStream转为表，为"age"字段指定别名myAge, 为"name"字段指定别名myName</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"age as myAge, name as myName"</span>);</span><br><span class="line"><span class="comment">//  将DataStream转为表，只选择一个name字段</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"name"</span>);</span><br><span class="line"><span class="comment">//  将DataStream转为表，只选择一个name字段，并起一个别名myName</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"name as myName"</span>);</span><br></pre></td></tr></table></figure>

<h4 id="Row类型"><a href="#Row类型" class="headerlink" title="Row类型"></a>Row类型</h4><p>Row类型的DataStream或者DataSet转为表的过程中，可以根据字段的位置或者字段名称进行映射，同时也可以为字段起一个别名，或者只选择部分字段。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// Row类型的DataStream，通过RowTypeInfo指定两个字段"name"和"age"</span></span><br><span class="line">DataStream&lt;Row&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为原始字段名"name"和"age"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据位置映射，为第一个字段指定myName别名，为第二个字段指定myAge别名</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"myName, myAge"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据字段名映射，为name字段起别名myName，为age字段起别名myAge</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"name as myName, age as myAge"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据字段名映射，只选择name字段</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"name"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据字段名映射，只选择name字段，并起一个别名"myName"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"name as myName"</span>);</span><br></pre></td></tr></table></figure>

<h2 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a>查询优化</h2><h3 id="Old-planner-1"><a href="#Old-planner-1" class="headerlink" title="Old planner"></a>Old planner</h3><p>Apache Flink利用Apache Calcite来优化和转换查询。当前执行的优化包括投影和过滤器下推，去相关子查询以及其他类型的查询重写。Old Planner目前不支持优化JOIN的顺序，而是按照查询中定义的顺序执行它们。</p>
<p>通过提供一个<code>CalciteConfig</code>对象，可以调整在不同阶段应用的优化规则集。这可通过调用<code>CalciteConfig.createBuilder()</code>方法来进行创建，并通过调用<code>tableEnv.getConfig.setPlannerConfig(calciteConfig)</code>方法将该对象传递给TableEnvironment。</p>
<h3 id="Blink-planner-1"><a href="#Blink-planner-1" class="headerlink" title="Blink planner"></a>Blink planner</h3><p>Apache Flink利用并扩展了Apache Calcite来执行复杂的查询优化。这包括一系列基于规则和基于成本的优化(cost_based)，例如：</p>
<ul>
<li>基于Apache Calcite的去相关子查询</li>
<li>投影裁剪</li>
<li>分区裁剪</li>
<li>过滤器谓词下推</li>
<li>过滤器下推</li>
<li>子计划重复数据删除以避免重复计算</li>
<li>特殊的子查询重写，包括两个部分：<ul>
<li>将IN和EXISTS转换为左半联接( left semi-join)</li>
<li>将NOT IN和NOT EXISTS转换为left anti-join</li>
</ul>
</li>
<li>调整join的顺序，需要启用 <code>table.optimizer.join-reorder-enabled</code></li>
</ul>
<p><strong>注意：</strong> IN / EXISTS / NOT IN / NOT EXISTS当前仅在子查询重写的结合条件下受支持。</p>
<p>查询优化器不仅基于计划，而且还可以基于数据源的统计信息以及每个操作的细粒度开销(例如io，cpu，网络和内存）,从而做出更加明智且合理的优化决策。</p>
<p>高级用户可以通过<code>CalciteConfig</code>对象提供自定义优化规则，通过调用tableEnv.getConfig.setPlannerConfig(calciteConfig)，将参数传递给TableEnvironment。</p>
<h3 id="查看执行计划"><a href="#查看执行计划" class="headerlink" title="查看执行计划"></a>查看执行计划</h3><p>SQL语言支持通过explain来查看某条SQL的执行计划，Flink Table API也可以通过调用explain()方法来查看具体的执行计划。该方法返回一个字符串用来描述三个部分计划，分别为：</p>
<ol>
<li>关系查询的抽象语法树，即未优化的逻辑查询计划，</li>
<li>优化的逻辑查询计划</li>
<li>实际执行计划</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);</span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream1 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>, <span class="string">"hello"</span>));</span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream2 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>, <span class="string">"hello"</span>));</span><br><span class="line">Table table1 = tEnv.fromDataStream(stream1, <span class="string">"count, word"</span>);</span><br><span class="line">Table table2 = tEnv.fromDataStream(stream2, <span class="string">"count, word"</span>);</span><br><span class="line">Table table = table1</span><br><span class="line">  .where(<span class="string">"LIKE(word, 'F%')"</span>)</span><br><span class="line">  .unionAll(table2);</span><br><span class="line"><span class="comment">// 查看执行计划</span></span><br><span class="line">String explanation = tEnv.explain(table);</span><br><span class="line">System.out.println(explanation);</span><br></pre></td></tr></table></figure>

<p>执行计划的结果为：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">== 抽象语法树 ==</span><br><span class="line">LogicalUnion(all=[<span class="keyword">true</span>])</span><br><span class="line">  LogicalFilter(condition=[LIKE($<span class="number">1</span>, _UTF-<span class="number">16L</span>E<span class="string">'F%'</span>)])</span><br><span class="line">    FlinkLogicalDataStreamScan(id=[<span class="number">1</span>], fields=[count, word])</span><br><span class="line">  FlinkLogicalDataStreamScan(id=[<span class="number">2</span>], fields=[count, word])</span><br><span class="line"></span><br><span class="line">== 优化的逻辑执行计划 ==</span><br><span class="line">DataStreamUnion(all=[<span class="keyword">true</span>], union all=[count, word])</span><br><span class="line">  DataStreamCalc(select=[count, word], where=[LIKE(word, _UTF-<span class="number">16L</span>E<span class="string">'F%'</span>)])</span><br><span class="line">    DataStreamScan(id=[<span class="number">1</span>], fields=[count, word])</span><br><span class="line">  DataStreamScan(id=[<span class="number">2</span>], fields=[count, word])</span><br><span class="line"></span><br><span class="line">== 物理执行计划 ==</span><br><span class="line">Stage <span class="number">1</span> : Data Source</span><br><span class="line">	content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">Stage <span class="number">2</span> : Data Source</span><br><span class="line">	content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">	Stage <span class="number">3</span> : Operator</span><br><span class="line">		content : from: (count, word)</span><br><span class="line">		ship_strategy : REBALANCE</span><br><span class="line"></span><br><span class="line">		Stage <span class="number">4</span> : Operator</span><br><span class="line">			content : where: (LIKE(word, _UTF-<span class="number">16L</span>E<span class="string">'F%'</span>)), select: (count, word)</span><br><span class="line">			ship_strategy : FORWARD</span><br><span class="line"></span><br><span class="line">			Stage <span class="number">5</span> : Operator</span><br><span class="line">				content : from: (count, word)</span><br><span class="line">				ship_strategy : REBALANCE</span><br></pre></td></tr></table></figure>

<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了Flink TableAPI &amp;SQL，首先介绍了Flink Table API &amp;SQL的基本概念 ，然后介绍了构建Flink Table API &amp; SQL程序所需要的依赖，接着介绍了Flink的两种planner，还介绍了如何注册表以及DataStream、DataSet与表的相互转换，最后介绍了Flink的两种planner对应的查询优化并给出了一个查看执行计划的案例。</p>
</div><div class="recommended_posts"><h3>相关推荐 ☟</h3><li><a href="https://jiamaoxiang.top/2020/05/28/数仓开发需要了解的BI数据分析方法/" target="_blank">数仓开发需要了解的BI数据分析方法</a></li><li><a href="https://jiamaoxiang.top/2020/05/28/Flink-Table-API-SQL编程指南-2/" target="_blank">Flink Table API &amp; SQL编程指南之动态表(2)</a></li><li><a href="https://jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/" target="_blank">一统江湖的数仓开发辅助神器--DBeaver</a></li><li><a href="https://jiamaoxiang.top/2020/05/20/Hive的条件函数与日期函数全面汇总解析/" target="_blank">Hive的条件函数与日期函数全面汇总解析</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Jia MaoXiang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2020/05/25/Flink-Table-API-SQL编程指南/">https://jiamaoxiang.top/2020/05/25/Flink-Table-API-SQL编程指南/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本文为博主原创文章，遵循CC BY-SA 4.0版权协议，转载请附上原文出处链接和本声明</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://jiamaoxiang.top/2020/05/25/Flink-Table-API-SQL编程指南/" data-id="ckdqpdsj3004kb47q32sjtw0v" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACvElEQVR42u3au3IjMQwEQP//T9vpuezVzQCkS0FvpJL2wd6AgIb8+IiPz4fj319/fs5/ff390znHDjw8PLxDQ/85rNdDb19Hfjy9pugV4OHh4V3jPQ0xme7zxydPSUpOcjc8PDy8d+bNmt3XmM2ZeHh4eO/Ga6fyPIDISXh4eHjvwMuDgM21bTzxmno4a8HDw8OLeXn7+z6fr6zv4eHh4a1X1TdFoi0DeeNejBYPDw/vAi+fcBNk3grvd0nl48TDw8M7y9v8vd+fn7ymNur9dk88PDy8C7z9BZsodt8u11EFHh4e3lFeWwDaBbB2eWy2EeE/JQEPDw/vEG+zRSBvmvPv2wW54dodHh4e3po3a2dnuxU2wXEdoODh4eEd5SUhaRtGzGKOJP7IR1tfgIeHh7fm5RHAsM0to5Dkm6Iw4OHh4a15m0Y5wefh7Gbh7Zfz8fDw8C7w2syzLQnJtck9i2LwGomHh4e35m3CiGRJLCkVe0a0aQAPDw/vEG+z4DS7w6wU5YUkqnh4eHh4C17eKM8m6DaA2CyP/XI+Hh4e3gXe2Wm6DWf3DXTdXuPh4eEd4s2a6bOLVbM2OvoVDw8P7yhvtrmqDSaSqXyzISBKqfHw8PCO8k5N6LNtB3WgkMcZeHh4eBd4+dSfl5D93fJAJBobHh4e3jXefkKfxcHtIly95QsPDw/vz3n7xa1ZGWiLVvS/AQ8PD+8oL98S2m4C2GwvmJWiIozAw8PDK3mf5TGLejcBcRJnPP6Kh4eHd4G3DxeSq2Yx7uZF1AceHh5eyWtj3HzQeV9/qgV//MeAh4eHd4GXN75tbJGHwvmmqyK2wMPDw3sD3ubB7UaBPOQ9Vhjw8PDw/rzn3C+btQUm2nSFh4eHd4HXNr5J0DCLfZMlt+IpeHh4eBd4+z/8yUSfLPbnd9iEF3h4eHgL3hfVTtpPTro6zAAAAABJRU5ErkJggg==">分享</a><div class="tags"><a href="/tags/flink/">flink</a></div><div class="post-nav"><a class="pre" href="/2020/05/28/Flink-Table-API-SQL编程指南-2/">Flink Table API &amp; SQL编程指南之动态表(2)</a><a class="next" href="/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/">一统江湖的数仓开发辅助神器--DBeaver</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar-toc"><div class="stoc-article" id="sidebar-stoc"><strong class="stoc-title"><i class="fa fa-list-ul"> 目录</i></strong><div class="toc-nav" id="stoc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#依赖"><span class="toc-number">1.</span> <span class="toc-text">依赖</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Blink-planner-amp-old-planner"><span class="toc-number">2.</span> <span class="toc-text">Blink planner &amp; old planner</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Flink-Table-amp-SQL程序的pom依赖"><span class="toc-number">3.</span> <span class="toc-text">Flink Table &amp; SQL程序的pom依赖</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Table-API-amp-SQL的编程模板"><span class="toc-number">4.</span> <span class="toc-text">Table API &amp; SQL的编程模板</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#创建TableEnvironment"><span class="toc-number">5.</span> <span class="toc-text">创建TableEnvironment</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#在catalog中创建表"><span class="toc-number">6.</span> <span class="toc-text">在catalog中创建表</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#临时表与永久表"><span class="toc-number">6.1.</span> <span class="toc-text">临时表与永久表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#创建表"><span class="toc-number">6.2.</span> <span class="toc-text">创建表</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#虚表-Virtual-Tables"><span class="toc-number">6.2.1.</span> <span class="toc-text">虚表(Virtual Tables)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#外部数据源表-Connector-Tables"><span class="toc-number">6.2.2.</span> <span class="toc-text">外部数据源表(Connector Tables)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#扩展创建表的标识属性"><span class="toc-number">6.3.</span> <span class="toc-text">扩展创建表的标识属性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#查询表"><span class="toc-number">7.</span> <span class="toc-text">查询表</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Table-API"><span class="toc-number">7.1.</span> <span class="toc-text">Table API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SQL"><span class="toc-number">7.2.</span> <span class="toc-text">SQL</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#输出表"><span class="toc-number">8.</span> <span class="toc-text">输出表</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Table-API-amp-SQL底层的转换与执行"><span class="toc-number">9.</span> <span class="toc-text">Table API &amp; SQL底层的转换与执行</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Old-planner"><span class="toc-number">9.0.1.</span> <span class="toc-text">Old planner</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Blink-planner"><span class="toc-number">9.0.2.</span> <span class="toc-text">Blink planner</span></a></li></ol></li></ol><li class="toc-item toc-level-2"><a class="toc-link" href="#与DataStream-amp-DataSet-API集成"><span class="toc-number">10.</span> <span class="toc-text">与DataStream &amp; DataSet API集成</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#从DataStream或者DataSet中注册临时表-视图"><span class="toc-number">10.1.</span> <span class="toc-text">从DataStream或者DataSet中注册临时表(视图)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#将DataStream或者DataSet转化为Table对象"><span class="toc-number">10.2.</span> <span class="toc-text">将DataStream或者DataSet转化为Table对象</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#将表转换为DataStream或者DataSet"><span class="toc-number">10.3.</span> <span class="toc-text">将表转换为DataStream或者DataSet</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#将表转换为DataStream"><span class="toc-number">10.3.1.</span> <span class="toc-text">将表转换为DataStream</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#将表转换为DataSet"><span class="toc-number">10.3.2.</span> <span class="toc-text">将表转换为DataSet</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#表的Schema与数据类型之间的映射"><span class="toc-number">10.4.</span> <span class="toc-text">表的Schema与数据类型之间的映射</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#基于字段下标位置的映射"><span class="toc-number">10.4.1.</span> <span class="toc-text">基于字段下标位置的映射</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#基于字段名称的映射"><span class="toc-number">10.4.2.</span> <span class="toc-text">基于字段名称的映射</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#原子类型"><span class="toc-number">10.4.3.</span> <span class="toc-text">原子类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Tuple类型"><span class="toc-number">10.4.4.</span> <span class="toc-text">Tuple类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#POJO类型"><span class="toc-number">10.4.5.</span> <span class="toc-text">POJO类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Row类型"><span class="toc-number">10.4.6.</span> <span class="toc-text">Row类型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#查询优化"><span class="toc-number">11.</span> <span class="toc-text">查询优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Old-planner-1"><span class="toc-number">11.1.</span> <span class="toc-text">Old planner</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Blink-planner-1"><span class="toc-number">11.2.</span> <span class="toc-text">Blink planner</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#查看执行计划"><span class="toc-number">11.3.</span> <span class="toc-text">查看执行计划</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#小结"><span class="toc-number">12.</span> <span class="toc-text">小结</span></a></li></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 - 2020 <a href="/." rel="nofollow">Jmx's Blog.</a>All rights reserved.<br>Thoughts on technology, life and everything else.</div></div></div><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" color="100,99,98" opacity="0.5" zindex="0.7" count="150" src="//lib.baomitu.com/canvas-nest.js/2.0.4/canvas-nest.umd.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>