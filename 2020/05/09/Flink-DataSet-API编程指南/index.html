<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记录朴实无华且枯燥的生活"><title>Flink DataSet API编程指南 | Jmx's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '912b6cfc43243cd27aeb428f7dbf7823';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Flink DataSet API编程指南</h1><a id="logo" href="/.">Jmx's Blog</a><p class="description">Keep it Simple and Stupid!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-pied-piper-alt"> 关于</i></a><a href="/tags"><i class="fa fa-tags"> 标签</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Flink DataSet API编程指南</h1><div class="post-meta">May 9, 2020<span> | </span><span class="category"><a href="/categories/Flink/">Flink</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 6.9k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 30</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><a id="more"></a>

<p>Flink最大的亮点是实时处理部分，Flink认为批处理是流处理的特殊情况，可以通过一套引擎处理批量和流式数据，而Flink在未来也会重点投入更多的资源到批流融合中。我在<a href="https://mp.weixin.qq.com/s/rllW7XS9m-BH-2lxp_N8QA" target="_blank" rel="noopener">Flink DataStream API编程指南</a>中介绍了DataStream API的使用，在本文中将介绍Flink批处理计算的DataSet API的使用。通过本文你可以了解：</p>
<ul>
<li>DataSet转换操作(Transformation)</li>
<li>Source与Sink的使用</li>
<li>广播变量的基本概念与使用Demo</li>
<li>分布式缓存的概念及使用Demo</li>
<li>DataSet API的Transformation使用Demo案例</li>
</ul>
<h2 id="WordCount示例"><a href="#WordCount示例" class="headerlink" title="WordCount示例"></a>WordCount示例</h2><p>在开始讲解DataSet API之前，先看一个Word Count的简单示例，来直观感受一下DataSet API的编程模型，具体代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 用于批处理的执行环境</span></span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 数据源</span></span><br><span class="line">        DataSource&lt;String&gt; stringDataSource = env.fromElements(<span class="string">"hello Flink What is Apache Flink"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 转换</span></span><br><span class="line">        AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; wordCnt = stringDataSource</span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        String[] split = value.split(<span class="string">" "</span>);</span><br><span class="line">                        <span class="keyword">for</span> (String word : split) &#123;</span><br><span class="line">                            out.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .groupBy(<span class="number">0</span>)</span><br><span class="line">                .sum(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 输出</span></span><br><span class="line">        wordCnt.print();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从上面的示例中可以看出，基本的编程模型是：</p>
<ul>
<li>获取批处理的执行环境ExecutionEnvironment</li>
<li>加载数据源</li>
<li>转换操作</li>
<li>数据输出</li>
</ul>
<p>下面会对数据源、转换操作、数据输出进行一一解读。</p>
<h2 id="Data-Source"><a href="#Data-Source" class="headerlink" title="Data Source"></a>Data Source</h2><p>DataSet API支持从多种数据源中将批量数据集读到Flink系统中，并转换成DataSet数据集。主要包括三种类型：分别是基于文件的、基于集合的及通用类数据源。同时在DataSet API中可以自定义实现InputFormat/RichInputFormat接口，以接入不同数据格式类型的数据源，比如CsvInputFormat、TextInputFormat等。从ExecutionEnvironment类提供的方法中可以看出支持的数据源方法，如下图所示：</p>
<p><img src="//jiamaoxiang.top/2020/05/09/Flink-DataSet-API编程指南/dataset%E6%95%B0%E6%8D%AE%E6%BA%90.png" alt></p>
<h3 id="基于文件的数据源"><a href="#基于文件的数据源" class="headerlink" title="基于文件的数据源"></a>基于文件的数据源</h3><h4 id="readTextFile-path-TextInputFormat"><a href="#readTextFile-path-TextInputFormat" class="headerlink" title="readTextFile(path) / TextInputFormat"></a>readTextFile(path) / TextInputFormat</h4><ul>
<li>解释</li>
</ul>
<p>读取文本文件，传递文件路径参数，并将文件内容转换成DataSet<string>类型数据集。</string></p>
<ul>
<li>使用</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 读取本地文件</span></span><br><span class="line">DataSet&lt;String&gt; localLines = env.readTextFile(<span class="string">"file:///path/to/my/textfile"</span>);</span><br><span class="line"><span class="comment">// 读取HDSF文件</span></span><br><span class="line">DataSet&lt;String&gt; hdfsLines = env.readTextFile(<span class="string">"hdfs://nnHost:nnPort/path/to/my/textfile"</span>);</span><br></pre></td></tr></table></figure>

<h4 id="readTextFileWithValue-path-TextValueInputFormat"><a href="#readTextFileWithValue-path-TextValueInputFormat" class="headerlink" title="readTextFileWithValue(path)/ TextValueInputFormat"></a>readTextFileWithValue(path)/ TextValueInputFormat</h4><ul>
<li>解释</li>
</ul>
<p>读取文本文件内容，将文件内容转换成DataSet[StringValue]类型数据集。该方法与readTextFile(String)不同的是，其泛型是StringValue，是一种可变的String类型，通过StringValue存储文本数据可以有效降低String对象创建数量，减小垃圾回收的压力。</p>
<ul>
<li>使用</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 读取本地文件</span></span><br><span class="line">DataSet&lt;StringValue&gt; localLines = env.readTextFileWithValue(<span class="string">"file:///some/local/file"</span>);</span><br><span class="line"><span class="comment">// 读取HDSF文件</span></span><br><span class="line">DataSet&lt;StringValue&gt; hdfsLines = env.readTextFileWithValue(<span class="string">"hdfs://host:port/file/path"</span>);</span><br></pre></td></tr></table></figure>

<h4 id="readCsvFile-path-CsvInputFormat"><a href="#readCsvFile-path-CsvInputFormat" class="headerlink" title="readCsvFile(path)/ CsvInputFormat"></a>readCsvFile(path)/ CsvInputFormat</h4><ul>
<li>解释</li>
</ul>
<p>创建一个CSV的reader，读取逗号分隔(或其他分隔符)的文件。可以直接转换成Tuple类型、POJOs类的DataSet。在方法中可以指定行切割符、列切割符、字段等信息。</p>
<ul>
<li>使用</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// read a CSV file with five fields, taking only two of them</span></span><br><span class="line"><span class="comment">// 读取一个具有5个字段的CSV文件，只取第一个和第四个字段</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Double&gt;&gt; csvInput = env.readCsvFile(<span class="string">"hdfs:///the/CSV/file"</span>)</span><br><span class="line">                               .includeFields(<span class="string">"10010"</span>)  </span><br><span class="line">	                          .types(String.class, Double.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取一个有三个字段的CSV文件，将其转为POJO类型</span></span><br><span class="line">DataSet&lt;Person&gt;&gt; csvInput = env.readCsvFile(<span class="string">"hdfs:///the/CSV/file"</span>)</span><br><span class="line">                         .pojoType(Person.class, <span class="string">"name"</span>, <span class="string">"age"</span>, <span class="string">"zipcode"</span>);</span><br></pre></td></tr></table></figure>

<h4 id="readFileOfPrimitives-path-Class-PrimitiveInputFormat"><a href="#readFileOfPrimitives-path-Class-PrimitiveInputFormat" class="headerlink" title="readFileOfPrimitives(path, Class) / PrimitiveInputFormat"></a>readFileOfPrimitives(path, Class) / PrimitiveInputFormat</h4><ul>
<li>解释</li>
</ul>
<p>读取一个原始数据类型(如String,Integer)的文件,返回一个对应的原始类型的DataSet集合</p>
<ul>
<li>使用</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;String&gt; Data = env.readFileOfPrimitives(<span class="string">"file:///some/local/file"</span>, String.class);</span><br></pre></td></tr></table></figure>

<h3 id="基于集合的数据源"><a href="#基于集合的数据源" class="headerlink" title="基于集合的数据源"></a>基于集合的数据源</h3><h4 id="fromCollection-Collection"><a href="#fromCollection-Collection" class="headerlink" title="fromCollection(Collection)"></a>fromCollection(Collection)</h4><ul>
<li>解释</li>
</ul>
<p>从java的集合中创建DataSet数据集，集合中的元素数据类型相同</p>
<ul>
<li>使用</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;String&gt; data= env.fromCollection(arrayList);</span><br></pre></td></tr></table></figure>

<h4 id="fromElements-T-…"><a href="#fromElements-T-…" class="headerlink" title="fromElements(T …)"></a>fromElements(T …)</h4><ul>
<li>解释</li>
</ul>
<p>从给定数据元素序列中创建DataSet数据集，且所有的数据对象类型必须一致</p>
<ul>
<li>使用</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;String&gt; stringDataSource = env.fromElements(<span class="string">"hello Flink What is Apache Flink"</span>);</span><br></pre></td></tr></table></figure>

<h4 id="generateSequence-from-to"><a href="#generateSequence-from-to" class="headerlink" title="generateSequence(from, to)"></a>generateSequence(from, to)</h4><ul>
<li>解释</li>
</ul>
<p>指定from到to范围区间，然后在区间内部生成数字序列数据集,由于是并行处理的，所以最终的顺序不能保证一致。</p>
<ul>
<li>使用</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Long&gt; longDataSource = env.generateSequence(<span class="number">1</span>, <span class="number">20</span>);</span><br></pre></td></tr></table></figure>

<h3 id="通用类型数据源"><a href="#通用类型数据源" class="headerlink" title="通用类型数据源"></a>通用类型数据源</h3><p>DataSet API中提供了Inputformat通用的数据接口，以接入不同数据源和格式类型的数据。InputFormat接口主要分为两种类型：一种是基于文件类型，在DataSet API对应readFile()方法；另外一种是基于通用数据类型的接口，例如读取RDBMS或NoSQL数据库中等，在DataSet API中对应createInput()方法。</p>
<h4 id="readFile-inputFormat-path-FileInputFormat"><a href="#readFile-inputFormat-path-FileInputFormat" class="headerlink" title="readFile(inputFormat, path) / FileInputFormat"></a>readFile(inputFormat, path) / FileInputFormat</h4><ul>
<li>解释</li>
</ul>
<p>自定义文件类型输入源，将指定格式文件读取并转成DataSet数据集</p>
<ul>
<li>使用</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.readFile(<span class="keyword">new</span> MyInputFormat(), <span class="string">"file:///some/local/file"</span>);</span><br></pre></td></tr></table></figure>

<h4 id="createInput-inputFormat-InputFormat"><a href="#createInput-inputFormat-InputFormat" class="headerlink" title="createInput(inputFormat) / InputFormat"></a>createInput(inputFormat) / InputFormat</h4><ul>
<li>解释</li>
</ul>
<p>自定义通用型数据源，将读取的数据转换为DataSet数据集。如以下实例使用Flink内置的JDBCInputFormat，创建读取mysql数据源的JDBCInput Format，完成从mysql中读取Person表，并转换成DataSet [Row]数据集</p>
<ul>
<li>使用</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt; dbData =</span><br><span class="line">    env.createInput(</span><br><span class="line">      JDBCInputFormat.buildJDBCInputFormat()</span><br><span class="line">                     .setDrivername(<span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">                     .setDBUrl(<span class="string">"jdbc:mysql://localhost/mydb"</span>)</span><br><span class="line">                     .setQuery(<span class="string">"select name, age from stu"</span>)</span><br><span class="line">                     .setRowTypeInfo(<span class="keyword">new</span> RowTypeInfo(BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.INT_TYPE_INFO))</span><br><span class="line">                     .finish()</span><br><span class="line">    );</span><br></pre></td></tr></table></figure>

<h2 id="Data-Sink"><a href="#Data-Sink" class="headerlink" title="Data Sink"></a>Data Sink</h2><p>Flink在DataSet API中的数据输出共分为三种类型。第一种是基于文件实现，对应DataSet的write()方法，实现将DataSet数据输出到文件系统中。第二种是基于通用存储介质实现，对应DataSet的output()方法，例如使用JDBCOutputFormat将数据输出到关系型数据库中。最后一种是客户端输出，直接将DataSet数据从不同的节点收集到Client，并在客户端中输出，例如DataSet的print()方法。</p>
<h3 id="标准的数据输出方法"><a href="#标准的数据输出方法" class="headerlink" title="标准的数据输出方法"></a>标准的数据输出方法</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 文本数据</span></span><br><span class="line">DataSet&lt;String&gt; textData = <span class="comment">// [...]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 将数据写入本地文件</span></span><br><span class="line">textData.writeAsText(<span class="string">"file:///my/result/on/localFS"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将数据写入HDFS文件</span></span><br><span class="line">textData.writeAsText(<span class="string">"hdfs://nnHost:nnPort/my/result/on/localFS"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 写数据到本地文件，如果文件存在则覆盖</span></span><br><span class="line">textData.writeAsText(<span class="string">"file:///my/result/on/localFS"</span>, WriteMode.OVERWRITE);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将数据输出到本地的CSV文件，指定分隔符为"|"</span></span><br><span class="line">DataSet&lt;Tuple3&lt;String, Integer, Double&gt;&gt; values = <span class="comment">// [...]</span></span><br><span class="line">values.writeAsCsv(<span class="string">"file:///path/to/the/result/file"</span>, <span class="string">"\n"</span>, <span class="string">"|"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用自定义的TextFormatter对象</span></span><br><span class="line">values.writeAsFormattedText(<span class="string">"file:///path/to/the/result/file"</span>,</span><br><span class="line">    <span class="keyword">new</span> TextFormatter&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">format</span> <span class="params">(Tuple2&lt;Integer, Integer&gt; value)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> value.f1 + <span class="string">" - "</span> + value.f0;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure>

<h3 id="使用自定义的输出类型"><a href="#使用自定义的输出类型" class="headerlink" title="使用自定义的输出类型"></a>使用自定义的输出类型</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple3&lt;String, Integer, Double&gt;&gt; myResult = [...]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将tuple类型的数据写入关系型数据库</span></span><br><span class="line">myResult.output(</span><br><span class="line">    <span class="comment">// 创建并配置OutputFormat</span></span><br><span class="line">    JDBCOutputFormat.buildJDBCOutputFormat()</span><br><span class="line">                    .setDrivername(<span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">                    .setDBUrl(<span class="string">"jdbc:mysql://localhost/mydb"</span>)</span><br><span class="line">                    .setQuery(<span class="string">"insert into persons (name, age, height) values (?,?,?)"</span>)</span><br><span class="line">                    .finish()</span><br><span class="line">    );</span><br></pre></td></tr></table></figure>

<h2 id="DataSet转换"><a href="#DataSet转换" class="headerlink" title="DataSet转换"></a>DataSet转换</h2><p>转换(transformations)将一个DataSet转成另外一个DataSet，Flink提供了非常丰富的转换操作符。具体使用如下：</p>
<h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><p>一进一出</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;String&gt; source = env.fromElements(<span class="string">"I"</span>, <span class="string">"like"</span>, <span class="string">"flink"</span>);</span><br><span class="line">      source.map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">          <span class="meta">@Override</span></span><br><span class="line">          <span class="comment">// 将数据转为大写</span></span><br><span class="line">          <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">              <span class="keyword">return</span> value.toUpperCase();</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;).print();</span><br></pre></td></tr></table></figure>

<h3 id="FlatMap"><a href="#FlatMap" class="headerlink" title="FlatMap"></a>FlatMap</h3><p>输入一个元素，产生0个、1个或多个元素</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stringDataSource</span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        String[] split = value.split(<span class="string">" "</span>);</span><br><span class="line">                        <span class="keyword">for</span> (String word : split) &#123;</span><br><span class="line">                            out.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .groupBy(<span class="number">0</span>)</span><br><span class="line">                .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<h3 id="MapPartition"><a href="#MapPartition" class="headerlink" title="MapPartition"></a>MapPartition</h3><p>功能和Map函数相似，只是MapPartition操作是在DataSet中基于分区对数据进行处理，函数调用中会按照分区将数据通过Iteator的形式传入，每个分区中的元素数与并行度有关，并返回任意数量的结果值。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">source.mapPartition(<span class="keyword">new</span> MapPartitionFunction&lt;String, Long&gt;() &#123;</span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">           <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mapPartition</span><span class="params">(Iterable&lt;String&gt; values, Collector&lt;Long&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">               <span class="keyword">long</span> c = <span class="number">0</span>;</span><br><span class="line">               <span class="keyword">for</span> (String value : values) &#123;</span><br><span class="line">                   c++;</span><br><span class="line">               &#125;</span><br><span class="line">               <span class="comment">//输出每个分区元素个数</span></span><br><span class="line">               out.collect(c);</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;).print();</span><br></pre></td></tr></table></figure>

<h3 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h3><p>过滤数据，如果返回true则保留数据，如果返回false则过滤掉</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Long&gt; source = env.fromElements(<span class="number">1L</span>, <span class="number">2L</span>, <span class="number">3L</span>,<span class="number">4L</span>,<span class="number">5L</span>);</span><br><span class="line">        source.filter(<span class="keyword">new</span> FilterFunction&lt;Long&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Long value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value % <span class="number">2</span> == <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();</span><br></pre></td></tr></table></figure>

<h3 id="Project"><a href="#Project" class="headerlink" title="Project"></a><strong>Project</strong></h3><p>仅能用在Tuple类型的数据集，投影操作，选取Tuple数据的字段的子集</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Tuple3&lt;Long, Integer, String&gt;&gt; source = env.fromElements(</span><br><span class="line">              Tuple3.of(<span class="number">1L</span>, <span class="number">20</span>, <span class="string">"tom"</span>), </span><br><span class="line">              Tuple3.of(<span class="number">2L</span>, <span class="number">25</span>, <span class="string">"jack"</span>), </span><br><span class="line">              Tuple3.of(<span class="number">3L</span>, <span class="number">22</span>, <span class="string">"bob"</span>));</span><br><span class="line">      <span class="comment">// 去第一个和第三个元素</span></span><br><span class="line">      source.project(<span class="number">0</span>, <span class="number">2</span>).print();</span><br></pre></td></tr></table></figure>

<h3 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a><strong>Reduce</strong></h3><p>通过两两合并，将数据集中的元素合并成一个元素，可以在整个数据集上使用，也可以在分组之后的数据集上使用。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Tuple2&lt;String, Integer&gt;&gt; source = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Hadoop"</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Spark"</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1</span>));</span><br><span class="line">        source</span><br><span class="line">                .groupBy(<span class="number">0</span>)</span><br><span class="line">                .reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Tuple2.of(value1.f0, value1.f1 + value2.f1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();</span><br></pre></td></tr></table></figure>

<h3 id="ReduceGroup"><a href="#ReduceGroup" class="headerlink" title="ReduceGroup"></a><strong>ReduceGroup</strong></h3><p>将数据集中的元素合并成一个元素，可以在整个数据集上使用，也可以在分组之后的数据集上使用。reduce函数的输入值是一个分组元素的Iterable。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Tuple2&lt;String, Long&gt;&gt; source = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Hadoop"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Spark"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1L</span>));</span><br><span class="line">        source</span><br><span class="line">                .groupBy(<span class="number">0</span>)</span><br><span class="line">                .reduceGroup(<span class="keyword">new</span> GroupReduceFunction&lt;Tuple2&lt;String,Long&gt;, Tuple2&lt;String,Long&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Iterable&lt;Tuple2&lt;String, Long&gt;&gt; values, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        Long sum = <span class="number">0L</span>;</span><br><span class="line">                        String word = <span class="string">""</span>;</span><br><span class="line">                        <span class="keyword">for</span>(Tuple2&lt;String, Long&gt; value:values)&#123;</span><br><span class="line">                            sum += value.f1;</span><br><span class="line">                            word = value.f0;</span><br><span class="line"></span><br><span class="line">                        &#125;</span><br><span class="line">                        out.collect(Tuple2.of(word,sum));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).print();</span><br></pre></td></tr></table></figure>

<h3 id="Aggregate"><a href="#Aggregate" class="headerlink" title="Aggregate"></a><strong>Aggregate</strong></h3><p>通过Aggregate Function将一组元素值合并成单个值，可以在整个DataSet数据集上使用，也可以在分组之后的数据集上使用。仅仅用在Tuple类型的数据集上，主要包括Sum,Min,Max函数</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Tuple2&lt;String, Long&gt;&gt; source = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Hadoop"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Spark"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1L</span>));</span><br><span class="line">        source</span><br><span class="line">                .groupBy(<span class="number">0</span>)</span><br><span class="line">                .aggregate(SUM,<span class="number">1</span>)<span class="comment">// 按第2个值求和</span></span><br><span class="line">                 .print();</span><br></pre></td></tr></table></figure>

<h3 id="Distinct"><a href="#Distinct" class="headerlink" title="Distinct"></a><strong>Distinct</strong></h3><p>DataSet数据集元素去重</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Tuple&gt; source = env.fromElements(Tuple1.of(<span class="string">"Flink"</span>),Tuple1.of(<span class="string">"Flink"</span>),Tuple1.of(<span class="string">"hadoop"</span>));</span><br><span class="line">        source.distinct(<span class="number">0</span>).print();<span class="comment">// 按照tuple的第一个字段去重</span></span><br><span class="line"><span class="comment">// 结果：</span></span><br><span class="line">(Flink)</span><br><span class="line">(hadoop)</span><br></pre></td></tr></table></figure>

<h3 id="Join"><a href="#Join" class="headerlink" title="Join"></a><strong>Join</strong></h3><p>默认的join是产生一个Tuple2数据类型的DataSet，关联的key可以通过key表达式、Key-selector函数、字段位置以及CaseClass字段指定。对于两个Tuple类型的数据集可以通过字段位置进行关联，左边数据集的字段通过where方法指定，右边数据集的字段通过equalTo()方法指定。比如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Tuple2&lt;Integer,String&gt;&gt; source1 = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="number">1</span>,<span class="string">"jack"</span>),</span><br><span class="line">                Tuple2.of(<span class="number">2</span>,<span class="string">"tom"</span>),</span><br><span class="line">                Tuple2.of(<span class="number">3</span>,<span class="string">"Bob"</span>));</span><br><span class="line">        DataSource&lt;Tuple2&lt;String, Integer&gt;&gt; source2 = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"order1"</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"order2"</span>, <span class="number">2</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"order3"</span>, <span class="number">3</span>));</span><br><span class="line">        source1.join(source2).where(<span class="number">0</span>).equalTo(<span class="number">1</span>).print();</span><br></pre></td></tr></table></figure>

<p>可以在关联的过程中指定自定义Join Funciton, Funciton的入参为左边数据集中的数据元素和右边数据集的中的数据元素所组成的元祖，并返回一个经过计算处理后的数据。如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 用户id，购买商品名称，购买商品数量</span></span><br><span class="line">        DataSource&lt;Tuple3&lt;Integer,String,Integer&gt;&gt; source1 = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1</span>,<span class="string">"item1"</span>,<span class="number">2</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2</span>,<span class="string">"item2"</span>,<span class="number">3</span>),</span><br><span class="line">                Tuple3.of(<span class="number">3</span>,<span class="string">"item3"</span>,<span class="number">4</span>));</span><br><span class="line">        <span class="comment">//商品名称与商品单价</span></span><br><span class="line">        DataSource&lt;Tuple2&lt;String, Integer&gt;&gt; source2 = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"item1"</span>, <span class="number">10</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"item2"</span>, <span class="number">20</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"item3"</span>, <span class="number">15</span>));</span><br><span class="line">        source1.join(source2)</span><br><span class="line">                .where(<span class="number">1</span>)</span><br><span class="line">                .equalTo(<span class="number">0</span>)</span><br><span class="line">                .with(<span class="keyword">new</span> JoinFunction&lt;Tuple3&lt;Integer,String,Integer&gt;, Tuple2&lt;String,Integer&gt;, Tuple3&lt;Integer,String,Double&gt;&gt;() &#123;</span><br><span class="line">                    <span class="comment">// 用户每种商品购物总金额</span></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, String, Double&gt; <span class="title">join</span><span class="params">(Tuple3&lt;Integer, String, Integer&gt; first, Tuple2&lt;String, Integer&gt; second)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> Tuple3.of(first.f0,first.f1,first.f2 * second.f1.doubleValue());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).print();</span><br></pre></td></tr></table></figure>

<p>为了能够更好地引导Flink底层去正确地处理数据集，可以在DataSet数据集关联中，通过Size Hint标记数据集的大小，Flink可以根据用户给定的hint(提示)调整计算策略，例如可以使用joinWithTiny或joinWithHuge提示第二个数据集的大小。示例如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;Integer, String&gt;&gt; input1 = <span class="comment">// [...]</span></span><br><span class="line">DataSet&lt;Tuple2&lt;Integer, String&gt;&gt; input2 = <span class="comment">// [...]</span></span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;Tuple2&lt;Integer, String&gt;, Tuple2&lt;Integer, String&gt;&gt;&gt;</span><br><span class="line">            result1 =</span><br><span class="line">            <span class="comment">// 提示第二个数据集为小数据集</span></span><br><span class="line">            input1.joinWithTiny(input2)</span><br><span class="line">                  .where(<span class="number">0</span>)</span><br><span class="line">                  .equalTo(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;Tuple2&lt;Integer, String&gt;, Tuple2&lt;Integer, String&gt;&gt;&gt;</span><br><span class="line">            result2 =</span><br><span class="line">            <span class="comment">// h提示第二个数据集为大数据集</span></span><br><span class="line">            input1.joinWithHuge(input2)</span><br><span class="line">                  .where(<span class="number">0</span>)</span><br><span class="line">                  .equalTo(<span class="number">0</span>);</span><br></pre></td></tr></table></figure>

<p>Flink的runtime可以使用多种方式执行join。在不同的情况下，每种可能的方式都会胜过其他方式。系统会尝试自动选择一种合理的方法，但是允许用户手动选择一种策略， 可以让Flink更加灵活且高效地执行Join操作。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;SomeType&gt; input1 = <span class="comment">// [...]</span></span><br><span class="line">DataSet&lt;AnotherType&gt; input2 = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// 广播第一个输入并从中构建一个哈希表，第二个输入将对其进行探测，适用于第一个数据集非常小的场景</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result =</span><br><span class="line">      input1.join(input2, JoinHint.BROADCAST_HASH_FIRST)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br><span class="line"><span class="comment">// 广播第二个输入并从中构建一个哈希表，第一个输入将对其进行探测，适用于第二个数据集非常小的场景</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result =</span><br><span class="line">      input1.join(input2, JoinHint.BROADCAST_HASH_SECOND)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br><span class="line"><span class="comment">// 将两个数据集重新分区，并将第一个数据集转换成哈希表，适用于第一个数据集比第二个数据集小，但两个数据集都比较大的场景</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result =</span><br><span class="line">      input1.join(input2, JoinHint.REPARTITION_HASH_FIRST)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br><span class="line"><span class="comment">// 将两个数据集重新分区，并将第二个数据集转换成哈希表，适用于第二个数据集比第一个数据集小，但两个数据集都比较大的场景</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result =</span><br><span class="line">      input1.join(input2, JoinHint.REPARTITION_HASH_SECOND)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br><span class="line"><span class="comment">// 将两个数据集重新分区，并将每个分区排序，适用于两个数据集都已经排好序的场景</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result =</span><br><span class="line">      input1.join(input2, JoinHint.REPARTITION_SORT_MERGE)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br><span class="line"><span class="comment">// 相当于不指定，有系统自行处理</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result =</span><br><span class="line">      input1.join(input2, JoinHint.OPTIMIZER_CHOOSES)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br></pre></td></tr></table></figure>

<h3 id="OuterJoin"><a href="#OuterJoin" class="headerlink" title="OuterJoin"></a>OuterJoin</h3><p>OuterJoin对两个数据集进行外关联，包含left、right、full outer join三种关联方式，分别对应DataSet API中的leftOuterJoin、rightOuterJoin以及fullOuterJoin方法。注意外连接仅适用于Java 和 Scala DataSet API.</p>
<p>使用方式几乎和join类似：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//左外连接</span></span><br><span class="line">source1.leftOuterJoin(source2).where(<span class="number">1</span>).equalTo(<span class="number">0</span>);</span><br><span class="line"><span class="comment">//右外链接</span></span><br><span class="line">source1.rightOuterJoin(source2).where(<span class="number">1</span>).equalTo(<span class="number">0</span>);</span><br></pre></td></tr></table></figure>

<p>此外，外连接也提供了相应的关联算法提示，可以跟据左右数据集的分布情况选择合适的优化策略，提升数据处理的效率。下面代码可以参考上面join的解释。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;SomeType&gt; input1 = <span class="comment">// [...]</span></span><br><span class="line">DataSet&lt;AnotherType&gt; input2 = <span class="comment">// [...]</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result1 =</span><br><span class="line">      input1.leftOuterJoin(input2, JoinHint.REPARTITION_SORT_MERGE)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result2 =</span><br><span class="line">      input1.rightOuterJoin(input2, JoinHint.BROADCAST_HASH_FIRST)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br></pre></td></tr></table></figure>

<p>对于外连接的关联算法，与join有所不同。每种外连接只支持部分算法。如下：</p>
<ul>
<li><p>LeftOuterJoin支持：</p>
<ul>
<li><p>OPTIMIZER_CHOOSES</p>
</li>
<li><p>BROADCAST_HASH_SECOND</p>
</li>
<li><p>REPARTITION_HASH_SECOND</p>
</li>
<li><p>REPARTITION_SORT_MERGE</p>
<ul>
<li>RightOuterJoin支持：<pre><code>- OPTIMIZER_CHOOSES
- BROADCAST_HASH_FIRST
- REPARTITION_HASH_FIRST
- REPARTITION_SORT_MERGE</code></pre></li>
<li>FullOuterJoin支持：<ul>
<li>OPTIMIZER_CHOOSES</li>
<li>REPARTITION_SORT_MERGE</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="CoGroup"><a href="#CoGroup" class="headerlink" title="CoGroup"></a><strong>CoGroup</strong></h3><p> CoGroup是对分组之后的DataSet进行join操作，将两个DataSet数据集合并在一起，会先各自对每个DataSet按照key进行分组，然后将分组之后的DataSet传输到用户定义的CoGroupFunction，将两个数据集根据相同的Key记录组合在一起，相同Key的记录会存放在一个Group中，如果指定key仅在一个数据集中有记录，则co-groupFunction会将这个Group与空的Group关联。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 用户id，购买商品名称，购买商品数量</span></span><br><span class="line">        DataSource&lt;Tuple3&lt;Integer,String,Integer&gt;&gt; source1 = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1</span>,<span class="string">"item1"</span>,<span class="number">2</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2</span>,<span class="string">"item2"</span>,<span class="number">3</span>),</span><br><span class="line">                Tuple3.of(<span class="number">3</span>,<span class="string">"item2"</span>,<span class="number">4</span>));</span><br><span class="line">        <span class="comment">//商品名称与商品单价</span></span><br><span class="line">        DataSource&lt;Tuple2&lt;String, Integer&gt;&gt; source2 = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"item1"</span>, <span class="number">10</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"item2"</span>, <span class="number">20</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"item3"</span>, <span class="number">15</span>));</span><br><span class="line"></span><br><span class="line">        source1.coGroup(source2)</span><br><span class="line">                .where(<span class="number">1</span>)</span><br><span class="line">                .equalTo(<span class="number">0</span>)</span><br><span class="line">                .with(<span class="keyword">new</span> CoGroupFunction&lt;Tuple3&lt;Integer,String,Integer&gt;, Tuple2&lt;String,Integer&gt;, Tuple2&lt;String,Double&gt;&gt;() &#123;</span><br><span class="line">                    <span class="comment">// 每个Iterable存储的是分好组的数据，即相同key的数据组织在一起</span></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">coGroup</span><span class="params">(Iterable&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; first, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; second, Collector&lt;Tuple2&lt;String, Double&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">//存储每种商品购买数量</span></span><br><span class="line">                        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">                        <span class="keyword">for</span>(Tuple3&lt;Integer, String, Integer&gt; val1:first)&#123;</span><br><span class="line">                        sum += val1.f2;</span><br><span class="line"></span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="comment">// 每种商品数量 * 商品单价</span></span><br><span class="line">                    <span class="keyword">for</span>(Tuple2&lt;String, Integer&gt; val2:second)&#123;</span><br><span class="line">                        out.collect(Tuple2.of(val2.f0,sum * val2.f1.doubleValue()));</span><br><span class="line"></span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).print();</span><br></pre></td></tr></table></figure>

<h3 id="Cross"><a href="#Cross" class="headerlink" title="Cross"></a><strong>Cross</strong></h3><p>将两个数据集合并成一个数据集，返回被连接的两个数据集所有数据行的笛卡儿积，返回的数据行数等于第一个数据集中符合查询条件的数据行数乘以第二个数据集中符合查询条件的数据行数。Cross操作可以通过应用Cross Funciton将关联的数据集合并成目标格式的数据集，如果不指定Cross Funciton则返回Tuple2类型的数据集。Cross操作是计算密集型的算子，建议在使用时加上算法提示，比如<em>crossWithTiny()</em> and <em>crossWithHuge()</em>.</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//[id,x,y],坐标值</span></span><br><span class="line">        DataSet&lt;Tuple3&lt;Integer, Integer, Integer&gt;&gt; coords1 = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1</span>, <span class="number">20</span>, <span class="number">18</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2</span>, <span class="number">15</span>, <span class="number">20</span>),</span><br><span class="line">                Tuple3.of(<span class="number">3</span>, <span class="number">25</span>, <span class="number">10</span>));</span><br><span class="line">        DataSet&lt;Tuple3&lt;Integer, Integer, Integer&gt;&gt; coords2 = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1</span>, <span class="number">20</span>, <span class="number">18</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2</span>, <span class="number">15</span>, <span class="number">20</span>),</span><br><span class="line">                Tuple3.of(<span class="number">3</span>, <span class="number">25</span>, <span class="number">10</span>));</span><br><span class="line">        <span class="comment">// 求任意两点之间的欧氏距离</span></span><br><span class="line"></span><br><span class="line">        coords1.cross(coords2)</span><br><span class="line">                .with(<span class="keyword">new</span> CrossFunction&lt;Tuple3&lt;Integer, Integer, Integer&gt;, Tuple3&lt;Integer, Integer, Integer&gt;, Tuple3&lt;Integer, Integer, Double&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, Integer, Double&gt; <span class="title">cross</span><span class="params">(Tuple3&lt;Integer, Integer, Integer&gt; val1, Tuple3&lt;Integer, Integer, Integer&gt; val2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">// 计算欧式距离</span></span><br><span class="line">                        <span class="keyword">double</span> dist = sqrt(pow(val1.f1 - val2.f1, <span class="number">2</span>) + pow(val1.f2 - val2.f2, <span class="number">2</span>));</span><br><span class="line">                        <span class="comment">// 返回两点之间的欧式距离</span></span><br><span class="line">                        <span class="keyword">return</span> Tuple3.of(val1.f0,val2.f0,dist);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).print();</span><br></pre></td></tr></table></figure>

<h3 id="Union"><a href="#Union" class="headerlink" title="Union"></a><strong>Union</strong></h3><p>合并两个DataSet数据集，两个数据集的数据元素格式必须相同，多个数据集可以连续合并.</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals1 = env.fromElements(</span><br><span class="line">           Tuple2.of(<span class="string">"jack"</span>,<span class="number">20</span>),</span><br><span class="line">           Tuple2.of(<span class="string">"Tom"</span>,<span class="number">21</span>));</span><br><span class="line">   DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals2 = env.fromElements(</span><br><span class="line">           Tuple2.of(<span class="string">"Robin"</span>,<span class="number">25</span>),</span><br><span class="line">           Tuple2.of(<span class="string">"Bob"</span>,<span class="number">30</span>));</span><br><span class="line">   DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals3 = env.fromElements(</span><br><span class="line">           Tuple2.of(<span class="string">"Jasper"</span>,<span class="number">24</span>),</span><br><span class="line">           Tuple2.of(<span class="string">"jarry"</span>,<span class="number">21</span>));</span><br><span class="line">   DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; unioned = vals1</span><br><span class="line">           .union(vals2)</span><br><span class="line">           .union(vals3);</span><br><span class="line">   unioned.print();</span><br></pre></td></tr></table></figure>

<h3 id="Rebalance"><a href="#Rebalance" class="headerlink" title="Rebalance"></a><strong>Rebalance</strong></h3><p>对数据集中的数据进行平均分布，使得每个分区上的数据量相同,减轻数据倾斜造成的影响，注意仅仅是<code>Map-like</code>类型的算子(比如map，flatMap)才可以用在Rebalance算子之后。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;String&gt; in = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// rebalance DataSet,然后使用map算子.</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, String&gt;&gt; out = in.rebalance()</span><br><span class="line">                                        .map(<span class="keyword">new</span> Mapper());</span><br></pre></td></tr></table></figure>

<h3 id="Hash-Partition"><a href="#Hash-Partition" class="headerlink" title="Hash-Partition"></a><strong>Hash-Partition</strong></h3><p>根据给定的Key进行Hash分区，key相同的数据会被放入同一个分区内。可以使用通过元素的位置、元素的名称或者key selector函数指定key。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; in = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// 根据第一个值进行hash分区，然后使用 MapPartition转换操作.</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, String&gt;&gt; out = in.partitionByHash(<span class="number">0</span>)</span><br><span class="line">                                        .mapPartition(<span class="keyword">new</span> PartitionMapper());</span><br></pre></td></tr></table></figure>

<h3 id="Range-Partition"><a href="#Range-Partition" class="headerlink" title="Range-Partition"></a><strong>Range-Partition</strong></h3><p>根据给定的Key进行Range分区，key相同的数据会被放入同一个分区内。可以使用通过元素的位置、元素的名称或者key selector函数指定key。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; in = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// 根据第一个值进行Range分区，然后使用 MapPartition转换操作.</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, String&gt;&gt; out = in.partitionByRange(<span class="number">0</span>)</span><br><span class="line">                                        .mapPartition(<span class="keyword">new</span> PartitionMapper());</span><br></pre></td></tr></table></figure>

<h3 id="Custom-Partitioning"><a href="#Custom-Partitioning" class="headerlink" title="Custom Partitioning"></a><strong>Custom Partitioning</strong></h3><p>除了上面的分区外，还支持自定义分区函数。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String,Integer&gt;&gt; in = <span class="comment">// [...]</span></span><br><span class="line">DataSet&lt;Integer&gt; result = in.partitionCustom(partitioner, key)</span><br><span class="line">                            .mapPartition(<span class="keyword">new</span> PartitionMapper());</span><br></pre></td></tr></table></figure>

<h3 id="Sort-Partition"><a href="#Sort-Partition" class="headerlink" title="Sort Partition"></a><strong>Sort Partition</strong></h3><p>在本地对DataSet数据集中的所有分区根据指定字段进行重排序，排序方式通过Order.ASCENDING以及Order.DESCENDING关键字指定。支持指定多个字段进行分区排序，如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; in = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// 按照第一个字段升序排列，第二个字段降序排列.</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, String&gt;&gt; out = in.sortPartition(<span class="number">1</span>, Order.ASCENDING)</span><br><span class="line">                                        .sortPartition(<span class="number">0</span>, Order.DESCENDING)</span><br><span class="line">                                        .mapPartition(<span class="keyword">new</span> PartitionMapper());</span><br></pre></td></tr></table></figure>

<h3 id="First-n"><a href="#First-n" class="headerlink" title="First-n"></a>First-n</h3><p>返回数据集的n条随机结果，可以应用于常规类型数据集、Grouped类型数据集以及排序数据集上。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; in = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// 返回数据集中的任意5个元素</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; out1 = in.first(<span class="number">5</span>);</span><br><span class="line"><span class="comment">//返回每个分组内的任意两个元素</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; out2 = in.groupBy(<span class="number">0</span>)</span><br><span class="line">                                          .first(<span class="number">2</span>);</span><br><span class="line"><span class="comment">// 返回每个分组内的前三个元素</span></span><br><span class="line"><span class="comment">// 分组后的数据集按照第二个字段进行升序排序</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; out3 = in.groupBy(<span class="number">0</span>)</span><br><span class="line">                                          .sortGroup(<span class="number">1</span>, Order.ASCENDING)</span><br><span class="line">                                          .first(<span class="number">3</span>);</span><br></pre></td></tr></table></figure>

<h3 id="MinBy-MaxBy"><a href="#MinBy-MaxBy" class="headerlink" title="MinBy / MaxBy"></a>MinBy / MaxBy</h3><p>从数据集中返回指定字段或组合对应最小或最大的记录，如果选择的字段具有多个相同值，则在集合中随机选择一条记录返回。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; source = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"jack"</span>,<span class="number">20</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Tom"</span>,<span class="number">21</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Robin"</span>,<span class="number">25</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Bob"</span>,<span class="number">30</span>));</span><br><span class="line"><span class="comment">// 按照第2个元素比较，找出第二个元素为最小值的那个tuple</span></span><br><span class="line"><span class="comment">// 在整个DataSet上使用minBy</span></span><br><span class="line">ReduceOperator&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2Reduce = source.minBy(<span class="number">1</span>);</span><br><span class="line">tuple2Reduce.print();<span class="comment">// 返回(jack,20)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 也可以在分组的DataSet上使用minBy</span></span><br><span class="line">source.groupBy(<span class="number">0</span>) <span class="comment">// 按照第一个字段进行分组</span></span><br><span class="line">      .minBy(<span class="number">1</span>)  <span class="comment">// 找出每个分组内的按照第二个元素为最小值的那个tuple</span></span><br><span class="line">      .print();</span><br></pre></td></tr></table></figure>

<h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>广播变量是分布式计算框架中经常会用到的一种数据共享方式。其主要作用是将小数据集采用网络传输的方式，在每台机器上维护一个只读的缓存变量，所在的计算节点实例均可以在本地内存中直接读取被广播的数据集，这样能够避免在数据计算过程中多次通过远程的方式从其他节点中读取小数据集，从而提升整体任务的计算性能。</p>
<p>广播变量可以理解为一个公共的共享变量，可以把DataSet广播出去，这样不同的task都可以读取该数据，广播的数据只会在每个节点上存一份。如果不使用广播变量，则会在每个节点上的task中都要复制一份dataset数据集，导致浪费内存。</p>
<p>使用广播变量的基本步骤如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//第一步创建需要广播的数据集</span></span><br><span class="line">DataSet&lt;Integer&gt; toBroadcast = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">DataSet&lt;String&gt; data = env.fromElements(<span class="string">"a"</span>, <span class="string">"b"</span>);</span><br><span class="line"></span><br><span class="line">data.map(<span class="keyword">new</span> RichMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      <span class="comment">// 第三步访问集合形式的广播变量数据集</span></span><br><span class="line">      Collection&lt;Integer&gt; broadcastSet = getRuntimeContext().getBroadcastVariable(<span class="string">"broadcastSetName"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).withBroadcastSet(toBroadcast, <span class="string">"broadcastSetName"</span>); <span class="comment">// 第二步广播数据集</span></span><br></pre></td></tr></table></figure>

<p>从上面的代码可以看出，DataSet API支持在RichFunction接口中通过RuntimeContext读取到广播变量。</p>
<p>首先在RichFunction中实现Open()方法，然后调用getRuntimeContext()方法获取应用的RuntimeContext，接着调用getBroadcastVariable()方法通过广播名称获取广播变量。同时Flink直接通过collect操作将数据集转换为本地Collection。需要注意的是，Collection对象的数据类型必须和定义的数据集的类型保持一致，否则会出现类型转换问题。</p>
<p>注意事项：</p>
<ul>
<li>由于广播变量的内容是保存在每个节点的内存中，所以广播变量数据集不易过大。</li>
<li>广播变量初始化之后，不支持修改，这样方能保证每个节点的数据都是一样的。</li>
<li>如果多个算子都要使用一份数据集，那么需要在多个算子的后面分别注册广播变量。</li>
<li>只能在批处理中使用广播变量。</li>
</ul>
<h3 id="使用Demo"><a href="#使用Demo" class="headerlink" title="使用Demo"></a>使用Demo</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BroadcastExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;Tuple2&lt;Integer,String&gt;&gt; RawBroadCastData = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        RawBroadCastData.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>,<span class="string">"jack"</span>));</span><br><span class="line">        RawBroadCastData.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">2</span>,<span class="string">"tom"</span>));</span><br><span class="line">        RawBroadCastData.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">3</span>,<span class="string">"Bob"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 模拟数据源，[userId,userName]</span></span><br><span class="line">        DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; userInfoBroadCastData = env.fromCollection(RawBroadCastData);</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;Tuple2&lt;Integer,Double&gt;&gt; rawUserAount = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        rawUserAount.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>,<span class="number">1000.00</span>));</span><br><span class="line">        rawUserAount.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">2</span>,<span class="number">500.20</span>));</span><br><span class="line">        rawUserAount.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">3</span>,<span class="number">800.50</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理数据：用户id，用户购买金额 ，[UserId,amount]</span></span><br><span class="line">        DataSet&lt;Tuple2&lt;Integer, Double&gt;&gt; userAmount = env.fromCollection(rawUserAount);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 转换为map集合类型的DataSet</span></span><br><span class="line">        DataSet&lt;HashMap&lt;Integer, String&gt;&gt; userInfoBroadCast = userInfoBroadCastData.map(<span class="keyword">new</span> MapFunction&lt;Tuple2&lt;Integer, String&gt;, HashMap&lt;Integer, String&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> HashMap&lt;Integer, String&gt; <span class="title">map</span><span class="params">(Tuple2&lt;Integer, String&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                HashMap&lt;Integer, String&gt; userInfo = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">                userInfo.put(value.f0, value.f1);</span><br><span class="line">                <span class="keyword">return</span> userInfo;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">       DataSet&lt;String&gt; result = userAmount.map(<span class="keyword">new</span> RichMapFunction&lt;Tuple2&lt;Integer, Double&gt;, String&gt;() &#123;</span><br><span class="line">            <span class="comment">// 存放广播变量返回的list集合数据</span></span><br><span class="line">            List&lt;HashMap&lt;String, String&gt;&gt; broadCastList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="comment">// 存放广播变量的值</span></span><br><span class="line">            HashMap&lt;String, String&gt; allMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                <span class="comment">//获取广播数据,返回的是一个list集合</span></span><br><span class="line">                <span class="keyword">this</span>.broadCastList = getRuntimeContext().getBroadcastVariable(<span class="string">"userInfo"</span>);</span><br><span class="line">                <span class="keyword">for</span> (HashMap&lt;String, String&gt; value : broadCastList) &#123;</span><br><span class="line">                    allMap.putAll(value);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(Tuple2&lt;Integer, Double&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String userName = allMap.get(value.f0);</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"用户id： "</span> + value.f0 + <span class="string">" | "</span>+ <span class="string">"用户名： "</span> + userName + <span class="string">" | "</span> + <span class="string">"购买金额： "</span> + value.f1;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).withBroadcastSet(userInfoBroadCast, <span class="string">"userInfo"</span>);</span><br><span class="line"></span><br><span class="line">        result.print();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="分布式缓存"><a href="#分布式缓存" class="headerlink" title="分布式缓存"></a>分布式缓存</h2><h3 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h3><p>Flink提供了一个分布式缓存(distributed cache),类似于Hadoop，以使文件在本地可被用户函数的并行实例访问。分布式缓存的工作机制是为程序注册一个文件或目录(本地或者远程文件系统，如HDFS等)，通过ExecutionEnvironment注册一个缓存文件，并起一个别名。当程序执行的时候，Flink会自动把注册的文件或目录复制到所有TaskManager节点的本地文件系统，用户可以通过注册是起的别名来查找文件或目录，然后在TaskManager节点的本地文件系统访问该文件。</p>
<p>分布式缓存的使用步骤：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// 注册一个HDFS文件</span></span><br><span class="line">env.registerCachedFile(<span class="string">"hdfs:///path/to/your/file"</span>, <span class="string">"hdfsFile"</span>)</span><br><span class="line"><span class="comment">// 注册一个本地文件</span></span><br><span class="line">env.registerCachedFile(<span class="string">"file:///path/to/exec/file"</span>, <span class="string">"localExecFile"</span>, <span class="keyword">true</span>)</span><br><span class="line"><span class="comment">// 访问数据</span></span><br><span class="line">getRuntimeContext().getDistributedCache().getFile(<span class="string">"hdfsFile"</span>);</span><br></pre></td></tr></table></figure>

<p>获取缓存文件的方式和广播变量相似，也是实现RichFunction接口，并通过RichFunction接口获得RuntimeContext对象，然后通过RuntimeContext提供的接口获取对应的本地缓存文件。</p>
<h3 id="使用Demo-1"><a href="#使用Demo-1" class="headerlink" title="使用Demo"></a>使用Demo</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributeCacheExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//获取运行环境</span></span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         *  注册一个本地文件</span></span><br><span class="line"><span class="comment">         *   文件内容为：</span></span><br><span class="line"><span class="comment">         *   1,"jack"</span></span><br><span class="line"><span class="comment">         *   2,"tom"</span></span><br><span class="line"><span class="comment">         *   3,"Bob"</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        env.registerCachedFile(<span class="string">"file:///E://userinfo.txt"</span>, <span class="string">"localFileUserInfo"</span>, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;Tuple2&lt;Integer,Double&gt;&gt; rawUserAount = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        rawUserAount.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>,<span class="number">1000.00</span>));</span><br><span class="line">        rawUserAount.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">2</span>,<span class="number">500.20</span>));</span><br><span class="line">        rawUserAount.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">3</span>,<span class="number">800.50</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理数据：用户id，用户购买金额 ，[UserId,amount]</span></span><br><span class="line">        DataSet&lt;Tuple2&lt;Integer, Double&gt;&gt; userAmount = env.fromCollection(rawUserAount);</span><br><span class="line"></span><br><span class="line">        DataSet&lt;String&gt; result= userAmount.map(<span class="keyword">new</span> RichMapFunction&lt;Tuple2&lt;Integer, Double&gt;, String&gt;() &#123;</span><br><span class="line">            <span class="comment">// 保存缓存数据</span></span><br><span class="line">            HashMap&lt;String, String&gt; allMap = <span class="keyword">new</span> HashMap&lt;String, String&gt;();</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                <span class="comment">// 获取分布式缓存的数据</span></span><br><span class="line">                File userInfoFile = getRuntimeContext().getDistributedCache().getFile(<span class="string">"localFileUserInfo"</span>);</span><br><span class="line">                List&lt;String&gt; userInfo = FileUtils.readLines(userInfoFile);</span><br><span class="line">                <span class="keyword">for</span> (String value : userInfo) &#123;</span><br><span class="line"></span><br><span class="line">                    String[] split = value.split(<span class="string">","</span>);</span><br><span class="line">                    allMap.put(split[<span class="number">0</span>], split[<span class="number">1</span>]);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(Tuple2&lt;Integer, Double&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String userName = allMap.get(value.f0);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> <span class="string">"用户id： "</span> + value.f0 + <span class="string">" | "</span> + <span class="string">"用户名： "</span> + userName + <span class="string">" | "</span> + <span class="string">"购买金额： "</span> + value.f1;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        result.print();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要讲解了Flink DataSet API的基本使用。首先介绍了一个DataSet API的WordCount案例，接着介绍了DataSet API的数据源与Sink操作，以及基本的使用。然后对每一个转换操作进行了详细的解释，并给出了具体的使用案例。最后讲解了广播变量和分布式缓存的概念，并就如何使用这两种高级功能，提供了完整的Demo案例。</p>
<blockquote>
<p>公众号『大数据技术与数仓』，回复『资料』领取大数据资料包</p>
</blockquote>
</div><div class="recommended_posts"><h3>相关推荐 ☟</h3><li><a href="https://jiamaoxiang.top/2020/05/20/Hive的条件函数与日期函数全面汇总解析/" target="_blank">Hive的条件函数与日期函数全面汇总解析</a></li><li><a href="https://jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/" target="_blank">Greenplum集群Master与Segment节点故障检测与恢复</a></li><li><a href="https://jiamaoxiang.top/2020/04/30/Flink-DataStream-API-中的多面手——Process-Function详解/" target="_blank">Flink DataStream API 中的多面手——Process Function详解</a></li><li><a href="https://jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/" target="_blank">Flink内部Exactly Once三板斧:状态、状态后端与检查点</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Jia MaoXiang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2020/05/09/Flink-DataSet-API编程指南/">https://jiamaoxiang.top/2020/05/09/Flink-DataSet-API编程指南/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本文为博主原创文章，遵循CC BY-SA 4.0版权协议，转载请附上原文出处链接和本声明</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://jiamaoxiang.top/2020/05/09/Flink-DataSet-API编程指南/" data-id="ckjzju0yi007vrc7q68lw71zg" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACoklEQVR42u3aQW7DMAwEwP7/0+m1QBB7V5SMFBifAsdVND6ILMmfn/h6/bne779/zu9cP3m9/rYLDw8Pb7D19+vTMwny+vn8ul75em94eHh453jXh3VLTZBJAEioNxY8PDy8L+bdHNBxrnv9DB4eHt5/5LXhIQkeLQkPDw/vG3htISB5QWtBJU+7N9da8PDw8GJe3kX6ns9H+nt4eHh44676M4d4Dqt3i4eHh3eAlx+489b+3t8qhsPw8PDwHuTlgSEvsK6VFa7LwTd7wMPDw9vKyzc9SXnzLU4S6JvOHh4eHt5W3rx80KbXyZPzeiweHh7eCV4+Njpnt+XgdiDs4zp4eHh4W3nJ0m1ynBdb24Js/RLx8PDwHuTl9/MWV/5S2hU+/hUeHh7eg7y19n9y3OehYm1coJiJwMPDw1vitens5KDPS7fzsi8eHh7ek7xJsXWeLucl42KmDA8PD+8AL28yTRLotj3WhpC6ZoyHh4e3xFtra002tMZuhxjw8PDwzvEmBdMkhCRFhEnCHe0QDw8P7wBvPi6QA/IgNC/v4uHh4Z3j5Yl1y1gLFbuGD/Dw8PBO8PIjuC2k5u2rNqUuxrPw8PDwDvPyln/+bQ7IiyDF+nh4eHgHeHmQWAsk7fqTBlvRBsPDw8Mb8+ZFhPz4XsOsfYuHh4d3mjcpBEzKtWvliWKqCg8PD+8YL2+AteWD5M7kF4uZCDw8PLwx71Ve8zGCyQr1t3h4eHiP/Oe+lraulVzzJDv/vHjh4eHhlbwTwWDtRay9oHqyDA8PD28Tb574to2xerohSNC39ffw8PDwtvLWhgzWnpwHDDw8PLxv4OXUZOv53+ZJPx4eHt4zvF2J76Ql1u5kc60FDw8PL+ZtmNsqE+ik9TVJ8fHw8PCO8X4Bq8CIvaAXEgcAAAAASUVORK5CYII=">分享</a><div class="tags"><a href="/tags/flink/">flink</a></div><div class="post-nav"><a class="pre" href="/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/">Greenplum集群Master与Segment节点故障检测与恢复</a><a class="next" href="/2020/04/30/Flink-DataStream-API-中的多面手——Process-Function详解/">Flink DataStream API 中的多面手——Process Function详解</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar-toc"><div class="stoc-article" id="sidebar-stoc"><strong class="stoc-title"><i class="fa fa-list-ul"> 目录</i></strong><div class="toc-nav" id="stoc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#WordCount示例"><span class="toc-number">1.</span> <span class="toc-text">WordCount示例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Data-Source"><span class="toc-number">2.</span> <span class="toc-text">Data Source</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#基于文件的数据源"><span class="toc-number">2.1.</span> <span class="toc-text">基于文件的数据源</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#readTextFile-path-TextInputFormat"><span class="toc-number">2.1.1.</span> <span class="toc-text">readTextFile(path) / TextInputFormat</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#readTextFileWithValue-path-TextValueInputFormat"><span class="toc-number">2.1.2.</span> <span class="toc-text">readTextFileWithValue(path)/ TextValueInputFormat</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#readCsvFile-path-CsvInputFormat"><span class="toc-number">2.1.3.</span> <span class="toc-text">readCsvFile(path)/ CsvInputFormat</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#readFileOfPrimitives-path-Class-PrimitiveInputFormat"><span class="toc-number">2.1.4.</span> <span class="toc-text">readFileOfPrimitives(path, Class) / PrimitiveInputFormat</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基于集合的数据源"><span class="toc-number">2.2.</span> <span class="toc-text">基于集合的数据源</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#fromCollection-Collection"><span class="toc-number">2.2.1.</span> <span class="toc-text">fromCollection(Collection)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#fromElements-T-…"><span class="toc-number">2.2.2.</span> <span class="toc-text">fromElements(T …)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#generateSequence-from-to"><span class="toc-number">2.2.3.</span> <span class="toc-text">generateSequence(from, to)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#通用类型数据源"><span class="toc-number">2.3.</span> <span class="toc-text">通用类型数据源</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#readFile-inputFormat-path-FileInputFormat"><span class="toc-number">2.3.1.</span> <span class="toc-text">readFile(inputFormat, path) / FileInputFormat</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#createInput-inputFormat-InputFormat"><span class="toc-number">2.3.2.</span> <span class="toc-text">createInput(inputFormat) / InputFormat</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Data-Sink"><span class="toc-number">3.</span> <span class="toc-text">Data Sink</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#标准的数据输出方法"><span class="toc-number">3.1.</span> <span class="toc-text">标准的数据输出方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用自定义的输出类型"><span class="toc-number">3.2.</span> <span class="toc-text">使用自定义的输出类型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataSet转换"><span class="toc-number">4.</span> <span class="toc-text">DataSet转换</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Map"><span class="toc-number">4.1.</span> <span class="toc-text">Map</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#FlatMap"><span class="toc-number">4.2.</span> <span class="toc-text">FlatMap</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MapPartition"><span class="toc-number">4.3.</span> <span class="toc-text">MapPartition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Filter"><span class="toc-number">4.4.</span> <span class="toc-text">Filter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Project"><span class="toc-number">4.5.</span> <span class="toc-text">Project</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reduce"><span class="toc-number">4.6.</span> <span class="toc-text">Reduce</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReduceGroup"><span class="toc-number">4.7.</span> <span class="toc-text">ReduceGroup</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Aggregate"><span class="toc-number">4.8.</span> <span class="toc-text">Aggregate</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Distinct"><span class="toc-number">4.9.</span> <span class="toc-text">Distinct</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Join"><span class="toc-number">4.10.</span> <span class="toc-text">Join</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#OuterJoin"><span class="toc-number">4.11.</span> <span class="toc-text">OuterJoin</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CoGroup"><span class="toc-number">4.12.</span> <span class="toc-text">CoGroup</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cross"><span class="toc-number">4.13.</span> <span class="toc-text">Cross</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Union"><span class="toc-number">4.14.</span> <span class="toc-text">Union</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Rebalance"><span class="toc-number">4.15.</span> <span class="toc-text">Rebalance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hash-Partition"><span class="toc-number">4.16.</span> <span class="toc-text">Hash-Partition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Range-Partition"><span class="toc-number">4.17.</span> <span class="toc-text">Range-Partition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Custom-Partitioning"><span class="toc-number">4.18.</span> <span class="toc-text">Custom Partitioning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sort-Partition"><span class="toc-number">4.19.</span> <span class="toc-text">Sort Partition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#First-n"><span class="toc-number">4.20.</span> <span class="toc-text">First-n</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MinBy-MaxBy"><span class="toc-number">4.21.</span> <span class="toc-text">MinBy / MaxBy</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#广播变量"><span class="toc-number">5.</span> <span class="toc-text">广播变量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#基本概念"><span class="toc-number">5.1.</span> <span class="toc-text">基本概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用Demo"><span class="toc-number">5.2.</span> <span class="toc-text">使用Demo</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#分布式缓存"><span class="toc-number">6.</span> <span class="toc-text">分布式缓存</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#基本概念-1"><span class="toc-number">6.1.</span> <span class="toc-text">基本概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用Demo-1"><span class="toc-number">6.2.</span> <span class="toc-text">使用Demo</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#小结"><span class="toc-number">7.</span> <span class="toc-text">小结</span></a></li></ol></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 - 2021 <a href="/." rel="nofollow">Jmx's Blog.</a>All rights reserved.<br>Thoughts on technology, life and everything else.</div></div></div><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" color="100,99,98" opacity="0.5" zindex="0.7" count="150" src="//lib.baomitu.com/canvas-nest.js/2.0.4/canvas-nest.umd.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>