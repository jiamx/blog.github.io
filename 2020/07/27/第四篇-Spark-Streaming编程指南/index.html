<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记录朴实无华且枯燥的生活"><title>第四篇|Spark Streaming编程指南(1) | Jmx's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '912b6cfc43243cd27aeb428f7dbf7823';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">第四篇|Spark Streaming编程指南(1)</h1><a id="logo" href="/.">Jmx's Blog</a><p class="description">Keep it Simple and Stupid!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-pied-piper-alt"> 关于</i></a><a href="/tags"><i class="fa fa-tags"> 标签</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">第四篇|Spark Streaming编程指南(1)</h1><div class="post-meta">Jul 27, 2020<span> | </span><span class="category"><a href="/categories/Spark/">Spark</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 5.5k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 24</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><a id="more"></a>

<p>Spark Streaming是构建在Spark Core基础之上的流处理框架，是Spark非常重要的组成部分。Spark Streaming于2013年2月在Spark0.7.0版本中引入，发展至今已经成为了在企业中广泛使用的流处理平台。在2016年7月，Spark2.0版本中引入了Structured Streaming，并在Spark2.2版本中达到了生产级别，Structured Streaming是构建在Spark SQL之上的流处理引擎，用户可以使用DataSet/DataFreame API进行流处理，目前Structured Streaming在不同的版本中发展速度很快。值得注意的是，本文不会对Structured Streaming做过多讲解，主要针对Spark Streaming进行讨论，包括以下内容：</p>
<ul>
<li>Spark Streaming介绍</li>
<li>Transformations与Output Operations</li>
<li>Spark Streaming数据源(Sources)</li>
<li>Spark Streaming 数据汇(Sinks)</li>
</ul>
<h2 id="Spark-Streaming介绍"><a href="#Spark-Streaming介绍" class="headerlink" title="Spark Streaming介绍"></a>Spark Streaming介绍</h2><h3 id="什么是DStream"><a href="#什么是DStream" class="headerlink" title="什么是DStream"></a>什么是DStream</h3><p>Spark Streaming是构建在Spark Core的RDD基础之上的，与此同时Spark Streaming引入了一个新的概念：DStream（Discretized Stream，离散化数据流)，表示连续不断的数据流。DStream抽象是Spark Streaming的流处理模型，在内部实现上，Spark Streaming会对输入数据按照时间间隔（如1秒）分段，每一段数据转换为Spark中的RDD，这些分段就是Dstream，并且对DStream的操作都最终转变为对相应的RDD的操作。如下图所示：</p>
<p><img src="//jiamaoxiang.top/2020/07/27/第四篇-Spark-Streaming编程指南/%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B.png" alt></p>
<p>如上图，这些底层的RDD转换操作是由Spark引擎来完成的，DStream的操作屏蔽了许多底层的细节，为用户提供了比较方便使用的高级API。</p>
<h3 id="计算模型"><a href="#计算模型" class="headerlink" title="计算模型"></a>计算模型</h3><p>在Flink中，批处理是流处理的特例，所以Flink是天然的流处理引擎。而Spark Streaming则不然，Spark Streaming认为流处理是批处理的特例，即Spark Streaming并不是纯实时的流处理引擎，在其内部使用的是<code>microBatch</code>模型，即将流处理看做是在较小时间间隔内(batch interval)的一些列的批处理。关于时间间隔的设定，需要结合具体的业务延迟需求，可以实现秒级或者分钟级的间隔。</p>
<p>Spark Streaming会将每个短时间间隔内接收的数据存储在集群中，然后对其作用一系列的算子操作(map,reduce, groupBy等)。执行过程见下图：</p>
<p><img src="//jiamaoxiang.top/2020/07/27/第四篇-Spark-Streaming编程指南/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B.png" alt></p>
<p>如上图：Spark Streaming会将输入的数据流分割成一个个小的batch，每一个batch都代表这一些列的RDD，然后将这些batch存储在内存中。通过启动Spark作业来处理这些batch数据，从而实现一个流处理应用。</p>
<h3 id="Spark-Streaming的工作机制"><a href="#Spark-Streaming的工作机制" class="headerlink" title="Spark Streaming的工作机制"></a>Spark Streaming的工作机制</h3><h4 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h4><p><img src="//jiamaoxiang.top/2020/07/27/第四篇-Spark-Streaming编程指南/%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt></p>
<ul>
<li>在Spark Streaming中，会有一个组件Receiver，作为一个长期运行的task跑在一个Executor上</li>
<li>每个Receiver都会负责一个input DStream（比如从文件中读取数据的文件流，比如套接字流，或者从Kafka中读取的一个输入流等等）</li>
<li>Spark Streaming通过input DStream与外部数据源进行连接，读取相关数据</li>
</ul>
<h4 id="执行细节"><a href="#执行细节" class="headerlink" title="执行细节"></a>执行细节</h4><p><img src="//jiamaoxiang.top/2020/07/27/第四篇-Spark-Streaming编程指南/%E5%86%85%E9%83%A8%E7%BB%86%E8%8A%82.png" alt></p>
<ul>
<li>1.启动StreamingContext</li>
<li>2.StreamingContext启动receiver，该receiver会一直运行在Executor的task中。用于连续不断地接收数据源，有两种主要的reciver，一种是可靠的reciver，当数据被接收并且存储到spark，发送回执确认，另一种是不可靠的reciver，对于数据源不发送回执确认。接收的数据会被缓存到work节点内存中，也会被复制到其他executor的所在的节点内存中，用于容错处理。</li>
<li>3.Streaming context周期触发job(根据batch-interval时间间隔)进行数据处理。</li>
<li>4.将数据输出。</li>
</ul>
<h3 id="Spark-Streaming编程步骤"><a href="#Spark-Streaming编程步骤" class="headerlink" title="Spark Streaming编程步骤"></a>Spark Streaming编程步骤</h3><p>经过上面的分析，对Spark Streaming有了初步的认识。那么该如何编写一个Spark Streaming应用程序呢？一个Spark Streaming一般包括一下几个步骤：</p>
<ul>
<li><p>1.创建<code>StreamingContext</code></p>
</li>
<li><p>2.创建输入<code>DStream</code>来定义输入源</p>
</li>
<li><p>3.通过对DStream应用转换操作和输出操作来定义处理逻辑</p>
</li>
<li><p>4.用streamingContext.start()来开始接收数据和处理流程</p>
</li>
<li><p>5.streamingContext.awaitTermination()方法来等待处理结束</p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StartSparkStreaming</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">      .setAppName(<span class="string">"Streaming"</span>)</span><br><span class="line">    <span class="comment">// 1.创建StreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.spark"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.hadoop"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line">    <span class="comment">// 2.创建DStream</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="comment">// 3.定义流计算处理逻辑</span></span><br><span class="line">    <span class="keyword">val</span> count = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">      .reduceByKey(_ + _)</span><br><span class="line">    <span class="comment">// 4.输出结果</span></span><br><span class="line">    count.print()</span><br><span class="line">    <span class="comment">// 5.启动</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">// 6.等待执行</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Transformations与Output-Operations"><a href="#Transformations与Output-Operations" class="headerlink" title="Transformations与Output Operations"></a>Transformations与Output Operations</h2><p>DStream是不可变的， 这意味着不能直接改变它们的内容，而是通过对DStream进行一系列转换(Transformation)来实现预期的应用程序逻辑。 每次转换都会创建一个新的DStream，该DStream表示来自父DStream的转换后的数据。 DStream转换是惰性(lazy)的，这意味只有执行output操作之后，才会去执行转换操作，这些触发执行的操作称之为<code>output operation</code>。 </p>
<h3 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h3><p>Spark Streaming提供了丰富的transformation操作，这些transformation又分为了<strong>有状态的transformation</strong>和<strong>无状态的transformation</strong>。除此之外，Spark Streaming也提供了一些window操作，值得注意的是window操作也是有状态的。具体细节如下：</p>
<h4 id="无状态的transformation"><a href="#无状态的transformation" class="headerlink" title="无状态的transformation"></a>无状态的transformation</h4><p>无状态的transformation是指每一个micro-batch的处理是相互独立的，即当前的计算结果不受之前计算结果的影响，Spark Streaming的大部分算子都是无状态的，比如常见的map(),flatMap(),reduceByKey()等等。</p>
<ul>
<li><strong>map(func)</strong></li>
</ul>
<p>对源DStream的每个元素，采用func函数进行转换，得到一个新的Dstream</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/** Return a new DStream by applying a function to all elements of this DStream. */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](mapFunc: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">DStream</span>[<span class="type">U</span>] = ssc.withScope &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MappedDStream</span>(<span class="keyword">this</span>, context.sparkContext.clean(mapFunc))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>flatMap(func)</strong></li>
</ul>
<p>与map相似，但是每个输入项可用被映射为0个或者多个输出项</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new DStream by applying a function to all elements of this DStream,</span></span><br><span class="line"><span class="comment"> * and then flattening the results</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](flatMapFunc: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">DStream</span>[<span class="type">U</span>] = ssc.withScope &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">FlatMappedDStream</span>(<span class="keyword">this</span>, context.sparkContext.clean(flatMapFunc))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>filter(func)</strong></li>
</ul>
<p>返回一个新的DStream，仅包含源DStream中满足函数func的项</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/** Return a new DStream containing only the elements that satisfy a predicate. */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(filterFunc: <span class="type">T</span> =&gt; <span class="type">Boolean</span>): <span class="type">DStream</span>[<span class="type">T</span>] = ssc.withScope &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">FilteredDStream</span>(<span class="keyword">this</span>, context.sparkContext.clean(filterFunc))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>repartition(numPartitions)</strong></li>
</ul>
<p>通过创建更多或者更少的分区改变DStream的并行程度</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return a new DStream with an increased or decreased level of parallelism. Each RDD in the</span></span><br><span class="line"><span class="comment">   * returned DStream has exactly numPartitions partitions.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(numPartitions: <span class="type">Int</span>): <span class="type">DStream</span>[<span class="type">T</span>] = ssc.withScope &#123;</span><br><span class="line">    <span class="keyword">this</span>.transform(_.repartition(numPartitions))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>reduce(func)</strong></li>
</ul>
<p>利用函数func聚集源DStream中每个RDD的元素，返回一个包含单元素RDDs的新DStream</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new DStream in which each RDD has a single element generated by reducing each RDD</span></span><br><span class="line"><span class="comment"> * of this DStream.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(reduceFunc: (<span class="type">T</span>, <span class="type">T</span>) =&gt; <span class="type">T</span>): <span class="type">DStream</span>[<span class="type">T</span>] = ssc.withScope &#123;</span><br><span class="line">  <span class="keyword">this</span>.map((<span class="literal">null</span>, _)).reduceByKey(reduceFunc, <span class="number">1</span>).map(_._2)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>count()</strong></li>
</ul>
<p>统计源DStream中每个RDD的元素数量</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return a new DStream in which each RDD has a single element generated by counting each RDD</span></span><br><span class="line"><span class="comment">   * of this DStream.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">DStream</span>[<span class="type">Long</span>] = ssc.withScope &#123;</span><br><span class="line">    <span class="keyword">this</span>.map(_ =&gt; (<span class="literal">null</span>, <span class="number">1</span>L))</span><br><span class="line">        .transform(_.union(context.sparkContext.makeRDD(<span class="type">Seq</span>((<span class="literal">null</span>, <span class="number">0</span>L)), <span class="number">1</span>)))</span><br><span class="line">        .reduceByKey(_ + _)</span><br><span class="line">        .map(_._2)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>union(otherStream)</strong></li>
</ul>
<p>返回一个新的DStream，包含源DStream和其他DStream的元素</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return a new DStream by unifying data of another DStream with this DStream.</span></span><br><span class="line"><span class="comment">   * @param that Another DStream having the same slideDuration as this DStream.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">union</span></span>(that: <span class="type">DStream</span>[<span class="type">T</span>]): <span class="type">DStream</span>[<span class="type">T</span>] = ssc.withScope &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">UnionDStream</span>[<span class="type">T</span>](<span class="type">Array</span>(<span class="keyword">this</span>, that))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>countByValue()</strong></li>
</ul>
<p>应用于元素类型为K的DStream上，返回一个（K，V）键值对类型的新DStream，每个键的值是在原DStream的每个RDD中的出现次数,比如<code>lines.flatMap(_.split(&quot; &quot;)).countByValue().print()</code>,对于输入：<code>spark spark flink</code>,将输出：<code>(spark,2),(flink,1)</code>,即按照元素值进行分组，然后统计每个分组的元素个数。</p>
<p>从源码可以看出：底层实现为map((_,1L)).reduceByKey((x: Long, y: Long) =&gt; x + y, numPartitions)，即先按当前的元素映射为一个tuple，其中key即为当前元素的值，然后再按照key做汇总。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return a new DStream in which each RDD contains the counts of each distinct value in</span></span><br><span class="line"><span class="comment">   * each RDD of this DStream. Hash partitioning is used to generate</span></span><br><span class="line"><span class="comment">   * the RDDs with `numPartitions` partitions (Spark's default number of partitions if</span></span><br><span class="line"><span class="comment">   * `numPartitions` not specified).</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">countByValue</span></span>(numPartitions: <span class="type">Int</span> = ssc.sc.defaultParallelism)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>)</span><br><span class="line">      : <span class="type">DStream</span>[(<span class="type">T</span>, <span class="type">Long</span>)] = ssc.withScope &#123;</span><br><span class="line">    <span class="keyword">this</span>.map((_, <span class="number">1</span>L)).reduceByKey((x: <span class="type">Long</span>, y: <span class="type">Long</span>) =&gt; x + y, numPartitions)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>reduceByKey(func, [numTasks])</strong></li>
</ul>
<p>当在一个由(K,V)键值对组成的DStream上执行该操作时，返回一个新的由(K,V)键值对组成的DStream，每一个key的值均由给定的recuce函数（func）聚集起来</p>
<p>比如：<code>lines.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_ + _).print()</code></p>
<p>对于输入：spark spark flink，将输出：(spark,2),(flink,1)</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new DStream by applying `reduceByKey` to each RDD. The values for each key are</span></span><br><span class="line"><span class="comment"> * merged using the associative and commutative reduce function. Hash partitioning is used to</span></span><br><span class="line"><span class="comment"> * generate the RDDs with Spark's default number of partitions.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(reduceFunc: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">DStream</span>[(<span class="type">K</span>, <span class="type">V</span>)] = ssc.withScope &#123;</span><br><span class="line">  reduceByKey(reduceFunc, defaultPartitioner())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>join(otherStream, [numTasks])</strong></li>
</ul>
<p>当应用于两个DStream（一个包含（K,V）键值对,一个包含(K,W)键值对），返回一个包含(K, (V, W))键值对的新Dstream</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new DStream by applying 'join' between RDDs of `this` DStream and `other` DStream.</span></span><br><span class="line"><span class="comment"> * Hash partitioning is used to generate the RDDs with Spark's default number of partitions.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>: <span class="type">ClassTag</span>](other: <span class="type">DStream</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">DStream</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))] = ssc.withScope &#123;</span><br><span class="line">  join[<span class="type">W</span>](other, defaultPartitioner())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>cogroup(otherStream, [numTasks])</strong></li>
</ul>
<p>当应用于两个DStream（一个包含（K,V）键值对,一个包含(K,W)键值对），返回一个包含(K, Seq[V], Seq[W])的元组</p>
<blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">&gt;    <span class="comment">// 输入：spark</span></span><br><span class="line">&gt;    <span class="comment">// 输出：(spark,(CompactBuffer(1),CompactBuffer(1)))</span></span><br><span class="line">&gt;    <span class="keyword">val</span> <span class="type">DS1</span> = lines.flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>))</span><br><span class="line">&gt;    <span class="keyword">val</span> <span class="type">DS2</span> = lines.flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>))</span><br><span class="line">&gt;    <span class="type">DS1</span>.cogroup(<span class="type">DS2</span>).print()</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new DStream by applying 'cogroup' between RDDs of `this` DStream and `other` DStream.</span></span><br><span class="line"><span class="comment"> * Hash partitioning is used to generate the RDDs with Spark's default number</span></span><br><span class="line"><span class="comment"> * of partitions.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    other: <span class="type">DStream</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">DStream</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W</span>]))] = ssc.withScope &#123;</span><br><span class="line">  cogroup(other, defaultPartitioner())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>transform(func)</strong></li>
</ul>
<p>通过对源DStream的每个RDD应用RDD-to-RDD函数，创建一个新的DStream。支持在新的DStream中做任何RDD操作</p>
<blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">&gt;<span class="comment">// 输入：spark spark flink</span></span><br><span class="line">&gt;<span class="comment">// 输出：(spark,2)、(flink,1)</span></span><br><span class="line">&gt;<span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">&gt;<span class="keyword">val</span> resultDStream = lines.transform(rdd =&gt; &#123;</span><br><span class="line">&gt;  rdd.flatMap(_.split(<span class="string">"\\W"</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">&gt;&#125;)</span><br><span class="line">&gt;resultDStream.print()</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new DStream in which each RDD is generated by applying a function</span></span><br><span class="line"><span class="comment"> * on each RDD of 'this' DStream.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](transformFunc: <span class="type">RDD</span>[<span class="type">T</span>] =&gt; <span class="type">RDD</span>[<span class="type">U</span>]): <span class="type">DStream</span>[<span class="type">U</span>] = ssc.withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanedF = context.sparkContext.clean(transformFunc, <span class="literal">false</span>)</span><br><span class="line">  transform((r: <span class="type">RDD</span>[<span class="type">T</span>], _: <span class="type">Time</span>) =&gt; cleanedF(r))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="有状态的transformation"><a href="#有状态的transformation" class="headerlink" title="有状态的transformation"></a>有状态的transformation</h4><p>有状态的transformation是指每个micro-batch的处理不是相互独立的，即当前的micro-batch处理依赖于之前的micro-batch计算结果。常见的有状态的transformation主要有countByValueAndWindow, reduceByKeyAndWindow , mapWithState, updateStateByKey等等。其实所有的基于window的操作都是有状态的，因为追踪整个窗口内的数据。</p>
<p>关于有状态的transformation和Window Operations，参见下文。</p>
<h3 id="Output-Operations"><a href="#Output-Operations" class="headerlink" title="Output Operations"></a>Output Operations</h3><p>使用Output operations可以将DStream写入多外部存储设备或打印到控制台。上文提到，Spark Streaming的transformation是lazy的，因此需要Output Operation进行触发计算，其功能类似于RDD的action操作。具体详见下文Spark Streaming 数据汇(Sinks)。</p>
<h2 id="Spark-Streaming数据源"><a href="#Spark-Streaming数据源" class="headerlink" title="Spark Streaming数据源"></a>Spark Streaming数据源</h2><p>Spark Streaming的目的是成为一个通用的流处理框架，为了实现这一目标，Spark Streaming使用<strong>Receiver</strong>来集成各种各样的数据源。但是，对于有些数据源(如kafka),Spark Streaming支持使用<strong>Direct</strong>的方式去接收数据，这种方式比Receiver方式性能要好。</p>
<h3 id="基于Receiver的方式"><a href="#基于Receiver的方式" class="headerlink" title="基于Receiver的方式"></a>基于Receiver的方式</h3><p><img src="//jiamaoxiang.top/2020/07/27/第四篇-Spark-Streaming编程指南/receiver.png" alt></p>
<p>Receiver的作用是从数据源收集数据，然后将数据传送给Spark Streaming。基本原理是：随着数据的不断到来，在相对应的batch interval时间间隔内，这些数据会被收集并且打包成block，只要等到batch interval时间完成了，收集的数据block会被发送给spark进行处理。</p>
<p>如上图：当Spark Streaming启动时，receiver开始收集数据。在<code>t0</code>的batch interval结束时(即收集完了该时间段内的数据)，收集到的block <strong>#0</strong>会被发送到Spark进行处理。在<code>t2</code>时刻，Spark会处理<code>t1</code>的batch interval的数据block，与此同时会不停地收集<code>t2</code>的batch interval对应的block<strong>#2</strong>。</p>
<p>常见的基于Receiver的数据源包括：Kafka, Kinesis, Flume,Twitter。除此之外，用户也可以通过继承 <strong>Receiver</strong>抽象类，实现<code>onStart()</code>与<code>onStop()</code>两个方法，进行自定义Receiver。本文不会对基于Receiver的数据源做过多讨论，主要针对基于Direct的Kafka数据源进行详细解释。</p>
<h3 id="基于Direct的方式"><a href="#基于Direct的方式" class="headerlink" title="基于Direct的方式"></a>基于Direct的方式</h3><p>Spark 1.3中引入了这种新的无Receiver的Direct方法，以确保更强的端到端保证。该方法不是使用Receiver来接收数据，而是定期查询Kafka每个topic+partition中的最新偏移量，并相应地定义要在每个批次中处理的偏移量范围。启动用于处理数据的作业时，Kafka的简单consumer API用于读取Kafka定义的偏移量范围（类似于从文件系统读取文件）。请注意，此功能是在Scala和Java API的Spark 1.3引入的，在Python API的Spark 1.4中引入的。</p>
<p>基于Direct的方式具有以下优点：</p>
<ul>
<li><strong>简化并行读取</strong></li>
</ul>
<p>如果要读取多个partition，不需要创建多个输入DStream然后对他们进行union操作。Spark会创建跟Kafka partition一样多的RDD partition，并且会并行从kafka中读取数据。所以在kafka partition和RDD partition之间，有一一对应的关系。</p>
<ul>
<li><strong>高性能</strong></li>
</ul>
<p>如果要保证数据零丢失，在基于Receiver的方式中，需要开启WAL机制。这种方式其实效率很低，因为数据实际被复制了两份，kafka自己本身就有高可靠的机制，会对数据复制一份，而这里又会复制一份到WAL中。而基于Direct的方式，不依赖于Receiver，不需要开启WAL机制，只要kafka中做了数据的复制，那么就可以通过kafka的副本进行恢复。</p>
<ul>
<li><strong>Exactly-once语义</strong></li>
</ul>
<p>基于Receiver的方式，使用kafka的高阶API来在Zookeeper中保存消费过的offset。这是消费kafka数据的传统方式。这种方式配合WAL机制，可以保证数据零丢失的高可靠性，但是却无法保证Exactly-once语义(Spark和Zookeeper之间可能是不同步的)。基于Direct的方式，使用kafka的简单API，Spark Streaming自己就负责追踪消费的offset，并保存在checkpoint中。Spark自己一定是同步的，因此可以保证数据时消费一次且仅消费一次。</p>
<h3 id="Spark-Streaming集成kafka"><a href="#Spark-Streaming集成kafka" class="headerlink" title="Spark Streaming集成kafka"></a>Spark Streaming集成kafka</h3><h4 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h4><p>使用<strong>KafkaUtils</strong>添加Kafka数据源，源码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDirectStream</span></span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">    ssc: <span class="type">StreamingContext</span>,</span><br><span class="line">    locationStrategy: <span class="type">LocationStrategy</span>,</span><br><span class="line">    consumerStrategy: <span class="type">ConsumerStrategy</span>[<span class="type">K</span>, <span class="type">V</span>]</span><br><span class="line">  ): <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">K</span>, <span class="type">V</span>]] = &#123;</span><br><span class="line">  <span class="keyword">val</span> ppc = <span class="keyword">new</span> <span class="type">DefaultPerPartitionConfig</span>(ssc.sparkContext.getConf)</span><br><span class="line">  createDirectStream[<span class="type">K</span>, <span class="type">V</span>](ssc, locationStrategy, consumerStrategy, ppc)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>具体参数解释：</p>
<ul>
<li><p><strong>K</strong>：Kafka消息key的类型</p>
</li>
<li><p><strong>V</strong>：Kafka消息value的类型</p>
</li>
<li><p><strong>ssc</strong>：StreamingContext</p>
</li>
<li><p><strong>locationStrategy</strong>: LocationStrategy，根据Executor中的主题的分区来调度consumer，即尽可能地让consumer靠近leader partition。该配置可以提升性能，但对于location的选择只是一种参考，并不是绝对的。可以选择如下方式：</p>
<ul>
<li>PreferBrokers：Spark和Kafka运行在同一个节点上，可以使用此种方式</li>
<li>PreferConsistent：大部分情况使用此方式，它将一致地在所有Executor之间分配分区</li>
<li>PreferFixed：将特定的主题分区放置到特定的主机上，在数据负载不均衡时使用</li>
</ul>
<p><strong>注意</strong>：多数情况下使用PreferConsisten，其他两种方式只是在特定的场景使用。这种配置只是一种参考，具体的情况还是会根据集群的资源自动调整。</p>
</li>
<li><p><strong>consumerStrategy</strong>：消费策略，主要有下面三种方式：</p>
<ul>
<li>Subscribe：订阅指定主题名称的主题集合</li>
<li>SubscribePattern：通过正则匹配，订阅相匹配的主题数据</li>
<li>Assign：订阅一个主题+分区的集合</li>
</ul>
<p><strong>注意</strong>：大多数情况下使用Subscribe方式。</p>
</li>
</ul>
<h4 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h4>  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TolerateWCTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createContext</span></span>(checkpointDirectory: <span class="type">String</span>): <span class="type">StreamingContext</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .set(<span class="string">"spark.streaming.backpressure.enabled"</span>, <span class="string">"true"</span>)</span><br><span class="line">      <span class="comment">//每秒钟从kafka分区中读取的records数量,默认not set</span></span><br><span class="line">      .set(<span class="string">"spark.streaming.kafka.maxRatePerPartition"</span>, <span class="string">"1000"</span>) <span class="comment">//</span></span><br><span class="line">      <span class="comment">//Driver为了获取每个leader分区的最近offsets，连续进行重试的次数，</span></span><br><span class="line">      <span class="comment">//默认是1，表示最多重试2次，仅仅适用于 new Kafka direct stream API</span></span><br><span class="line">      .set(<span class="string">"spark.streaming.kafka.maxRetries"</span>, <span class="string">"2"</span>)</span><br><span class="line">      .setAppName(<span class="string">"TolerateWCTest"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    ssc.checkpoint(checkpointDirectory)</span><br><span class="line">    <span class="keyword">val</span> topic = <span class="type">Array</span>(<span class="string">"testkafkasource2"</span>)</span><br><span class="line">    <span class="keyword">val</span> kafkaParam = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="string">"bootstrap.servers"</span> -&gt; <span class="string">"kms-1:9092"</span>,</span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">"group.id"</span> -&gt; <span class="string">"group0"</span>,</span><br><span class="line">      <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"latest"</span>, <span class="comment">//默认latest，</span></span><br><span class="line">      <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)) <span class="comment">//默认true,false:手动提交</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lines = <span class="type">KafkaUtils</span>.createDirectStream(</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topic, kafkaParam))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.value().split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordDstream = words.map(x =&gt; (x, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> stateDstream = wordDstream.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    stateDstream.cache()</span><br><span class="line">    <span class="comment">//参照batch interval设置，</span></span><br><span class="line">    <span class="comment">//不得低于batch interval，否则会报错，</span></span><br><span class="line">    <span class="comment">//设为batch interval的2倍</span></span><br><span class="line">    stateDstream.checkpoint(<span class="type">Seconds</span>(<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把DStream保存到MySQL数据库中</span></span><br><span class="line">    stateDstream.foreachRDD(rdd =&gt;</span><br><span class="line">      rdd.foreachPartition &#123; record =&gt;</span><br><span class="line">        <span class="keyword">var</span> conn: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> stmt: <span class="type">PreparedStatement</span> = <span class="literal">null</span></span><br><span class="line">        <span class="comment">// 给每个partition，获取一个连接</span></span><br><span class="line">        conn = <span class="type">ConnectionPool</span>.getConnection</span><br><span class="line">        <span class="comment">// 遍历partition中的数据，使用一个连接，插入数据库</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (record.hasNext) &#123;</span><br><span class="line">          <span class="keyword">val</span> wordcounts = record.next()</span><br><span class="line">          <span class="keyword">val</span> sql = <span class="string">"insert into wctbl(word,count) values (?,?)"</span></span><br><span class="line">          stmt = conn.prepareStatement(sql);</span><br><span class="line">          stmt.setString(<span class="number">1</span>, wordcounts._1.trim)</span><br><span class="line">          stmt.setInt(<span class="number">2</span>, wordcounts._2.toInt)</span><br><span class="line">          stmt.executeUpdate()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 用完以后，将连接还回去</span></span><br><span class="line">        <span class="type">ConnectionPool</span>.returnConnection(conn)</span><br><span class="line">      &#125;)</span><br><span class="line">    ssc</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> checkpointDirectory = <span class="string">"hdfs://kms-1:8020/docheckpoint"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="type">StreamingContext</span>.getOrCreate(</span><br><span class="line">      checkpointDirectory,</span><br><span class="line">      () =&gt; createContext(checkpointDirectory))</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Spark-Streaming-数据汇-Sinks"><a href="#Spark-Streaming-数据汇-Sinks" class="headerlink" title="Spark Streaming 数据汇(Sinks)"></a>Spark Streaming 数据汇(Sinks)</h2><h3 id="Output-Operation介绍"><a href="#Output-Operation介绍" class="headerlink" title="Output Operation介绍"></a>Output Operation介绍</h3><p>Spark Streaming提供了下面内置的Output Operation，如下：</p>
<ul>
<li><strong>print</strong>()</li>
</ul>
<p>打印数据数据到标准输出，如果不传递参数，默认打印前10个元素</p>
<ul>
<li><strong>saveAsTextFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</li>
</ul>
<p>将DStream内容存储到文件系统，每个batch interval的文件名称为`<em>prefix-TIME_IN_MS[.suffix]</em></p>
<ul>
<li><strong>saveAsObjectFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</li>
</ul>
<p>将DStream的内容保存为序列化的java对象的SequenceFile，每个batch interval的文件名称为<code>prefix-TIME_IN_MS[.suffix]</code>,Python API不支持此方法。</p>
<ul>
<li><strong>saveAsHadoopFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</li>
</ul>
<p>将DStream内容保存为Hadoop文件，每个batch interval的文件名称为<code>prefix-TIME_IN_MS[.suffix]</code>,Python API不支持此方法。</p>
<ul>
<li><strong>foreachRDD</strong>(<em>func</em>)</li>
</ul>
<p>通用的数据输出算子，func函数将每个RDD的数据输出到外部存储设备，比如将RDD写入到文件或者数据库。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Apply a function to each RDD in this DStream. This is an output operator, so</span></span><br><span class="line"><span class="comment">  * 'this' DStream will be registered as an output stream and therefore materialized.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">foreachRDD</span></span>(foreachFunc: <span class="type">RDD</span>[<span class="type">T</span>] =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = ssc.withScope &#123;</span><br><span class="line">   <span class="keyword">val</span> cleanedF = context.sparkContext.clean(foreachFunc, <span class="literal">false</span>)</span><br><span class="line">   foreachRDD((r: <span class="type">RDD</span>[<span class="type">T</span>], _: <span class="type">Time</span>) =&gt; cleanedF(r), displayInnerRDDOps = <span class="literal">true</span>)</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Apply a function to each RDD in this DStream. This is an output operator, so</span></span><br><span class="line"><span class="comment">  * 'this' DStream will be registered as an output stream and therefore materialized.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">foreachRDD</span></span>(foreachFunc: (<span class="type">RDD</span>[<span class="type">T</span>], <span class="type">Time</span>) =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = ssc.withScope &#123;</span><br><span class="line">   <span class="comment">// because the DStream is reachable from the outer object here, and because</span></span><br><span class="line">   <span class="comment">// DStreams can't be serialized with closures, we can't proactively check</span></span><br><span class="line">   <span class="comment">// it for serializability and so we pass the optional false to SparkContext.clean</span></span><br><span class="line">   foreachRDD(foreachFunc, displayInnerRDDOps = <span class="literal">true</span>)</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">foreachRDD</span></span>(</span><br><span class="line">     foreachFunc: (<span class="type">RDD</span>[<span class="type">T</span>], <span class="type">Time</span>) =&gt; <span class="type">Unit</span>,</span><br><span class="line">     displayInnerRDDOps: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">   <span class="keyword">new</span> <span class="type">ForEachDStream</span>(<span class="keyword">this</span>,</span><br><span class="line">     context.sparkContext.clean(foreachFunc, <span class="literal">false</span>), displayInnerRDDOps).register()</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p><strong>foreachRDD</strong>是一个非常重要的操作，用户可以使用它将处理的数据输出到外部存储设备。关于foreachRDD的使用，需要特点别注意一些细节问题。具体分析如下：</p>
<p>如果将数据写入到MySQL，需要获取连接Connection。用户可能不经意的在Spark Driver中创建一个连接对象，然后在Work中使用它将数据写入外部设备，代码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">dstream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  <span class="keyword">val</span> connection = createNewConnection()  <span class="comment">// ①注意：该段代码在driver上执行</span></span><br><span class="line">  rdd.foreach &#123; record =&gt;</span><br><span class="line">    connection.send(record) <span class="comment">// ②注意：该段代码在worker上执行</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>尖叫提示：上面的使用方式是错误的，因为需要将connection对象进行序列化，然后发送到driver节点，而这种connection对象是不能被序列化，所以不能跨节点传输。上面代码会报序列化错误，正确的使用方式是在worker节点创建connection，即在<code>rdd.foreach</code>内部创建connection。方式如下：</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">dstream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  rdd.foreach &#123; record =&gt;</span><br><span class="line">    <span class="keyword">val</span> connection = createNewConnection()</span><br><span class="line">    connection.send(record)</span><br><span class="line">    connection.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面的方式解决了不能序列化的问题，但是会为每个RDD的record创建一个connection，通常创建一个connection对象是会存在一定性能开销的，所以频繁创建和销毁connection对象会造成整体的吞吐量降低。一个比较好的做法是将<code>rdd.foreach</code>替换为``rdd.foreachPartition<code></code>,这样就不用频繁为每个record创建connection，而是为RDD的partition创建connection，大大减少了创建connection带来的开销。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">dstream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  rdd.foreachPartition &#123; partitionOfRecords =&gt;</span><br><span class="line">    <span class="keyword">val</span> connection = createNewConnection()</span><br><span class="line">    partitionOfRecords.foreach(record =&gt; connection.send(record))</span><br><span class="line">    connection.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其实上面的使用方式还可以进一步优化，可以通过在多个RDD或者批数据间重用连接对象。用户可以维护一个静态的连接对象池，重复使用池中的对象将多批次的RDD推送到外部系统，以进一步节省开销：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">dstream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  rdd.foreachPartition &#123; partitionOfRecords =&gt;</span><br><span class="line">    <span class="keyword">val</span> connection = <span class="type">ConnectionPool</span>.getConnection()</span><br><span class="line">    partitionOfRecords.foreach(record =&gt; connection.send(record))</span><br><span class="line">    <span class="type">ConnectionPool</span>.returnConnection(connection)  </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="使用案例-1"><a href="#使用案例-1" class="headerlink" title="使用案例"></a>使用案例</h3><ul>
<li>模拟数据库连接池</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 简易版的连接池</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConnectionPool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 静态的Connection队列</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> LinkedList&lt;Connection&gt; connectionQueue;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 加载驱动</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Class.forName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ClassNotFoundException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取连接，多线程访问并发控制</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (connectionQueue == <span class="keyword">null</span>) &#123;</span><br><span class="line">                connectionQueue = <span class="keyword">new</span> LinkedList&lt;Connection&gt;();</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">                    Connection conn = DriverManager.getConnection(<span class="string">"jdbc:mysql://localhost:3306/wordcount"</span>, <span class="string">"root"</span>,</span><br><span class="line">                            <span class="string">"123qwe"</span>);</span><br><span class="line">                    connectionQueue.push(conn);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> connectionQueue.poll();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 用完之后，返回一个连接</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">returnConnection</span><span class="params">(Connection conn)</span> </span>&#123;</span><br><span class="line">        connectionQueue.push(conn);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>实时统计写入MySQL</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"NetworkWordCount"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">    wordCounts.print()</span><br><span class="line">    <span class="comment">// 存储到MySQL</span></span><br><span class="line">    wordCounts.foreachRDD &#123; rdd =&gt;</span><br><span class="line">      rdd.foreachPartition &#123; partition =&gt;</span><br><span class="line">        <span class="keyword">var</span> conn: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> stmt: <span class="type">PreparedStatement</span> = <span class="literal">null</span></span><br><span class="line">        <span class="comment">// 给每个partition，获取一个连接</span></span><br><span class="line">        conn = <span class="type">ConnectionPool</span>.getConnection</span><br><span class="line">        <span class="comment">// 遍历partition中的数据，使用一个连接，插入数据库</span></span><br><span class="line">        <span class="keyword">while</span> (partition.hasNext) &#123;</span><br><span class="line">          <span class="keyword">val</span> wordcounts = partition.next()</span><br><span class="line">          <span class="keyword">val</span> sql = <span class="string">"insert into wctbl(word,count) values (?,?)"</span></span><br><span class="line">          stmt = conn.prepareStatement(sql);</span><br><span class="line">          stmt.setString(<span class="number">1</span>, wordcounts._1.trim)</span><br><span class="line">          stmt.setInt(<span class="number">2</span>, wordcounts._2.toInt)</span><br><span class="line">          stmt.executeUpdate()</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 用完以后，将连接还回去</span></span><br><span class="line">        <span class="type">ConnectionPool</span>.returnConnection(conn)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>由于篇幅限制，本文主要对Spark Streaming执行机制、Transformations与Output Operations、Spark Streaming数据源(Sources)、Spark Streaming 数据汇(Sinks)进行了讨论。下一篇将分享<strong>基于时间的窗口操作</strong>、<strong>有状态的计算</strong>、<strong>检查点Checkpoint</strong>、<strong>性能调优</strong>等内容。</p>
<blockquote>
<p>公众号『大数据技术与数仓』，回复『资料』领取大数据资料包</p>
</blockquote>
</div><div class="recommended_posts"><h3>相关推荐 ☟</h3><li><a href="https://jiamaoxiang.top/2020/07/31/第六篇-Spark-MLLib机器学习/" target="_blank">第六篇|Spark MLLib机器学习(1)</a></li><li><a href="https://jiamaoxiang.top/2020/07/29/第五篇-Spark-Streaming编程指南-2/" target="_blank">第五篇|Spark-Streaming编程指南(2)</a></li><li><a href="https://jiamaoxiang.top/2020/07/23/第三篇-Spark-SQL编程指南/" target="_blank">第三篇|Spark SQL编程指南</a></li><li><a href="https://jiamaoxiang.top/2020/07/18/第二篇-Spark-core编程指南/" target="_blank">第二篇|Spark core编程指南</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Jia MaoXiang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2020/07/27/第四篇-Spark-Streaming编程指南/">https://jiamaoxiang.top/2020/07/27/第四篇-Spark-Streaming编程指南/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本文为博主原创文章，遵循CC BY-SA 4.0版权协议，转载请附上原文出处链接和本声明</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://jiamaoxiang.top/2020/07/27/第四篇-Spark-Streaming编程指南/" data-id="ckkb752or007g187qzl113w85" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACsUlEQVR42u3aS27DMAwFwNz/0u0BGjuPpGgExXhl5KNovJCYR71e8fVzcd1//v7+78j3r9//4ujCw8PDOzT1v0MnvPvpXo02H+FyVnh4eHhrvHxxv59i8koy6d4mgYeHh/edvLy0TcrufJPAw8PD+0+8pPxNiuZ8M8DDw8P7Bl4SRiQLev7dHJyHI3h4eHjP8KrF8Tfcr/T38PDw8MZd9Wq7qxrCTl6PZouHh4e3wMsX3GrZmsev1ag3/108PDy8DV5vga42/vMxq+HCh+NZeHh4eGu8U7FpNd5NeL3SPOrs4eHh4bV41Sw0OQqQTK5XZFdHxsPDw9vjVZfvpD2WHynIR+6V8nh4eHhneRNYUkBXJz15QJf9PTw8PLw1XlK8TkrbJMDND36NOnt4eHh4A16y0FcPP/XK5aR0TqKQy9Hw8PDwFnjzsDU5EFCdUN42K2fSeHh4eC1edXGvRgzV5tmkcH9zj4eHh7fAq5a2kzhgfliq15zDw8PDO8tLFtbqQlwNansbUvSw8PDw8JZ5eZzaOy7QizxGwQceHh7eI7w8hO0lpfkmkbffmicj8PDw8I7yksg1+fmzYUd5s8HDw8Nb4FUZeYybfLe3YeRxBh4eHt4GLyl/8y3h7Emnaoz75l08PDy8Bd5GzJo/oPnUm8cR8PDw8Ma8Xvxa3UImY442Azw8PLzHedVAtneMoBrmFoISPDw8vMd5van0pth7QB8CXzw8PLyjvJ/iNYl3q8FE0paLTkbg4eHhHeVN9pNqm78XSfTu8fDw8LZ5yWZwNnrIjxTkHbwPZ8rw8PDwFnh5w2le7FZheRBceBsPDw/vcV4e1FbL7rOfxMPDw/sG3j043zaqD6gXNOPh4eHt8fLCdyOkmEe9HzYGPDw8vKO8yR/+3iSSkOJUxIyHh4d3lPcLg04NAX3gWTcAAAAASUVORK5CYII=">分享</a><div class="tags"><a href="/tags/Spark/">Spark</a></div><div class="post-nav"><a class="pre" href="/2020/07/29/第五篇-Spark-Streaming编程指南-2/">第五篇|Spark-Streaming编程指南(2)</a><a class="next" href="/2020/07/23/第三篇-Spark-SQL编程指南/">第三篇|Spark SQL编程指南</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar-toc"><div class="stoc-article" id="sidebar-stoc"><strong class="stoc-title"><i class="fa fa-list-ul"> 目录</i></strong><div class="toc-nav" id="stoc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Streaming介绍"><span class="toc-number">1.</span> <span class="toc-text">Spark Streaming介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#什么是DStream"><span class="toc-number">1.1.</span> <span class="toc-text">什么是DStream</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#计算模型"><span class="toc-number">1.2.</span> <span class="toc-text">计算模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-Streaming的工作机制"><span class="toc-number">1.3.</span> <span class="toc-text">Spark Streaming的工作机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#概览"><span class="toc-number">1.3.1.</span> <span class="toc-text">概览</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#执行细节"><span class="toc-number">1.3.2.</span> <span class="toc-text">执行细节</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-Streaming编程步骤"><span class="toc-number">1.4.</span> <span class="toc-text">Spark Streaming编程步骤</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformations与Output-Operations"><span class="toc-number">2.</span> <span class="toc-text">Transformations与Output Operations</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformations"><span class="toc-number">2.1.</span> <span class="toc-text">Transformations</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#无状态的transformation"><span class="toc-number">2.1.1.</span> <span class="toc-text">无状态的transformation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#有状态的transformation"><span class="toc-number">2.1.2.</span> <span class="toc-text">有状态的transformation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Output-Operations"><span class="toc-number">2.2.</span> <span class="toc-text">Output Operations</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Streaming数据源"><span class="toc-number">3.</span> <span class="toc-text">Spark Streaming数据源</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#基于Receiver的方式"><span class="toc-number">3.1.</span> <span class="toc-text">基于Receiver的方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基于Direct的方式"><span class="toc-number">3.2.</span> <span class="toc-text">基于Direct的方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-Streaming集成kafka"><span class="toc-number">3.3.</span> <span class="toc-text">Spark Streaming集成kafka</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#使用方式"><span class="toc-number">3.3.1.</span> <span class="toc-text">使用方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用案例"><span class="toc-number">3.3.2.</span> <span class="toc-text">使用案例</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Streaming-数据汇-Sinks"><span class="toc-number">4.</span> <span class="toc-text">Spark Streaming 数据汇(Sinks)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Output-Operation介绍"><span class="toc-number">4.1.</span> <span class="toc-text">Output Operation介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用案例-1"><span class="toc-number">4.2.</span> <span class="toc-text">使用案例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">5.</span> <span class="toc-text">总结</span></a></li></ol></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 - 2021 <a href="/." rel="nofollow">Jmx's Blog.</a>All rights reserved.<br>Thoughts on technology, life and everything else.</div></div></div><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" color="100,99,98" opacity="0.5" zindex="0.7" count="150" src="//lib.baomitu.com/canvas-nest.js/2.0.4/canvas-nest.umd.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>