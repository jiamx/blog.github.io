<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记录朴实无华且枯燥的生活"><title>第三篇|Spark SQL编程指南 | Jmx's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '912b6cfc43243cd27aeb428f7dbf7823';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">第三篇|Spark SQL编程指南</h1><a id="logo" href="/.">Jmx's Blog</a><p class="description">Keep it Simple and Stupid!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-pied-piper-alt"> 关于</i></a><a href="/tags"><i class="fa fa-tags"> 标签</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">第三篇|Spark SQL编程指南</h1><div class="post-meta">Jul 23, 2020<span> | </span><span class="category"><a href="/categories/Spark/">Spark</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 5.8k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 24</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><a id="more"></a>

<p>在<a href="https://mp.weixin.qq.com/s/z22uJwTnBxeZnYlCKIXzNQ" target="_blank" rel="noopener">《第二篇|Spark Core编程指南》</a>一文中，对Spark的核心模块进行了讲解。本文将讨论Spark的另外一个重要模块–Spark SQL，Spark SQL是在Shark的基础之上构建的，于2014年5月发布。从名称上可以看出，该模块是Spark提供的关系型操作API，实现了SQL-on-Spark的功能。对于一些熟悉SQL的用户，可以直接使用SQL在Spark上进行复杂的数据处理。通过本文，你可以了解到：</p>
<ul>
<li>Spark SQL简介</li>
<li>DataFrame API&amp;DataSet API</li>
<li>Catalyst Optimizer优化器</li>
<li>Spark SQL基本操作</li>
<li>Spark SQL的数据源</li>
<li>RDD与DataFrame相互转换</li>
<li>Thrift  server与Spark SQL CLI</li>
</ul>
<h2 id="Spark-SQL简介"><a href="#Spark-SQL简介" class="headerlink" title="Spark SQL简介"></a>Spark SQL简介</h2><p>Spark SQL是Spark的其中一个模块，用于结构化数据处理。与基本的Spark RDD API不同，Spark SQL提供的接口为Spark提供了有关数据结构和正在执行的计算的更多信息，Spark SQL会使用这些额外的信息来执行额外的优化。使用SparkSQL的方式有很多种，包括SQL、DataFrame API以及Dataset API。值得注意的是，无论使用何种方式何种语言，其执行引擎都是相同的。实现这种统一，意味着开发人员可以轻松地在不同的API之间来回切换，从而使数据处理更加地灵活。</p>
<h2 id="DataFrame-API-amp-DataSet-API"><a href="#DataFrame-API-amp-DataSet-API" class="headerlink" title="DataFrame API&amp;DataSet API"></a>DataFrame API&amp;DataSet API</h2><h3 id="DataFrame-API"><a href="#DataFrame-API" class="headerlink" title="DataFrame API"></a>DataFrame API</h3><p>DataFrame代表一个不可变的分布式数据集合，其核心目的是让开发者面对数据处理时，只关心要做什么，而不用关心怎么去做，将一些优化的工作交由Spark框架本身去处理。DataFrame是具有Schema信息的，也就是说可以被看做具有字段名称和类型的数据，类似于关系型数据库中的表，但是底层做了很多的优化。创建了DataFrame之后，就可以使用SQL进行数据处理。</p>
<p>用户可以从多种数据源中构造DataFrame，例如：结构化数据文件，Hive中的表，外部数据库或现有RDD。DataFrame API支持Scala，Java，Python和R，在Scala和Java中，row类型的DataSet代表DataFrame，即<code>Dataset[Row]</code>等同于DataFrame。</p>
<h3 id="DataSet-API"><a href="#DataSet-API" class="headerlink" title="DataSet API"></a>DataSet API</h3><p>DataSet是Spark 1.6中添加的新接口，是DataFrame的扩展，它具有RDD的优点（强类型输入，支持强大的lambda函数）以及Spark SQL的优化执行引擎的优点。可以通过JVM对象构建DataSet，然后使用函数转换（map<code>，</code>flatMap<code>，</code>filter)。值得注意的是，Dataset API在Scala和 Java中可用，Python不支持Dataset API。</p>
<p>另外，DataSet API可以减少内存的使用，由于Spark框架知道DataSet的数据结构，因此在持久化DataSet时可以节省很多的内存空间。</p>
<h2 id="Catalyst-Optimizer优化器"><a href="#Catalyst-Optimizer优化器" class="headerlink" title="Catalyst Optimizer优化器"></a>Catalyst Optimizer优化器</h2><p>在Catalyst中，存在两种类型的计划：</p>
<ul>
<li><strong>逻辑计划（Logical Plan）</strong>：定义数据集上的计算，尚未定义如何去执行计算。每个逻辑计划定义了一系列的用户代码所需要的属性(查询字段)和约束(where条件)，但是不定义该如何执行。具体如下图所示：</li>
</ul>
<p><img src="//jiamaoxiang.top/2020/07/23/第三篇-Spark-SQL编程指南/%E9%80%BB%E8%BE%91%E8%AE%A1%E5%88%92.png" alt></p>
<ul>
<li><strong>物理计划(Physical Plan)</strong>:物理计划是从逻辑计划生成的，定义了如何执行计算，是可执行的。举个栗子：逻辑计划中的JOIN会被转换为物理计划中的sort merge JOIN。需要注意，Spark会生成多个物理计划，然后选择成本最低的物理计划。具体如下图所示：</li>
</ul>
<p><img src="//jiamaoxiang.top/2020/07/23/第三篇-Spark-SQL编程指南/%E7%89%A9%E7%90%86%E8%AE%A1%E5%88%92.png" alt></p>
<p>在Spark SQL中，所有的算子操作会被转换成AST(abstract syntax tree,抽象语法树)，然后将其传递给Catalyst优化器。该优化器是在Scala的函数式编程基础会上构建的，Catalyst支持基于规则的(rule-based)和基于成本的(cost-based)优化策略。</p>
<p>Spark SQL的查询计划包括4个阶段(见下图)：</p>
<p><img src="//jiamaoxiang.top/2020/07/23/第三篇-Spark-SQL编程指南/%E6%89%A7%E8%A1%8C.png" alt></p>
<ul>
<li>1.分析</li>
<li>2.逻辑优化</li>
<li>3.物理计划</li>
<li>4.生成代码，将查询部分编译成Java字节码</li>
</ul>
<p><strong>注意：</strong>在物理计划阶段，Catalyst会生成多个计划，并且会计算每个计划的成本，然后比较这些计划的成本的大小，即基于成本的策略。在其他阶段，都是基于规则的的优化策略。</p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p><strong>Unresolved Logical plan –&gt; Logical plan</strong>。Spark SQL的查询计划首先起始于由SQL解析器返回的AST，或者是由API构建的DataFrame对象。在这两种情况下，都会存在未处理的属性引用(某个查询字段可能不存在，或者数据类型错误)，比如查询语句:<code>SELECT col FROM sales</code>,关于字段<code>col</code>的类型，或者该字段是否是一个有效的字段，只有等到查看该<code>sales</code>表时才会清楚。当不能确定一个属性字段的类型或者没能够与输入表进行匹配时，称之为<code>未处理的</code>。Spark SQL使用Catalyst的规则以及Catalog对象(能够访问数据源的表信息)来处理这些属性。首先会构建一个<strong>Unresolved Logical Plan</strong>树，然后作用一系列的规则，最后生成Logical Plan。</p>
<h3 id="逻辑优化"><a href="#逻辑优化" class="headerlink" title="逻辑优化"></a>逻辑优化</h3><p><strong>Logical plan –&gt; Optimized Logical Plan</strong>。逻辑优化阶段使用基于规则的优化策略，比如谓词下推、投影裁剪等。经过一些列优化过后，生成优化的逻辑计划Optimized Logical Plan。</p>
<h3 id="物理计划"><a href="#物理计划" class="headerlink" title="物理计划"></a>物理计划</h3><p><strong>Optimized Logical Plan –&gt;physical Plan</strong>。在物理计划阶段，Spark SQL会将优化的逻辑计划生成多个物理执行计划，然后使用Cost Model计算每个物理计划的成本，最终选择一个物理计划。在这个阶段，如果确定一张表很小(可以持久化到内存)，Spark SQL会使用broadcast join。</p>
<p>需要注意的是，物理计划器也会使用基于规则的优化策略，比如将投影、过滤操作管道化一个Spark的map算子。此外，还会将逻辑计划阶段的操作推到数据源端(支持谓词下推、投影下推)。</p>
<h3 id="代码生成"><a href="#代码生成" class="headerlink" title="代码生成"></a>代码生成</h3><p>查询优化的最终阶段是生成Java字节码，使用<strong>Quasi quotes</strong>来完成这项工作的。</p>
<p>经过上面的分析，对Catalyst Optimizer有了初步的了解。关于Spark的其他组件是如何与Catalyst Optimizer交互的呢？具体如下图所示：</p>
<p><img src="//jiamaoxiang.top/2020/07/23/第三篇-Spark-SQL编程指南/%E5%85%B6%E4%BB%96%E7%BB%84%E4%BB%B6.png" alt></p>
<p>如上图所示：ML Pipelines, Structured streaming以及 GraphFrames都使用了DataFrame/Dataset<br>APIs，并且都得益于 Catalyst optimiser。</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="创建SparkSession"><a href="#创建SparkSession" class="headerlink" title="创建SparkSession"></a>创建SparkSession</h3><p>SparkSession是Dataset与DataFrame API的编程入口，从Spark2.0开始支持。用于统一原来的HiveContext和SQLContext，为了兼容两者，仍然保留这两个入口。通过一个SparkSession入口，提高了Spark的易用性。下面的代码展示了如何创建一个SparkSession：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .appName(<span class="string">"Spark SQL basic example"</span>)</span><br><span class="line">  .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">  .getOrCreate()</span><br><span class="line"><span class="comment">//导入隐式转换，比如将RDD转为DataFrame</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure>

<h3 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h3><p>创建完SparkSession之后，可以使用SparkSession从已经存在的RDD、Hive表或者其他数据源中创建DataFrame。下面的示例使用的是从一个JSON文件数据源中创建DataFrame：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* &#123;"name":"Michael"&#125;</span></span><br><span class="line"><span class="comment">* &#123;"name":"Andy", "age":30&#125;</span></span><br><span class="line"><span class="comment">* &#123;"name":"Justin", "age":19&#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">"E://people.json"</span>)</span><br><span class="line"><span class="comment">//输出DataFrame的内容</span></span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure>

<h3 id="DataFrame基本操作"><a href="#DataFrame基本操作" class="headerlink" title="DataFrame基本操作"></a>DataFrame基本操作</h3><p>创建完DataFrame之后，可以对其进行一些列的操作，具体如下面代码所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 打印该DataFrame的信息</span></span><br><span class="line">df.printSchema()</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">// |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 查询name字段</span></span><br><span class="line">df.select(<span class="string">"name"</span>).show()</span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"><span class="comment">// |   name|</span></span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"><span class="comment">// |Michael|</span></span><br><span class="line"><span class="comment">// |   Andy|</span></span><br><span class="line"><span class="comment">// | Justin|</span></span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 将每个人的age + 1</span></span><br><span class="line">df.select($<span class="string">"name"</span>, $<span class="string">"age"</span> + <span class="number">1</span>).show()</span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |   name|(age + 1)|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |Michael|     null|</span></span><br><span class="line"><span class="comment">// |   Andy|       31|</span></span><br><span class="line"><span class="comment">// | Justin|       20|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 查找age大于21的人员信息</span></span><br><span class="line">df.filter($<span class="string">"age"</span> &gt; <span class="number">21</span>).show()</span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// |age|name|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// | 30|Andy|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 按照age分组，统计每种age的个数</span></span><br><span class="line">df.groupBy(<span class="string">"age"</span>).count().show()</span><br><span class="line"><span class="comment">// +----+-----+</span></span><br><span class="line"><span class="comment">// | age|count|</span></span><br><span class="line"><span class="comment">// +----+-----+</span></span><br><span class="line"><span class="comment">// |  19|    1|</span></span><br><span class="line"><span class="comment">// |null|    1|</span></span><br><span class="line"><span class="comment">// |  30|    1|</span></span><br><span class="line"><span class="comment">// +----+-----+</span></span><br></pre></td></tr></table></figure>

<h3 id="在程序中使用SQL查询"><a href="#在程序中使用SQL查询" class="headerlink" title="在程序中使用SQL查询"></a>在程序中使用SQL查询</h3><p>上面的操作使用的是<strong>DSL(domain-specific language)</strong>方式，还可以直接使用SQL对DataFrame进行操作，具体如下所示：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 将DataFrame注册为SQL的临时视图</span></span><br><span class="line"><span class="comment">// 该方法创建的是一个本地的临时视图，生命周期与其绑定的SparkSession会话相关</span></span><br><span class="line"><span class="comment">// 即如果创建该view的session结束了，该view也就消失了</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"><span class="keyword">val</span> sqlDF = spark.sql(<span class="string">"SELECT * FROM people"</span>)</span><br><span class="line">sqlDF.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure>

<h3 id="Global-Temporary-View"><a href="#Global-Temporary-View" class="headerlink" title="Global Temporary View"></a>Global Temporary View</h3><p>上面使用的是Temporary views的方式，该方式是Spark Session范围的。如果将创建的view可以在所有session之间共享，可以使用Global Temporary View的方式创建view，具体如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 将DataFrame注册为全局临时视图(global temporary view)</span></span><br><span class="line"><span class="comment">// 该方法创建的是一个全局的临时视图，生命周期与其绑定的Spark应用程序相关，</span></span><br><span class="line"><span class="comment">// 即如果应用程序结束，会自动被删除</span></span><br><span class="line"><span class="comment">// 全局临时视图是可以跨Spark Session的，系统保留的数据库名为`global_temp`</span></span><br><span class="line"><span class="comment">// 当查询时，必须要加上全限定名，如`SELECT * FROM global_temp.view1`</span></span><br><span class="line">df.createGlobalTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 全局临时视图默认的保留数据库为:`global_temp` </span></span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 全局临时视图支持跨Spark Session会话</span></span><br><span class="line">spark.newSession().sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure>

<h3 id="创建DataSet"><a href="#创建DataSet" class="headerlink" title="创建DataSet"></a>创建DataSet</h3><p>DataSet与RDD很类似，但是，RDD使用的Java的序列化器或者Kyro序列化，而DataSet使用的是Encoder对在网络间传输的对象进行序列化的。创建DataSet的示例如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">创建DataSet</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">caseClassDS</span> </span>= <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Andy"</span>, <span class="number">32</span>)).toDS()</span><br><span class="line">caseClassDS.show()</span><br><span class="line"><span class="comment">// +----+---+</span></span><br><span class="line"><span class="comment">// |name|age|</span></span><br><span class="line"><span class="comment">// +----+---+</span></span><br><span class="line"><span class="comment">// |Andy| 32|</span></span><br><span class="line"><span class="comment">// +----+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过导入Spark的隐式转换spark.implicits._</span></span><br><span class="line"><span class="comment">// 可以自动识别数据类型</span></span><br><span class="line"><span class="keyword">val</span> primitiveDS = <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).toDS()</span><br><span class="line">primitiveDS.map(_ + <span class="number">1</span>).collect() <span class="comment">// 返回: Array(2, 3, 4)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过调用as方法，DataFrame可以转为DataSet，</span></span><br><span class="line"><span class="keyword">val</span> path = <span class="string">"E://people.json"</span></span><br><span class="line"><span class="keyword">val</span> peopleDS = spark.read.json(path).as[<span class="type">Person</span>]</span><br><span class="line">peopleDS.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure>

<h2 id="RDD与DataFrame相互转换"><a href="#RDD与DataFrame相互转换" class="headerlink" title="RDD与DataFrame相互转换"></a>RDD与DataFrame相互转换</h2><p>Spark SQL支持两种不同的方式将RDD转换为DataFrame。第一种是使用反射来推断包含特定类型对象的RDD的模式，这种基于反射的方式可以提供更简洁的代码，如果在编写Spark应用程序时，已经明确了schema，可以使用这种方式。第二种方式是通过可编程接口来构建schema，然后将其应用于现有的RDD。此方式编写的代码更冗长，此种方式创建的DataFrame，直到运行时才知道该DataFrame的列及其类型。</p>
<p>下面案例的数据集如下people.txt：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Tom, 29</span><br><span class="line">Bob, 30</span><br><span class="line">Jack, 19</span><br></pre></td></tr></table></figure>

<h3 id="通过反射的方式"><a href="#通过反射的方式" class="headerlink" title="通过反射的方式"></a>通过反射的方式</h3><p>Spark SQL的Scala接口支持自动将包含样例类的RDD转换为DataFrame。样例类定义表的schema。通过反射读取样例类的参数名称，并映射成column的名称。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">object RDD2DF_m1 &#123;</span><br><span class="line">  <span class="comment">//创建样例类</span></span><br><span class="line">  <span class="function"><span class="keyword">case</span> class  <span class="title">Person</span><span class="params">(name: String, age: Int)</span></span></span><br><span class="line"><span class="function">  def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"RDD2DF_m1"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    Logger.getLogger(<span class="string">"org.apache.spark"</span>).setLevel(Level.OFF)</span><br><span class="line">    Logger.getLogger(<span class="string">"org.apache.hadoop"</span>).setLevel(Level.OFF)</span><br><span class="line">    runRDD2DF(spark)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> def <span class="title">runRDD2DF</span><span class="params">(spark: SparkSession)</span> </span>= &#123;</span><br><span class="line">    <span class="comment">//导入隐式转换,用于RDD转为DataFrame</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">//从文本文件中创建RDD，并将其转换为DataFrame</span></span><br><span class="line">    val peopleDF = spark.sparkContext</span><br><span class="line">      .textFile(<span class="string">"file:///E:/people.txt"</span>)</span><br><span class="line">      .map(_.split(<span class="string">","</span>))</span><br><span class="line">      .map(attributes =&gt; Person(attributes(<span class="number">0</span>), attributes(<span class="number">1</span>).trim.toInt))</span><br><span class="line">      .toDF()</span><br><span class="line">    <span class="comment">//将DataFrame注册成临时视图</span></span><br><span class="line">    peopleDF.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line">    <span class="comment">// 运行SQL语句</span></span><br><span class="line">    val teenagersDF = spark.sql(<span class="string">"SELECT name, age FROM people WHERE age BETWEEN 13 AND 19"</span>)</span><br><span class="line">    <span class="comment">// 使用字段索引访问列</span></span><br><span class="line">    teenagersDF.map(teenager =&gt; <span class="string">"Name: "</span> + teenager(<span class="number">0</span>)).show()</span><br><span class="line">    <span class="comment">// +----------+</span></span><br><span class="line">    <span class="comment">// |     value|</span></span><br><span class="line">    <span class="comment">// +----------+</span></span><br><span class="line">    <span class="comment">// |Name: Jack|</span></span><br><span class="line">    <span class="comment">// +----------+</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 通过字段名访问列</span></span><br><span class="line">    teenagersDF.map(teenager =&gt; <span class="string">"Name: "</span> + teenager.getAs[String](<span class="string">"name"</span>)).show()</span><br><span class="line">    <span class="comment">// +------------+</span></span><br><span class="line">    <span class="comment">// |       value|</span></span><br><span class="line">    <span class="comment">// +------------+</span></span><br><span class="line">    <span class="comment">// |Name: Jack|</span></span><br><span class="line">    <span class="comment">// +------------+</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="通过构建schema的方式"><a href="#通过构建schema的方式" class="headerlink" title="通过构建schema的方式"></a>通过构建schema的方式</h3><p>通过构建schema的方式创建DataFrame主要包括三步：</p>
<ul>
<li>1.从原始RDD创建Row类型的RDD</li>
<li>2.使用StructType，创建schema</li>
<li>3.通过createDataFrame方法将schema应用于Row类型的RDD </li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">object RDD2DF_m2 &#123;</span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"RDD2DF_m1"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    Logger.getLogger(<span class="string">"org.apache.spark"</span>).setLevel(Level.OFF)</span><br><span class="line">    Logger.getLogger(<span class="string">"org.apache.hadoop"</span>).setLevel(Level.OFF)</span><br><span class="line">    runRDD2DF(spark)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> def <span class="title">runRDD2DF</span><span class="params">(spark: SparkSession)</span> </span>= &#123;</span><br><span class="line">    <span class="comment">//导入隐式转换,用于RDD转为DataFrame</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">//创建原始RDD</span></span><br><span class="line">    val peopleRDD = spark.sparkContext.textFile(<span class="string">"E:/people.txt"</span>)</span><br><span class="line">    <span class="comment">//step 1 将原始RDD转换为ROW类型的RDD</span></span><br><span class="line">    val rowRDD = peopleRDD</span><br><span class="line">      .map(_.split(<span class="string">","</span>))</span><br><span class="line">      .map(attributes =&gt; Row(attributes(<span class="number">0</span>), attributes(<span class="number">1</span>).trim.toInt))</span><br><span class="line">    <span class="comment">//step 2 创建schema</span></span><br><span class="line">    val schema = StructType(Array(</span><br><span class="line">      StructField(<span class="string">"name"</span>, StringType, <span class="keyword">true</span>),</span><br><span class="line">      StructField(<span class="string">"age"</span>, IntegerType, <span class="keyword">true</span>)</span><br><span class="line">    ))</span><br><span class="line">    <span class="comment">//step 3 创建DF</span></span><br><span class="line">    val peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line">    <span class="comment">// 将DataFrame注册成临时视图</span></span><br><span class="line">    peopleDF.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line">    <span class="comment">// 运行SQL语句</span></span><br><span class="line">    val results = spark.sql(<span class="string">"SELECT name FROM people"</span>)</span><br><span class="line">    <span class="comment">// 使用字段索引访问列</span></span><br><span class="line">    results.map(attributes =&gt; <span class="string">"Name: "</span> + attributes(<span class="number">0</span>)).show()</span><br><span class="line">    <span class="comment">// +----------+</span></span><br><span class="line">    <span class="comment">// |     value|</span></span><br><span class="line">    <span class="comment">// +----------+</span></span><br><span class="line">    <span class="comment">// | Name: Tom|</span></span><br><span class="line">    <span class="comment">// | Name: Bob|</span></span><br><span class="line">    <span class="comment">// | Name: Jack|</span></span><br><span class="line">    <span class="comment">// +----------+</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Spark-SQL的数据源"><a href="#Spark-SQL的数据源" class="headerlink" title="Spark SQL的数据源"></a>Spark SQL的数据源</h2><p>Spark SQL支持通过DataFrame接口对各种数据源进行操作，可以使用关系转换以及临时视图对DataFrame进行操作。常见的数据源包括以下几种：</p>
<h3 id="文件数据源"><a href="#文件数据源" class="headerlink" title="文件数据源"></a>文件数据源</h3><ul>
<li><p>Parquet文件</p>
</li>
<li><p>JSON文件</p>
</li>
<li><p>CSV文件</p>
</li>
<li><p>ORC文件</p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runBasicDataSourceExample</span></span>(spark: <span class="type">SparkSession</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 读取parquet文件数据源,并将结果写入到parquet文件</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> usersDF = spark</span><br><span class="line">      .read</span><br><span class="line">      .load(<span class="string">"E://users.parquet"</span>)</span><br><span class="line">    usersDF.show()</span><br><span class="line">    <span class="comment">// 将DF保存到parquet文件</span></span><br><span class="line">    usersDF</span><br><span class="line">      .select(<span class="string">"name"</span>, <span class="string">"favorite_color"</span>)</span><br><span class="line">      .write</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">      .save(<span class="string">"E://namesAndFavColors.parquet"</span>)</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 读取json文件数据源,并将结果写入到parquet文件</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> peopleDF = spark</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">"json"</span>)</span><br><span class="line">      .load(<span class="string">"E://people.json"</span>)</span><br><span class="line">    peopleDF.show()</span><br><span class="line">    <span class="comment">// 将DF保存到parquet文件</span></span><br><span class="line">    peopleDF</span><br><span class="line">      .select(<span class="string">"name"</span>, <span class="string">"age"</span>)</span><br><span class="line">      .write</span><br><span class="line">      .format(<span class="string">"parquet"</span>)</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">      .save(<span class="string">"E://namesAndAges.parquet"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 读取CSV文件数据源</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="keyword">val</span> peopleDFCsv = spark.read.format(<span class="string">"csv"</span>)</span><br><span class="line">      .option(<span class="string">"sep"</span>, <span class="string">";"</span>)</span><br><span class="line">      .option(<span class="string">"inferSchema"</span>, <span class="string">"true"</span>)</span><br><span class="line">      .option(<span class="string">"header"</span>, <span class="string">"true"</span>)</span><br><span class="line">      .load(<span class="string">"E://people.csv"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 将usersDF写入到ORC文件</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    usersDF.write.format(<span class="string">"orc"</span>)</span><br><span class="line">      .option(<span class="string">"orc.bloom.filter.columns"</span>, <span class="string">"favorite_color"</span>)</span><br><span class="line">      .option(<span class="string">"orc.dictionary.key.threshold"</span>, <span class="string">"1.0"</span>)</span><br><span class="line">      .option(<span class="string">"orc.column.encoding.direct"</span>, <span class="string">"name"</span>)</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">      .save(<span class="string">"E://users_with_options.orc"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 将peopleDF保存为持久化表，一般保存为Hive中</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    peopleDF</span><br><span class="line">      .write</span><br><span class="line">      .option(<span class="string">"path"</span>,<span class="string">"E://warehouse/people_bucketed"</span>) <span class="comment">// 保存路径</span></span><br><span class="line">      .bucketBy(<span class="number">42</span>, <span class="string">"name"</span>)           <span class="comment">// 按照name字段分桶</span></span><br><span class="line">      .sortBy(<span class="string">"age"</span>)                  <span class="comment">// 按照age字段排序</span></span><br><span class="line">      .saveAsTable(<span class="string">"people_bucketed"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 将userDF保存为分区文件，类似于Hive分区表</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    usersDF</span><br><span class="line">      .write</span><br><span class="line">      .partitionBy(<span class="string">"favorite_color"</span>)  <span class="comment">// 分区字段</span></span><br><span class="line">      .format(<span class="string">"parquet"</span>)        <span class="comment">// 文件格式</span></span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>) <span class="comment">// 保存模式</span></span><br><span class="line">      .save(<span class="string">"E://namesPartByColor.parquet"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      *</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    usersDF</span><br><span class="line">      .write</span><br><span class="line">      .option(<span class="string">"path"</span>,<span class="string">"E://warehouse/users_partitioned_bucketed"</span>) <span class="comment">// 保存路径</span></span><br><span class="line">      .partitionBy(<span class="string">"favorite_color"</span>)  <span class="comment">// 分区</span></span><br><span class="line">      .bucketBy(<span class="number">42</span>, <span class="string">"name"</span>)           <span class="comment">// 分桶</span></span><br><span class="line">      .saveAsTable(<span class="string">"users_partitioned_bucketed"</span>)</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">"DROP TABLE IF EXISTS people_bucketed"</span>)</span><br><span class="line">    spark.sql(<span class="string">"DROP TABLE IF EXISTS users_partitioned_bucketed"</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p><strong>保存模式</strong></p>
<table>
<thead>
<tr>
<th align="left">Scala/Java</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>SaveMode.ErrorIfExists</code>(default)</td>
<td align="left">如果目标文件已经存在，则报异常</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Append</code></td>
<td align="left">如果目标文件或表已经存在，则将结果追加进去</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Overwrite</code></td>
<td align="left">如果目标文件或表已经存在，则覆盖原有的内容</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Ignore</code></td>
<td align="left">类似于SQL中的CREATE TABLE IF NOT EXISTS，如果目标文件或表已经存在，则不做任何操作</td>
</tr>
</tbody></table>
<p><strong>保存为持久化表</strong></p>
<p>DataFrame可以被保存为Hive的持久化表，值得注意的是，这种方式并不依赖与Hive的部署，也就是说Spark会使用Derby创建一个默认的本地Hive metastore，与createOrReplaceTempView不同，该方式会直接将结果物化。</p>
<p>对于基于文件的数据源( text, parquet, json等)，在保存的时候可以指定一个具体的路径，比如 df.write.option(“path”, “/some/path”).saveAsTable(“t”)(存储在指定路径下的文件格式为parquet)<code>。</code>当表被删除时，自定义的表的路径和表数据不会被移除。如果没有指定具体的路径，spark默认的是warehouse的目录(/user/hive/warehouse),当表被删除时，默认的表路径也会被删除。</p>
<h3 id="Hive数据源"><a href="#Hive数据源" class="headerlink" title="Hive数据源"></a>Hive数据源</h3><p>见下面小节：Spark SQL集成Hive</p>
<h3 id="JDBC数据源"><a href="#JDBC数据源" class="headerlink" title="JDBC数据源"></a>JDBC数据源</h3><p>Spark SQL还包括一个可以使用JDBC从其他数据库读取数据的数据源。与使用JdbcRDD相比，应优先使用此功能。这是因为结果作为DataFrame返回，它们可以在Spark SQL中轻松处理或与其他数据源连接。JDBC数据源也更易于使用Java或Python，因为它不需要用户提供ClassTag。</p>
<p> 可以使用Data Sources API将远程数据库中的表加载为DataFrame或Spark SQL临时视图。用户可以在数据源选项中指定JDBC连接属性。 user并且password通常作为用于登录数据源的连接属性提供。除连接属性外，Spark还支持以下不区分大小写的选项：</p>
<table>
<thead>
<tr>
<th align="left">属性名称</th>
<th align="left">解释</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>url</code></td>
<td align="left">要连接的JDBC URL</td>
</tr>
<tr>
<td align="left"><code>dbtable</code></td>
<td align="left">读取或写入的JDBC表</td>
</tr>
<tr>
<td align="left"><code>query</code></td>
<td align="left">指定查询语句</td>
</tr>
<tr>
<td align="left"><code>driver</code></td>
<td align="left">用于连接到该URL的JDBC驱动类名</td>
</tr>
<tr>
<td align="left"><code>partitionColumn, lowerBound, upperBound</code></td>
<td align="left">如果指定了这些选项，则必须全部指定。另外， <code>numPartitions</code>必须指定</td>
</tr>
<tr>
<td align="left"><code>numPartitions</code></td>
<td align="left">表读写中可用于并行处理的最大分区数。这也确定了并发JDBC连接的最大数量。如果要写入的分区数超过此限制，我们可以通过<code>coalesce(numPartitions)</code>在写入之前进行调用将其降低到此限制</td>
</tr>
<tr>
<td align="left"><code>queryTimeout</code></td>
<td align="left">默认为<code>0</code>，查询超时时间</td>
</tr>
<tr>
<td align="left"><code>fetchsize</code></td>
<td align="left">JDBC的获取大小，它确定每次要获取多少行。这可以帮助提高JDBC驱动程序的性能</td>
</tr>
<tr>
<td align="left"><code>batchsize</code></td>
<td align="left">默认为1000，JDBC批处理大小，这可以帮助提高JDBC驱动程序的性能。</td>
</tr>
<tr>
<td align="left"><code>isolationLevel</code></td>
<td align="left">事务隔离级别，适用于当前连接。它可以是一个<code>NONE</code>，<code>READ_COMMITTED</code>，<code>READ_UNCOMMITTED</code>，<code>REPEATABLE_READ</code>，或<code>SERIALIZABLE</code>，对应于由JDBC的连接对象定义，缺省值为标准事务隔离级别<code>READ_UNCOMMITTED</code>。此选项仅适用于写作。</td>
</tr>
<tr>
<td align="left"><code>sessionInitStatement</code></td>
<td align="left">在向远程数据库打开每个数据库会话之后，在开始读取数据之前，此选项将执行自定义SQL语句，使用它来实现会话初始化代码。</td>
</tr>
<tr>
<td align="left"><code>truncate</code></td>
<td align="left">这是与JDBC writer相关的选项。当<code>SaveMode.Overwrite</code>启用时，就会清空目标表的内容，而不是删除和重建其现有的表。默认为<code>false</code></td>
</tr>
<tr>
<td align="left"><code>pushDownPredicate</code></td>
<td align="left">用于启用或禁用谓词下推到JDBC数据源的选项。默认值为true，在这种情况下，Spark将尽可能将过滤器下推到JDBC数据源。</td>
</tr>
</tbody></table>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JdbcDatasetExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"JdbcDatasetExample"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>) <span class="comment">//设置为本地运行</span></span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.spark"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.hadoop"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line">    runJdbcDatasetExample(spark)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runJdbcDatasetExample</span></span>(spark: <span class="type">SparkSession</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//注意：从JDBC源加载数据</span></span><br><span class="line">    <span class="keyword">val</span> jdbcPersonDF = spark.read</span><br><span class="line">      .format(<span class="string">"jdbc"</span>)</span><br><span class="line">      .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://localhost/mydb"</span>)</span><br><span class="line">      .option(<span class="string">"dbtable"</span>, <span class="string">"person"</span>)</span><br><span class="line">      .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">      .option(<span class="string">"password"</span>, <span class="string">"123qwe"</span>)</span><br><span class="line">      .load()</span><br><span class="line">    <span class="comment">//打印jdbcDF的schema</span></span><br><span class="line">    jdbcPersonDF.printSchema()</span><br><span class="line">    <span class="comment">//打印数据</span></span><br><span class="line">    jdbcPersonDF.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> connectionProperties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    connectionProperties.put(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">    connectionProperties.put(<span class="string">"password"</span>, <span class="string">"123qwe"</span>)</span><br><span class="line">    <span class="comment">//通过.jdbc的方式加载数据</span></span><br><span class="line">    <span class="keyword">val</span> jdbcStudentDF = spark</span><br><span class="line">      .read</span><br><span class="line">      .jdbc(<span class="string">"jdbc:mysql://localhost/mydb"</span>, <span class="string">"student"</span>, connectionProperties)</span><br><span class="line">    <span class="comment">//打印jdbcDF的schema</span></span><br><span class="line">    jdbcStudentDF.printSchema()</span><br><span class="line">    <span class="comment">//打印数据</span></span><br><span class="line">    jdbcStudentDF.show()</span><br><span class="line">    <span class="comment">// 保存数据到JDBC源</span></span><br><span class="line">    jdbcStudentDF.write</span><br><span class="line">      .format(<span class="string">"jdbc"</span>)</span><br><span class="line">      .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://localhost/mydb"</span>)</span><br><span class="line">      .option(<span class="string">"dbtable"</span>, <span class="string">"student2"</span>)</span><br><span class="line">      .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">      .option(<span class="string">"password"</span>, <span class="string">"123qwe"</span>)</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Append</span>)</span><br><span class="line">      .save()</span><br><span class="line"></span><br><span class="line">    jdbcStudentDF</span><br><span class="line">      .write</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Append</span>)</span><br><span class="line">      .jdbc(<span class="string">"jdbc:mysql://localhost/mydb"</span>, <span class="string">"student2"</span>, connectionProperties)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Spark-SQL集成Hive"><a href="#Spark-SQL集成Hive" class="headerlink" title="Spark SQL集成Hive"></a>Spark SQL集成Hive</h2><p>Spark SQL还支持读取和写入存储在Apache Hive中的数据。但是，由于Hive具有大量依赖项，因此这些依赖项不包含在默认的Spark发布包中。如果可以在类路径上找到Hive依赖项，Spark将自动加载它们。请注意，这些Hive依赖项也必须存在于所有工作节点(worker nodes)上，因为它们需要访问Hive序列化和反序列化库（SerDes）才能访问存储在Hive中的数据。</p>
<p><strong>将hive-site.xml，core-site.xml以及hdfs-site.xml文件放在conf/下</strong>。</p>
<p>在使用Hive时，必须实例化一个支持Hive的SparkSession，包括连接到持久性Hive Metastore，支持Hive 的序列化、反序列化（serdes）和Hive用户定义函数。没有部署Hive的用户仍可以启用Hive支持。如果未配置hive-site.xml，则上下文(context)会在当前目录中自动创建metastore_db，并且会创建一个由spark.sql.warehouse.dir配置的目录，其默认目录为spark-warehouse，位于启动Spark应用程序的当前目录中。请注意，自Spark 2.0.0以来，该在hive-site.xml中的hive.metastore.warehouse.dir属性已被标记过时(deprecated)。使用spark.sql.warehouse.dir用于指定warehouse中的默认位置。可能需要向启动Spark应用程序的用户授予写入的权限。</p>
<p>​      下面的案例为在本地运行(为了方便查看打印的结果)，运行结束之后会发现在项目的目录下E:\IdeaProjects\myspark创建了spark-warehouse和metastore_db的文件夹。可以看出没有部署Hive的用户仍可以启用Hive支持，同时也可以将代码打包，放在集群上运行。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkHiveExample</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span>(<span class="params">key: <span class="type">Int</span>, value: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">def</span> <span class="title">main</span>(<span class="params">args: <span class="type">Array</span>[<span class="type">String</span>]</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"Spark Hive Example"</span>)</span><br><span class="line">      .config(<span class="string">"spark.sql.warehouse.dir"</span>, <span class="string">"e://warehouseLocation"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)<span class="comment">//设置为本地运行</span></span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.spark"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.hadoop"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">import</span> spark.sql</span><br><span class="line">    <span class="comment">//使用Spark SQL 的语法创建Hive中的表</span></span><br><span class="line">    sql(<span class="string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive"</span>)</span><br><span class="line">    sql(<span class="string">"LOAD DATA LOCAL INPATH 'file:///e:/kv1.txt' INTO TABLE src"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用HiveQL查询</span></span><br><span class="line">    sql(<span class="string">"SELECT * FROM src"</span>).show()</span><br><span class="line">    <span class="comment">// +---+-------+</span></span><br><span class="line">    <span class="comment">// |key|  value|</span></span><br><span class="line">    <span class="comment">// +---+-------+</span></span><br><span class="line">    <span class="comment">// |238|val_238|</span></span><br><span class="line">    <span class="comment">// | 86| val_86|</span></span><br><span class="line">    <span class="comment">// |311|val_311|</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 支持使用聚合函数</span></span><br><span class="line">    sql(<span class="string">"SELECT COUNT(*) FROM src"</span>).show()</span><br><span class="line">    <span class="comment">// +--------+</span></span><br><span class="line">    <span class="comment">// |count(1)|</span></span><br><span class="line">    <span class="comment">// +--------+</span></span><br><span class="line">    <span class="comment">// |    500 |</span></span><br><span class="line">    <span class="comment">// +--------+</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// SQL查询的结果是一个DataFrame，支持使用所有的常规的函数</span></span><br><span class="line">    <span class="keyword">val</span> sqlDF = sql(<span class="string">"SELECT key, value FROM src WHERE key &lt; 10 AND key &gt; 0 ORDER BY key"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// DataFrames是Row类型的, 允许你按顺序访问列.</span></span><br><span class="line">    <span class="keyword">val</span> stringsDS = sqlDF.map &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Row</span>(key: <span class="type">Int</span>, value: <span class="type">String</span>) =&gt; <span class="string">s"Key: <span class="subst">$key</span>, Value: <span class="subst">$value</span>"</span></span><br><span class="line">    &#125;</span><br><span class="line">    stringsDS.show()</span><br><span class="line">    <span class="comment">// +--------------------+</span></span><br><span class="line">    <span class="comment">// |               value|</span></span><br><span class="line">    <span class="comment">// +--------------------+</span></span><br><span class="line">    <span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line">    <span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line">    <span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//可以通过SparkSession使用DataFrame创建一个临时视图</span></span><br><span class="line">    <span class="keyword">val</span> recordsDF = spark.createDataFrame((<span class="number">1</span> to <span class="number">100</span>).map(i =&gt; <span class="type">Record</span>(i, <span class="string">s"val_<span class="subst">$i</span>"</span>)))</span><br><span class="line">    recordsDF.createOrReplaceTempView(<span class="string">"records"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//可以用DataFrame与Hive中的表进行join查询</span></span><br><span class="line">    sql(<span class="string">"SELECT * FROM records r JOIN src s ON r.key = s.key"</span>).show()</span><br><span class="line">    <span class="comment">// +---+------+---+------+</span></span><br><span class="line">    <span class="comment">// |key| value|key| value|</span></span><br><span class="line">    <span class="comment">// +---+------+---+------+</span></span><br><span class="line">    <span class="comment">// |  2| val_2|  2| val_2|</span></span><br><span class="line">    <span class="comment">// |  4| val_4|  4| val_4|</span></span><br><span class="line">    <span class="comment">// |  5| val_5|  5| val_5|</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建一个Parquet格式的hive托管表，使用的是HQL语法，没有使用Spark SQL的语法("USING hive")</span></span><br><span class="line">    sql(<span class="string">"CREATE TABLE IF NOT EXISTS hive_records(key int, value string) STORED AS PARQUET"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//读取Hive中的表，转换成了DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.table(<span class="string">"src"</span>)</span><br><span class="line">    <span class="comment">//将该DataFrame保存为Hive中的表，使用的模式(mode)为复写模式(Overwrite)</span></span><br><span class="line">    <span class="comment">//即如果保存的表已经存在，则会覆盖掉原来表中的内容</span></span><br><span class="line">    df.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).saveAsTable(<span class="string">"hive_records"</span>)</span><br><span class="line">    <span class="comment">// 查询表中的数据</span></span><br><span class="line">    sql(<span class="string">"SELECT * FROM hive_records"</span>).show()</span><br><span class="line">    <span class="comment">// +---+-------+</span></span><br><span class="line">    <span class="comment">// |key|  value|</span></span><br><span class="line">    <span class="comment">// +---+-------+</span></span><br><span class="line">    <span class="comment">// |238|val_238|</span></span><br><span class="line">    <span class="comment">// | 86| val_86|</span></span><br><span class="line">    <span class="comment">// |311|val_311|</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置Parquet数据文件路径</span></span><br><span class="line">    <span class="keyword">val</span> dataDir = <span class="string">"/tmp/parquet_data"</span></span><br><span class="line">    <span class="comment">//spark.range(10)返回的是DataSet[Long]</span></span><br><span class="line">    <span class="comment">//将该DataSet直接写入parquet文件</span></span><br><span class="line">    spark.range(<span class="number">10</span>).write.parquet(dataDir)</span><br><span class="line">    <span class="comment">// 在Hive中创建一个Parquet格式的外部表</span></span><br><span class="line">    sql(<span class="string">s"CREATE EXTERNAL TABLE IF NOT EXISTS hive_ints(key int) STORED AS PARQUET LOCATION '<span class="subst">$dataDir</span>'"</span>)</span><br><span class="line">    <span class="comment">// 查询上面创建的表</span></span><br><span class="line">    sql(<span class="string">"SELECT * FROM hive_ints"</span>).show()</span><br><span class="line">    <span class="comment">// +---+</span></span><br><span class="line">    <span class="comment">// |key|</span></span><br><span class="line">    <span class="comment">// +---+</span></span><br><span class="line">    <span class="comment">// |  0|</span></span><br><span class="line">    <span class="comment">// |  1|</span></span><br><span class="line">    <span class="comment">// |  2|</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 开启Hive动态分区</span></span><br><span class="line">    spark.sqlContext.setConf(<span class="string">"hive.exec.dynamic.partition"</span>, <span class="string">"true"</span>)</span><br><span class="line">    spark.sqlContext.setConf(<span class="string">"hive.exec.dynamic.partition.mode"</span>, <span class="string">"nonstrict"</span>)</span><br><span class="line">    <span class="comment">// 使用DataFrame API创建Hive的分区表</span></span><br><span class="line">    df.write.partitionBy(<span class="string">"key"</span>).format(<span class="string">"hive"</span>).saveAsTable(<span class="string">"hive_part_tbl"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分区键‘key’将会在最终的schema中被移除</span></span><br><span class="line">    sql(<span class="string">"SELECT * FROM hive_part_tbl"</span>).show()</span><br><span class="line">    <span class="comment">// +-------+---+</span></span><br><span class="line">    <span class="comment">// |  value|key|</span></span><br><span class="line">    <span class="comment">// +-------+---+</span></span><br><span class="line">    <span class="comment">// |val_238|238|</span></span><br><span class="line">    <span class="comment">// | val_86| 86|</span></span><br><span class="line">    <span class="comment">// |val_311|311|</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Thrift-server与Spark-SQL-CLI"><a href="#Thrift-server与Spark-SQL-CLI" class="headerlink" title="Thrift  server与Spark SQL CLI"></a>Thrift  server与Spark SQL CLI</h2><p>可以使用JDBC/ODBC或者命令行访问Spark SQL，通过这种方式，用户可以直接使用SQL运行查询，而不用编写代码。</p>
<h3 id="Thrift-JDBC-ODBC-server"><a href="#Thrift-JDBC-ODBC-server" class="headerlink" title="Thrift JDBC/ODBC server"></a>Thrift JDBC/ODBC server</h3><p>Thrift JDBC/ODBC server与Hive的HiveServer2向对应，可以使用Beeline访问JDBC服务器。在Spark的sbin目录下存在start-thriftserver.sh脚本，使用此脚本启动JDBC/ODBC服务器：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/start-thriftserver.sh</span><br></pre></td></tr></table></figure>

<p>使用beeline访问JDBC/ODBC服务器,Beeline会要求提供用户名和密码,在非安全模式下，只需输入用户名和空白密码即可</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">beeline&gt; !connect jdbc:hive2://localhost:10000</span><br></pre></td></tr></table></figure>

<p><img src="//jiamaoxiang.top/2020/07/23/第三篇-Spark-SQL编程指南/thrift.png" alt></p>
<h3 id="Spark-SQL-CLI"><a href="#Spark-SQL-CLI" class="headerlink" title="Spark SQL CLI"></a>Spark SQL CLI</h3><p>Spark SQL CLI是在本地模式下运行Hive Metastore服务并执行从命令行输入的查询的便捷工具。请注意，Spark SQL CLI无法与Thrift JDBC服务器通信。</p>
<p>要启动Spark SQL CLI，只需要在Spark的bin目录中运行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./spark-sql</span><br></pre></td></tr></table></figure>

<p><img src="//jiamaoxiang.top/2020/07/23/第三篇-Spark-SQL编程指南/sparkSQL.png" alt></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要对Spark SQL进行了阐述，主要包括Spark SQL的介绍、DataFrame&amp;DataSet API基本使用、Catalyst Optimizer优化器的基本原理、Spark SQL编程、Spark SQL数据源以及与Hive集成、Thrift  server与Spark SQL CLI。下一篇将分享Spark Streaming编程指南。</p>
</div><div class="recommended_posts"><h3>相关推荐 ☟</h3><li><a href="https://jiamaoxiang.top/2020/07/29/第五篇-Spark-Streaming编程指南-2/" target="_blank">第五篇|Spark-Streaming编程指南(2)</a></li><li><a href="https://jiamaoxiang.top/2020/07/27/第四篇-Spark-Streaming编程指南/" target="_blank">第四篇|Spark Streaming编程指南(1)</a></li><li><a href="https://jiamaoxiang.top/2020/07/18/第二篇-Spark-core编程指南/" target="_blank">第二篇|Spark core编程指南</a></li><li><a href="https://jiamaoxiang.top/2020/07/14/第一篇-Spark概览/" target="_blank">第一篇|Spark概览</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Jia MaoXiang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2020/07/23/第三篇-Spark-SQL编程指南/">https://jiamaoxiang.top/2020/07/23/第三篇-Spark-SQL编程指南/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本文为博主原创文章，遵循CC BY-SA 4.0版权协议，转载请附上原文出处链接和本声明</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://jiamaoxiang.top/2020/07/23/第三篇-Spark-SQL编程指南/" data-id="ckdny48210050u07q1b1uuth3" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACsklEQVR42u3aUW7jMAwFwNz/0t0DdOO8R0rdLjD+MlJH1qiAxJB8veLr69v1/Nd3zz9/NxnzdePCw8PDW0/93bSeP/8+5uyNydIkz+Ph4eHd5r2bRLuhP0+lfTK5/2DBw8PD+wW8JMCdPfM8Ezw8PLz/ndcmFM7i8fDw8H4Dr02/JkneWVJjNs6BXAseHh5ezMurSL/n/kp9Dw8PD29dVd+0C7THQJ7+KGaLh4eHd4GXb7iz0lQbmu/DZTw8PLzbvOs/8uOERfJkXmbDw8PDu8fL2wLyz/NU7D5cjhYCDw8P7yjv1MEwY7QheDsyHh4e3j3eLHFwo/y/Kci9PRjw8PDwjvLyVtRT39onjutjDA8PD+8ob7Ot5+WxTZqjXbIopMbDw8Nb8DYTnU2xPSpmb/8QXuPh4eEd4rXBdJuoTZKzpwL612yd8PDw8Epe3iaVvyw/kZ5h7WFQ/D/x8PDwSl5Szt+XyvLDIG/JigJ3PDw8vGu8PIDOy1Gb8TdtXkVlDw8PD2/Ea3+5t20BsyVrE8RFAxYeHh7eUV6yQW+27Fki+FRzAx4eHt4pXsvIg+Zk698sXHTY4OHh4V3gbZKn++p8i0/e8pfn8fDw8I7y8pJS2yaVpxuS6bYpDzw8PLx7vIR0oLy0KKTlTQMf5omHh4d3lPdcpmrvk2u2xdeHAR4eHt4/4rXJ2f2Ym1atKNeCh4eHd5SXB9DtFj9LaswC9+Er8fDw8IJZfZVXm7Y4FazP0sd4eHh4N3hnk7DJouRp3H1CBA8PD+8erz0MTrVh5UmKNgTHw8PD+xnegcam+FDJGcP8yqa+h4eHh3eBN2sLmDUKtCNH3WR4eHh4P85rt+l862/BUeoWDw8P7xpvH/jmLVNtsjhfrLcHAx4eHt5RXrvdz0LqvOFg32SAh4eHd433B2c2AycDyY7TAAAAAElFTkSuQmCC">分享</a><div class="tags"><a href="/tags/Spark/">Spark</a></div><div class="post-nav"><a class="pre" href="/2020/07/27/第四篇-Spark-Streaming编程指南/">第四篇|Spark Streaming编程指南(1)</a><a class="next" href="/2020/07/18/第二篇-Spark-core编程指南/">第二篇|Spark core编程指南</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar-toc"><div class="stoc-article" id="sidebar-stoc"><strong class="stoc-title"><i class="fa fa-list-ul"> 目录</i></strong><div class="toc-nav" id="stoc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-SQL简介"><span class="toc-number">1.</span> <span class="toc-text">Spark SQL简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataFrame-API-amp-DataSet-API"><span class="toc-number">2.</span> <span class="toc-text">DataFrame API&amp;DataSet API</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#DataFrame-API"><span class="toc-number">2.1.</span> <span class="toc-text">DataFrame API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataSet-API"><span class="toc-number">2.2.</span> <span class="toc-text">DataSet API</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Catalyst-Optimizer优化器"><span class="toc-number">3.</span> <span class="toc-text">Catalyst Optimizer优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#分析"><span class="toc-number">3.1.</span> <span class="toc-text">分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#逻辑优化"><span class="toc-number">3.2.</span> <span class="toc-text">逻辑优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#物理计划"><span class="toc-number">3.3.</span> <span class="toc-text">物理计划</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#代码生成"><span class="toc-number">3.4.</span> <span class="toc-text">代码生成</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Quick-Start"><span class="toc-number">4.</span> <span class="toc-text">Quick Start</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#创建SparkSession"><span class="toc-number">4.1.</span> <span class="toc-text">创建SparkSession</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#创建DataFrame"><span class="toc-number">4.2.</span> <span class="toc-text">创建DataFrame</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataFrame基本操作"><span class="toc-number">4.3.</span> <span class="toc-text">DataFrame基本操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#在程序中使用SQL查询"><span class="toc-number">4.4.</span> <span class="toc-text">在程序中使用SQL查询</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Global-Temporary-View"><span class="toc-number">4.5.</span> <span class="toc-text">Global Temporary View</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#创建DataSet"><span class="toc-number">4.6.</span> <span class="toc-text">创建DataSet</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD与DataFrame相互转换"><span class="toc-number">5.</span> <span class="toc-text">RDD与DataFrame相互转换</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#通过反射的方式"><span class="toc-number">5.1.</span> <span class="toc-text">通过反射的方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#通过构建schema的方式"><span class="toc-number">5.2.</span> <span class="toc-text">通过构建schema的方式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-SQL的数据源"><span class="toc-number">6.</span> <span class="toc-text">Spark SQL的数据源</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#文件数据源"><span class="toc-number">6.1.</span> <span class="toc-text">文件数据源</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive数据源"><span class="toc-number">6.2.</span> <span class="toc-text">Hive数据源</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#JDBC数据源"><span class="toc-number">6.3.</span> <span class="toc-text">JDBC数据源</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-SQL集成Hive"><span class="toc-number">7.</span> <span class="toc-text">Spark SQL集成Hive</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Thrift-server与Spark-SQL-CLI"><span class="toc-number">8.</span> <span class="toc-text">Thrift  server与Spark SQL CLI</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Thrift-JDBC-ODBC-server"><span class="toc-number">8.1.</span> <span class="toc-text">Thrift JDBC/ODBC server</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-SQL-CLI"><span class="toc-number">8.2.</span> <span class="toc-text">Spark SQL CLI</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">9.</span> <span class="toc-text">总结</span></a></li></ol></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 - 2020 <a href="/." rel="nofollow">Jmx's Blog.</a>All rights reserved.<br>Thoughts on technology, life and everything else.</div></div></div><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" color="100,99,98" opacity="0.5" zindex="0.7" count="150" src="//lib.baomitu.com/canvas-nest.js/2.0.4/canvas-nest.umd.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>