<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记录朴实无华且枯燥的生活"><title>第二篇|Spark core编程指南 | Jmx's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '912b6cfc43243cd27aeb428f7dbf7823';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">第二篇|Spark core编程指南</h1><a id="logo" href="/.">Jmx's Blog</a><p class="description">Keep it Simple and Stupid!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-pied-piper-alt"> 关于</i></a><a href="/tags"><i class="fa fa-tags"> 标签</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">第二篇|Spark core编程指南</h1><div class="post-meta">Jul 18, 2020<span> | </span><span class="category"><a href="/categories/Spark/">Spark</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 6.2k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 25</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><a id="more"></a>

<p>在<a href="https://mp.weixin.qq.com/s/e37gLo-Mq_93U12DmFDyPQ" target="_blank" rel="noopener">《第一篇|Spark概览》</a>一文中，对Spark的整体面貌进行了阐述。本文将深入探究Spark的核心组件–<strong>Spark core</strong>，Spark Core是Spark平台的基础通用执行引擎，所有其他功能均建立在该引擎之上。它不仅提供了内存计算功能来提高速度，而且还提供了通用的执行模型以支持各种应用程序，另外，用户可以使用Java，Scala和Python API开发应用程序。Spark core是建立在统一的抽象RDD之上的，这使得Spark的各个组件可以随意集成，可以在同一个应用程序中使用不同的组件以完成复杂的大数据处理任务。本文主要讨论的内容有：</p>
<ul>
<li>什么是RDD<ul>
<li>RDD的设计初衷</li>
<li>RDD的基本概念与主要特点</li>
<li>宽依赖与窄依赖</li>
<li>stage划分与作业调度</li>
</ul>
</li>
<li>RDD操作算子<ul>
<li>Transformations</li>
<li>Actions</li>
</ul>
</li>
<li>共享变量<ul>
<li>广播变量</li>
<li>累加器</li>
</ul>
</li>
<li>持久化</li>
<li>综合案例</li>
</ul>
<h2 id="什么是RDD"><a href="#什么是RDD" class="headerlink" title="什么是RDD"></a>什么是RDD</h2><h3 id="设计初衷"><a href="#设计初衷" class="headerlink" title="设计初衷"></a>设计初衷</h3><p>RDD(Resilient Distributed Datasets)的设计之初是为了解决目前存在的一些计算框架对于两类应用场景的处理效率不高的问题，这两类应用场景是<strong>迭代式算法</strong>和<strong>交互式数据挖掘</strong>。在这两种应用场景中，通过将数据保存在内存中，可以将性能提高到几个数量级。对于<strong>迭代式算法</strong>而言，比如PageRank、K-means聚类、逻辑回归等，经常需要重用中间结果。另一种应用场景是<strong>交互式数据挖掘</strong>，比如在同一份数据集上运行多个即席查询。大部分的计算框架(比如Hadoop)，使用中间计算结果的方式是将其写入到一个外部存储设备(比如HDFS)，这会增加额外的负载(数据复制、磁盘IO和序列化)，由此会增加应用的执行时间。</p>
<p>RDD可以有效地支持多数应用中的数据重用，它是一种容错的、并行的数据结构，可以让用户显性地将中间结果持久化到内存中，并且可以通过分区来优化数据的存放，另外，RDD支持丰富的算子操作，用户可以很容易地使用这些算子对RDD进行操作。</p>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>一个RDD是一个分布式对象集合，其本质是一个只读的、分区的记录集合。每个RDD可以分成多个分区，不同的分区保存在不同的集群节点上(<strong>具体如下图所示</strong>)。RDD是一种高度受限的共享内存模型，即RDD是只读的分区记录集合，所以也就不能对其进行修改。只能通过两种方式创建RDD，一种是基于物理存储的数据创建RDD，另一种是通过在其他RDD上作用转换操作(transformation，比如map、filter、join等)得到新的RDD。</p>
<p><img src="//jiamaoxiang.top/2020/07/18/第二篇-Spark-core编程指南/RDD%E5%88%86%E5%B8%83.png" alt></p>
<p>RDD不需要被物化，它通过血缘关系(lineage)来确定其是从RDD计算得来的。另外，用户可以控制RDD的<strong>持久化</strong>和<strong>分区</strong>，用户可以将需要被重用的RDD进行持久化操作(比如内存、或者磁盘)以提高计算效率。也可以按照记录的key将RDD的元素分布在不同的机器上，比如在对两个数据集进行JOIN操作时，可以确保以相同的方式进行hash分区。</p>
<h3 id="主要特点"><a href="#主要特点" class="headerlink" title="主要特点"></a>主要特点</h3><ul>
<li><p><strong>基于内存</strong></p>
<p>RDD是位于内存中的对象集合。RDD可以存储在内存、磁盘或者内存加磁盘中，但是，Spark之所以速度快，是基于这样一个事实：数据存储在内存中，并且每个算子不会从磁盘上提取数据。</p>
</li>
<li><p><strong>分区</strong></p>
<p>分区是对逻辑数据集划分成不同的独立部分，分区是分布式系统性能优化的一种技术手段，可以减少网络流量传输，将相同的key的元素分布在相同的分区中可以减少shuffle带来的影响。RDD被分成了多个分区，这些分区分布在集群中的不同节点。</p>
</li>
<li><p><strong>强类型</strong></p>
<p>RDD中的数据是强类型的，当创建RDD的时候，所有的元素都是相同的类型，该类型依赖于数据集的数据类型。</p>
</li>
<li><p><strong>懒加载</strong></p>
<p>Spark的转换操作是懒加载模式，这就意味着只有在执行了action(比如count、collect等)操作之后，才会去执行一些列的算子操作。</p>
</li>
<li><p><strong>不可修改</strong></p>
<p>RDD一旦被创建，就不能被修改。只能从一个RDD转换成另外一个RDD。</p>
</li>
<li><p><strong>并行化</strong></p>
<p>RDD是可以被并行操作的，由于RDD是分区的，每个分区分布在不同的机器上，所以每个分区可以被并行操作。</p>
</li>
<li><p><strong>持久化</strong></p>
<p>由于RDD是懒加载的，只有action操作才会导致RDD的转换操作被执行，进而创建出相对应的RDD。对于一些被重复使用的RDD，可以对其进行持久化操作(比如将其保存在内存或磁盘中，Spark支持多种持久化策略)，从而提高计算效率。</p>
</li>
</ul>
<h3 id="宽依赖和窄依赖"><a href="#宽依赖和窄依赖" class="headerlink" title="宽依赖和窄依赖"></a>宽依赖和窄依赖</h3><p>RDD中不同的操作会使得不同RDD中的分区产不同的依赖，主要有两种依赖：<strong>宽依赖</strong>和<strong>窄依赖</strong>。宽依赖是指一个父RDD的一个分区对应一个子RDD的多个分区，窄依赖是指一个父RDD的分区对应与一个子RDD的分区，或者多个父RDD的分区对应一个子RDD分区。关于宽依赖与窄依赖，如下图所示：</p>
<p><img src="//jiamaoxiang.top/2020/07/18/第二篇-Spark-core编程指南/%E4%BE%9D%E8%B5%961.png" alt></p>
<h3 id="Stage划分"><a href="#Stage划分" class="headerlink" title="Stage划分"></a>Stage划分</h3><p>窄依赖会被划分到同一个stage中，这样可以以管道的形式迭代执行。宽依赖所依赖的分区一般有多个，所以需要跨节点传输数据。从容灾方面看，两种依赖的计算结果恢复的方式是不同的，窄依赖只需要恢复父RDD丢失的分区即可，而宽依赖则需要考虑恢复所有父RDD丢失的分区。</p>
<p>DAGScheduler会将Job的RDD划分到不同的stage中，并构建一个stage的依赖关系，即DAG。这样划分的目的是既可以保障没有依赖关系的stage可以并行执行，又可以保证存在依赖关系的stage顺序执行。stage主要分为两种类型，一种是<strong>ShuffleMapStage</strong>，另一种是<strong>ResultStage</strong>。其中ShuffleMapStage是属于上游的stage，而ResulStage属于最下游的stage，这意味着上游的stage先执行，最后执行ResultStage。</p>
<ul>
<li>ShuffleMapStage</li>
</ul>
<p>ShuffleMapStage是DAG调度流程的中间stage，它可以包含一个或者多个ShuffleMapTask，用与生成Shuffle的数据，ShuffleMapStage可以是ShuffleMapStage的前置stage，但一定是ResultStage的前置stage。部分源码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">ShuffleMapStage</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    id: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    rdd: <span class="type">RDD</span>[_],</span></span></span><br><span class="line"><span class="class"><span class="params">    numTasks: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    parents: <span class="type">List</span>[<span class="type">Stage</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">    firstJobId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    callSite: <span class="type">CallSite</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val shuffleDep: <span class="type">ShuffleDependency</span>[_, _, _],</span></span></span><br><span class="line"><span class="class"><span class="params">    mapOutputTrackerMaster: <span class="type">MapOutputTrackerMaster</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Stage</span>(<span class="params">id, rdd, numTasks, parents, firstJobId, callSite</span>) </span>&#123;</span><br><span class="line">      <span class="comment">// 省略代码</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>ResultStage</li>
</ul>
<p>ResultStage可以使用指定的函数对RDD中的分区进行计算并得到最终结果，ResultStage是最后执行的stage，比如打印数据到控制台，或者将数据写入到外部存储设备等。部分源码如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">ResultStage</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    id: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    rdd: <span class="type">RDD</span>[_],</span></span></span><br><span class="line"><span class="class"><span class="params">    val func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]</span>) <span class="title">=&gt;</span> <span class="title">_</span>,</span></span><br><span class="line"><span class="class">    <span class="title">val</span> <span class="title">partitions</span></span>: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">    parents: <span class="type">List</span>[<span class="type">Stage</span>],</span><br><span class="line">    firstJobId: <span class="type">Int</span>,</span><br><span class="line">    callSite: <span class="type">CallSite</span>)</span><br><span class="line">  <span class="keyword">extends</span> <span class="type">Stage</span>(id, rdd, partitions.length, parents, firstJobId, callSite) &#123;</span><br><span class="line"><span class="comment">// 省略代码</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面提到Spark通过分析各个RDD的依赖关系生成DAG，通过各个RDD中的分区之间的依赖关系来决定如何划分stage。具体的思路是：在DAG中进行反向解析，遇到宽依赖就断开、遇到窄依赖就把当前的RDD加入到当前的stage中。即将窄依赖划分到同一个stage中，从而形成一个pipeline，提升计算效率。所以一个DAG图可以划分为多个stage，每个stage都代表了一组关联的，相互之间没有shuffle依赖关系的任务组成的task集合，每个task集合会被提交到TaskScheduler进行调度处理，最终将任务分发到Executor中进行执行。</p>
<h3 id="Spark作业调度流程"><a href="#Spark作业调度流程" class="headerlink" title="Spark作业调度流程"></a>Spark作业调度流程</h3><p>Spark首先会对Job进行一系列的RDD转换操作，并通过RDD之间的依赖关系构建DAG(Direct Acyclic Graph,有向无环图)。然后根据RDD依赖关系将RDD划分到不同的stage中，每个stage按照partition的数量创建多个Task，最后将这些Task提交到集群的work节点上执行。具体流程如下图所示：</p>
<p><img src="//jiamaoxiang.top/2020/07/18/第二篇-Spark-core编程指南/%E8%B0%83%E5%BA%A6.png" alt></p>
<ul>
<li><p>1.构建DAG，将DAG提交到调度系统；</p>
</li>
<li><p>2.DAGScheduler负责接收DAG，并将DAG划分成多个stage，最后将每个stage中的Task以任务集合(TaskSet)的形式提交个TaskScheduler做下一步处理；</p>
</li>
<li><p>3.使用集群管理器分配资源与任务调度，对于失败的任务会有相应的重试机制。TaskScheduler负责从DAGScheduler接收TaskSet，然后会创建TaskSetManager对TaskSet进行管理，最后由SchedulerBackend对Task进行调度；</p>
</li>
<li><p>4.执行具体的任务，并将任务的中间结果和最终结果存入存储体系。</p>
</li>
</ul>
<h2 id="RDD操作算子"><a href="#RDD操作算子" class="headerlink" title="RDD操作算子"></a>RDD操作算子</h2><p>Spark提供了丰富的RDD操作算子，主要包括两大类：<strong>Transformation</strong>与<strong>Action</strong>，下面会对一些常见的算子进行说明。</p>
<h3 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h3><p>下面是一些常见的transformation操作，值得注意的是，对于普通的RDD，支持Scala、Java、Python和R的API，对于pairRDD，仅支持Scala和JavaAPI。下面将对一些常见的算子进行解释：</p>
<ul>
<li><strong>map</strong>(<em>func</em>)</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将每个元素传递到func函数中，并返回一个新的RDD</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.map(cleanF))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>filter</strong>(<em>func</em>)</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 筛选出满足func函数的元素，并返回一个新的RDD</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Boolean</span>): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">T</span>, <span class="type">T</span>](</span><br><span class="line">      <span class="keyword">this</span>,</span><br><span class="line">      (context, pid, iter) =&gt; iter.filter(cleanF),</span><br><span class="line">      preservesPartitioning = <span class="literal">true</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>flatMap</strong>(<em>func</em>)</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 首先对该RDD所有元素应用func函数，然后将结果打平，一个元素会映射到0或者多个元素，返回一个新RDD </span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.flatMap(cleanF))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>mapPartitions</strong>(<em>func</em>)</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 将func作用于该RDD的每个分区，返回一个新的RDD</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">      preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanedF = sc.clean(f)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>(</span><br><span class="line">      <span class="keyword">this</span>,</span><br><span class="line">      (context: <span class="type">TaskContext</span>, index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedF(iter),</span><br><span class="line">      preservesPartitioning)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>union</strong>(<em>otherDataset</em>)</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 返回一个新的RDD，包含两个RDD的元素，类似于SQL的UNION ALL</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">union</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    sc.union(<span class="keyword">this</span>, other)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>intersection</strong>(<em>otherDataset</em>)</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 返回一个新的RDD，包含两个RDD的交集</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">intersection</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">this</span>.map(v =&gt; (v, <span class="literal">null</span>)).cogroup(other.map(v =&gt; (v, <span class="literal">null</span>)))</span><br><span class="line">        .filter &#123; <span class="keyword">case</span> (_, (leftGroup, rightGroup)) =&gt; leftGroup.nonEmpty &amp;&amp; rightGroup.nonEmpty &#125;</span><br><span class="line">        .keys</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>distinct</strong>([<em>numPartitions</em>]))</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 返回一个新的RDD，对原RDD元素去重</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>(): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">   distinct(partitions.length)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>groupByKey</strong>([<em>numPartitions</em>])</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 将pairRDD按照key进行分组，该算子的性能开销较大，可以使用PairRDDFunctions.aggregateByKey</span></span><br><span class="line"><span class="comment">   *或者PairRDDFunctions.reduceByKey进行代替</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])] = self.withScope &#123;</span><br><span class="line">    groupByKey(defaultPartitioner(self))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>reduceByKey</strong>(<em>func</em>, [<em>numPartitions</em>])</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 使用reduce函数对每个key对应的值进行聚合，该算子会在本地先对每个mapper结果进行合并，然后再将结果发送到reducer，类似于MapReduce的combiner功能</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">    reduceByKey(defaultPartitioner(self), func)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>aggregateByKey</strong>(<em>zeroValue</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numPartitions</em>])</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 使用给定的聚合函数和初始值对每个key对应的value值进行聚合</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">aggregateByKey</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">V</span>) =&gt; <span class="type">U</span>,</span><br><span class="line">      combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)] = self.withScope &#123;</span><br><span class="line">    aggregateByKey(zeroValue, defaultPartitioner(self))(seqOp, combOp)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>sortByKey</strong>([<em>ascending</em>], [<em>numPartitions</em>])</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 按照key对RDD进行排序，所以每个分区的元素都是排序的</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sortByKey</span></span>(ascending: <span class="type">Boolean</span> = <span class="literal">true</span>, numPartitions: <span class="type">Int</span> = self.partitions.length)</span><br><span class="line">      : <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">val</span> part = <span class="keyword">new</span> <span class="type">RangePartitioner</span>(numPartitions, self, ascending)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](self, part)</span><br><span class="line">      .setKeyOrdering(<span class="keyword">if</span> (ascending) ordering <span class="keyword">else</span> ordering.reverse)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>join</strong>(<em>otherDataset</em>, [<em>numPartitions</em>])</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 将相同key的pairRDD JOIN在一起，返回(k, (v1, v2))tuple类型</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))] = self.withScope &#123;</span><br><span class="line">    join(other, defaultPartitioner(self, other))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numPartitions</em>])</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 将相同key的元素放在一组，返回的RDD类型为(K, (Iterable[V], Iterable[W1], Iterable[W2])</span></span><br><span class="line"><span class="comment">   * 第一个Iterable里面包含当前RDD的key对应的value值，第二个Iterable里面包含W1 RDD的key对应的    * value值，第三个Iterable里面包含W2 RDD的key对应的value值</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W1</span>, <span class="type">W2</span>](other1: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W1</span>)], other2: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W2</span>)], numPartitions: <span class="type">Int</span>)</span><br><span class="line">      : <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W1</span>], <span class="type">Iterable</span>[<span class="type">W2</span>]))] = self.withScope &#123;</span><br><span class="line">    cogroup(other1, other2, <span class="keyword">new</span> <span class="type">HashPartitioner</span>(numPartitions))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>coalesce</strong>(<em>numPartitions</em>)</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 该函数用于将RDD进行重分区，使用HashPartitioner。第一个参数为重分区的数目，第二个为是否进行      * shuffle，默认为false;</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coalesce</span></span>(numPartitions: <span class="type">Int</span>, shuffle: <span class="type">Boolean</span> = <span class="literal">false</span>,</span><br><span class="line">               partitionCoalescer: <span class="type">Option</span>[<span class="type">PartitionCoalescer</span>] = <span class="type">Option</span>.empty)</span><br><span class="line">              (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>)</span><br><span class="line">      : <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    require(numPartitions &gt; <span class="number">0</span>, <span class="string">s"Number of partitions (<span class="subst">$numPartitions</span>) must be positive."</span>)</span><br><span class="line">    <span class="keyword">if</span> (shuffle) &#123;</span><br><span class="line">      <span class="comment">/** Distributes elements evenly across output partitions, starting from a random partition. */</span></span><br><span class="line">      <span class="keyword">val</span> distributePartition = (index: <span class="type">Int</span>, items: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; &#123;</span><br><span class="line">        <span class="keyword">var</span> position = <span class="keyword">new</span> <span class="type">Random</span>(hashing.byteswap32(index)).nextInt(numPartitions)</span><br><span class="line">        items.map &#123; t =&gt;</span><br><span class="line">          <span class="comment">// Note that the hash code of the key will just be the key itself. The HashPartitioner</span></span><br><span class="line">          <span class="comment">// will mod it with the number of total partitions.</span></span><br><span class="line">          position = position + <span class="number">1</span></span><br><span class="line">          (position, t)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; : <span class="type">Iterator</span>[(<span class="type">Int</span>, <span class="type">T</span>)]</span><br><span class="line"></span><br><span class="line">      <span class="comment">// include a shuffle step so that our upstream tasks are still distributed</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">CoalescedRDD</span>(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">Int</span>, <span class="type">T</span>, <span class="type">T</span>](mapPartitionsWithIndex(distributePartition),</span><br><span class="line">        <span class="keyword">new</span> <span class="type">HashPartitioner</span>(numPartitions)),</span><br><span class="line">        numPartitions,</span><br><span class="line">        partitionCoalescer).values</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">CoalescedRDD</span>(<span class="keyword">this</span>, numPartitions, partitionCoalescer)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>repartition</strong>(<em>numPartitions</em>)</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 可以增加或者减少分区，底层调用的是coalesce方法。如果要减少分区，建议使用coalesce，因为可以避    * 免shuffle</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    coalesce(numPartitions, shuffle = <span class="literal">true</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h3><p>一些常见的action算子如下表所示</p>
<table>
<thead>
<tr>
<th><strong>操作</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody><tr>
<td>count()</td>
<td>返回数据集中的元素个数</td>
</tr>
<tr>
<td>collect()</td>
<td>以数组的形式返回数据集中的所有元素</td>
</tr>
<tr>
<td>first()</td>
<td>返回数据集中的第一个元素</td>
</tr>
<tr>
<td>take(n)</td>
<td>以数组的形式返回数据集中的前n个元素</td>
</tr>
<tr>
<td>reduce(func)</td>
<td>通过函数func（输入两个参数并返回一个值）聚合数据集中的元素</td>
</tr>
<tr>
<td>foreach(func)</td>
<td>将数据集中的每个元素传递到函数func中运行</td>
</tr>
</tbody></table>
<h2 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h2><p>Spark提供了两种类型的共享变量：<strong>广播变量</strong>和<strong>累加器</strong>。广播变量(Broadcast variables)是一个只读的变量，并且在每个节点都保存一份副本，而不需要在集群中发送数据。累加器(Accumulators)可以将所有任务的数据累加到一个共享结果中。</p>
<h3 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h3><p>广播变量允许用户在集群中共享一个不可变的值，该共享的、不可变的值被持计划到集群的每台节点上。通常在需要将一份小数据集(比如维表)复制到集群中的每台节点时使用，比如日志分析的应用，web日志通常只包含pageId，而每个page的标题保存在一张表中，如果要分析日志(比如哪些page被访问的最多)，则需要将两者join在一起，这时就可以使用广播变量，将该表广播到集群的每个节点。具体如下图所示：</p>
<p><img src="//jiamaoxiang.top/2020/07/18/第二篇-Spark-core编程指南/%E5%B9%BF%E6%92%AD.png" alt></p>
<p>如上图，首先Driver将序列化对象分割成小的数据库，然后将这些数据块存储在Driver节点的BlockManager上。当ececutor中执行具体的task时，每个executor首先尝试从自己所在节点的BlockManager提取数据，如果之前已经提取的该广播变量的值，就直接使用它。如果没有找到，则会向远程的Driver或者其他的Executor中提取广播变量的值，一旦获取该值，就将其存储在自己节点的BlockManager中。这种机制可以避免Driver端向多个executor发送数据而造成的性能瓶颈。</p>
<p>基本使用方式如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 模拟一个数据集合</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> mockCollection = <span class="string">"Spark Flink Hadoop Hive"</span>.split(<span class="string">" "</span>)</span><br><span class="line">mockCollection: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="type">Spark</span>, <span class="type">Flink</span>, <span class="type">Hadoop</span>, <span class="type">Hive</span>)</span><br><span class="line"><span class="comment">// 构造RDD</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> words = sc.parallelize(mockCollection,<span class="number">2</span>)</span><br><span class="line">words: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">7</span>] at parallelize at &lt;console&gt;:<span class="number">29</span></span><br><span class="line"><span class="comment">// 模拟广播变量数据</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> mapData = <span class="type">Map</span>(<span class="string">"Spark"</span> -&gt; <span class="number">10</span>, <span class="string">"Flink"</span> -&gt; <span class="number">20</span>,<span class="string">"Hadoop"</span> -&gt; <span class="number">15</span>, <span class="string">"Hive"</span> -&gt; <span class="number">9</span>)</span><br><span class="line">mapData: scala.collection.immutable.<span class="type">Map</span>[<span class="type">String</span>,<span class="type">Int</span>] = <span class="type">Map</span>(<span class="type">Spark</span> -&gt; <span class="number">10</span>, <span class="type">Flink</span> -&gt; <span class="number">20</span>, <span class="type">Hadoop</span> -&gt; <span class="number">15</span>, <span class="type">Hive</span> -&gt; <span class="number">9</span>)</span><br><span class="line"><span class="comment">// 创建一个广播变量</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> broadCast = sc.broadcast(mapData)</span><br><span class="line">broadCast: org.apache.spark.broadcast.<span class="type">Broadcast</span>[scala.collection.immutable.<span class="type">Map</span>[<span class="type">String</span>,<span class="type">Int</span>]] = <span class="type">Broadcast</span>(<span class="number">4</span>)</span><br><span class="line"><span class="comment">// 在算子内部使用广播变量,根据key取出value值，按value升序排列</span></span><br><span class="line">scala&gt; words.map(word =&gt; (word,broadCast.value.getOrElse(word,<span class="number">0</span>))).sortBy(wordPair =&gt; wordPair._2).collect</span><br><span class="line">res5: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="type">Hive</span>,<span class="number">9</span>), (<span class="type">Spark</span>,<span class="number">10</span>), (<span class="type">Hadoop</span>,<span class="number">15</span>), (<span class="type">Flink</span>,<span class="number">20</span>))</span><br></pre></td></tr></table></figure>

<h3 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h3><p>累加器(Accumulator)是Spark提供的另外一个共享变量，与广播变量不同，累加器是可以被修改的，是可变的。每个transformation会将修改的累加器值传输到Driver节点，累加器可以实现一个累加的功能，类似于一个计数器。Spark本身支持数字类型的累加器，用户也可以自定义累加器的类型。</p>
<p><img src="//jiamaoxiang.top/2020/07/18/第二篇-Spark-core编程指南/%E7%B4%AF%E5%8A%A0.png" alt></p>
<h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><p>可以通过sparkContext.<code>longAccumulator()</code> 或者<code>SparkContext.doubleAccumulator()</code>分别创建Long和Double类型的累加器。运行在集群中的task可以调用add方法对该累加器变量进行累加，但是不能够读取累加器的值，只有Driver程序可以通过调用value方法读取累加器的值。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkAccumulator</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="type">SparkShareVariable</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.spark"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.hadoop"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line">    <span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>)</span><br><span class="line">    <span class="keyword">val</span> listRDD = sc.parallelize(list)</span><br><span class="line">    <span class="keyword">var</span> counter = <span class="number">0</span> <span class="comment">//外部变量</span></span><br><span class="line">    <span class="comment">//初始化一个accumulator，初始值默认为0</span></span><br><span class="line">    <span class="keyword">val</span> countAcc = sc.longAccumulator(<span class="string">"my accumulator"</span>)</span><br><span class="line">    <span class="keyword">val</span> mapRDD = listRDD.map(num =&gt; &#123;</span><br><span class="line">      counter += <span class="number">1</span> <span class="comment">//在算子内部使用了外部变量，这样操作不会改变外部变量的值</span></span><br><span class="line">      <span class="keyword">if</span> (num % <span class="number">3</span> == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">//遇到3的倍数，累加器+1</span></span><br><span class="line">        countAcc.add(<span class="number">1</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      num * <span class="number">2</span></span><br><span class="line">    &#125;)</span><br><span class="line">    mapRDD.foreach(println)</span><br><span class="line">    println(<span class="string">"counter = "</span> + counter) <span class="comment">// counter = 0</span></span><br><span class="line">    println(<span class="string">"countAcc = "</span> + countAcc.value) <span class="comment">// countAcc = 4</span></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>尖叫提示：</p>
<p>我们在dirver中声明的一些局部变量或者成员变量，可以直接在transformation中使用，但是经过transformation操作之后，是不会将最终的结果重新赋值给dirver中的对应的变量。因为通过action触发transformation操作之后，transformation的操作都是通过DAGScheduler将代码打包，然后序列化，最后交由TaskScheduler传送到各个Worker节点中的Executor去执行，在transformation中执行的这些变量，是自己节点上的变量，不是dirver上最初的变量，只不过是将driver上的对应的变量拷贝了一份而已。</p>
</blockquote>
<h3 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h3><p>Spark提供了一些默认类型的累加器，同时也支持自定义累加器。通过继承AccumulatorV2类即可实现自定义累加器，具体代码如下:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">customAccumulator</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[<span class="type">BigInt</span>, <span class="type">BigInt</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> num:<span class="type">BigInt</span> = <span class="number">0</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 返回该accumulator是否为0值，比如一个计数器，0代表zero，如果是一个list，Nil代表zero </span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.num == <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 创建一个该accumulator副本</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">BigInt</span>, <span class="type">BigInt</span>] = &#123;</span><br><span class="line">    <span class="keyword">new</span> customAccumulator</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 重置accumulator的值, 该值为0，调用 `isZero` 必须返回true</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.num = <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 根据输入的值，进行累加，</span></span><br><span class="line">  <span class="comment">// 判断为偶数时，累加器加上该值</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(intVal: <span class="type">BigInt</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span>(intVal % <span class="number">2</span> == <span class="number">0</span>)&#123;</span><br><span class="line">      <span class="keyword">this</span>.num += intVal</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 合并其他的同一类型的accumulator，并更新该accumulator值</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">BigInt</span>, <span class="type">BigInt</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.num += other.value</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 定义当前accumulator的值</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: <span class="type">BigInt</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.num</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用该自定义累加器</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> acc = <span class="keyword">new</span> customAccumulator</span><br><span class="line"><span class="keyword">val</span> newAcc = sc.register(acc, <span class="string">"evenAcc"</span>)</span><br><span class="line">println(acc.value)</span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)).foreach(x =&gt; acc.add(x))</span><br><span class="line">println(acc.value)</span><br></pre></td></tr></table></figure>

<h2 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h2><h3 id="持久化方法"><a href="#持久化方法" class="headerlink" title="持久化方法"></a>持久化方法</h3><p>在Spark中，RDD采用惰性求值的机制，每次遇到action操作，都会从头开始执行计算。每次调用action操作，都会触发一次从头开始的计算。对于需要被重复使用的RDD，spark支持对其进行持久化，通过调用persist()或者cache()方法即可实现RDD的持计划。通过持久化机制可以避免重复计算带来的开销。值得注意的是，当调用持久化的方法时，只是对该RDD标记为了持久化，需要等到第一次执行action操作之后，才会把计算结果进行持久化。持久化后的RDD将会被保留在计算节点的内存中被后面的行动操作重复使用。</p>
<p>Spark提供的两个持久化方法的主要区别是：cache()方法默认使用的是内存级别，其底层调用的是persist()方法，具体源码片段如下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (isLocallyCheckpointed) &#123;</span><br><span class="line">      persist(<span class="type">LocalRDDCheckpointData</span>.transformStorageLevel(newLevel), allowOverride = <span class="literal">true</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      persist(newLevel, allowOverride = <span class="literal">false</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 使用默认的存储级别持久化RDD (`MEMORY_ONLY`).</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 使用默认的存储级别持久化RDD (`MEMORY_ONLY`).</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist()</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 手动地把持久化的RDD从缓存中移除</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">unpersist</span></span>(blocking: <span class="type">Boolean</span> = <span class="literal">true</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line">    logInfo(<span class="string">"Removing RDD "</span> + id + <span class="string">" from persistence list"</span>)</span><br><span class="line">    sc.unpersistRDD(id, blocking)</span><br><span class="line">    storageLevel = <span class="type">StorageLevel</span>.<span class="type">NONE</span></span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="持计划存储级别"><a href="#持计划存储级别" class="headerlink" title="持计划存储级别"></a>持计划存储级别</h3><p>Spark的提供了多种持久化级别，比如内存、磁盘、内存+磁盘等。具体如下表所示：</p>
<table>
<thead>
<tr>
<th align="left">Storage Level</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left">MEMORY_ONLY</td>
<td align="left">默认，表示将RDD作为反序列化的Java对象存储于JVM中，如果内存不够用，则部分分区不会被持久化，等到使用到这些分区时，会重新计算。</td>
</tr>
<tr>
<td align="left">MEMORY_AND_DISK</td>
<td align="left">将RDD作为反序列化的Java对象存储在JVM中，如果内存不足，超出的分区将会被存放在硬盘上.</td>
</tr>
<tr>
<td align="left">MEMORY_ONLY_SER  (Java and Scala)</td>
<td align="left">将RDD序列化为Java对象进行持久化，每个分区对应一个字节数组。此方式比反序列化要节省空间，但是会占用更多cpu资源</td>
</tr>
<tr>
<td align="left">MEMORY_AND_DISK_SER  (Java and Scala)</td>
<td align="left">与 MEMORY_ONLY_SER, 如果内存放不下，则溢写到磁盘。</td>
</tr>
<tr>
<td align="left">DISK_ONLY</td>
<td align="left">将RDD的分区数据存储到磁盘</td>
</tr>
<tr>
<td align="left">MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.</td>
<td align="left">与上面的方式类似,但是会将分区数据复制到两个集群</td>
</tr>
<tr>
<td align="left">OFF_HEAP (experimental)</td>
<td align="left">与MEMORY_ONLY_SER类似,将数据存储到堆外内存 off-heap，需要将off-heap 开启</td>
</tr>
</tbody></table>
<h3 id="持久化级别的选择"><a href="#持久化级别的选择" class="headerlink" title="持久化级别的选择"></a>持久化级别的选择</h3><p>Spark提供的持久化存储级别是在<strong>内存使用</strong>与<strong>CPU效率</strong>之间做权衡，通常推荐下面的选择方式：</p>
<ul>
<li><p>如果内存可以容纳RDD，可以使用默认的持久化级别，即MEMORY_ONLY。这是CPU最有效率的选择，可以使作用在RDD上的算子尽可能第快速执行。</p>
</li>
<li><p>如果内存不够用，可以尝试使用MEMORY_ONLY_SER，使用一个快速的序列化库可以节省很多空间，比如 Kryo 。</p>
<blockquote>
<p>tips：在一些shuffle算子中，比如reduceByKey，即便没有显性调用persist方法，Spark也会自动将中间结果进行持久化，这样做的目的是避免在shuffle期间发生故障而造成重新计算整个输入。即便如此，还是推荐对需要被重复使用的RDD进行持久化处理。</p>
</blockquote>
</li>
</ul>
<h2 id="综合案例"><a href="#综合案例" class="headerlink" title="综合案例"></a>综合案例</h2><ul>
<li><strong>case 1</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *  1.数据集</span></span><br><span class="line"><span class="comment">  *          [orderId,userId,payment,productId]</span></span><br><span class="line"><span class="comment">  *          1,108,280,1002</span></span><br><span class="line"><span class="comment">  *          2,202,300,2004</span></span><br><span class="line"><span class="comment">  *          3,210,588,3241</span></span><br><span class="line"><span class="comment">  *          4,198,5000,3567</span></span><br><span class="line"><span class="comment">  *          5,200,590,2973</span></span><br><span class="line"><span class="comment">  *          6,678,8000,18378</span></span><br><span class="line"><span class="comment">  *          7,243,200,2819</span></span><br><span class="line"><span class="comment">  *          8,236,7890,2819</span></span><br><span class="line"><span class="comment">  *  2.需求描述</span></span><br><span class="line"><span class="comment">  *           计算Top3订单金额</span></span><br><span class="line"><span class="comment">  *           </span></span><br><span class="line"><span class="comment">  *  3.结果输出</span></span><br><span class="line"><span class="comment">  *    		 1	8000</span></span><br><span class="line"><span class="comment">  *		     2	7890</span></span><br><span class="line"><span class="comment">  *          3	5000       </span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TopOrder</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"TopN"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    sc.setLogLevel(<span class="string">"ERROR"</span>)</span><br><span class="line">    <span class="keyword">val</span> lines = sc.textFile(<span class="string">"E://order.txt"</span>)</span><br><span class="line">    <span class="keyword">var</span> num = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">val</span> result = lines.filter(line =&gt; (line.trim().length &gt; <span class="number">0</span>) &amp;&amp; (line.split(<span class="string">","</span>).length == <span class="number">4</span>))</span><br><span class="line">      .map(_.split(<span class="string">","</span>)(<span class="number">2</span>))     <span class="comment">// 取出支付金额</span></span><br><span class="line">      .map(x =&gt; (x.toInt,<span class="string">""</span>))   </span><br><span class="line">      .sortByKey(<span class="literal">false</span>)         <span class="comment">// 按照支付金额降序排列    </span></span><br><span class="line">      .map(x =&gt; x._1).take(<span class="number">3</span>)   <span class="comment">// 取出前3个</span></span><br><span class="line">      .foreach(x =&gt; &#123;</span><br><span class="line">        num = num + <span class="number">1</span></span><br><span class="line">        println(num + <span class="string">"\t"</span> + x)</span><br><span class="line">      &#125;)</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>case 2</strong></li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 1.数据集(movielensSet)</span></span><br><span class="line"><span class="comment">  *        用户电影评分数据[UserID::MovieID::Rating::Timestamp]</span></span><br><span class="line"><span class="comment">  *        电影名称数据[MovieId::MovieName::MovieType]</span></span><br><span class="line"><span class="comment">  * 2.需求描述</span></span><br><span class="line"><span class="comment">  *        求平均评分大于5的电影名称</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MovieRating</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"MovieRating"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    sc.setLogLevel(<span class="string">"ERROR"</span>)</span><br><span class="line">    <span class="comment">// 用户电影评分数据[UserID::MovieID::Rating::Timestamp]</span></span><br><span class="line">    <span class="keyword">val</span> userRating = sc.textFile(<span class="string">"E://ml-1m/ratings.dat"</span>)</span><br><span class="line">    <span class="comment">// 电影名称数据[MovieId::MovieName::MovieType]</span></span><br><span class="line">    <span class="keyword">val</span> movies = sc.textFile(<span class="string">"E://ml-1m/movies.dat"</span>)</span><br><span class="line">    <span class="comment">//提取电影id和评分,(MovieID, Rating)</span></span><br><span class="line">    <span class="keyword">val</span> movieRating = userRating.map &#123; line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> rating = line.split(<span class="string">"::"</span>)</span><br><span class="line">      (rating(<span class="number">1</span>).toInt, rating(<span class="number">2</span>).toDouble)</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 计算电影id及其平均评分,(MovieId,AvgRating)</span></span><br><span class="line">    <span class="keyword">val</span> movieAvgRating = movieRating</span><br><span class="line">      .groupByKey()</span><br><span class="line">      .map &#123; rating =&gt;</span><br><span class="line">          <span class="keyword">val</span> avgRating = rating._2.sum / rating._2.size</span><br><span class="line">          (rating._1, avgRating)</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="comment">//提取电影id和电影名称,(MovieId,MovieName)</span></span><br><span class="line">   <span class="keyword">val</span> movieName =  movies.map &#123; movie =&gt;</span><br><span class="line">        <span class="keyword">val</span> fields = movie.split(<span class="string">"::"</span>)</span><br><span class="line">        (fields(<span class="number">0</span>).toInt, fields(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    &#125;.keyBy(_._1)</span><br><span class="line"></span><br><span class="line">    movieAvgRating</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line">      .join(movieName) <span class="comment">// Join的结果(MovieID,((MovieID,AvgRating),(MovieID,MovieName)))</span></span><br><span class="line">      .filter(joinData =&gt; joinData._2._1._2 &gt; <span class="number">5.0</span>)</span><br><span class="line">      .map(rs =&gt; (rs._1,rs._2._1._2,rs._2._2._2))</span><br><span class="line">      .saveAsTextFile(<span class="string">"E:/MovieRating/"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文对Spark Core进行了详细讲解，主要包括RDD的基本概念、RDD的操作算子、共享变量以及持计划，最后给出两个完整的Spark Core编程案例。下一篇将分享Spark SQL编程指南。</p>
</div><div class="recommended_posts"><h3>相关推荐 ☟</h3><li><a href="https://jiamaoxiang.top/2020/07/27/第四篇-Spark-Streaming编程指南/" target="_blank">第四篇|Spark Streaming编程指南(1)</a></li><li><a href="https://jiamaoxiang.top/2020/07/23/第三篇-Spark-SQL编程指南/" target="_blank">第三篇|Spark SQL编程指南</a></li><li><a href="https://jiamaoxiang.top/2020/07/14/第一篇-Spark概览/" target="_blank">第一篇|Spark概览</a></li><li><a href="https://jiamaoxiang.top/2020/07/11/数仓-大数据时代-维度建模过时了吗/" target="_blank">数仓|大数据时代,维度建模过时了吗?</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Jia MaoXiang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2020/07/18/第二篇-Spark-core编程指南/">https://jiamaoxiang.top/2020/07/18/第二篇-Spark-core编程指南/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本文为博主原创文章，遵循CC BY-SA 4.0版权协议，转载请附上原文出处链接和本声明</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://jiamaoxiang.top/2020/07/18/第二篇-Spark-core编程指南/" data-id="ckdny482m0052u07qs9jno46q" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACuUlEQVR42u3aQW4bMRAEQP//084lhxysVfcMaShA8SRI8oJFAyTVM19f8fh+HM/ff/WEf9/Pn/B1Y+Dh4eFdmHr+6avnv3rdjnq58fDw8K7x8mk9v/PMft7uk0/zOePh4eF9Di+fXPLMzQUdDw8P73/n5ZFBjvmggwEPDw9vFEbkjPzC3YIvZi14eHh4XcUqqiJ9zusr9T08PDy8dVX9ecPNQ4T2Mr2f59/n4OHh4V3gbeKD50t223bQFsOKmAMPDw/vKC8v828Oj7MNB0mEgYeHh3ePN4tZZ/Fuexjk4XIU4OLh4eEd4m3KVG0hvw0a8oaDIobAw8PDO8RrA9n2yjuLdNuFK3oi8PDw8Na8ZLNOrrDttTuBbeISPDw8vBu8dltvC2BtuasOZ5Mlw8PDwzvKm/34b0v7dYhQIl/OBA8PD+8Cr/55X27QUeUtPlTagBgPDw/vBq8tYs2Oh7z8n/xVcVnHw8PDu8DbFO9n4cWpgCP/B+Dh4eGd5Z0q+beRQV7EalsHhqUvPDw8vAWvfVwbJewxRfEMDw8P7wKvLSwV9/RRwJFfwfNQAw8PD+8sL4loZ00Am7h2szQ/nHt4eHh4R3mzy+5sW0/GrFGgaLrCw8PDW/OSzTqZUH4k5Is1CyPw8PDwbvPOFqhmhbG2iWEWH+Ph4eGd5bVbc3tdzlsTWkzReoWHh4d3gZdfedtv3li4ovkADw8P79d5yVQ2oW1ydV4NPDw8vKO873Js2ghmwUe7HD90RuDh4eEd5c322CRoaAFt00AS5uLh4eHd47WNVpsYd19OawtjeHh4ePd4+aact2dtDoB2+eqP8fDw8H6RNzswNmFHEvK+KYDh4eHhfQAvjyHaAKItub05SPDw8PCu8dr4NQ8RzpbN8uXGw8PDu8eb/eBvp5i/M1v6A+1ZeHh4eO95fwDbPl/0RFKT4gAAAABJRU5ErkJggg==">分享</a><div class="tags"><a href="/tags/Spark/">Spark</a></div><div class="post-nav"><a class="pre" href="/2020/07/23/第三篇-Spark-SQL编程指南/">第三篇|Spark SQL编程指南</a><a class="next" href="/2020/07/14/第一篇-Spark概览/">第一篇|Spark概览</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar-toc"><div class="stoc-article" id="sidebar-stoc"><strong class="stoc-title"><i class="fa fa-list-ul"> 目录</i></strong><div class="toc-nav" id="stoc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是RDD"><span class="toc-number">1.</span> <span class="toc-text">什么是RDD</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#设计初衷"><span class="toc-number">1.1.</span> <span class="toc-text">设计初衷</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基本概念"><span class="toc-number">1.2.</span> <span class="toc-text">基本概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#主要特点"><span class="toc-number">1.3.</span> <span class="toc-text">主要特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#宽依赖和窄依赖"><span class="toc-number">1.4.</span> <span class="toc-text">宽依赖和窄依赖</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stage划分"><span class="toc-number">1.5.</span> <span class="toc-text">Stage划分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark作业调度流程"><span class="toc-number">1.6.</span> <span class="toc-text">Spark作业调度流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD操作算子"><span class="toc-number">2.</span> <span class="toc-text">RDD操作算子</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformation"><span class="toc-number">2.1.</span> <span class="toc-text">Transformation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Action"><span class="toc-number">2.2.</span> <span class="toc-text">Action</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#共享变量"><span class="toc-number">3.</span> <span class="toc-text">共享变量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#广播变量"><span class="toc-number">3.1.</span> <span class="toc-text">广播变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#累加器"><span class="toc-number">3.2.</span> <span class="toc-text">累加器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基本使用"><span class="toc-number">3.3.</span> <span class="toc-text">基本使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#自定义累加器"><span class="toc-number">3.4.</span> <span class="toc-text">自定义累加器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#持久化"><span class="toc-number">4.</span> <span class="toc-text">持久化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#持久化方法"><span class="toc-number">4.1.</span> <span class="toc-text">持久化方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#持计划存储级别"><span class="toc-number">4.2.</span> <span class="toc-text">持计划存储级别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#持久化级别的选择"><span class="toc-number">4.3.</span> <span class="toc-text">持久化级别的选择</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#综合案例"><span class="toc-number">5.</span> <span class="toc-text">综合案例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">6.</span> <span class="toc-text">总结</span></a></li></ol></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 - 2020 <a href="/." rel="nofollow">Jmx's Blog.</a>All rights reserved.<br>Thoughts on technology, life and everything else.</div></div></div><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" color="100,99,98" opacity="0.5" zindex="0.7" count="150" src="//lib.baomitu.com/canvas-nest.js/2.0.4/canvas-nest.umd.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>