<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记录朴实无华且枯燥的生活"><title>基于Canal与Flink实现数据实时增量同步(二) | Jmx's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '912b6cfc43243cd27aeb428f7dbf7823';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">基于Canal与Flink实现数据实时增量同步(二)</h1><a id="logo" href="/.">Jmx's Blog</a><p class="description">Keep it Simple and Stupid!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-pied-piper-alt"> 关于</i></a><a href="/tags"><i class="fa fa-tags"> 标签</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">基于Canal与Flink实现数据实时增量同步(二)</h1><div class="post-meta">Mar 24, 2020<span> | </span><span class="category"><a href="/categories/Flink/">Flink</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2.1k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 9</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>本文主要从Binlog实时采集和离线处理Binlog还原业务数据两个方面，来介绍如何实现DB数据准确、高效地进入Hive数仓。</p>
<a id="more"></a>

<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在数据仓库建模中，未经任何加工处理的原始业务层数据，我们称之为ODS(Operational Data Store)数据。在互联网企业中，常见的ODS数据有业务日志数据（Log）和业务DB数据（DB）两类。对于业务DB数据来说，从MySQL等关系型数据库的业务数据进行采集，然后导入到Hive中，是进行数据仓库生产的重要环节。如何准确、高效地把MySQL数据同步到Hive中？一般常用的解决方案是批量取数并Load：直连MySQL去Select表中的数据，然后存到本地文件作为中间存储，最后把文件Load到Hive表中。这种方案的优点是实现简单，但是随着业务的发展，缺点也逐渐暴露出来：</p>
<ul>
<li><p>性能瓶颈：随着业务规模的增长，Select From MySQL -&gt; Save to Localfile -&gt; Load to Hive这种数据流花费的时间越来越长，无法满足下游数仓生产的时间要求。</p>
</li>
<li><p>直接从MySQL中Select大量数据，对MySQL的影响非常大，容易造成慢查询，影响业务线上的正常服务。</p>
</li>
<li><p>由于Hive本身的语法不支持更新、删除等SQL原语(高版本Hive支持，但是需要分桶+ORC存储格式)，对于MySQL中发生Update/Delete的数据无法很好地进行支持。</p>
</li>
</ul>
<p>为了彻底解决这些问题，我们逐步转向CDC (Change Data Capture) + Merge的技术方案，即实时Binlog采集 + 离线处理Binlog还原业务数据这样一套解决方案。Binlog是MySQL的二进制日志，记录了MySQL中发生的所有数据变更，MySQL集群自身的主从同步就是基于Binlog做的。</p>
<h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><p>首先，采用Flink负责把Kafka上的Binlog数据拉取到HDFS上。</p>
<p>然后，对每张ODS表，首先需要一次性制作快照（Snapshot），把MySQL里的存量数据读取到Hive上，这一过程底层采用直连MySQL去Select数据的方式，可以使用Sqoop进行一次性全量导入。</p>
<p>最后，对每张ODS表，每天基于存量数据和当天增量产生的Binlog做Merge，从而还原出业务数据。</p>
<p>Binlog是流式产生的，通过对Binlog的实时采集，把部分数据处理需求由每天一次的批处理分摊到实时流上。无论从性能上还是对MySQL的访问压力上，都会有明显地改善。Binlog本身记录了数据变更的类型（Insert/Update/Delete），通过一些语义方面的处理，完全能够做到精准的数据还原。</p>
<h2 id="实现方案"><a href="#实现方案" class="headerlink" title="实现方案"></a>实现方案</h2><h3 id="Flink处理Kafka的binlog日志"><a href="#Flink处理Kafka的binlog日志" class="headerlink" title="Flink处理Kafka的binlog日志"></a>Flink处理Kafka的binlog日志</h3><p>使用kafka source，对读取的数据进行JSON解析，将解析的字段拼接成字符串，符合Hive的schema格式，具体代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.etl.kafka2hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONArray;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.parser.Feature;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FilterFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringEncoder;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.core.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.StateBackend;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.filesystem.FsStateBackend;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.CheckpointConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.RollingPolicy;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Created</span> with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@author</span> : jmx</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Date</span>: 2020/3/27</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Time</span>: 12:52</span></span><br><span class="line"><span class="comment"> *  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsSink</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String fieldDelimiter = <span class="string">","</span>;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// checkpoint</span></span><br><span class="line">        env.enableCheckpointing(<span class="number">10_000</span>);</span><br><span class="line">        <span class="comment">//env.setStateBackend((StateBackend) new FsStateBackend("file:///E://checkpoint"));</span></span><br><span class="line">        env.setStateBackend((StateBackend) <span class="keyword">new</span> FsStateBackend(<span class="string">"hdfs://kms-1:8020/checkpoint"</span>));</span><br><span class="line">        CheckpointConfig config = env.getCheckpointConfig();</span><br><span class="line">        config.enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// source</span></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.setProperty(<span class="string">"bootstrap.servers"</span>, <span class="string">"kms-2:9092,kms-3:9092,kms-4:9092"</span>);</span><br><span class="line">        <span class="comment">// only required for Kafka 0.8</span></span><br><span class="line">        props.setProperty(<span class="string">"zookeeper.connect"</span>, <span class="string">"kms-2:2181,kms-3:2181,kms-4:2181"</span>);</span><br><span class="line">        props.setProperty(<span class="string">"group.id"</span>, <span class="string">"test123"</span>);</span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; consumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(</span><br><span class="line">                <span class="string">"qfbap_ods.code_city"</span>, <span class="keyword">new</span> SimpleStringSchema(), props);</span><br><span class="line">        consumer.setStartFromEarliest();</span><br><span class="line">        DataStream&lt;String&gt; stream = env.addSource(consumer);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// transform</span></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; cityDS = stream</span><br><span class="line">                .filter(<span class="keyword">new</span> FilterFunction&lt;String&gt;() &#123;</span><br><span class="line">                    <span class="comment">// 过滤掉DDL操作</span></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(String jsonVal)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        JSONObject record = JSON.parseObject(jsonVal, Feature.OrderedField);</span><br><span class="line">                        <span class="keyword">return</span> record.getString(<span class="string">"isDdl"</span>).equals(<span class="string">"false"</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        StringBuilder fieldsBuilder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">                        <span class="comment">// 解析JSON数据</span></span><br><span class="line">                        JSONObject record = JSON.parseObject(value, Feature.OrderedField);</span><br><span class="line">                        <span class="comment">// 获取最新的字段值</span></span><br><span class="line">                        JSONArray data = record.getJSONArray(<span class="string">"data"</span>);</span><br><span class="line">                        <span class="comment">// 遍历，字段值的JSON数组，只有一个元素</span></span><br><span class="line">                        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; data.size(); i++) &#123;</span><br><span class="line">                            <span class="comment">// 获取到JSON数组的第i个元素</span></span><br><span class="line">                            JSONObject obj = data.getJSONObject(i);</span><br><span class="line">                            <span class="keyword">if</span> (obj != <span class="keyword">null</span>) &#123;</span><br><span class="line">                                fieldsBuilder.append(record.getLong(<span class="string">"id"</span>)); <span class="comment">// 序号id</span></span><br><span class="line">                                fieldsBuilder.append(fieldDelimiter); <span class="comment">// 字段分隔符</span></span><br><span class="line">                                fieldsBuilder.append(record.getLong(<span class="string">"es"</span>)); <span class="comment">//业务时间戳</span></span><br><span class="line">                                fieldsBuilder.append(fieldDelimiter);</span><br><span class="line">                                fieldsBuilder.append(record.getLong(<span class="string">"ts"</span>)); <span class="comment">// 日志时间戳</span></span><br><span class="line">                                fieldsBuilder.append(fieldDelimiter);</span><br><span class="line">                                fieldsBuilder.append(record.getString(<span class="string">"type"</span>)); <span class="comment">// 操作类型</span></span><br><span class="line">                                <span class="keyword">for</span> (Map.Entry&lt;String, Object&gt; entry : obj.entrySet()) &#123;</span><br><span class="line"></span><br><span class="line">                                    fieldsBuilder.append(fieldDelimiter);</span><br><span class="line">                                    fieldsBuilder.append(entry.getValue()); <span class="comment">// 表字段数据</span></span><br><span class="line">                                &#125;</span><br><span class="line"></span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">return</span> fieldsBuilder.toString();</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//cityDS.print();</span></span><br><span class="line">        <span class="comment">//stream.print();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// sink</span></span><br><span class="line">        <span class="comment">// 以下条件满足其中之一就会滚动生成新的文件</span></span><br><span class="line">        RollingPolicy&lt;String, String&gt; rollingPolicy = DefaultRollingPolicy.create()</span><br><span class="line">                .withRolloverInterval(<span class="number">60L</span> * <span class="number">1000L</span>) <span class="comment">//滚动写入新文件的时间，默认60s。根据具体情况调节</span></span><br><span class="line">                .withMaxPartSize(<span class="number">1024</span> * <span class="number">1024</span> * <span class="number">128L</span>) <span class="comment">//设置每个文件的最大大小 ,默认是128M，这里设置为128M</span></span><br><span class="line">                .withInactivityInterval(<span class="number">60L</span> * <span class="number">1000L</span>) <span class="comment">//默认60秒,未写入数据处于不活跃状态超时会滚动新文件</span></span><br><span class="line">                .build();</span><br><span class="line">        </span><br><span class="line">        StreamingFileSink&lt;String&gt; sink = StreamingFileSink</span><br><span class="line">                <span class="comment">//.forRowFormat(new Path("file:///E://binlog_db/city"), new SimpleStringEncoder&lt;String&gt;())</span></span><br><span class="line">                .forRowFormat(<span class="keyword">new</span> Path(<span class="string">"hdfs://kms-1:8020/binlog_db/code_city_delta"</span>), <span class="keyword">new</span> SimpleStringEncoder&lt;String&gt;())</span><br><span class="line">                .withBucketAssigner(<span class="keyword">new</span> EventTimeBucketAssigner())</span><br><span class="line">                .withRollingPolicy(rollingPolicy)</span><br><span class="line">                .withBucketCheckInterval(<span class="number">1000</span>)  <span class="comment">// 桶检查间隔，这里设置1S</span></span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        cityDS.addSink(sink);</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对于Flink Sink到HDFS，<code>StreamingFileSink</code> 替代了先前的 <code>BucketingSink</code>，用来将上游数据存储到 HDFS 的不同目录中。它的核心逻辑是分桶，默认的分桶方式是 <code>DateTimeBucketAssigner</code>，即按照处理时间分桶。处理时间指的是消息到达 Flink 程序的时间，这点并不符合我们的需求。因此，我们需要自己编写代码将事件时间从消息体中解析出来，按规则生成分桶的名称，具体代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.etl.kafka2hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.core.io.SimpleVersionedSerializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.BucketAssigner;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.SimpleVersionedStringSerializer;</span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Created</span> with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@author</span> : jmx</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Date</span>: 2020/3/27</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Time</span>: 12:49</span></span><br><span class="line"><span class="comment"> *  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EventTimeBucketAssigner</span> <span class="keyword">implements</span> <span class="title">BucketAssigner</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getBucketId</span><span class="params">(String element, Context context)</span> </span>&#123;</span><br><span class="line">        String partitionValue;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            partitionValue = getPartitionValue(element);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            partitionValue = <span class="string">"00000000"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"dt="</span> + partitionValue;<span class="comment">//分区目录名称</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SimpleVersionedSerializer&lt;String&gt; <span class="title">getSerializer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> SimpleVersionedStringSerializer.INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> String <span class="title">getPartitionValue</span><span class="params">(String element)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 取出最后拼接字符串的es字段值，该值为业务时间</span></span><br><span class="line">        <span class="keyword">long</span> eventTime = Long.parseLong(element.split(<span class="string">","</span>)[<span class="number">1</span>]);</span><br><span class="line">        Date eventDate = <span class="keyword">new</span> Date(eventTime);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> SimpleDateFormat(<span class="string">"yyyyMMdd"</span>).format(eventDate);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="离线还原MySQL数据"><a href="#离线还原MySQL数据" class="headerlink" title="离线还原MySQL数据"></a>离线还原MySQL数据</h3><p>经过上述步骤，即可将Binlog日志记录写入到HDFS的对应的分区中，接下来就需要根据增量的数据和存量的数据还原最新的数据。Hive 表保存在 HDFS 上，该文件系统不支持修改，因此我们需要一些额外工作来写入数据变更。常用的方式包括：JOIN、Hive 事务、或改用 HBase、kudu。</p>
<p>如昨日的存量数据code_city,今日增量的数据为code_city_delta，可以通过 <code>FULL OUTER JOIN</code>，将存量和增量数据合并成一张最新的数据表，并作为明天的存量数据：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> code_city</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">        <span class="keyword">COALESCE</span>( t2.id, t1.id ) <span class="keyword">AS</span> <span class="keyword">id</span>,</span><br><span class="line">        <span class="keyword">COALESCE</span> ( t2.city, t1.city ) <span class="keyword">AS</span> city,</span><br><span class="line">        <span class="keyword">COALESCE</span> ( t2.province, t1.province ) <span class="keyword">AS</span> province,</span><br><span class="line">        <span class="keyword">COALESCE</span> ( t2.event_time, t1.event_time ) <span class="keyword">AS</span> event_time </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">        code_city t1</span><br><span class="line">        <span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> (</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">        <span class="keyword">id</span>,</span><br><span class="line">        city,</span><br><span class="line">        province,</span><br><span class="line">        event_time </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">        (<span class="comment">-- 取最后一条状态数据</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">        <span class="keyword">id</span>,</span><br><span class="line">        city,</span><br><span class="line">        province,</span><br><span class="line">        dml_type,</span><br><span class="line">        event_time,</span><br><span class="line">        row_number ( ) <span class="keyword">over</span> ( <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">id</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> event_time <span class="keyword">DESC</span> ) <span class="keyword">AS</span> <span class="keyword">rank</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">        code_city_delta </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">        dt = <span class="string">'20200324'</span> <span class="comment">-- 分区数据</span></span><br><span class="line">        ) temp </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">        <span class="keyword">rank</span> = <span class="number">1</span> </span><br><span class="line">        ) t2 <span class="keyword">ON</span> t1.id = t2.id;</span><br></pre></td></tr></table></figure>

<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要从Binlog流式采集和基于Binlog的ODS数据还原两方面，介绍了通过Flink实现实时的ETL，此外还可以将binlog日志写入kudu、HBase等支持事务操作的NoSQL中，这样就可以省去数据表还原的步骤。本文是《基于Canal与Flink实现数据实时增量同步》的第二篇，关于canal解析Binlog日志写入kafka的实现步骤，参见《基于Canal与Flink实现数据实时增量同步一》。</p>
<p><strong>refrence：</strong></p>
<p>[1]<a href="https://tech.meituan.com/2018/12/06/binlog-dw.html" target="_blank" rel="noopener">https://tech.meituan.com/2018/12/06/binlog-dw.html</a></p>
</div><div class="recommended_posts"><h3>相关推荐 ☟</h3><li><a href="https://jiamaoxiang.top/2020/03/22/分布式数据集成框架gobblin快速入门/" target="_blank">分布式数据集成框架gobblin快速入门</a></li><li><a href="https://jiamaoxiang.top/2020/03/05/基于Canal与Flink实现数据实时增量同步-一/" target="_blank">基于Canal与Flink实现数据实时增量同步(一)</a></li><li><a href="https://jiamaoxiang.top/2019/12/10/SQL中的相关子查询解析/" target="_blank">SQL中的相关子查询解析</a></li><li><a href="https://jiamaoxiang.top/2019/12/09/LeeCode数据库部分题目汇总/" target="_blank">LeeCode数据库部分题目汇总</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Jia MaoXiang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2020/03/24/基于Canal与Flink实现数据实时增量同步-二/">https://jiamaoxiang.top/2020/03/24/基于Canal与Flink实现数据实时增量同步-二/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本文为博主原创文章，遵循CC BY-SA 4.0版权协议，转载请附上原文出处链接和本声明</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://jiamaoxiang.top/2020/03/24/基于Canal与Flink实现数据实时增量同步-二/" data-id="ck8bpnf1h0024o87qqo576x2g" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPYAAAD2CAAAAADAeSUUAAADN0lEQVR42u3ay27bWgwF0Pz/T/sCRQe9aK3szSMDNrM0MhJZOoseEHx8fcXX49eV/Pfxx/Xszr/vv35Ocs/XKy5sbGzsD2E/Lq+EnR/05I3Xz8mfjI2Njb2Vnael/LjXyebZgYqjX77xGyM2Njb2j2RfJ6o8QEk42kSFjY2Njd22ivKgzKjXT5s9BxsbG3s3O39BUh7cFdDZgOHmXho2Njb227PbQe87f37hfBsbGxv7LdmP8poNAJIk1K7pzM7/+7vY2NjYi9j5csz54mO7xHOyHhSlN2xsbOx17JPEMBsn5CPettkUhQ8bGxt7BXu2OtPC2jSWJMjZ87GxsbH3sZOC5Hyg245yc/bNyQwbGxt7KTtJMNchmw0P2h8mT2zY2NjYm9gnrZ+7mlB5KppVE9jY2NjYs0WZtmBoAzoLyj82lbCxsbFXsNt2zKwAyJ88GyQUxRI2Njb2Inbb7j8vA2b3Jz9AkvywsbGx97HzNZ12lFu0eMrBwEnqiiowbGxs7A9nzxLbXQPdtlGV/2zY2NjYP4GdDwDyJtEr1m7ygEalCDY2NvYK9qx9MytUZkPiuwKBjY2NvY89G8fORsL5gs6spZWUItjY2Nhb2a+efublSrv60/4YT9MYNjY29oez8yQ0G+4WsS8TZFuQFDNqbGxs7A9hz+LULsq0Y+O2WGpXiLCxsbH3sZNCIm8/nbf180TVLhX9b2UHGxsbezX7pOkzG9OeLwMljTBsbGzsfew2YcyWZmbsWYrKP2NjY2PvYOcLLicNo7sY+Ri4yNvY2NjY69jtsPZkYPAYXXlonpZD2NjY2IvYRb1SNpjyQ8wWhpJm1jenxcbGxl7Bno1Or781Gx6cB7podWFjY2OvY+eYnJoXG7OVnbax9XQ8gI2Njb2C3TZ0TlpIJ6s2s/d+k7exsbGxP5ydX237pk2K+eLOyXCixmNjY2N/CHuWtGZpLwnK+XmiBIaNjY29jj1LRbMhbhKOWSMpUWBjY2P/ZHb7ueWdYJJmFjY2NjZ2u0zTli6zRZz878NSBBsbG/tD2HlLKG/ot0c8mWy058HGxsbexD5qzcTrkidvOVkbOil4sLGxsd+e/R+nqoMGV4jSjgAAAABJRU5ErkJggg==">分享</a><div class="tags"><a href="/tags/Flink/">Flink</a></div><div class="post-nav"><a class="next" href="/2020/03/22/分布式数据集成框架gobblin快速入门/">分布式数据集成框架gobblin快速入门</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar-toc"><div class="stoc-article" id="sidebar-stoc"><strong class="stoc-title"><i class="fa fa-list-ul"> 目录</i></strong><div class="toc-nav" id="stoc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#背景"><span class="toc-number">1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实现思路"><span class="toc-number">2.</span> <span class="toc-text">实现思路</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实现方案"><span class="toc-number">3.</span> <span class="toc-text">实现方案</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Flink处理Kafka的binlog日志"><span class="toc-number">3.1.</span> <span class="toc-text">Flink处理Kafka的binlog日志</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#离线还原MySQL数据"><span class="toc-number">3.2.</span> <span class="toc-text">离线还原MySQL数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#小结"><span class="toc-number">4.</span> <span class="toc-text">小结</span></a></li></ol></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 - 2020 <a href="/." rel="nofollow">Jmx's Blog.</a>All rights reserved.<br>Thoughts on technology, life and everything else.</div></div></div><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" color="100,99,98" opacity="0.5" zindex="0.7" count="150" src="//lib.baomitu.com/canvas-nest.js/2.0.4/canvas-nest.umd.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>