<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记录朴实无华且枯燥的生活"><title>你真的了解Flink Kafka source吗？ | Jmx's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '912b6cfc43243cd27aeb428f7dbf7823';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">你真的了解Flink Kafka source吗？</h1><a id="logo" href="/.">Jmx's Blog</a><p class="description">Keep it Simple and Stupid!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-pied-piper-alt"> 关于</i></a><a href="/tags"><i class="fa fa-tags"> 标签</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">你真的了解Flink Kafka source吗？</h1><div class="post-meta">Apr 2, 2020<span> | </span><span class="category"><a href="/categories/Flink/">Flink</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 7.9k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 33</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>Flink 提供了专门的 Kafka 连接器，向 Kafka topic 中读取或者写入数据。Flink Kafka Consumer 集成了 Flink 的 Checkpoint 机制，可提供 exactly-once 的处理语义。为此，Flink 并不完全依赖于跟踪 Kafka 消费组的偏移量，而是在内部跟踪和检查偏移量。</p>
<a id="more"></a>

<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>当我们在使用Spark Streaming、Flink等计算框架进行数据实时处理时，使用Kafka作为一款发布与订阅的消息系统成为了标配。Spark Streaming与Flink都提供了相对应的Kafka Consumer，使用起来非常的方便，只需要设置一下Kafka的参数，然后添加kafka的source就万事大吉了。如果你真的觉得事情就是如此的so easy，感觉妈妈再也不用担心你的学习了，那就真的是too young too simple sometimes naive了。本文以Flink 的Kafka Source为讨论对象，首先从基本的使用入手，然后深入源码逐一剖析，一并为你拨开Flink Kafka connector的神秘面纱。值得注意的是，本文假定读者具备了Kafka的相关知识，关于Kafka的相关细节问题，不在本文的讨论范围之内。</p>
<h2 id="Flink-Kafka-Consumer介绍"><a href="#Flink-Kafka-Consumer介绍" class="headerlink" title="Flink Kafka Consumer介绍"></a>Flink Kafka Consumer介绍</h2><p>Flink Kafka Connector有很多个版本，可以根据你的kafka和Flink的版本选择相应的包（maven artifact id）和类名。本文所涉及的Flink版本为1.10，Kafka的版本为2.3.4。Flink所提供的Maven依赖于类名如下表所示：</p>
<table>
<thead>
<tr>
<th align="left">Maven 依赖</th>
<th align="left">自从哪个版本 开始支持</th>
<th align="left">类名</th>
<th align="left">Kafka 版本</th>
<th align="left">注意</th>
</tr>
</thead>
<tbody><tr>
<td align="left">flink-connector-kafka-0.8_2.11</td>
<td align="left">1.0.0</td>
<td align="left">FlinkKafkaConsumer08 FlinkKafkaProducer08</td>
<td align="left">0.8.x</td>
<td align="left">这个连接器在内部使用 Kafka 的 <a href="https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+SimpleConsumer+Example" target="_blank" rel="noopener">SimpleConsumer</a> API。偏移量由 Flink 提交给 ZK。</td>
</tr>
<tr>
<td align="left">flink-connector-kafka-0.9_2.11</td>
<td align="left">1.0.0</td>
<td align="left">FlinkKafkaConsumer09 FlinkKafkaProducer09</td>
<td align="left">0.9.x</td>
<td align="left">这个连接器使用新的 Kafka <a href="http://kafka.apache.org/documentation.html#newconsumerapi" target="_blank" rel="noopener">Consumer API</a></td>
</tr>
<tr>
<td align="left">flink-connector-kafka-0.10_2.11</td>
<td align="left">1.2.0</td>
<td align="left">FlinkKafkaConsumer010 FlinkKafkaProducer010</td>
<td align="left">0.10.x</td>
<td align="left">这个连接器支持 <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message" target="_blank" rel="noopener">带有时间戳的 Kafka 消息</a>，用于生产和消费。</td>
</tr>
<tr>
<td align="left">flink-connector-kafka-0.11_2.11</td>
<td align="left">1.4.0</td>
<td align="left">FlinkKafkaConsumer011 FlinkKafkaProducer011</td>
<td align="left">&gt;=  0.11.x</td>
<td align="left">Kafka 从 0.11.x 版本开始不支持 Scala 2.10。此连接器支持了 <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging" target="_blank" rel="noopener">Kafka 事务性的消息传递</a>来为生产者提供 Exactly once 语义。</td>
</tr>
<tr>
<td align="left">flink-connector-kafka_2.11</td>
<td align="left">1.7.0</td>
<td align="left">FlinkKafkaConsumer FlinkKafkaProducer</td>
<td align="left">&gt;= 1.0.0</td>
<td align="left">这个通用的 Kafka 连接器尽力与 Kafka client 的最新版本保持同步。该连接器使用的 Kafka client 版本可能会在 Flink 版本之间发生变化。从 Flink 1.9 版本开始，它使用 Kafka 2.2.0 client。当前 Kafka 客户端向后兼容 0.10.0 或更高版本的 Kafka broker。 但是对于 Kafka 0.11.x 和 0.10.x 版本，我们建议你分别使用专用的 flink-connector-kafka-0.11_2.11 和 flink-connector-kafka-0.10_2.11 连接器。</td>
</tr>
</tbody></table>
<h2 id="Demo示例"><a href="#Demo示例" class="headerlink" title="Demo示例"></a>Demo示例</h2><h3 id="添加Maven依赖"><a href="#添加Maven依赖" class="headerlink" title="添加Maven依赖"></a>添加Maven依赖</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;!--本文使用的是通用型的connector--&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<h3 id="简单代码案例"><a href="#简单代码案例" class="headerlink" title="简单代码案例"></a>简单代码案例</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConnector</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment senv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">// 开启checkpoint，时间间隔为毫秒</span></span><br><span class="line">        senv.enableCheckpointing(<span class="number">5000L</span>);</span><br><span class="line">        <span class="comment">// 选择状态后端</span></span><br><span class="line">        senv.setStateBackend((StateBackend) <span class="keyword">new</span> FsStateBackend(<span class="string">"file:///E://checkpoint"</span>));</span><br><span class="line">        <span class="comment">//senv.setStateBackend((StateBackend) new FsStateBackend("hdfs://kms-1:8020/checkpoint"));</span></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// kafka broker地址</span></span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"kms-2:9092,kms-3:9092,kms-4:9092"</span>);</span><br><span class="line">        <span class="comment">// 仅kafka0.8版本需要配置</span></span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"kms-2:2181,kms-3:2181,kms-4:2181"</span>);</span><br><span class="line">        <span class="comment">// 消费者组</span></span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line">        <span class="comment">// 自动偏移量提交</span></span><br><span class="line">        props.put(<span class="string">"enable.auto.commit"</span>, <span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">// 偏移量提交的时间间隔，毫秒</span></span><br><span class="line">        props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="number">5000</span>);</span><br><span class="line">        <span class="comment">// kafka 消息的key序列化器</span></span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        <span class="comment">// kafka 消息的value序列化器</span></span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        <span class="comment">// 指定kafka的消费者从哪里开始消费数据</span></span><br><span class="line">        <span class="comment">// 共有三种方式，</span></span><br><span class="line">        <span class="comment">// #earliest</span></span><br><span class="line">        <span class="comment">// 当各分区下有已提交的offset时，从提交的offset开始消费；</span></span><br><span class="line">        <span class="comment">// 无提交的offset时，从头开始消费</span></span><br><span class="line">        <span class="comment">// #latest</span></span><br><span class="line">        <span class="comment">// 当各分区下有已提交的offset时，从提交的offset开始消费；</span></span><br><span class="line">        <span class="comment">// 无提交的offset时，消费新产生的该分区下的数据</span></span><br><span class="line">        <span class="comment">// #none</span></span><br><span class="line">        <span class="comment">// topic各分区都存在已提交的offset时，</span></span><br><span class="line">        <span class="comment">// 从offset后开始消费；</span></span><br><span class="line">        <span class="comment">// 只要有一个分区不存在已提交的offset，则抛出异常</span></span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; consumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(</span><br><span class="line">                <span class="string">"qfbap_ods.code_city"</span>,</span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props);</span><br><span class="line">        <span class="comment">//设置checkpoint后在提交offset，即oncheckpoint模式</span></span><br><span class="line">        <span class="comment">// 该值默认为true，</span></span><br><span class="line">        consumer.setCommitOffsetsOnCheckpoints(<span class="keyword">true</span>);</span><br><span class="line">     </span><br><span class="line">        <span class="comment">// 最早的数据开始消费</span></span><br><span class="line">        <span class="comment">// 该模式下，Kafka 中的 committed offset 将被忽略，不会用作起始位置。</span></span><br><span class="line">        <span class="comment">//consumer.setStartFromEarliest();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 消费者组最近一次提交的偏移量，默认。</span></span><br><span class="line">        <span class="comment">// 如果找不到分区的偏移量，那么将会使用配置中的 auto.offset.reset 设置</span></span><br><span class="line">        <span class="comment">//consumer.setStartFromGroupOffsets();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 最新的数据开始消费</span></span><br><span class="line">        <span class="comment">// 该模式下，Kafka 中的 committed offset 将被忽略，不会用作起始位置。</span></span><br><span class="line">        <span class="comment">//consumer.setStartFromLatest();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定具体的偏移量时间戳,毫秒</span></span><br><span class="line">        <span class="comment">// 对于每个分区，其时间戳大于或等于指定时间戳的记录将用作起始位置。</span></span><br><span class="line">        <span class="comment">// 如果一个分区的最新记录早于指定的时间戳，则只从最新记录读取该分区数据。</span></span><br><span class="line">        <span class="comment">// 在这种模式下，Kafka 中的已提交 offset 将被忽略，不会用作起始位置。</span></span><br><span class="line">        <span class="comment">//consumer.setStartFromTimestamp(1585047859000L);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 为每个分区指定偏移量</span></span><br><span class="line">        <span class="comment">/*Map&lt;KafkaTopicPartition, Long&gt; specificStartOffsets = new HashMap&lt;&gt;();</span></span><br><span class="line"><span class="comment">        specificStartOffsets.put(new KafkaTopicPartition("qfbap_ods.code_city", 0), 23L);</span></span><br><span class="line"><span class="comment">        specificStartOffsets.put(new KafkaTopicPartition("qfbap_ods.code_city", 1), 31L);</span></span><br><span class="line"><span class="comment">        specificStartOffsets.put(new KafkaTopicPartition("qfbap_ods.code_city", 2), 43L);</span></span><br><span class="line"><span class="comment">        consumer1.setStartFromSpecificOffsets(specificStartOffsets);*/</span></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 请注意：当 Job 从故障中自动恢复或使用 savepoint 手动恢复时，</span></span><br><span class="line"><span class="comment">         * 这些起始位置配置方法不会影响消费的起始位置。</span></span><br><span class="line"><span class="comment">         * 在恢复时，每个 Kafka 分区的起始位置由存储在 savepoint 或 checkpoint 中的 offset 确定</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; source = senv.addSource(consumer);</span><br><span class="line">        <span class="comment">// TODO</span></span><br><span class="line">        source.print();</span><br><span class="line">        senv.execute(<span class="string">"test kafka connector"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="参数配置解读"><a href="#参数配置解读" class="headerlink" title="参数配置解读"></a>参数配置解读</h3><p>在Demo示例中，给出了详细的配置信息，下面将对上面的参数配置进行逐一分析。</p>
<h4 id="kakfa的properties参数配置"><a href="#kakfa的properties参数配置" class="headerlink" title="kakfa的properties参数配置"></a>kakfa的properties参数配置</h4><ul>
<li><p>bootstrap.servers：kafka broker地址</p>
</li>
<li><p>zookeeper.connect：仅kafka0.8版本需要配置</p>
</li>
<li><p>group.id：消费者组</p>
</li>
<li><p>enable.auto.commit：</p>
<p>自动偏移量提交，该值的配置不是最终的偏移量提交模式，需要考虑用户是否开启了checkpoint，</p>
<p>在下面的源码分析中会进行解读</p>
</li>
<li><p>auto.commit.interval.ms：偏移量提交的时间间隔，毫秒</p>
</li>
<li><p>key.deserializer：</p>
<p>kafka 消息的key序列化器，如果不指定会使用ByteArrayDeserializer序列化器</p>
</li>
<li><p>value.deserializer：</p>
</li>
</ul>
<p>kafka 消息的value序列化器，如果不指定会使用ByteArrayDeserializer序列化器</p>
<ul>
<li><p>auto.offset.reset：</p>
<p>指定kafka的消费者从哪里开始消费数据，共有三种方式，</p>
<ul>
<li>第一种：earliest<br>当各分区下有已提交的offset时，从提交的offset开始消费； 无提交的offset时，从头开始消费</li>
<li>第二种：latest<br>当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据</li>
<li>第三种：none<br>topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常</li>
</ul>
<p>注意：上面的指定消费模式并不是最终的消费模式，取决于用户在Flink程序中配置的消费模式</p>
</li>
</ul>
<h4 id="Flink程序用户配置的参数"><a href="#Flink程序用户配置的参数" class="headerlink" title="Flink程序用户配置的参数"></a>Flink程序用户配置的参数</h4><ul>
<li>consumer.setCommitOffsetsOnCheckpoints(true)</li>
</ul>
<p>​    解释：设置checkpoint后在提交offset，即oncheckpoint模式，该值默认为true，该参数会影响偏移量的提交方式，下面的源码中会进行分析</p>
<ul>
<li><p>consumer.setStartFromEarliest()</p>
<p>解释： 最早的数据开始消费 ，该模式下，Kafka 中的 committed offset 将被忽略，不会用作起始位置。该方法为继承父类FlinkKafkaConsumerBase的方法。</p>
</li>
<li><p>consumer.setStartFromGroupOffsets()</p>
<p>解释：消费者组最近一次提交的偏移量，默认。 如果找不到分区的偏移量，那么将会使用配置中的 auto.offset.reset 设置，该方法为继承父类FlinkKafkaConsumerBase的方法。</p>
</li>
<li><p>consumer.setStartFromLatest()</p>
<p>解释：最新的数据开始消费，该模式下，Kafka 中的 committed offset 将被忽略，不会用作起始位置。该方法为继承父类FlinkKafkaConsumerBase的方法。</p>
</li>
<li><p>consumer.setStartFromTimestamp(1585047859000L)</p>
<p>解释：指定具体的偏移量时间戳,毫秒。对于每个分区，其时间戳大于或等于指定时间戳的记录将用作起始位置。 如果一个分区的最新记录早于指定的时间戳，则只从最新记录读取该分区数据。在这种模式下，Kafka 中的已提交 offset 将被忽略，不会用作起始位置。</p>
</li>
<li><p>consumer.setStartFromSpecificOffsets(specificStartOffsets)</p>
</li>
</ul>
<p>解释：为每个分区指定偏移量，该方法为继承父类FlinkKafkaConsumerBase的方法。</p>
<p>请注意：当 Job 从故障中自动恢复或使用 savepoint 手动恢复时，这些起始位置配置方法不会影响消费的起始位置。在恢复时，每个 Kafka 分区的起始位置由存储在 savepoint 或 checkpoint 中的 offset 确定。</p>
<h2 id="Flink-Kafka-Consumer源码解读"><a href="#Flink-Kafka-Consumer源码解读" class="headerlink" title="Flink Kafka Consumer源码解读"></a>Flink Kafka Consumer源码解读</h2><h3 id="继承关系"><a href="#继承关系" class="headerlink" title="继承关系"></a>继承关系</h3><p>Flink Kafka Consumer继承了FlinkKafkaConsumerBase抽象类，而FlinkKafkaConsumerBase抽象类又继承了RichParallelSourceFunction，所以要实现一个自定义的source时，有两种实现方式：一种是通过实现SourceFunction接口来自定义并行度为1的数据源；另一种是通过实现ParallelSourceFunction接口或者继承RichParallelSourceFunction来自定义具有并行度的数据源。FlinkKafkaConsumer的继承关系如下图所示。</p>
<p><img src="//jiamaoxiang.top/2020/04/02/你真的了解Flink-Kafka-connector吗？/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3Flink-Kafka-connector%E5%90%97%EF%BC%9F%5C%E7%BB%A7%E6%89%BF%E5%9B%BE.png" alt></p>
<h3 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h3><h4 id="FlinkKafkaConsumer源码"><a href="#FlinkKafkaConsumer源码" class="headerlink" title="FlinkKafkaConsumer源码"></a>FlinkKafkaConsumer源码</h4><p>先看一下FlinkKafkaConsumer的源码，为了方面阅读，本文将尽量给出本比较完整的源代码片段，具体如下所示：代码较长，在这里可以先有有一个总体的印象，下面会对重要的代码片段详细进行分析。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkKafkaConsumer</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">FlinkKafkaConsumerBase</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 配置轮询超时超时时间，使用flink.poll-timeout参数在properties进行配置</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String KEY_POLL_TIMEOUT = <span class="string">"flink.poll-timeout"</span>;</span><br><span class="line">	<span class="comment">// 如果没有可用数据，则等待轮询所需的时间（以毫秒为单位）。 如果为0，则立即返回所有可用的记录</span></span><br><span class="line">	<span class="comment">//默认轮询超时时间</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> DEFAULT_POLL_TIMEOUT = <span class="number">100L</span>;</span><br><span class="line">	<span class="comment">// 用户提供的kafka 参数配置</span></span><br><span class="line">	<span class="keyword">protected</span> <span class="keyword">final</span> Properties properties;</span><br><span class="line">	<span class="comment">// 如果没有可用数据，则等待轮询所需的时间（以毫秒为单位）。 如果为0，则立即返回所有可用的记录</span></span><br><span class="line">	<span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">long</span> pollTimeout;</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> topic                   消费的主题名称</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> valueDeserializer       反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> props                   用户传入的kafka参数</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(String topic, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>(Collections.singletonList(topic), valueDeserializer, props);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment">	 * 该构造方法允许传入KafkaDeserializationSchema，该反序列化类支持访问kafka消费的额外信息</span></span><br><span class="line"><span class="comment">	 * 比如：key/value对，offsets(偏移量)，topic(主题名称)</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> topic                消费的主题名称</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> deserializer         反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> props                用户传入的kafka参数</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(String topic, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>(Collections.singletonList(topic), deserializer, props);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment">	 * 该构造方法允许传入多个topic(主题)，支持消费多个主题</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> topics          消费的主题名称，多个主题为List集合</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> deserializer    反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> props           用户传入的kafka参数</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(List&lt;String&gt; topics, DeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>(topics, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(deserializer), props);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment">	 * 该构造方法允许传入多个topic(主题)，支持消费多个主题,</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> topics         消费的主题名称，多个主题为List集合</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> deserializer   反序列化类型，用于将kafka的字节消息转换为Flink的对象,支持获取额外信息</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> props          用户传入的kafka参数</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(List&lt;String&gt; topics, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>(topics, <span class="keyword">null</span>, deserializer, props);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 基于正则表达式订阅多个topic</span></span><br><span class="line"><span class="comment">	 * 如果开启了分区发现，即FlinkKafkaConsumer.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值为非负数</span></span><br><span class="line"><span class="comment">	 * 只要是能够正则匹配上，主题一旦被创建就会立即被订阅</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> subscriptionPattern   主题的正则表达式</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> valueDeserializer   反序列化类型，用于将kafka的字节消息转换为Flink的对象,支持获取额外信息</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> props               用户传入的kafka参数</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(Pattern subscriptionPattern, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>(<span class="keyword">null</span>, subscriptionPattern, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(valueDeserializer), props);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 基于正则表达式订阅多个topic</span></span><br><span class="line"><span class="comment">	 * 如果开启了分区发现，即FlinkKafkaConsumer.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值为非负数</span></span><br><span class="line"><span class="comment">	 * 只要是能够正则匹配上，主题一旦被创建就会立即被订阅</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> subscriptionPattern   主题的正则表达式</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> deserializer          该反序列化类支持访问kafka消费的额外信息,比如：key/value对，offsets(偏移量)，topic(主题名称)</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> props                 用户传入的kafka参数</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(Pattern subscriptionPattern, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>(<span class="keyword">null</span>, subscriptionPattern, deserializer, props);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">private</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">		List&lt;String&gt; topics,</span></span></span><br><span class="line"><span class="function"><span class="params">		Pattern subscriptionPattern,</span></span></span><br><span class="line"><span class="function"><span class="params">		KafkaDeserializationSchema&lt;T&gt; deserializer,</span></span></span><br><span class="line"><span class="function"><span class="params">		Properties props)</span> </span>&#123;</span><br><span class="line">		<span class="comment">// 调用父类(FlinkKafkaConsumerBase)构造方法，PropertiesUtil.getLong方法第一个参数为Properties，第二个参数为key，第三个参数为value默认值</span></span><br><span class="line">		<span class="keyword">super</span>(</span><br><span class="line">			topics,</span><br><span class="line">			subscriptionPattern,</span><br><span class="line">			deserializer,</span><br><span class="line">			getLong(</span><br><span class="line">				checkNotNull(props, <span class="string">"props"</span>),</span><br><span class="line">				KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS, PARTITION_DISCOVERY_DISABLED),</span><br><span class="line">			!getBoolean(props, KEY_DISABLE_METRICS, <span class="keyword">false</span>));</span><br><span class="line"></span><br><span class="line">		<span class="keyword">this</span>.properties = props;</span><br><span class="line">		setDeserializer(<span class="keyword">this</span>.properties);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 配置轮询超时时间，如果在properties中配置了KEY_POLL_TIMEOUT参数，则返回具体的配置值，否则返回默认值DEFAULT_POLL_TIMEOUT</span></span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			<span class="keyword">if</span> (properties.containsKey(KEY_POLL_TIMEOUT)) &#123;</span><br><span class="line">				<span class="keyword">this</span>.pollTimeout = Long.parseLong(properties.getProperty(KEY_POLL_TIMEOUT));</span><br><span class="line">			&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">				<span class="keyword">this</span>.pollTimeout = DEFAULT_POLL_TIMEOUT;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Cannot parse poll timeout for '"</span> + KEY_POLL_TIMEOUT + <span class="string">'\''</span>, e);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">   <span class="comment">// 父类(FlinkKafkaConsumerBase)方法重写，该方法的作用是返回一个fetcher实例，</span></span><br><span class="line">	<span class="comment">// fetcher的作用是连接kafka的broker，拉去数据并进行反序列化，然后将数据输出为数据流(data stream)</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="keyword">protected</span> AbstractFetcher&lt;T, ?&gt; createFetcher(</span><br><span class="line">		SourceContext&lt;T&gt; sourceContext,</span><br><span class="line">		Map&lt;KafkaTopicPartition, Long&gt; assignedPartitionsWithInitialOffsets,</span><br><span class="line">		SerializedValue&lt;AssignerWithPeriodicWatermarks&lt;T&gt;&gt; watermarksPeriodic,</span><br><span class="line">		SerializedValue&lt;AssignerWithPunctuatedWatermarks&lt;T&gt;&gt; watermarksPunctuated,</span><br><span class="line">		StreamingRuntimeContext runtimeContext,</span><br><span class="line">		OffsetCommitMode offsetCommitMode,</span><br><span class="line">		MetricGroup consumerMetricGroup,</span><br><span class="line">		<span class="keyword">boolean</span> useMetrics) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 确保当偏移量的提交模式为ON_CHECKPOINTS(条件1：开启checkpoint，条件2：consumer.setCommitOffsetsOnCheckpoints(true))时，禁用自动提交</span></span><br><span class="line">		<span class="comment">// 该方法为父类(FlinkKafkaConsumerBase)的静态方法</span></span><br><span class="line">		<span class="comment">// 这将覆盖用户在properties中配置的任何设置</span></span><br><span class="line">		<span class="comment">// 当offset的模式为ON_CHECKPOINTS，或者为DISABLED时，会将用户配置的properties属性进行覆盖</span></span><br><span class="line">		<span class="comment">// 具体是将ENABLE_AUTO_COMMIT_CONFIG = "enable.auto.commit"的值重置为"false</span></span><br><span class="line">        <span class="comment">// 可以理解为：如果开启了checkpoint，并且设置了consumer.setCommitOffsetsOnCheckpoints(true)，默认为true，</span></span><br><span class="line">		<span class="comment">// 就会将kafka properties的enable.auto.commit强制置为false</span></span><br><span class="line">		adjustAutoCommitConfig(properties, offsetCommitMode);</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">new</span> KafkaFetcher&lt;&gt;(</span><br><span class="line">			sourceContext,</span><br><span class="line">			assignedPartitionsWithInitialOffsets,</span><br><span class="line">			watermarksPeriodic,</span><br><span class="line">			watermarksPunctuated,</span><br><span class="line">			runtimeContext.getProcessingTimeService(),</span><br><span class="line">			runtimeContext.getExecutionConfig().getAutoWatermarkInterval(),</span><br><span class="line">			runtimeContext.getUserCodeClassLoader(),</span><br><span class="line">			runtimeContext.getTaskNameWithSubtasks(),</span><br><span class="line">			deserializer,</span><br><span class="line">			properties,</span><br><span class="line">			pollTimeout,</span><br><span class="line">			runtimeContext.getMetricGroup(),</span><br><span class="line">			consumerMetricGroup,</span><br><span class="line">			useMetrics);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">//父类(FlinkKafkaConsumerBase)方法重写</span></span><br><span class="line">	<span class="comment">// 返回一个分区发现类，分区发现可以使用kafka broker的高级consumer API发现topic和partition的元数据</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> AbstractPartitionDiscoverer <span class="title">createPartitionDiscoverer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">		KafkaTopicsDescriptor topicsDescriptor,</span></span></span><br><span class="line"><span class="function"><span class="params">		<span class="keyword">int</span> indexOfThisSubtask,</span></span></span><br><span class="line"><span class="function"><span class="params">		<span class="keyword">int</span> numParallelSubtasks)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">new</span> KafkaPartitionDiscoverer(topicsDescriptor, indexOfThisSubtask, numParallelSubtasks, properties);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 *判断是否在kafka的参数开启了自动提交，即enable.auto.commit=true，</span></span><br><span class="line"><span class="comment">	 * 并且auto.commit.interval.ms&gt;0,</span></span><br><span class="line"><span class="comment">	 * 注意：如果没有没有设置enable.auto.commit的参数，则默认为true</span></span><br><span class="line"><span class="comment">	 *       如果没有设置auto.commit.interval.ms的参数，则默认为5000毫秒</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">getIsAutoCommitEnabled</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="comment">//</span></span><br><span class="line">		<span class="keyword">return</span> getBoolean(properties, ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">true</span>) &amp;&amp;</span><br><span class="line">			PropertiesUtil.getLong(properties, ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="number">5000</span>) &gt; <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 确保配置了kafka消息的key与value的反序列化方式，</span></span><br><span class="line"><span class="comment">	 * 如果没有配置，则使用ByteArrayDeserializer序列化器，</span></span><br><span class="line"><span class="comment">	 * 该类的deserialize方法是直接将数据进行return，未做任何处理</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> props</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">setDeserializer</span><span class="params">(Properties props)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">final</span> String deSerName = ByteArrayDeserializer.class.getName();</span><br><span class="line"></span><br><span class="line">		Object keyDeSer = props.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">		Object valDeSer = props.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (keyDeSer != <span class="keyword">null</span> &amp;&amp; !keyDeSer.equals(deSerName)) &#123;</span><br><span class="line">			LOG.warn(<span class="string">"Ignoring configured key DeSerializer (&#123;&#125;)"</span>, ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> (valDeSer != <span class="keyword">null</span> &amp;&amp; !valDeSer.equals(deSerName)) &#123;</span><br><span class="line">			LOG.warn(<span class="string">"Ignoring configured value DeSerializer (&#123;&#125;)"</span>, ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">		&#125;</span><br><span class="line">		props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, deSerName);</span><br><span class="line">		props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, deSerName);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>上面的代码已经给出了非常详细的注释，下面将对比较关键的部分进行分析。</p>
<ul>
<li><p>构造方法分析</p>
<p><img src="//jiamaoxiang.top/2020/04/02/你真的了解Flink-Kafka-connector吗？/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3Flink-Kafka-connector%E5%90%97%EF%BC%9F%5C%E6%9E%84%E9%80%A0%E6%96%B9%E6%B3%95%E9%87%8D%E5%86%99.png" alt></p>
</li>
</ul>
<p>FlinkKakfaConsumer提供了7种构造方法，如上图所示。不同的构造方法分别具有不同的功能，通过传递的参数也可以大致分析出每种构造方法特有的功能，为了方便理解，本文将对其进行分组讨论，具体如下：</p>
<p><strong>单topic</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> topic                   消费的主题名称</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> valueDeserializer       反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> props                   用户传入的kafka参数</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(String topic, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>(Collections.singletonList(topic), valueDeserializer, props);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment">	 * 该构造方法允许传入KafkaDeserializationSchema，该反序列化类支持访问kafka消费的额外信息</span></span><br><span class="line"><span class="comment">	 * 比如：key/value对，offsets(偏移量)，topic(主题名称)</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> topic                消费的主题名称</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> deserializer         反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> props                用户传入的kafka参数</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(String topic, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>(Collections.singletonList(topic), deserializer, props);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<p>上面两种构造方法只支持单个topic，区别在于反序列化的方式不一样。第一种使用的是DeserializationSchema，第二种使用的是KafkaDeserializationSchema，其中使用带有KafkaDeserializationSchema参数的构造方法可以获取更多的附属信息，比如在某些场景下需要获取key/value对，offsets(偏移量)，topic(主题名称)等信息，可以选择使用此方式的构造方法。以上两种方法都调用了私有的构造方法，私有构造方法的分析见下面。</p>
<p><strong>多topic</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment">	 * 该构造方法允许传入多个topic(主题)，支持消费多个主题</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> topics          消费的主题名称，多个主题为List集合</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> deserializer    反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> props           用户传入的kafka参数</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(List&lt;String&gt; topics, DeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>(topics, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(deserializer), props);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment">	 * 该构造方法允许传入多个topic(主题)，支持消费多个主题,</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> topics         消费的主题名称，多个主题为List集合</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> deserializer   反序列化类型，用于将kafka的字节消息转换为Flink的对象,支持获取额外信息</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> props          用户传入的kafka参数</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(List&lt;String&gt; topics, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>(topics, <span class="keyword">null</span>, deserializer, props);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<p>上面的两种多topic的构造方法，可以使用一个list集合接收多个topic进行消费，区别在于反序列化的方式不一样。第一种使用的是DeserializationSchema，第二种使用的是KafkaDeserializationSchema，其中使用带有KafkaDeserializationSchema参数的构造方法可以获取更多的附属信息，比如在某些场景下需要获取key/value对，offsets(偏移量)，topic(主题名称)等信息，可以选择使用此方式的构造方法。以上两种方法都调用了私有的构造方法，私有构造方法的分析见下面。</p>
<p><strong>正则匹配topic</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 基于正则表达式订阅多个topic</span></span><br><span class="line"><span class="comment">	 * 如果开启了分区发现，即FlinkKafkaConsumer.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值为非负数</span></span><br><span class="line"><span class="comment">	 * 只要是能够正则匹配上，主题一旦被创建就会立即被订阅</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> subscriptionPattern   主题的正则表达式</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> valueDeserializer   反序列化类型，用于将kafka的字节消息转换为Flink的对象,支持获取额外信息</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> props               用户传入的kafka参数</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(Pattern subscriptionPattern, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>(<span class="keyword">null</span>, subscriptionPattern, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(valueDeserializer), props);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 基于正则表达式订阅多个topic</span></span><br><span class="line"><span class="comment">	 * 如果开启了分区发现，即FlinkKafkaConsumer.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值为非负数</span></span><br><span class="line"><span class="comment">	 * 只要是能够正则匹配上，主题一旦被创建就会立即被订阅</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> subscriptionPattern   主题的正则表达式</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> deserializer          该反序列化类支持访问kafka消费的额外信息,比如：key/value对，offsets(偏移量)，topic(主题名称)</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> props                 用户传入的kafka参数</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(Pattern subscriptionPattern, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>(<span class="keyword">null</span>, subscriptionPattern, deserializer, props);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<p>实际的生产环境中可能有这样一些需求，比如有一个flink作业需要将多种不同的数据聚合到一起，而这些数据对应着不同的kafka topic，随着业务增长，新增一类数据，同时新增了一个kafka topic，如何在不重启作业的情况下作业自动感知新的topic。首先需要在构建FlinkKafkaConsumer时的properties中设置flink.partition-discovery.interval-millis参数为非负值，表示开启动态发现的开关，以及设置的时间间隔。此时FLinkKafkaConsumer内部会启动一个单独的线程定期去kafka获取最新的meta信息。具体的调用执行信息，参见下面的私有构造方法</p>
<p><strong>私有构造方法</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">	List&lt;String&gt; topics,</span></span></span><br><span class="line"><span class="function"><span class="params">	Pattern subscriptionPattern,</span></span></span><br><span class="line"><span class="function"><span class="params">	KafkaDeserializationSchema&lt;T&gt; deserializer,</span></span></span><br><span class="line"><span class="function"><span class="params">	Properties props)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 调用父类(FlinkKafkaConsumerBase)构造方法，PropertiesUtil.getLong方法第一个参数为Properties，第二个参数为key，第三个参数为value默认值。KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值是开启分区发现的配置参数，在properties里面配置flink.partition-discovery.interval-millis=5000(大于0的数),如果没有配置则使用PARTITION_DISCOVERY_DISABLED=Long.MIN_VALUE(表示禁用分区发现)</span></span><br><span class="line">	<span class="keyword">super</span>(</span><br><span class="line">		topics,</span><br><span class="line">		subscriptionPattern,</span><br><span class="line">		deserializer,</span><br><span class="line">		getLong(</span><br><span class="line">			checkNotNull(props, <span class="string">"props"</span>),</span><br><span class="line">			KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS, PARTITION_DISCOVERY_DISABLED),</span><br><span class="line">		!getBoolean(props, KEY_DISABLE_METRICS, <span class="keyword">false</span>));</span><br><span class="line"></span><br><span class="line">	<span class="keyword">this</span>.properties = props;</span><br><span class="line">	setDeserializer(<span class="keyword">this</span>.properties);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 配置轮询超时时间，如果在properties中配置了KEY_POLL_TIMEOUT参数，则返回具体的配置值，否则返回默认值DEFAULT_POLL_TIMEOUT</span></span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		<span class="keyword">if</span> (properties.containsKey(KEY_POLL_TIMEOUT)) &#123;</span><br><span class="line">			<span class="keyword">this</span>.pollTimeout = Long.parseLong(properties.getProperty(KEY_POLL_TIMEOUT));</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="keyword">this</span>.pollTimeout = DEFAULT_POLL_TIMEOUT;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">		<span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Cannot parse poll timeout for '"</span> + KEY_POLL_TIMEOUT + <span class="string">'\''</span>, e);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>其他方法分析</li>
</ul>
<p><strong>KafkaFetcher对象创建</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">// 父类(FlinkKafkaConsumerBase)方法重写，该方法的作用是返回一个fetcher实例，</span></span><br><span class="line"><span class="comment">// fetcher的作用是连接kafka的broker，拉去数据并进行反序列化，然后将数据输出为数据流(data stream)</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">protected</span> AbstractFetcher&lt;T, ?&gt; createFetcher(</span><br><span class="line">	SourceContext&lt;T&gt; sourceContext,</span><br><span class="line">	Map&lt;KafkaTopicPartition, Long&gt; assignedPartitionsWithInitialOffsets,</span><br><span class="line">	SerializedValue&lt;AssignerWithPeriodicWatermarks&lt;T&gt;&gt; watermarksPeriodic,</span><br><span class="line">	SerializedValue&lt;AssignerWithPunctuatedWatermarks&lt;T&gt;&gt; watermarksPunctuated,</span><br><span class="line">	StreamingRuntimeContext runtimeContext,</span><br><span class="line">	OffsetCommitMode offsetCommitMode,</span><br><span class="line">	MetricGroup consumerMetricGroup,</span><br><span class="line">	<span class="keyword">boolean</span> useMetrics) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">       <span class="comment">// 确保当偏移量的提交模式为ON_CHECKPOINTS(条件1：开启checkpoint，条件2：consumer.setCommitOffsetsOnCheckpoints(true))时，禁用自动提交</span></span><br><span class="line">	<span class="comment">// 该方法为父类(FlinkKafkaConsumerBase)的静态方法</span></span><br><span class="line">	<span class="comment">// 这将覆盖用户在properties中配置的任何设置</span></span><br><span class="line">	<span class="comment">// 当offset的模式为ON_CHECKPOINTS，或者为DISABLED时，会将用户配置的properties属性进行覆盖</span></span><br><span class="line">	<span class="comment">// 具体是将ENABLE_AUTO_COMMIT_CONFIG = "enable.auto.commit"的值重置为"false</span></span><br><span class="line">       <span class="comment">// 可以理解为：如果开启了checkpoint，并且设置了consumer.setCommitOffsetsOnCheckpoints(true)，默认为true，</span></span><br><span class="line">	<span class="comment">// 就会将kafka properties的enable.auto.commit强制置为false</span></span><br><span class="line">	adjustAutoCommitConfig(properties, offsetCommitMode);</span><br><span class="line">	<span class="keyword">return</span> <span class="keyword">new</span> KafkaFetcher&lt;&gt;(</span><br><span class="line">		sourceContext,</span><br><span class="line">		assignedPartitionsWithInitialOffsets,</span><br><span class="line">		watermarksPeriodic,</span><br><span class="line">		watermarksPunctuated,</span><br><span class="line">		runtimeContext.getProcessingTimeService(),</span><br><span class="line">		runtimeContext.getExecutionConfig().getAutoWatermarkInterval(),</span><br><span class="line">		runtimeContext.getUserCodeClassLoader(),</span><br><span class="line">		runtimeContext.getTaskNameWithSubtasks(),</span><br><span class="line">		deserializer,</span><br><span class="line">		properties,</span><br><span class="line">		pollTimeout,</span><br><span class="line">		runtimeContext.getMetricGroup(),</span><br><span class="line">		consumerMetricGroup,</span><br><span class="line">		useMetrics);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该方法的作用是返回一个fetcher实例，fetcher的作用是连接kafka的broker，拉去数据并进行反序列化，然后将数据输出为数据流(data stream)，在这里对自动偏移量提交模式进行了强制调整，即确保当偏移量的提交模式为ON_CHECKPOINTS(条件1：开启checkpoint，条件2：consumer.setCommitOffsetsOnCheckpoints(true))时，禁用自动提交。这将覆盖用户在properties中配置的任何设置，简单可以理解为：如果开启了checkpoint，并且设置了consumer.setCommitOffsetsOnCheckpoints(true)，默认为true，就会将kafka properties的enable.auto.commit强制置为false。关于offset的提交模式，见下文的偏移量提交模式分析。</p>
<p><strong>判断是否设置了自动提交</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">getIsAutoCommitEnabled</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="comment">//</span></span><br><span class="line">	<span class="keyword">return</span> getBoolean(properties, ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">true</span>) &amp;&amp;</span><br><span class="line">		PropertiesUtil.getLong(properties, ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="number">5000</span>) &gt; <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>判断是否在kafka的参数开启了自动提交，即enable.auto.commit=true，并且auto.commit.interval.ms&gt;0, 注意：如果没有没有设置enable.auto.commit的参数，则默认为true, 如果没有设置auto.commit.interval.ms的参数，则默认为5000毫秒。该方法会在FlinkKafkaConsumerBase的open方法进行初始化的时候调用。</p>
<p><strong>反序列化</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">setDeserializer</span><span class="params">(Properties props)</span> </span>&#123;</span><br><span class="line">         <span class="comment">// 默认的反序列化方式 </span></span><br><span class="line">		<span class="keyword">final</span> String deSerName = ByteArrayDeserializer.class.getName();</span><br><span class="line">         <span class="comment">//获取用户配置的properties关于key与value的反序列化模式</span></span><br><span class="line">		Object keyDeSer = props.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">		Object valDeSer = props.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">         <span class="comment">// 如果配置了，则使用用户配置的值</span></span><br><span class="line">		<span class="keyword">if</span> (keyDeSer != <span class="keyword">null</span> &amp;&amp; !keyDeSer.equals(deSerName)) &#123;</span><br><span class="line">			LOG.warn(<span class="string">"Ignoring configured key DeSerializer (&#123;&#125;)"</span>, ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> (valDeSer != <span class="keyword">null</span> &amp;&amp; !valDeSer.equals(deSerName)) &#123;</span><br><span class="line">			LOG.warn(<span class="string">"Ignoring configured value DeSerializer (&#123;&#125;)"</span>, ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">		&#125;</span><br><span class="line">        <span class="comment">// 没有配置，则使用ByteArrayDeserializer进行反序列化</span></span><br><span class="line">		props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, deSerName);</span><br><span class="line">		props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, deSerName);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<p>确保配置了kafka消息的key与value的反序列化方式，如果没有配置，则使用ByteArrayDeserializer序列化器，<br>ByteArrayDeserializer类的deserialize方法是直接将数据进行return，未做任何处理。</p>
<h4 id="FlinkKafkaConsumerBase源码"><a href="#FlinkKafkaConsumerBase源码" class="headerlink" title="FlinkKafkaConsumerBase源码"></a>FlinkKafkaConsumerBase源码</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkKafkaConsumerBase</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">		<span class="title">CheckpointListener</span>,</span></span><br><span class="line"><span class="class">		<span class="title">ResultTypeQueryable</span>&lt;<span class="title">T</span>&gt;,</span></span><br><span class="line"><span class="class">		<span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAX_NUM_PENDING_CHECKPOINTS = <span class="number">100</span>;</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> PARTITION_DISCOVERY_DISABLED = Long.MIN_VALUE;</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String KEY_DISABLE_METRICS = <span class="string">"flink.disable-metrics"</span>;</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS = <span class="string">"flink.partition-discovery.interval-millis"</span>;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String OFFSETS_STATE_NAME = <span class="string">"topic-partition-offset-states"</span>;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">boolean</span> enableCommitOnCheckpoints = <span class="keyword">true</span>;</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 偏移量的提交模式，仅能通过在FlinkKafkaConsumerBase#open(Configuration)进行配置</span></span><br><span class="line"><span class="comment">	 * 该值取决于用户是否开启了checkpoint</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="keyword">private</span> OffsetCommitMode offsetCommitMode;</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 配置从哪个位置开始消费kafka的消息，</span></span><br><span class="line"><span class="comment">	 * 默认为StartupMode#GROUP_OFFSETS，即从当前提交的偏移量开始消费</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="keyword">private</span> StartupMode startupMode = StartupMode.GROUP_OFFSETS;</span><br><span class="line">	<span class="keyword">private</span> Map&lt;KafkaTopicPartition, Long&gt; specificStartupOffsets;</span><br><span class="line">	<span class="keyword">private</span> Long startupOffsetsTimestamp;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 确保当偏移量的提交模式为ON_CHECKPOINTS时，禁用自动提交，</span></span><br><span class="line"><span class="comment">	 * 这将覆盖用户在properties中配置的任何设置。</span></span><br><span class="line"><span class="comment">	 * 当offset的模式为ON_CHECKPOINTS，或者为DISABLED时，会将用户配置的properties属性进行覆盖</span></span><br><span class="line"><span class="comment">	 * 具体是将ENABLE_AUTO_COMMIT_CONFIG = "enable.auto.commit"的值重置为"false，即禁用自动提交</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> properties       kafka配置的properties，会通过该方法进行覆盖</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> offsetCommitMode    offset提交模式</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">adjustAutoCommitConfig</span><span class="params">(Properties properties, OffsetCommitMode offsetCommitMode)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS || offsetCommitMode == OffsetCommitMode.DISABLED) &#123;</span><br><span class="line">			properties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="string">"false"</span>);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 决定是否在开启checkpoint时，在checkpoin之后提交偏移量，</span></span><br><span class="line"><span class="comment">	 * 只有用户配置了启用checkpoint，该参数才会其作用</span></span><br><span class="line"><span class="comment">	 * 如果没有开启checkpoint，则使用kafka的配置参数：enable.auto.commit</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> commitOnCheckpoints</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setCommitOffsetsOnCheckpoints</span><span class="params">(<span class="keyword">boolean</span> commitOnCheckpoints)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.enableCommitOnCheckpoints = commitOnCheckpoints;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 从最早的偏移量开始消费，</span></span><br><span class="line"><span class="comment">	 *该模式下，Kafka 中的已经提交的偏移量将被忽略，不会用作起始位置。</span></span><br><span class="line"><span class="comment">	 *可以通过consumer1.setStartFromEarliest()进行设置</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromEarliest</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.startupMode = StartupMode.EARLIEST;</span><br><span class="line">		<span class="keyword">this</span>.startupOffsetsTimestamp = <span class="keyword">null</span>;</span><br><span class="line">		<span class="keyword">this</span>.specificStartupOffsets = <span class="keyword">null</span>;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 从最新的数据开始消费,</span></span><br><span class="line"><span class="comment">	 *  该模式下，Kafka 中的 已提交的偏移量将被忽略，不会用作起始位置。</span></span><br><span class="line"><span class="comment">	 *</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromLatest</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.startupMode = StartupMode.LATEST;</span><br><span class="line">		<span class="keyword">this</span>.startupOffsetsTimestamp = <span class="keyword">null</span>;</span><br><span class="line">		<span class="keyword">this</span>.specificStartupOffsets = <span class="keyword">null</span>;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 *指定具体的偏移量时间戳,毫秒</span></span><br><span class="line"><span class="comment">	 *对于每个分区，其时间戳大于或等于指定时间戳的记录将用作起始位置。</span></span><br><span class="line"><span class="comment">	 * 如果一个分区的最新记录早于指定的时间戳，则只从最新记录读取该分区数据。</span></span><br><span class="line"><span class="comment">	 * 在这种模式下，Kafka 中的已提交 offset 将被忽略，不会用作起始位置。</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromTimestamp</span><span class="params">(<span class="keyword">long</span> startupOffsetsTimestamp)</span> </span>&#123;</span><br><span class="line">		checkArgument(startupOffsetsTimestamp &gt;= <span class="number">0</span>, <span class="string">"The provided value for the startup offsets timestamp is invalid."</span>);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">long</span> currentTimestamp = System.currentTimeMillis();</span><br><span class="line">		checkArgument(startupOffsetsTimestamp &lt;= currentTimestamp,</span><br><span class="line">			<span class="string">"Startup time[%s] must be before current time[%s]."</span>, startupOffsetsTimestamp, currentTimestamp);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">this</span>.startupMode = StartupMode.TIMESTAMP;</span><br><span class="line">		<span class="keyword">this</span>.startupOffsetsTimestamp = startupOffsetsTimestamp;</span><br><span class="line">		<span class="keyword">this</span>.specificStartupOffsets = <span class="keyword">null</span>;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 *</span></span><br><span class="line"><span class="comment">	 * 从具体的消费者组最近提交的偏移量开始消费，为默认方式</span></span><br><span class="line"><span class="comment">	 * 如果没有发现分区的偏移量，使用auto.offset.reset参数配置的值</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromGroupOffsets</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.startupMode = StartupMode.GROUP_OFFSETS;</span><br><span class="line">		<span class="keyword">this</span>.startupOffsetsTimestamp = <span class="keyword">null</span>;</span><br><span class="line">		<span class="keyword">this</span>.specificStartupOffsets = <span class="keyword">null</span>;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 *为每个分区指定偏移量进行消费</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromSpecificOffsets</span><span class="params">(Map&lt;KafkaTopicPartition, Long&gt; specificStartupOffsets)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.startupMode = StartupMode.SPECIFIC_OFFSETS;</span><br><span class="line">		<span class="keyword">this</span>.startupOffsetsTimestamp = <span class="keyword">null</span>;</span><br><span class="line">		<span class="keyword">this</span>.specificStartupOffsets = checkNotNull(specificStartupOffsets);</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration configuration)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		<span class="comment">// determine the offset commit mode</span></span><br><span class="line">		<span class="comment">// 决定偏移量的提交模式，</span></span><br><span class="line">		<span class="comment">// 第一个参数为是否开启了自动提交，</span></span><br><span class="line">		<span class="comment">// 第二个参数为是否开启了CommitOnCheckpoint模式</span></span><br><span class="line">		<span class="comment">// 第三个参数为是否开启了checkpoint</span></span><br><span class="line">		<span class="keyword">this</span>.offsetCommitMode = OffsetCommitModes.fromConfiguration(</span><br><span class="line">				getIsAutoCommitEnabled(),</span><br><span class="line">				enableCommitOnCheckpoints,</span><br><span class="line">				((StreamingRuntimeContext) getRuntimeContext()).isCheckpointingEnabled());</span><br><span class="line">       </span><br><span class="line">	   <span class="comment">// 省略的代码</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 省略的代码</span></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 创建一个fetcher用于连接kafka的broker，拉去数据并进行反序列化，然后将数据输出为数据流(data stream)</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> sourceContext   数据输出的上下文</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> subscribedPartitionsToStartOffsets  当前sub task需要处理的topic分区集合，即topic的partition与offset的Map集合</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> watermarksPeriodic    可选,一个序列化的时间戳提取器，生成periodic类型的 watermark</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> watermarksPunctuated  可选,一个序列化的时间戳提取器，生成punctuated类型的 watermark</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> runtimeContext        task的runtime context上下文</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> offsetCommitMode      offset的提交模式,有三种，分别为：DISABLED(禁用偏移量自动提交),ON_CHECKPOINTS(仅仅当checkpoints完成之后，才提交偏移量给kafka)</span></span><br><span class="line"><span class="comment">	 * KAFKA_PERIODIC(使用kafka自动提交函数，周期性自动提交偏移量)</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> kafkaMetricGroup   Flink的Metric</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> useMetrics         是否使用Metric</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@return</span>                   返回一个fetcher实例</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="keyword">protected</span> <span class="keyword">abstract</span> AbstractFetcher&lt;T, ?&gt; createFetcher(</span><br><span class="line">			SourceContext&lt;T&gt; sourceContext,</span><br><span class="line">			Map&lt;KafkaTopicPartition, Long&gt; subscribedPartitionsToStartOffsets,</span><br><span class="line">			SerializedValue&lt;AssignerWithPeriodicWatermarks&lt;T&gt;&gt; watermarksPeriodic,</span><br><span class="line">			SerializedValue&lt;AssignerWithPunctuatedWatermarks&lt;T&gt;&gt; watermarksPunctuated,</span><br><span class="line">			StreamingRuntimeContext runtimeContext,</span><br><span class="line">			OffsetCommitMode offsetCommitMode,</span><br><span class="line">			MetricGroup kafkaMetricGroup,</span><br><span class="line">			<span class="keyword">boolean</span> useMetrics) <span class="keyword">throws</span> Exception;</span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">getIsAutoCommitEnabled</span><span class="params">()</span></span>;</span><br><span class="line">	<span class="comment">// 省略的代码</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述代码是FlinkKafkaConsumerBase的部分代码片段，基本上对其做了详细注释，里面的有些方法是FlinkKafkaConsumer继承的，有些是重写的。之所以在这里给出，可以对照FlinkKafkaConsumer的源码，从而方便理解。</p>
<h3 id="偏移量提交模式分析"><a href="#偏移量提交模式分析" class="headerlink" title="偏移量提交模式分析"></a>偏移量提交模式分析</h3><p>Flink Kafka Consumer 允许有配置如何将 offset 提交回 Kafka broker（或 0.8 版本的 Zookeeper）的行为。请注意：Flink Kafka Consumer 不依赖于提交的 offset 来实现容错保证。提交的 offset 只是一种方法，用于公开 consumer 的进度以便进行监控。</p>
<p>配置 offset 提交行为的方法是否相同，取决于是否为 job 启用了 checkpointing。在这里先给出提交模式的具体结论，下面会对两种方式进行具体的分析。基本的结论为：</p>
<ul>
<li><p>开启checkpoint</p>
<ul>
<li><p>情况1：用户通过调用 consumer 上的 setCommitOffsetsOnCheckpoints(true) 方法来启用 offset 的提交(默认情况下为 true )<br>那么当 checkpointing 完成时，Flink Kafka Consumer 将提交的 offset 存储在 checkpoint 状态中。<br>这确保 Kafka broker 中提交的 offset 与 checkpoint 状态中的 offset 一致。<br>注意，在这个场景中，Properties 中的自动定期 offset 提交设置会被完全忽略。<br>此情况使用的是ON_CHECKPOINTS</p>
</li>
<li><p>情况2：用户通过调用 consumer 上的 setCommitOffsetsOnCheckpoints(“false”) 方法来禁用 offset 的提交，则使用DISABLED模式提交offset</p>
</li>
</ul>
</li>
<li><p>未开启checkpoint<br>Flink Kafka Consumer 依赖于内部使用的 Kafka client 自动定期 offset 提交功能，因此，要禁用或启用 offset 的提交</p>
</li>
<li><p>情况1：配置了Kafka properties的参数配置了”enable.auto.commit” = “true”或者 Kafka 0.8 的 auto.commit.enable=true，使用KAFKA_PERIODIC模式提交offset，即自动提交offset</p>
<ul>
<li>情况2：没有配置enable.auto.commit参数，使用DISABLED模式提交offset，这意味着kafka不知道当前的消费者组的消费者每次消费的偏移量。</li>
</ul>
</li>
</ul>
<h4 id="提交模式源码分析"><a href="#提交模式源码分析" class="headerlink" title="提交模式源码分析"></a>提交模式源码分析</h4><ul>
<li>offset的提交模式</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> OffsetCommitMode &#123;</span><br><span class="line">	<span class="comment">// 禁用偏移量自动提交</span></span><br><span class="line">	DISABLED,</span><br><span class="line">	<span class="comment">// 仅仅当checkpoints完成之后，才提交偏移量给kafka</span></span><br><span class="line">	ON_CHECKPOINTS,</span><br><span class="line">	<span class="comment">// 使用kafka自动提交函数，周期性自动提交偏移量</span></span><br><span class="line">	KAFKA_PERIODIC;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>提交模式的调用</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OffsetCommitModes</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> OffsetCommitMode <span class="title">fromConfiguration</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">			<span class="keyword">boolean</span> enableAutoCommit,</span></span></span><br><span class="line"><span class="function"><span class="params">			<span class="keyword">boolean</span> enableCommitOnCheckpoint,</span></span></span><br><span class="line"><span class="function"><span class="params">			<span class="keyword">boolean</span> enableCheckpointing)</span> </span>&#123;</span><br><span class="line">		<span class="comment">// 如果开启了checkinpoint，执行下面判断</span></span><br><span class="line">		<span class="keyword">if</span> (enableCheckpointing) &#123;</span><br><span class="line">			<span class="comment">// 如果开启了checkpoint，进一步判断是否在checkpoin启用时提交(setCommitOffsetsOnCheckpoints(true))，如果是则使用ON_CHECKPOINTS模式</span></span><br><span class="line">			<span class="comment">// 否则使用DISABLED模式</span></span><br><span class="line">			<span class="keyword">return</span> (enableCommitOnCheckpoint) ? OffsetCommitMode.ON_CHECKPOINTS : OffsetCommitMode.DISABLED;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="comment">// 若Kafka properties的参数配置了"enable.auto.commit" = "true"，则使用KAFKA_PERIODIC模式提交offset</span></span><br><span class="line">			<span class="comment">// 否则使用DISABLED模式</span></span><br><span class="line">			<span class="keyword">return</span> (enableAutoCommit) ? OffsetCommitMode.KAFKA_PERIODIC : OffsetCommitMode.DISABLED;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了Flink Kafka Consumer，首先对FlinkKafkaConsumer的不同版本进行了对比，然后给出了一个完整的Demo案例，并对案例的配置参数进行了详细解释，接着分析了FlinkKafkaConsumer的继承关系，并分别对FlinkKafkaConsumer以及其父类FlinkKafkaConsumerBase的源码进行了解读，最后从源码层面分析了Flink Kafka Consumer的偏移量提交模式，并对每一种提交模式进行了梳理。</p>
</div><div class="recommended_posts"><h3>相关推荐 ☟</h3><li><a href="https://jiamaoxiang.top/2020/04/12/Flink-DataStream-API编程指南/" target="_blank">Flink DataStream API编程指南</a></li><li><a href="https://jiamaoxiang.top/2020/04/09/如何使用Hive进行OLAP分析/" target="_blank">如何使用Hive进行OLAP分析</a></li><li><a href="https://jiamaoxiang.top/2020/03/31/Flink1-10集成Hive快速入门/" target="_blank">Flink1.10集成Hive快速入门</a></li><li><a href="https://jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/" target="_blank">Flink的八种分区策略源码解读</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Jia MaoXiang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2020/04/02/你真的了解Flink-Kafka-connector吗？/">https://jiamaoxiang.top/2020/04/02/你真的了解Flink-Kafka-connector吗？/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本文为博主原创文章，遵循CC BY-SA 4.0版权协议，转载请附上原文出处链接和本声明</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://jiamaoxiang.top/2020/04/02/你真的了解Flink-Kafka-connector吗？/" data-id="ckaf5u7xb003bz47qzt1fwbnv" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACp0lEQVR42u3awW7CQAwE0P7/T7dXDg2dsbMRql5OCFKyL5Ww49mvr/j4fjmSc17P/L44Xj+9en316c0HHh4e3mLpyRJz0uyK76+VrxkPDw/vNO9qEVfnzKjJX70/J18zHh4e3qfxNs13DsPDw8P7f7xkWfn7mwKDh4eH9zwvH7bOSsJsAPG+MNw8a8HDw8PrviFqlD/n9ZF8Dw8PD2+dqrdFYrZ1YPMNf6wWDw8P7wBv3y4nYdVmNNy2y3h4eHjP8xJAHkrNxrv5xq+odOHh4eE9wsu3Rt1LatcQhWp4eHh4x3izoeq+gW5/+utShIeHh3eAV59aLm42tmhvSpHg4eHh4d3Ea4enSePbLrpttYdbr/Dw8PBu4uUNa9tMt8HYZqgxfG7Aw8PDW/PyCCpvpmdB2mazFx4eHt5pXvtz/P6dJKxqlz4k4eHh4R3g5cWgRSajh83YN/pn4OHh4R3gtQvdHJvaVVc5PDw8vAO8PF56v6A6YRs14nmrjYeHh3eO14ZVs5FuOxmYMS5vDR4eHt6tvPwC9c9xMGhotybU4RweHh7eAV4bgG22ZG0+nZUcPDw8vBO8zYC1DbryMGwWoeHh4eE9w7sr7J9tPphtKaibdTw8PLzHefkXtcPW/Cd+1Y7j4eHhHeO1sX27eSu5TZvrRnu18PDw8I7xWnYegOXUtpBEKR8eHh7erbw8AJtR8w0ExVaApITg4eHh3cqbPfzPBrizYUcey/1yI/Dw8PAO8M490eejh+T8zXAZDw8P7wSvveQmKpuFW21hwMPDw3uG1y69Hd22DfRwsrKZMePh4eEd5rXfc64MRBMXPDw8vA/g5e/MSkjemv9RHvDw8PCO8fIoK4/E6vZ3vaWgTvnw8PDwRrz2gX8zhtjfsmGCh4eHh3cP7weRWg0B/eK12AAAAABJRU5ErkJggg==">分享</a><div class="tags"><a href="/tags/flink/">flink</a></div><div class="post-nav"><a class="pre" href="/2020/04/09/如何使用Hive进行OLAP分析/">如何使用Hive进行OLAP分析</a><a class="next" href="/2020/03/31/Flink1-10集成Hive快速入门/">Flink1.10集成Hive快速入门</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar-toc"><div class="stoc-article" id="sidebar-stoc"><strong class="stoc-title"><i class="fa fa-list-ul"> 目录</i></strong><div class="toc-nav" id="stoc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#引言"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Flink-Kafka-Consumer介绍"><span class="toc-number">2.</span> <span class="toc-text">Flink Kafka Consumer介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Demo示例"><span class="toc-number">3.</span> <span class="toc-text">Demo示例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#添加Maven依赖"><span class="toc-number">3.1.</span> <span class="toc-text">添加Maven依赖</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#简单代码案例"><span class="toc-number">3.2.</span> <span class="toc-text">简单代码案例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#参数配置解读"><span class="toc-number">3.3.</span> <span class="toc-text">参数配置解读</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#kakfa的properties参数配置"><span class="toc-number">3.3.1.</span> <span class="toc-text">kakfa的properties参数配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Flink程序用户配置的参数"><span class="toc-number">3.3.2.</span> <span class="toc-text">Flink程序用户配置的参数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Flink-Kafka-Consumer源码解读"><span class="toc-number">4.</span> <span class="toc-text">Flink Kafka Consumer源码解读</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#继承关系"><span class="toc-number">4.1.</span> <span class="toc-text">继承关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#源码解读"><span class="toc-number">4.2.</span> <span class="toc-text">源码解读</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#FlinkKafkaConsumer源码"><span class="toc-number">4.2.1.</span> <span class="toc-text">FlinkKafkaConsumer源码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#分析"><span class="toc-number">4.2.2.</span> <span class="toc-text">分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#FlinkKafkaConsumerBase源码"><span class="toc-number">4.2.3.</span> <span class="toc-text">FlinkKafkaConsumerBase源码</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#偏移量提交模式分析"><span class="toc-number">4.3.</span> <span class="toc-text">偏移量提交模式分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#提交模式源码分析"><span class="toc-number">4.3.1.</span> <span class="toc-text">提交模式源码分析</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#小结"><span class="toc-number">5.</span> <span class="toc-text">小结</span></a></li></ol></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 - 2020 <a href="/." rel="nofollow">Jmx's Blog.</a>All rights reserved.<br>Thoughts on technology, life and everything else.</div></div></div><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" color="100,99,98" opacity="0.5" zindex="0.7" count="150" src="//lib.baomitu.com/canvas-nest.js/2.0.4/canvas-nest.umd.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>