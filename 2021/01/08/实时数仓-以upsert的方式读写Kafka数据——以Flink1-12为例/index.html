<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="记录朴实无华且枯燥的生活"><title>实时数仓|以upsert的方式读写Kafka数据——以Flink1.12为例 | Jmx's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + '912b6cfc43243cd27aeb428f7dbf7823';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">实时数仓|以upsert的方式读写Kafka数据——以Flink1.12为例</h1><a id="logo" href="/.">Jmx's Blog</a><p class="description">Keep it Simple and Stupid!</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-pied-piper-alt"> 关于</i></a><a href="/tags"><i class="fa fa-tags"> 标签</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">实时数仓|以upsert的方式读写Kafka数据——以Flink1.12为例</h1><div class="post-meta">Jan 8, 2021<span> | </span><span class="category"><a href="/categories/Flink/">Flink</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 12</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>在某些场景中，比如GROUP BY聚合之后的结果，需要去更新之前的结果值。这个时候，需要将 Kafka 消息记录的 key 当成主键处理，用来确定一条数据是应该作为插入、删除还是更新记录来处理。在Flink1.11中，可以通过 <strong>flink-cdc-connectors</strong> 项目提供的 *<em>changelog-json format *</em>来实现该功能。关于该功能的使用，见之前的分享<a href="https://mp.weixin.qq.com/s/h0RvUH8upF8Cqn7PKfo6Qw" target="_blank" rel="noopener">Flink1.11中的CDC Connectors操作实践</a>。</p>
<p>在Flink1.12版本中， 新增了一个 <strong>upsert connector(upsert-kafka)</strong>，该 connector 扩展自现有的 Kafka connector，工作在 upsert 模式（FLIP-149）下。新的 upsert-kafka connector 既可以作为 source 使用，也可以作为 sink 使用，并且提供了与现有的 kafka connector 相同的基本功能和持久性保证，因为两者之间复用了大部分代码。本文将以Flink1.12为例，介绍该功能的基本使用步骤，以下是全文，希望对你有所帮助。</p>
<blockquote>
<p>公众号『大数据技术与数仓』，回复『资料』领取大数据资料包</p>
</blockquote>
<h2 id="Upsert-Kafka-connector简介"><a href="#Upsert-Kafka-connector简介" class="headerlink" title="Upsert Kafka connector简介"></a>Upsert Kafka connector简介</h2><p><strong>Upsert Kafka Connector</strong>允许用户以upsert的方式从Kafka主题读取数据或将数据写入Kafka主题。</p>
<p><strong>当作为数据源时</strong>，upsert-kafka Connector会生产一个changelog流，其中每条数据记录都表示一个更新或删除事件。更准确地说，如果不存在对应的key，则视为<strong>INSERT</strong>操作。如果已经存在了相对应的key，则该key对应的value值为最后一次更新的值。</p>
<p>用表来类比，changelog 流中的数据记录被解释为 UPSERT，也称为 INSERT/UPDATE，因为任何具有相同 key 的现有行都被覆盖。另外，value 为空的消息将会被视作为 DELETE 消息。</p>
<p><strong>当作为数据汇时</strong>，upsert-kafka Connector会消费一个changelog流。它将<strong>INSERT / UPDATE_AFTER</strong>数据作为正常的Kafka消息值写入(即INSERT和UPDATE操作，都会进行正常写入，如果是更新，则同一个key会存储多条数据，但在读取该表数据时，只保留最后一次更新的值)，并将 DELETE 数据以 value 为空的 Kafka 消息写入（key被打上墓碑标记，表示对应 key 的消息被删除）。Flink 将根据主键列的值对数据进行分区，从而保证主键上的消息有序，因此同一主键上的更新/删除消息将落在同一分区中</p>
<h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><p>为了使用Upsert Kafka连接器，需要添加下面的依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.12.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>如果使用SQL Client，需要下载<strong>flink-sql-connector-kafka_2.11-1.12.0.jar</strong>，并将其放置在Flink安装目录的lib文件夹下。</p>
<h2 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h2><h3 id="使用样例"><a href="#使用样例" class="headerlink" title="使用样例"></a>使用样例</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建一张kafka表，用户存储sink的数据</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> pageviews_per_region (</span><br><span class="line">  user_region <span class="keyword">STRING</span>,</span><br><span class="line">  pv <span class="built_in">BIGINT</span>,</span><br><span class="line">  uv <span class="built_in">BIGINT</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (user_region) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">'connector'</span> = <span class="string">'upsert-kafka'</span>,</span><br><span class="line">  <span class="string">'topic'</span> = <span class="string">'pageviews_per_region'</span>,</span><br><span class="line">  <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-2:9092,kms-3:9092,kms-4:9092'</span>,</span><br><span class="line">  <span class="string">'key.format'</span> = <span class="string">'avro'</span>,</span><br><span class="line">  <span class="string">'value.format'</span> = <span class="string">'avro'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<blockquote>
<p>尖叫提示：</p>
<p>要使用 upsert-kafka connector，必须在创建表时使用<strong>PRIMARY KEY</strong>定义主键，并为键（key.format）和值（value.format）指定序列化反序列化格式。</p>
</blockquote>
<h3 id="upsert-kafka-connector参数"><a href="#upsert-kafka-connector参数" class="headerlink" title="upsert-kafka connector参数"></a>upsert-kafka connector参数</h3><ul>
<li><strong>connector</strong></li>
</ul>
<p><strong>必选</strong>。指定要使用的连接器，Upsert Kafka 连接器使用：<code>&#39;upsert-kafka&#39;</code>。</p>
<ul>
<li><strong>topic</strong></li>
</ul>
<p><strong>必选</strong>。用于读取和写入的 Kafka topic 名称。</p>
<ul>
<li><strong>properties.bootstrap.servers</strong></li>
</ul>
<p><strong>必选</strong>。以逗号分隔的 Kafka brokers 列表。</p>
<ul>
<li><strong>key.format</strong></li>
</ul>
<p><strong>必选</strong>。用于对 Kafka 消息中 key 部分序列化和反序列化的格式。key 字段由 <strong>PRIMARY KEY</strong> 语法指定。支持的格式包括 <code>&#39;csv&#39;</code>、<code>&#39;json&#39;</code>、<code>&#39;avro&#39;</code>。</p>
<ul>
<li><strong>value.format</strong></li>
</ul>
<p><strong>必选</strong>。用于对 Kafka 消息中 value 部分序列化和反序列化的格式。支持的格式包括 <code>&#39;csv&#39;</code>、<code>&#39;json&#39;</code>、<code>&#39;avro&#39;</code>。</p>
<ul>
<li><em>*properties.</em> **</li>
</ul>
<p><strong>可选</strong>。 该选项可以传递任意的 Kafka 参数。选项的后缀名必须匹配定义在 <a href="https://kafka.apache.org/documentation/#configuration" target="_blank" rel="noopener">Kafka 参数文档</a>中的参数名。 Flink 会自动移除 选项名中的 “properties.” 前缀，并将转换后的键名以及值传入 KafkaClient。 例如，你可以通过 <code>&#39;properties.allow.auto.create.topics&#39; = &#39;false&#39;</code> 来禁止自动创建 topic。 但是，某些选项，例如<code>&#39;key.deserializer&#39;</code> 和 <code>&#39;value.deserializer&#39;</code> 是不允许通过该方式传递参数，因为 Flink 会重写这些参数的值。</p>
<ul>
<li><strong>value.fields-include</strong></li>
</ul>
<p><strong>可选</strong>，默认为<strong>ALL</strong>。控制key字段是否出现在 value 中。当取<strong>ALL</strong>时，表示<code>消息的 value 部分将包含 schema 中所有的字段，包括定义为主键的字段。</code>当取<strong>EXCEPT_KEY</strong>时，表示记录的 value 部分包含 schema 的所有字段，定义为主键的字段除外。</p>
<ul>
<li><strong>key.fields-prefix</strong></li>
</ul>
<p><strong>可选</strong>。为了避免与value字段命名冲突，为key字段添加一个自定义前缀。默认前缀为空。一旦指定了key字段的前缀，必须在DDL中指明前缀的名称，但是在构建key的序列化数据类型时，将移除该前缀。见下面的示例。在需要注意的是：使用该配置属性，<strong>value.fields-include</strong>的值必须为<strong>EXCEPT_KEY</strong>。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建一张upsert表，当指定了qwe前缀，涉及的key必须指定qwe前缀</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> result_total_pvuv_min_prefix (</span><br><span class="line">    qwedo_date     <span class="keyword">STRING</span>,     <span class="comment">-- 统计日期，必须包含qwe前缀</span></span><br><span class="line">    qwedo_min      <span class="keyword">STRING</span>,      <span class="comment">-- 统计分钟，必须包含qwe前缀</span></span><br><span class="line">    pv          <span class="built_in">BIGINT</span>,     <span class="comment">-- 点击量</span></span><br><span class="line">    uv          <span class="built_in">BIGINT</span>,     <span class="comment">-- 一天内同个访客多次访问仅计算一个UV</span></span><br><span class="line">    currenttime <span class="built_in">TIMESTAMP</span>,  <span class="comment">-- 当前时间</span></span><br><span class="line">    PRIMARY <span class="keyword">KEY</span> (qwedo_date, qwedo_min) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span> <span class="comment">-- 必须包含qwe前缀</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">'connector'</span> = <span class="string">'upsert-kafka'</span>,</span><br><span class="line">  <span class="string">'topic'</span> = <span class="string">'result_total_pvuv_min_prefix'</span>,</span><br><span class="line">  <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-2:9092,kms-3:9092,kms-4:9092'</span>,</span><br><span class="line">  <span class="string">'key.json.ignore-parse-errors'</span> = <span class="string">'true'</span>,</span><br><span class="line">  <span class="string">'value.json.fail-on-missing-field'</span> = <span class="string">'false'</span>,</span><br><span class="line">  <span class="string">'key.format'</span> = <span class="string">'json'</span>,</span><br><span class="line">  <span class="string">'value.format'</span> = <span class="string">'json'</span>,</span><br><span class="line">  <span class="string">'key.fields-prefix'</span>=<span class="string">'qwe'</span>, <span class="comment">-- 指定前缀qwe</span></span><br><span class="line">  <span class="string">'value.fields-include'</span> = <span class="string">'EXCEPT_KEY'</span> <span class="comment">-- key不出现kafka消息的value中</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">-- 向该表中写入数据</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> result_total_pvuv_min_prefix</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  do_date,    <span class="comment">--  时间分区</span></span><br><span class="line">  <span class="keyword">cast</span>(<span class="keyword">DATE_FORMAT</span> (access_time,<span class="string">'HH:mm'</span>) <span class="keyword">AS</span> <span class="keyword">STRING</span>) <span class="keyword">AS</span> do_min,<span class="comment">-- 分钟级别的时间</span></span><br><span class="line">  pv,</span><br><span class="line">  uv,</span><br><span class="line">  <span class="keyword">CURRENT_TIMESTAMP</span> <span class="keyword">AS</span> currenttime <span class="comment">-- 当前时间</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  view_total_pvuv_min;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>尖叫提示：</p>
<p>如果指定了key字段前缀，但在DDL中并没有添加该前缀字符串，那么在向该表写入数时，会抛出下面异常：</p>
<p>[ERROR] Could not execute SQL statement. Reason:<br>org.apache.flink.table.api.ValidationException: All fields in ‘key.fields’ must be prefixed with ‘qwe’ when option ‘key.fields-prefix’ is set but field ‘do_date’ is not prefixed.</p>
</blockquote>
<ul>
<li><strong>sink.parallelism</strong></li>
</ul>
<p><strong>可选</strong>。定义 upsert-kafka sink 算子的并行度。默认情况下，由框架确定并行度，与上游链接算子的并行度保持一致。</p>
<h2 id="其他注意事项"><a href="#其他注意事项" class="headerlink" title="其他注意事项"></a>其他注意事项</h2><h3 id="Key和Value的序列化格式"><a href="#Key和Value的序列化格式" class="headerlink" title="Key和Value的序列化格式"></a>Key和Value的序列化格式</h3><p>关于Key、value的序列化可以参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html#key-and-value-formats" target="_blank" rel="noopener">Kafka connector</a>。值得注意的是，必须指定Key和Value的序列化格式，其中Key是通过<strong>PRIMARY KEY</strong>指定的。</p>
<h3 id="Primary-Key约束"><a href="#Primary-Key约束" class="headerlink" title="Primary Key约束"></a>Primary Key约束</h3><p><strong>Upsert Kafka</strong> 工作在 upsert 模式（FLIP-149）下。当我们创建表时，需要在 DDL 中定义主键。具有相同key的数据，会存在相同的分区中。在 changlog source 上定义主键意味着在物化后的 changelog 上主键具有唯一性。定义的主键将决定哪些字段出现在 Kafka 消息的 key 中。</p>
<h3 id="一致性保障"><a href="#一致性保障" class="headerlink" title="一致性保障"></a>一致性保障</h3><p>默认情况下，如果<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/stream/state/checkpointing.html#enabling-and-configuring-checkpointing" target="_blank" rel="noopener">启用 checkpoint</a>，Upsert Kafka sink 会保证至少一次将数据插入 Kafka topic。</p>
<p>这意味着，Flink 可以将具有相同 key 的重复记录写入 Kafka topic。但由于该连接器以 upsert 的模式工作，该连接器作为 source 读入时，可以确保具有相同主键值下仅最后一条消息会生效。因此，upsert-kafka 连接器可以像 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/connectors/hbase.html" target="_blank" rel="noopener">HBase sink</a> 一样实现幂等写入。</p>
<h3 id="分区水位线"><a href="#分区水位线" class="headerlink" title="分区水位线"></a>分区水位线</h3><p>Flink 支持根据 Upsert Kafka 的 每个分区的数据特性发送相应的 watermark。当使用这个特性的时候，watermark 是在 Kafka consumer 内部生成的。 合并每个分区生成的 watermark 的方式和 streaming shuffle 的方式是一致的(<strong>单个分区的输入取最大值，多个分区的输入取最小值</strong>)。 数据源产生的 watermark 是取决于该 consumer 负责的所有分区中当前最小的 watermark。如果该 consumer 负责的部分分区是空闲的，那么整体的 watermark 并不会前进。在这种情况下，可以通过设置合适的 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/config.html#table-exec-source-idle-timeout" target="_blank" rel="noopener">table.exec.source.idle-timeout</a> 来缓解这个问题。</p>
<h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><p>Upsert Kafka 用字节bytes存储消息的 key 和 value，因此没有 schema 或数据类型。消息按格式进行序列化和反序列化，例如：csv、json、avro。不同的序列化格式所提供的数据类型有所不同，因此需要根据使用的序列化格式进行确定表字段的数据类型是否与该序列化类型提供的数据类型兼容。</p>
<h2 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h2><p>本文以实时地统计网页PV和UV的总量为例，介绍upsert-kafka基本使用方式：</p>
<ul>
<li>Kafka 数据源</li>
</ul>
<p>用户的ippv信息，一个用户在一天内可以有很多次pv</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_ods_fact_user_ippv (</span><br><span class="line">    user_id      <span class="keyword">STRING</span>,       <span class="comment">-- 用户ID</span></span><br><span class="line">    client_ip    <span class="keyword">STRING</span>,       <span class="comment">-- 客户端IP</span></span><br><span class="line">    client_info  <span class="keyword">STRING</span>,       <span class="comment">-- 设备机型信息</span></span><br><span class="line">    pagecode     <span class="keyword">STRING</span>,       <span class="comment">-- 页面代码</span></span><br><span class="line">    access_time  <span class="built_in">TIMESTAMP</span>,    <span class="comment">-- 请求时间</span></span><br><span class="line">    dt           <span class="keyword">STRING</span>,       <span class="comment">-- 时间分区天</span></span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> access_time <span class="keyword">AS</span> access_time - <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span>  <span class="comment">-- 定义watermark</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">   <span class="string">'connector'</span> = <span class="string">'kafka'</span>, <span class="comment">-- 使用 kafka connector</span></span><br><span class="line">    <span class="string">'topic'</span> = <span class="string">'user_ippv'</span>, <span class="comment">-- kafka主题</span></span><br><span class="line">    <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span>, <span class="comment">-- 偏移量</span></span><br><span class="line">    <span class="string">'properties.group.id'</span> = <span class="string">'group1'</span>, <span class="comment">-- 消费者组</span></span><br><span class="line">    <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-2:9092,kms-3:9092,kms-4:9092'</span>, </span><br><span class="line">    <span class="string">'format'</span> = <span class="string">'json'</span>, <span class="comment">-- 数据源格式为json</span></span><br><span class="line">    <span class="string">'json.fail-on-missing-field'</span> = <span class="string">'false'</span>,</span><br><span class="line">    <span class="string">'json.ignore-parse-errors'</span> = <span class="string">'true'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<ul>
<li>Kafka Sink表</li>
</ul>
<p>统计每分钟的PV、UV，并将结果存储在Kafka中</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> result_total_pvuv_min (</span><br><span class="line">    do_date     <span class="keyword">STRING</span>,     <span class="comment">-- 统计日期</span></span><br><span class="line">    do_min      <span class="keyword">STRING</span>,      <span class="comment">-- 统计分钟</span></span><br><span class="line">    pv          <span class="built_in">BIGINT</span>,     <span class="comment">-- 点击量</span></span><br><span class="line">    uv          <span class="built_in">BIGINT</span>,     <span class="comment">-- 一天内同个访客多次访问仅计算一个UV</span></span><br><span class="line">    currenttime <span class="built_in">TIMESTAMP</span>,  <span class="comment">-- 当前时间</span></span><br><span class="line">    PRIMARY <span class="keyword">KEY</span> (do_date, do_min) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">'connector'</span> = <span class="string">'upsert-kafka'</span>,</span><br><span class="line">  <span class="string">'topic'</span> = <span class="string">'result_total_pvuv_min'</span>,</span><br><span class="line">  <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-2:9092,kms-3:9092,kms-4:9092'</span>,</span><br><span class="line">  <span class="string">'key.json.ignore-parse-errors'</span> = <span class="string">'true'</span>,</span><br><span class="line">  <span class="string">'value.json.fail-on-missing-field'</span> = <span class="string">'false'</span>,</span><br><span class="line">  <span class="string">'key.format'</span> = <span class="string">'json'</span>,</span><br><span class="line">  <span class="string">'value.format'</span> = <span class="string">'json'</span>,</span><br><span class="line">  <span class="string">'value.fields-include'</span> = <span class="string">'EXCEPT_KEY'</span> <span class="comment">-- key不出现kafka消息的value中</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<ul>
<li>计算逻辑</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建视图</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> view_total_pvuv_min <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">     dt <span class="keyword">AS</span> do_date,                    <span class="comment">-- 时间分区</span></span><br><span class="line">     <span class="keyword">count</span> (client_ip) <span class="keyword">AS</span> pv,          <span class="comment">-- 客户端的IP</span></span><br><span class="line">     <span class="keyword">count</span> (<span class="keyword">DISTINCT</span> client_ip) <span class="keyword">AS</span> uv, <span class="comment">-- 客户端去重</span></span><br><span class="line">     <span class="keyword">max</span>(access_time) <span class="keyword">AS</span> access_time   <span class="comment">-- 请求的时间</span></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">    source_ods_fact_user_ippv</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> dt;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 写入数据</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> result_total_pvuv_min</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  do_date,    <span class="comment">--  时间分区</span></span><br><span class="line">  <span class="keyword">cast</span>(<span class="keyword">DATE_FORMAT</span> (access_time,<span class="string">'HH:mm'</span>) <span class="keyword">AS</span> <span class="keyword">STRING</span>) <span class="keyword">AS</span> do_min,<span class="comment">-- 分钟级别的时间</span></span><br><span class="line">  pv,</span><br><span class="line">  uv,</span><br><span class="line">  <span class="keyword">CURRENT_TIMESTAMP</span> <span class="keyword">AS</span> currenttime <span class="comment">-- 当前时间</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  view_total_pvuv_min;</span><br></pre></td></tr></table></figure>

<ul>
<li>生产用户访问数据到kafka，向kafka中的user_ippv插入数据：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">&#123;<span class="string">"user_id"</span>:<span class="string">"1"</span>,<span class="string">"client_ip"</span>:<span class="string">"192.168.12.1"</span>,<span class="string">"client_info"</span>:<span class="string">"phone"</span>,<span class="string">"pagecode"</span>:<span class="string">"1001"</span>,<span class="string">"access_time"</span>:<span class="string">"2021-01-08 11:32:24"</span>,<span class="string">"dt"</span>:<span class="string">"2021-01-08"</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">"user_id"</span>:<span class="string">"1"</span>,<span class="string">"client_ip"</span>:<span class="string">"192.168.12.1"</span>,<span class="string">"client_info"</span>:<span class="string">"phone"</span>,<span class="string">"pagecode"</span>:<span class="string">"1201"</span>,<span class="string">"access_time"</span>:<span class="string">"2021-01-08 11:32:55"</span>,<span class="string">"dt"</span>:<span class="string">"2021-01-08"</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">"user_id"</span>:<span class="string">"2"</span>,<span class="string">"client_ip"</span>:<span class="string">"192.165.12.1"</span>,<span class="string">"client_info"</span>:<span class="string">"pc"</span>,<span class="string">"pagecode"</span>:<span class="string">"1031"</span>,<span class="string">"access_time"</span>:<span class="string">"2021-01-08 11:32:59"</span>,<span class="string">"dt"</span>:<span class="string">"2021-01-08"</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">"user_id"</span>:<span class="string">"1"</span>,<span class="string">"client_ip"</span>:<span class="string">"192.168.12.1"</span>,<span class="string">"client_info"</span>:<span class="string">"phone"</span>,<span class="string">"pagecode"</span>:<span class="string">"1101"</span>,<span class="string">"access_time"</span>:<span class="string">"2021-01-08 11:33:24"</span>,<span class="string">"dt"</span>:<span class="string">"2021-01-08"</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">"user_id"</span>:<span class="string">"3"</span>,<span class="string">"client_ip"</span>:<span class="string">"192.168.10.3"</span>,<span class="string">"client_info"</span>:<span class="string">"pc"</span>,<span class="string">"pagecode"</span>:<span class="string">"1001"</span>,<span class="string">"access_time"</span>:<span class="string">"2021-01-08 11:33:30"</span>,<span class="string">"dt"</span>:<span class="string">"2021-01-08"</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">"user_id"</span>:<span class="string">"1"</span>,<span class="string">"client_ip"</span>:<span class="string">"192.168.12.1"</span>,<span class="string">"client_info"</span>:<span class="string">"phone"</span>,<span class="string">"pagecode"</span>:<span class="string">"1001"</span>,<span class="string">"access_time"</span>:<span class="string">"2021-01-08 11:34:24"</span>,<span class="string">"dt"</span>:<span class="string">"2021-01-08"</span>&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>查询结果表：</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> result_total_pvuv_min;</span><br></pre></td></tr></table></figure>

<p><img src="//jiamaoxiang.top/2021/01/08/实时数仓-以upsert的方式读写Kafka数据——以Flink1-12为例/%E7%BB%93%E6%9E%9C1.png" alt></p>
<p><strong>可以看出</strong>：每分钟的pv、uv只显示一条数据，即代表着截止到当前时间点的pv和uv</p>
<p>查看Kafka中result_total_pvuv_min主题的数据，如下：</p>
<p><img src="//jiamaoxiang.top/2021/01/08/实时数仓-以upsert的方式读写Kafka数据——以Flink1-12为例/kafka%E7%BB%93%E6%9E%9C.png" alt></p>
<p><strong>可以看出</strong>：针对每一条访问数据，触发计算了一次PV、UV，每一条数据都是截止到当前时间的累计PV和UV。</p>
<blockquote>
<p>尖叫提示：</p>
<p>默认情况下，如果在<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/checkpointing.html#enabling-and-configuring-checkpointing" target="_blank" rel="noopener">启用</a>了<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/checkpointing.html#enabling-and-configuring-checkpointing" target="_blank" rel="noopener">检查点的</a>情况下执行查询，Upsert Kafka接收器会将具有至少一次保证的数据提取到Kafka主题中。</p>
<p>这意味着，Flink可能会将具有相同键的重复记录写入Kafka主题。但是，由于连接器在upsert模式下工作，因此作为源读回时，同一键上的最后一条记录将生效。因此，upsert-kafka连接器就像<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/hbase.html" target="_blank" rel="noopener">HBase接收器</a>一样实现幂等写入。</p>
</blockquote>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文以Flink1.12为例，介绍了<strong>upsert connector(upsert-kafka)</strong>的基本使用，该方式允许用户以upsert 的方式读写Kafka中的表，使用起来非常方便。另外本文也给出了一个具体的使用案例，可以进一步加深对该功能的使用。</p>
<blockquote>
<p>公众号『大数据技术与数仓』，回复『资料』领取大数据资料包</p>
</blockquote>
</div><div class="recommended_posts"><h3>相关推荐 ☟</h3><li><a href="https://jiamaoxiang.top/2021/01/16/Greenplum5-9生产环境集群部署/" target="_blank">Greenplum5.9生产环境集群部署</a></li><li><a href="https://jiamaoxiang.top/2020/12/22/Flink-on-Hive构建流批一体数仓/" target="_blank">Flink on Hive构建流批一体数仓</a></li><li><a href="https://jiamaoxiang.top/2020/12/21/Flink集成Hive之Hive-Catalog与Hive-Dialect-以Flink1-12为例/" target="_blank">Flink集成Hive之Hive Catalog与Hive Dialect--以Flink1.12为例</a></li><li><a href="https://jiamaoxiang.top/2020/12/18/Flink集成Hive之快速入门-以Flink1-12为例/" target="_blank">Flink集成Hive之快速入门--以Flink1.12为例</a></li></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Jia MaoXiang</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2021/01/08/实时数仓-以upsert的方式读写Kafka数据——以Flink1-12为例/">https://jiamaoxiang.top/2021/01/08/实时数仓-以upsert的方式读写Kafka数据——以Flink1-12为例/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本文为博主原创文章，遵循CC BY-SA 4.0版权协议，转载请附上原文出处链接和本声明</li></ul></div><br><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="https://jiamaoxiang.top/2021/01/08/实时数仓-以upsert的方式读写Kafka数据——以Flink1-12为例/" data-id="ckk9d320s004jxo7q9sji9ew2" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAQ4AAAEOCAAAAABd2qZ5AAAD8klEQVR42u3aS24bMRAFQN3/0gqQlQNnpPe6OYIWNSsjnlBk0UCrP49H/Dz/Pj9//vn8fvPqt7/fef0vyftXn3j16QceHDhw4MARHDVZuj3Aa4JTB2t3++bCcODAgQPHIY4kuJ6K58kBEsrZVb05Ow4cOHDg+CBHkmLNttUe8ipgJ9eJAwcOHDi+jWOWts3Cc1s0xIEDBw4c38ORl+fyxtKp9tXZPtGxWikOHDhw4JjHu6hQ+M0/3z7fgQMHDhw4gvDTPrP0aZaG5V8Civ3jwIEDB441xz6tuq842K6W7/bNVeHAgQMHjgXH7MBtE6gtO+bhuT386gZw4MCBA8e6FLiBaNtUbUDdcNThHwcOHDhwxBxJYS5J0tr3Z+MO7U7yVPA/tVIcOHDgwHGUo2WaHbglbhtI9TXjwIEDB441x6lmT56kzQJtQtOWF6NxOhw4cODAUXK8XnTT+NmkW7PkLW9l4cCBAweOuznagJc3e/YBuC35tUkaDhw4cOC4j6OYdxhRzlZOmNpW0+XKOHDgwIFjzXEqrTr1ZhsyZ22zx+xOcODAgQNHTJCX5NqRgrak2AbyWbPqck0cOHDgwLHmaJOcpMC32fosadyPYkTVShw4cODAEXPMtnX4NkZFxjw856fAgQMHDhybU+StmiQxKxKk9fr7veHAgQMHjvs48vZPm6ptRt/y99vPqttOOHDgwIHjKEc7ZNaOJuSBcHZtBRYOHDhw4FhztAW1YfVxVLxrA22+ftR/w4EDBw4chzjy1GiT+LVh9Y7W1JsuHA4cOHDgGHE8g2fT5vnMEEMSYpNEDgcOHDhwnOKYbej1m/uwOkvMZoMU/9RKceDAgQPHgiPZSp5QtYFwM1o3u7zoqwAOHDhw4DjEEQWhxQDBfUedJWw4cODAgeMOjllbaDOOdmDg4OhXgTd/Fzhw4MCBY8GRB7+2sRS1fEZNqTwVjK4NBw4cOHCsOdqgmKdnm1Qt2c/mf0UXiQMHDhw4DnG0KPkow2aQroVo07zLQIsDBw4cOEqOdsSt5WvXzLee33N+wThw4MCB4xRHsqE8iLblubyVlV/bcOgBBw4cOHCsOdrDnBopaBO2TfpXj03gwIEDB44Fx7N88jRsHzjzdlESSuv6KA4cOHDgGHHMgl9eesuD92aUIU8yH6ceHDhw4MARJzN58JutnP82TxQ3Dao6kcOBAwcOHDHHLIjmH9mmf7M12/UPf6fAgQMHDhwjjtmgQ9tAasN8O5zxpmKKAwcOHDg+wnF23KEdYpvtod0nDhw4cOA4xZEXB9swmW83Tw5nzapo/zhw4MCBY83RDjRsht5mAxPt6MN9jTQcOHDgwPFyb38AfO78lx2qgusAAAAASUVORK5CYII=">分享</a><div class="tags"><a href="/tags/Flink/">Flink</a></div><div class="post-nav"><a class="pre" href="/2021/01/16/Greenplum5-9生产环境集群部署/">Greenplum5.9生产环境集群部署</a><a class="next" href="/2020/12/22/Flink-on-Hive构建流批一体数仓/">Flink on Hive构建流批一体数仓</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar-toc"><div class="stoc-article" id="sidebar-stoc"><strong class="stoc-title"><i class="fa fa-list-ul"> 目录</i></strong><div class="toc-nav" id="stoc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Upsert-Kafka-connector简介"><span class="toc-number">1.</span> <span class="toc-text">Upsert Kafka connector简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#依赖"><span class="toc-number">2.</span> <span class="toc-text">依赖</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用方式"><span class="toc-number">3.</span> <span class="toc-text">使用方式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#使用样例"><span class="toc-number">3.1.</span> <span class="toc-text">使用样例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#upsert-kafka-connector参数"><span class="toc-number">3.2.</span> <span class="toc-text">upsert-kafka connector参数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#其他注意事项"><span class="toc-number">4.</span> <span class="toc-text">其他注意事项</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Key和Value的序列化格式"><span class="toc-number">4.1.</span> <span class="toc-text">Key和Value的序列化格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Primary-Key约束"><span class="toc-number">4.2.</span> <span class="toc-text">Primary Key约束</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#一致性保障"><span class="toc-number">4.3.</span> <span class="toc-text">一致性保障</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#分区水位线"><span class="toc-number">4.4.</span> <span class="toc-text">分区水位线</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据类型"><span class="toc-number">4.5.</span> <span class="toc-text">数据类型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用案例"><span class="toc-number">5.</span> <span class="toc-text">使用案例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">6.</span> <span class="toc-text">总结</span></a></li></ol></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 - 2021 <a href="/." rel="nofollow">Jmx's Blog.</a>All rights reserved.<br>Thoughts on technology, life and everything else.</div></div></div><script type="text/javascript" src="/js/toc.js?v=0.0.0"></script><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" color="100,99,98" opacity="0.5" zindex="0.7" count="150" src="//lib.baomitu.com/canvas-nest.js/2.0.4/canvas-nest.umd.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>