<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>经典Hive SQL面试题</title>
      <link href="/2019/10/15/%E7%BB%8F%E5%85%B8Hive-SQL%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
      <url>/2019/10/15/%E7%BB%8F%E5%85%B8Hive-SQL%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>HQL练习</p><a id="more"></a><h2 id="第一题："><a href="#第一题：" class="headerlink" title="第一题："></a>第一题：</h2><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">我们有如下的用户访问数据</span><br><span class="line">userId  visitDate   visitCount</span><br><span class="line">u01 2017/1/21   5</span><br><span class="line">u02 2017/1/23   6</span><br><span class="line">u03 2017/1/22   8</span><br><span class="line">u04 2017/1/20   3</span><br><span class="line">u01 2017/1/23   6</span><br><span class="line">u01 2017/2/21   8</span><br><span class="line">U02 2017/1/23   6</span><br><span class="line">U01 2017/2/22   4</span><br><span class="line">要求使用SQL统计出每个用户的累积访问次数，如下表所示：</span><br><span class="line">用户id    月份  小计  累积</span><br><span class="line">u01 2017-01 11  11</span><br><span class="line">u01 2017-02 12  23</span><br><span class="line">u02 2017-01 12  12</span><br><span class="line">u03 2017-01 8   8</span><br><span class="line">u04 2017-01 3   3</span><br></pre></td></tr></table></figure><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><pre><code class="bash">CREATE TABLE test_sql.test1 (         userId string,         visitDate string,        visitCount INT )    ROW format delimited FIELDS TERMINATED BY <span class="string">"\t"</span>;    INSERT INTO TABLE test_sql.test1    VALUES        ( <span class="string">'u01'</span>, <span class="string">'2017/1/21'</span>, 5 ),        ( <span class="string">'u02'</span>, <span class="string">'2017/1/23'</span>, 6 ),        ( <span class="string">'u03'</span>, <span class="string">'2017/1/22'</span>, 8 ),        ( <span class="string">'u04'</span>, <span class="string">'2017/1/20'</span>, 3 ),        ( <span class="string">'u01'</span>, <span class="string">'2017/1/23'</span>, 6 ),        ( <span class="string">'u01'</span>, <span class="string">'2017/2/21'</span>, 8 ),        ( <span class="string">'u02'</span>, <span class="string">'2017/1/23'</span>, 6 ),        ( <span class="string">'u01'</span>, <span class="string">'2017/2/22'</span>, 4 );</code></pre>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的Window Function介绍</title>
      <link href="/2019/09/26/Flink%E7%9A%84Window-Function%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/09/26/Flink%E7%9A%84Window-Function%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Impala使用的端口</title>
      <link href="/2019/08/29/Impala%E4%BD%BF%E7%94%A8%E7%9A%84%E7%AB%AF%E5%8F%A3/"/>
      <url>/2019/08/29/Impala%E4%BD%BF%E7%94%A8%E7%9A%84%E7%AB%AF%E5%8F%A3/</url>
      
        <content type="html"><![CDATA[<p>本文主要介绍了Impala所使用的端口号，在部署Impala的时候，确保下面列出的端口是开启的。</p><a id="more"></a><table><thead><tr><th align="center">组件</th><th>服务</th><th>端口</th><th align="center"><span style="white-space:nowrap;">访问需求&emsp;&emsp;</span></th><th>备注</th></tr></thead><tbody><tr><td align="center">Impala Daemon</td><td>Impala Daemon Frontend Port</td><td>21000</td><td align="center">外部</td><td>被 impala-shell, Beeswax, Cloudera ODBC 1.2 驱动 用于传递命令和接收结果</td></tr><tr><td align="center">Impala Daemon</td><td>Impala Daemon Frontend Port</td><td>21050</td><td align="center">外部</td><td>被使用 JDBC 或 Cloudera ODBC 2.0 及以上驱动的诸如 BI 工具之类的应用用来传递命令和接收结果</td></tr><tr><td align="center">Impala Daemon</td><td>Impala Daemon Backend Port</td><td>22000</td><td align="center">内部</td><td>仅内部使用。Impala</td></tr><tr><td align="center">Impala Daemon</td><td>StateStoreSubscriber Service Port</td><td>23000</td><td align="center">内部</td><td>仅内部使用。Impala 守护进程监听该端口接收来源于 state store 的更新</td></tr><tr><td align="center">Catalog Daemon</td><td>StateStoreSubscriber Service Port</td><td>23020</td><td align="center">内部</td><td>仅内部使用，catalog daemon监听该端口接收来源于 state store 的更新</td></tr><tr><td align="center">Impala Daemon</td><td>Impala Daemon HTTP Server Port</td><td>25000</td><td align="center">外部</td><td>Impala Daemon的Web端口，用于管理员监控和线上故障排查</td></tr><tr><td align="center">Impala StateStore Daemon</td><td>StateStore HTTP Server Port</td><td>25010</td><td align="center">外部</td><td>StateStore的Web端口，用于管理员监控和线上故障排查</td></tr><tr><td align="center">Impala Catalog Daemon</td><td>Catalog HTTP Server Port</td><td>25020</td><td align="center">外部</td><td>Catalog的Web端口，用于管理员监控和线上故障排查，从Impala1.2开始加入</td></tr><tr><td align="center">Impala StateStore Daemon</td><td>StateStore Service Port</td><td>24000</td><td align="center">内部</td><td>仅内部使用，statestore daemon监听的端口，用于registration/unregistration请求</td></tr><tr><td align="center">Impala Catalog Daemon</td><td>Catalog Service Port</td><td>26000</td><td align="center">内部</td><td>仅内部使用，catalog服务使用此端口与Impala Daemon进行通信，从Impala1.2开始加入</td></tr><tr><td align="center">Impala Daemon</td><td>KRPC Port</td><td>27000</td><td align="center">内部</td><td>仅内部使用，Impala daemon使用此端口进行基于krpc的相互通信。</td></tr></tbody></table><hr><p>Refrence:<a href="https://www.cloudera.com/documentation/enterprise/6/latest/topics/impala_ports.html#ports" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/6/latest/topics/impala_ports.html#ports</a></p>]]></content>
      
      
      <categories>
          
          <category> Impala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Impala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban安装部署</title>
      <link href="/2019/08/28/Azkaban%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
      <url>/2019/08/28/Azkaban%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<p>Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。</p><a id="more"></a><h1 id="安装前准备"><a href="#安装前准备" class="headerlink" title="安装前准备"></a>安装前准备</h1><p>(1)将Azkaban Web服务器、Azkaban执行服务器要安装机器的/opt/software目录下：  </p><ul><li>azkaban-web-server-2.5.0.tar.gz  </li><li>azkaban-executor-server-2.5.0.tar.gz  </li><li>azkaban-sql-script-2.5.0.tar.gz  </li><li>mysql-libs.zip  </li></ul><p>(2)目前azkaban只支持 mysql作为元数据库，需安装mysql，本文档中默认已安装好mysql服务器</p><h1 id="安装Azkaban"><a href="#安装Azkaban" class="headerlink" title="安装Azkaban"></a>安装Azkaban</h1><p>(1)在/opt/module/目录下创建azkaban目录<br>(2)解压azkaban-web-server-2.5.0.tar.gz、azkaban-executor-server-2.5.0.tar.gz、azkaban-sql-script-2.5.0.tar.gz到/opt/module/azkaban目录下<br>解压完成后的文件夹如下图所示：<br><img src="//jiamaoxiang.top/2019/08/28/Azkaban安装部署/1.png" alt><br>(3)初始化Azkaban的元数据库<br>登录mysql，创建azkaban的数据库，并执行脚本create-all-sql-2.5.0.sql，如下所示：  </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; create database azkaban;  </span><br><span class="line">Query OK, 1 row affected (0.00 sec)  </span><br><span class="line">mysql&gt; use azkaban;  </span><br><span class="line">Database changed  </span><br><span class="line">mysql&gt; <span class="built_in">source</span> /opt/module/azkaban/azkaban-2.5.0/create-all-sql-2.5.0.sql</span><br></pre></td></tr></table></figure><h2 id="创建SSL配置"><a href="#创建SSL配置" class="headerlink" title="创建SSL配置"></a>创建SSL配置</h2><p>(1)生成 keystore的密码及相应信息  </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban]$ keytool -keystore keystore -<span class="built_in">alias</span> jetty -genkey -keyalg RSA  </span><br><span class="line">输入keystore密码：   </span><br><span class="line">再次输入新密码:    </span><br><span class="line">您的名字与姓氏是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您的组织单位名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您的组织名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您所在的城市或区域名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您所在的州或省份名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">该单位的两字母国家代码是什么    </span><br><span class="line">[Unknown]：  CN    </span><br><span class="line">CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN 正确吗？  </span><br><span class="line">[否]：  y    </span><br><span class="line"> </span><br><span class="line">输入&lt;jetty&gt;的主密码    </span><br><span class="line">（如果和 keystore 密码相同，按回车）  ：</span><br></pre></td></tr></table></figure><p>(2)将keystore 考贝到 azkaban web服务器根目录中</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban]<span class="comment"># mv keystore  azkaban-web-2.5.0/</span></span><br></pre></td></tr></table></figure><h2 id="Web服务器配置"><a href="#Web服务器配置" class="headerlink" title="Web服务器配置"></a>Web服务器配置</h2><p>(1)进入azkaban web服务器安装目录 conf目录，修改azkaban.properties文件<br>(2)按照如下配置修改azkaban.properties文件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Azkaban Personalization Settings      </span></span><br><span class="line"><span class="comment">#服务器UI名称,用于服务器上方显示的名字      </span></span><br><span class="line">azkaban.name=Test      </span><br><span class="line"><span class="comment">#描述                            </span></span><br><span class="line">azkaban.label=My Local Azkaban     </span><br><span class="line"><span class="comment">#UI颜色                         </span></span><br><span class="line">azkaban.color=<span class="comment">#FF3601                                         </span></span><br><span class="line">azkaban.default.servlet.path=/index  </span><br><span class="line"><span class="comment">#根web目录,配置绝对路径，即web的目录  </span></span><br><span class="line">web.resource.dir=/opt/module/azkaban/azkaban-web-2.5.0/web   </span><br><span class="line"><span class="comment">#默认时区,已改为亚洲/上海 默认为美国                                            </span></span><br><span class="line">default.timezone.id=Asia/Shanghai                           </span><br><span class="line"><span class="comment">#用户权限管理默认类  </span></span><br><span class="line">user.manager.class=azkaban.user.XmlUserManager    </span><br><span class="line"><span class="comment">#用户配置,配置绝对路径，即azkaban-users.xml的路径     </span></span><br><span class="line">user.manager.xml.file=/opt/module/azkaban/azkaban-web-2.5.0/conf/azkaban-users.xml             </span><br><span class="line"><span class="comment">#Loader for projects .global配置文件所在位置,即global.properties绝对路径    </span></span><br><span class="line">executor.global.properties=/opt/module/azkaban/azkaban-executor-2.5.0/conf/global.properties    </span><br><span class="line">azkaban.project.dir=projects                                                 </span><br><span class="line"><span class="comment">#数据库类型  </span></span><br><span class="line">database.type=mysql                                                            </span><br><span class="line">mysql.port=3306                                                                  </span><br><span class="line">mysql.host=cdh01                                                    </span><br><span class="line">mysql.database=azkaban                                                      </span><br><span class="line">mysql.user=root  </span><br><span class="line"><span class="comment">#数据库密码                                                               </span></span><br><span class="line">mysql.password=123qwe                                                     </span><br><span class="line">mysql.numconnections=100                                                </span><br><span class="line"><span class="comment"># Velocity dev mode   </span></span><br><span class="line">velocity.dev.mode=<span class="literal">false</span>  </span><br><span class="line"><span class="comment"># Jetty服务器属性.  </span></span><br><span class="line"><span class="comment">#最大线程数     </span></span><br><span class="line">jetty.maxThreads=25   </span><br><span class="line"><span class="comment">#Jetty SSL端口                                                                 </span></span><br><span class="line">jetty.ssl.port=8443  </span><br><span class="line"><span class="comment">#Jetty端口                                                                      </span></span><br><span class="line">jetty.port=8081    </span><br><span class="line"><span class="comment">#SSL文件名,即keystore绝对路径                                                                              </span></span><br><span class="line">jetty.keystore=/opt/module/azkaban/azkaban-web-2.5.0/keystore    </span><br><span class="line"><span class="comment">#SSL文件密码,本配置与keystore密码相同                                                           </span></span><br><span class="line">jetty.password=123qwe  </span><br><span class="line"><span class="comment">#Jetty主密码 与 keystore文件相同                                                          </span></span><br><span class="line">jetty.keypassword=123qwe  </span><br><span class="line"><span class="comment">#SSL文件名,即keystore绝对路径                                                            </span></span><br><span class="line">jetty.truststore=/opt/module/azkaban/azkaban-web-2.5.0/keystore    </span><br><span class="line"><span class="comment"># SSL文件密码                                                             </span></span><br><span class="line">jetty.trustpassword=123qwe                                                    </span><br><span class="line"><span class="comment"># 执行服务器属性, 执行服务器端口  </span></span><br><span class="line">executor.port=12321                                                                </span><br><span class="line"><span class="comment"># 邮件设置,发送邮箱    </span></span><br><span class="line">mail.sender=xxxxxxxx@163.com    </span><br><span class="line"><span class="comment">#发送邮箱smtp地址                                           </span></span><br><span class="line">mail.host=smtp.163.com     </span><br><span class="line"><span class="comment">#发送邮件时显示的名称                                                            </span></span><br><span class="line">mail.user=xxxxxxxx  </span><br><span class="line"><span class="comment">#邮箱密码                                            </span></span><br><span class="line">mail.password=**********   </span><br><span class="line"><span class="comment">#任务失败时发送邮件的地址                                                        </span></span><br><span class="line">job.failure.email=xxxxxxxx@163.com   </span><br><span class="line"><span class="comment">#任务成功时发送邮件的地址                                 </span></span><br><span class="line">job.success.email=xxxxxxxx@163.com                            </span><br><span class="line">lockdown.create.projects=<span class="literal">false</span>    </span><br><span class="line"><span class="comment">#缓存目录                                            </span></span><br><span class="line">cache.directory=cache</span><br></pre></td></tr></table></figure><p>(3)web服务器用户配置<br>在azkaban web服务器安装目录 conf目录，按照如下配置修改azkaban-users.xml 文件，增加管理员用户。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;azkaban-users&gt;  </span><br><span class="line">       &lt;user username=<span class="string">"azkaban"</span> password=<span class="string">"azkaban"</span> roles=<span class="string">"admin"</span> groups=<span class="string">"azkaban"</span> /&gt;    </span><br><span class="line">       &lt;user username=<span class="string">"metrics"</span> password=<span class="string">"metrics"</span> roles=<span class="string">"metrics"</span>/&gt;  </span><br><span class="line">       &lt;user username=<span class="string">"admin"</span> password=<span class="string">"admin"</span> roles=<span class="string">"admin,metrics"</span> /&gt;  </span><br><span class="line">       &lt;role name=<span class="string">"admin"</span> permissions=<span class="string">"ADMIN"</span> /&gt;  </span><br><span class="line">       &lt;role name=<span class="string">"metrics"</span> permissions=<span class="string">"METRICS"</span>/&gt;    </span><br><span class="line">&lt;/azkaban-users&gt;</span><br></pre></td></tr></table></figure><h2 id="executor服务器配置"><a href="#executor服务器配置" class="headerlink" title="executor服务器配置"></a>executor服务器配置</h2><p>(1)进入executor安装目录，修改azkaban.properties</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Azkaban  </span></span><br><span class="line">default.timezone.id=Asia/Shanghai  </span><br><span class="line"><span class="comment"># Azkaban JobTypes Plugins  </span></span><br><span class="line">azkaban.jobtype.plugin.dir=./../plugins/jobtypes  </span><br><span class="line"><span class="comment">#Loader for projects  </span></span><br><span class="line">executor.global.properties=/opt/module/azkaban/azkaban-executor-2.5.0/conf/global.properties  </span><br><span class="line">azkaban.project.dir=projects  </span><br><span class="line">database.type=mysql  </span><br><span class="line">mysql.port=3306  </span><br><span class="line">mysql.host=cdh01  </span><br><span class="line">mysql.database=azkaban  </span><br><span class="line">mysql.user=root  </span><br><span class="line">mysql.password=123qwe    </span><br><span class="line">mysql.numconnections=100    </span><br><span class="line"><span class="comment"># Azkaban Executor settings  </span></span><br><span class="line">executor.maxThreads=50  </span><br><span class="line">executor.port=12321  </span><br><span class="line">executor.flow.threads=30</span><br></pre></td></tr></table></figure><h1 id="启动web服务器"><a href="#启动web服务器" class="headerlink" title="启动web服务器"></a>启动web服务器</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban-web-2.5.0]<span class="comment"># bin/azkaban-web-start.sh  &amp;</span></span><br></pre></td></tr></table></figure><h1 id="启动executor服务器"><a href="#启动executor服务器" class="headerlink" title="启动executor服务器"></a>启动executor服务器</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban-executor-2.5.0]<span class="comment"># bin/azkaban-executor-start.sh  &amp;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的数据类型</title>
      <link href="/2019/08/27/Flink%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
      <url>/2019/08/27/Flink%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>Flink使用type information来代表数据类型，Flink还具有一个类型提取系统，该系统分析函数的输入和返回类型，以自动获取类型信息(type information)，从而获得序列化程序和反序列化程序。但是，在某些情况下，例如lambda函数或泛型类型，需要显式地提供类型信息((type information)),从而提高其性能。本文主要讨论包括：(1)Flink支持的数据类型,(2)如何为数据类型创建type information，（3）如果无法自动推断函数的返回类型，如何使用提示(hints)来帮助Flink的类型系统识别类型信息。</p><a id="more"></a><h2 id="支持的数据类型"><a href="#支持的数据类型" class="headerlink" title="支持的数据类型"></a>支持的数据类型</h2><p>Flink支持Java和Scala中所有常见的数据类型，使用比较广泛的类型主要包括以下五种：</p><ul><li>原始类型  </li><li>Java和Scala的tuple类型  </li><li>Scala样例类  </li><li>POJO类型  </li><li>一些特殊的类型  </li></ul><p><strong>NOTE：</strong>不能被处理的类型将会被视为普通的数据类型，通过Kyro序列化框架进行序列化。</p><h3 id="原始类型"><a href="#原始类型" class="headerlink" title="原始类型"></a>原始类型</h3><p>Flink支持所有Java和Scala的原始类型，比如Int(Java中的Integer)，String、Double等。下面的例子是处理一个Long类型的数据流，处理每个元素+1  </p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> numbers: <span class="type">DataStream</span>[<span class="type">Long</span>] = env.fromElements(<span class="number">1</span>L, <span class="number">2</span>L,<span class="number">3</span>L, <span class="number">4</span>L)  </span><br><span class="line">numbers.map( n =&gt; n + <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="Java和Scala的tuple类型"><a href="#Java和Scala的tuple类型" class="headerlink" title="Java和Scala的tuple类型"></a>Java和Scala的tuple类型</h3><p>基于Scala的DataStream API使用的Scala的tuple。下面的例子是过滤一个具有两个字段的tuple类型的数据流.  </p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// DataStream of Tuple2[String, Integer] for Person(name,age)  </span></span><br><span class="line"><span class="keyword">val</span> persons: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Integer</span>)] = env.fromElements((<span class="string">"Adam"</span>, <span class="number">17</span>),(<span class="string">"Sarah"</span>, <span class="number">23</span>))  </span><br><span class="line"><span class="comment">// filter for persons of age &gt; 18  </span></span><br><span class="line">persons.filter(p =&gt; p._2 &gt; <span class="number">18</span>)</span><br></pre></td></tr></table></figure><p>Flink提供了有效的Java tuple实现，Flink的Java tuple最多包括25个字段，分别为tuple1，tuple2，直到tuple25，tuple类型是强类型的。使用Java DataStream API重写上面的例子:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// DataStream of Tuple2&lt;String, Integer&gt; for Person(name,age)  </span></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; persons =env.fromElements(Tuple2.of(<span class="string">"Adam"</span>, <span class="number">17</span>),Tuple2.of(<span class="string">"Sarah"</span>,<span class="number">23</span>));  </span><br><span class="line"><span class="comment">// filter for persons of age &gt; 18  </span></span><br><span class="line">persons.filter(p -&gt; p.f1 &gt; <span class="number">18</span>);</span><br></pre></td></tr></table></figure><p>Tuple字段可以通过使用f0，f1，f2的形式访问，也可以通过getField(int pos)方法访问，参数的索引起始值为0，比如:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Tuple2&lt;String, Integer&gt; personTuple = Tuple2.of(<span class="string">"Alex"</span>,<span class="string">"42"</span>);  </span><br><span class="line">Integer age = personTuple.getField(<span class="number">1</span>); <span class="comment">// age = 42</span></span><br></pre></td></tr></table></figure><p>与Scala相比，Flink的Java tuple是可变的，所以tuple的元素值是可以被重新复制的。Function可以重用Java tuple,从而减小垃圾回收的压力。下面的例子展示了如何更新一个tuple字段值</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">personTuple.f1 = <span class="number">42</span>; <span class="comment">// set the 2nd field to 42     </span></span><br><span class="line">personTuple.setField(<span class="number">43</span>, <span class="number">1</span>); <span class="comment">// set the 2nd field to 43</span></span><br></pre></td></tr></table></figure><h3 id="Scala的样例类"><a href="#Scala的样例类" class="headerlink" title="Scala的样例类"></a>Scala的样例类</h3><p>Flink支持Scala的样例类，可以通过字段名称来访问样例类的字段，下面的例子定义了一个<code>Person</code>样例类，该样例类有两个字段：<code>name</code>和<code>age</code>,按<code>age</code>过滤DataStream，如下所示</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)  </span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">persons</span></span>: <span class="type">DataStream</span>[<span class="type">Person</span>] = env.fromElements(<span class="type">Person</span>(<span class="string">"Adam"</span>, <span class="number">17</span>),<span class="type">Person</span>(<span class="string">"Sarah"</span>, <span class="number">23</span>))  </span><br><span class="line"><span class="comment">// filter for persons with age &gt; 18  </span></span><br><span class="line">persons.filter(p =&gt; p.age &gt; <span class="number">18</span>)</span><br></pre></td></tr></table></figure><h3 id="POJO"><a href="#POJO" class="headerlink" title="POJO"></a>POJO</h3><p>Flink接受的POJO类型需满足以下条件：</p><ul><li>public 类  </li><li>无参的共有构造方法  </li><li>所有字段都是public的，可以通过getter和setter方法访问  </li><li>所有字段类型必须是Flink能够支持的<br>下面的例子定义一个<code>Person</code>POJO</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;  </span><br><span class="line"><span class="comment">// both fields are public  </span></span><br><span class="line"><span class="keyword">public</span> String name;  </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">int</span> age;  </span><br><span class="line"><span class="comment">// default constructor is present  </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">()</span> </span>&#123;&#125;  </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(String name, <span class="keyword">int</span> age)</span> </span>&#123;  </span><br><span class="line"><span class="keyword">this</span>.name = name;  </span><br><span class="line"><span class="keyword">this</span>.age = age;  </span><br><span class="line">&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">DataStream&lt;Person&gt; persons = env.fromElements(   </span><br><span class="line"><span class="keyword">new</span> Person(<span class="string">"Alex"</span>, <span class="number">42</span>),  </span><br><span class="line"><span class="keyword">new</span> Person(<span class="string">"Wendy"</span>, <span class="number">23</span>));</span><br></pre></td></tr></table></figure><h3 id="一些特殊的类型"><a href="#一些特殊的类型" class="headerlink" title="一些特殊的类型"></a>一些特殊的类型</h3><p>Flink支持一些有特殊作用的数据类型，比如Array，Java中的ArrayList、HashMap和Enum等，也支持Hadoop的Writable类型。  </p><h2 id="为数据类型创建类型信息-type-information"><a href="#为数据类型创建类型信息-type-information" class="headerlink" title="为数据类型创建类型信息(type information)"></a>为数据类型创建类型信息(type information)</h2><h2 id="显示地指定类型信息-type-information"><a href="#显示地指定类型信息-type-information" class="headerlink" title="显示地指定类型信息(type information)"></a>显示地指定类型信息(type information)</h2>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于SparkStreaming的日志分析项目</title>
      <link href="/2019/08/26/%E5%9F%BA%E4%BA%8ESparkStreaming%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E9%A1%B9%E7%9B%AE/"/>
      <url>/2019/08/26/%E5%9F%BA%E4%BA%8ESparkStreaming%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E9%A1%B9%E7%9B%AE/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp;基于SparkStreaming实现实时的日志分析，首先基于discuz搭建一个论坛平台，然后将该论坛的日志写入到指定文件，最后通过SparkStreaming实时对日志进行分析。</p><a id="more"></a><h1 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h1><ul><li>统计指定时间段的热门文章</li></ul><ul><li>统计指定时间段内的最受欢迎的用户（以 ip 为单位）</li></ul><ul><li>统计指定时间段内的不同模块的访问量  </li></ul><h1 id="项目架构"><a href="#项目架构" class="headerlink" title="项目架构"></a>项目架构</h1><p><img src="//jiamaoxiang.top/2019/08/26/基于SparkStreaming的日志分析项目/%E9%A1%B9%E7%9B%AE%E6%9E%B6%E6%9E%84.png" alt></p><h1 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h1><p>resources<br>&emsp;&emsp;&emsp;&emsp;access_log.txt:日志样例<br>&emsp;&emsp;&emsp;&emsp;log_sta.conf ：配置文件<br>scala.com.jmx.analysis<br>&emsp;&emsp;&emsp;&emsp;AccessLogParser.scala :日志解析<br>&emsp;&emsp;&emsp;&emsp;logAnalysis：日志分析<br>scala.com.jmx.util<br>&emsp;&emsp;&emsp;&emsp;Utility.scala:工具类<br>scala<br>&emsp;&emsp;&emsp;&emsp;Run：驱动程序(main)<br>具体代码详见<a href="https://github.com/jiamx/log_analysis" target="_blank" rel="noopener">github</a></p><h1 id="搭建discuz论坛"><a href="#搭建discuz论坛" class="headerlink" title="搭建discuz论坛"></a>搭建discuz论坛</h1><h2 id="安装XAMPP"><a href="#安装XAMPP" class="headerlink" title="安装XAMPP"></a>安装XAMPP</h2><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><p><code>wget https://www.apachefriends.org/xampp-files/5.6.33/xampp-linux-x64-5.6.33-0-installer.run</code></p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><code># 赋予文件执行权限</code><br><code>chmod u+x xampp-linux-x64-5.6.33-0-installer.run</code><br><code># 运行安装文件</code><br>`./xampp-linux-x64-5.6.33-0-installer.run``</p><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><p>将以下内容加入到 ~/.bash_profile<br><code>export XAMPP=/opt/lampp/</code><br><code>export PATH=$PATH:$XAMPP:$XAMPP/bin</code>   </p><h3 id="刷新环境变量"><a href="#刷新环境变量" class="headerlink" title="刷新环境变量"></a>刷新环境变量</h3><p><code>source ~/.bash_profile</code></p><h3 id="启动XAMPP"><a href="#启动XAMPP" class="headerlink" title="启动XAMPP"></a>启动XAMPP</h3><p><code>xampp restart</code></p><h2 id="root用户密码和权限修改"><a href="#root用户密码和权限修改" class="headerlink" title="root用户密码和权限修改"></a>root用户密码和权限修改</h2><p><code>#修改root用户密码为123</code><br><code>update mysql.user set password=PASSWORD(&#39;123&#39;) where user=&#39;root&#39;;</code><br><code>flush privileges;</code><br><code>#赋予root用户远程登录权限</code><br><code>grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;123&#39; with grant option;</code><br><code>flush privileges;</code>  </p><h2 id="安装Discuz"><a href="#安装Discuz" class="headerlink" title="安装Discuz"></a>安装Discuz</h2><h3 id="下载discuz"><a href="#下载discuz" class="headerlink" title="下载discuz"></a>下载discuz</h3><p><code>wget http://download.comsenz.com/DiscuzX/3.2/Discuz_X3.2_SC_UTF8.zip</code>  </p><h3 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h3><p><code>#删除原有的web应用</code><br><code>rm -rf /opt/lampp/htdocs/*</code><br><code>unzip Discuz_X3.2_SC_UTF8.zip –d /opt/lampp/htdocs/</code><br><code>cd /opt/lampp/htdocs/</code><br><code>mv upload/*</code><br><code>#修改目录权限</code><br><code>chmod 777 -R /opt/lampp/htdocs/config/</code><br><code>chmod 777 -R /opt/lampp/htdocs/data/</code><br><code>chmod 777 -R /opt/lampp/htdocs/uc_client/</code><br><code>chmod 777 -R /opt/lampp/htdocs/uc_server/</code>  </p><h2 id="Discuz基本操作"><a href="#Discuz基本操作" class="headerlink" title="Discuz基本操作"></a>Discuz基本操作</h2><h3 id="自定义版块"><a href="#自定义版块" class="headerlink" title="自定义版块"></a>自定义版块</h3><ul><li>进入discuz后台：<a href="http://slave1/admin.php" target="_blank" rel="noopener">http://slave1/admin.php</a>  </li><li>点击顶部的“论坛”菜单  </li><li>按照页面提示创建所需版本，可以创建父子版块  </li></ul><h3 id="查看访问日志"><a href="#查看访问日志" class="headerlink" title="查看访问日志"></a>查看访问日志</h3><p>日志默认地址<br><code>/opt/lampp/logs/access_log</code><br>实时查看日志命令<br><code>tail –f /opt/lampp/logs/access_log</code>  </p><h2 id="Discuz帖子-版块存储简介"><a href="#Discuz帖子-版块存储简介" class="headerlink" title="Discuz帖子/版块存储简介"></a>Discuz帖子/版块存储简介</h2><p><code>mysql -uroot -p123 ultrax # 登录ultrax数据库</code><br><code>查看包含帖子id及标题对应关系的表</code><br><code>#tid, subject（文章id、标题）</code><br><code>select tid, subject from pre_forum_post limit 10;</code><br><code>#fid, name（版块id、标题）</code><br><code>select fid, name from pre_forum_forum limit 40;</code>  </p><h2 id="修改日志格式"><a href="#修改日志格式" class="headerlink" title="修改日志格式"></a>修改日志格式</h2><h3 id="找到Apache配置文件"><a href="#找到Apache配置文件" class="headerlink" title="找到Apache配置文件"></a>找到Apache配置文件</h3><p>Apache配置文件名称为httpd.conf，所在目录为 /opt/lampp/etc/ ，完整路径为 /opt/lampp/etc/httpd.conf</p><h3 id="修改日志格式-1"><a href="#修改日志格式-1" class="headerlink" title="修改日志格式"></a>修改日志格式</h3><p>关闭通用日志文件的使用<br><code>CustomLog &quot;logs/access_log&quot; common</code><br>启用组合日志文件<br><code>CustomLog &quot;logs/access_log&quot; combined</code><br>重新加载配置文件<br><code>xampp reload</code><br>检查访问日志<br><code>tail -f /opt/lampp/logs/access_log</code>  </p><h3 id="Flume与Kafka配置"><a href="#Flume与Kafka配置" class="headerlink" title="Flume与Kafka配置"></a>Flume与Kafka配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#agent的名称为a1  </span><br><span class="line">a1.sources = source1  </span><br><span class="line">a1.channels = channel1  </span><br><span class="line">a1.sinks = sink1</span><br><span class="line">#set source</span><br><span class="line">a1.sources.source1.type = TAILDIR  </span><br><span class="line">a1.sources.source1.filegroups = f1  </span><br><span class="line">a1.sources.source1.filegroups.f1 = /opt/lampp/logs/access_log  </span><br><span class="line">a1sources.source1.fileHeader = flase  </span><br><span class="line">#set sink</span><br><span class="line">a1.sinks.sink1.type = org.apache.flume.sink.kafka.KafkaSink  </span><br><span class="line">a1.sinks.sink1.brokerList=kms-2.apache.com:9092,kms-3.apache.com:9092,kms-4.apache.com:9092    </span><br><span class="line">​a1.sinks.sink1.topic= discuzlog  </span><br><span class="line">​a1.sinks.sink1.kafka.flumeBatchSize = 20  </span><br><span class="line">​a1.sinks.sink1.kafka.producer.acks = 1  </span><br><span class="line">​a1.sinks.sink1.kafka.producer.linger.ms = 1  </span><br><span class="line">​a1.sinks.sink1.kafka.producer.compression.type = snappy  </span><br><span class="line">#set channel</span><br><span class="line">​a1.channels.channel1.type = file  </span><br><span class="line">​a1.channels.channel1.checkpointDir = /home/kms/data/flume_data/checkpoint  </span><br><span class="line">​a1.channels.channel1.dataDirs= /home/kms/data/flume_data/data  </span><br><span class="line">#bind</span><br><span class="line">​a1.sources.source1.channels = channel1  </span><br><span class="line">​a1.sinks.sink1.channel = channel1</span><br></pre></td></tr></table></figure><h2 id="创建MySQL数据库和所需要的表"><a href="#创建MySQL数据库和所需要的表" class="headerlink" title="创建MySQL数据库和所需要的表"></a>创建MySQL数据库和所需要的表</h2><p><strong>创建数据库</strong>  </p><p><code>CREATE DATABASE</code>statistics<code>CHARACTER SET &#39;utf8&#39; COLLATE &#39;utf8_general_ci&#39;;</code>  </p><p><strong>创建表:</strong><br>&emsp;&emsp;特定时间段内不同ip的访问次数：client_ip_access<br>CREATE TABLE client_ip_access (<br>&emsp;&emsp;&emsp;&emsp;client_ip text COMMENT ‘客户端ip’,<br>&emsp;&emsp;&emsp;&emsp;sum BIGINT ( 20 ) NOT NULL COMMENT ‘访问次数’,<br>&emsp;&emsp;&emsp;&emsp;time text NOT NULL COMMENT ‘统计时间’<br>) ENGINE = INNODB DEFAULT CHARSET = utf8;  </p><p>&emsp;&emsp;特定时间段内不同文章的访问次数：hot_article<br>CREATE TABLE hot_article (<br>&emsp;&emsp;&emsp;&emsp;article_id text COMMENT ‘文章id’,<br>&emsp;&emsp;&emsp;&emsp;subject text NOT NULL COMMENT ‘文章标题’,<br>&emsp;&emsp;&emsp;&emsp;sum BIGINT ( 20 ) NOT NULL COMMENT ‘访问次数’,<br>&emsp;&emsp;&emsp;&emsp;time text NOT NULL COMMENT ‘统计时间’<br>) ENGINE = INNODB DEFAULT CHARSET = utf8;  </p><p>&emsp;&emsp;特定时间段内不同版块的访问次数：hot_section<br>CREATE TABLE hot_section (<br>&emsp;&emsp;&emsp;&emsp;section_id text COMMENT ‘版块id’,<br>&emsp;&emsp;&emsp;&emsp;name text NOT NULL COMMENT ‘版块标题’,<br>&emsp;&emsp;&emsp;&emsp;sum BIGINT ( 20 ) NOT NULL COMMENT ‘访问次数’,<br>&emsp;&emsp;&emsp;&emsp;time text NOT NULL COMMENT ‘统计时间’<br>) ENGINE = INNODB DEFAULT CHARSET = utf8; </p><h2 id="打包部署存在的问题"><a href="#打包部署存在的问题" class="headerlink" title="打包部署存在的问题"></a>打包部署存在的问题</h2><p><strong>问题1</strong> </p><pre><code>Exception in thread &quot;main&quot; java.lang.SecurityException: Invalid signature file digest for Manifest main attributes</code></pre><p><strong>解决方式</strong>  </p><p>原因:使用sbt打包的时候导致某些包的重复引用，所以打包之后的META-INF的目录下多出了一些<em>.SF,</em>.DSA,*.RSA文件  </p><p>解决办法：删除掉多于的<em>.SF,</em>.DSA,*.RSA文件  </p><p><code>zip -d log_analysis-1.0-SNAPSHOT.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF</code>  </p><p><strong>问题2</strong>    </p><pre><code>Exception in thread &quot;main&quot; java.io.FileNotFoundException: File file:/data/spark_data/history/event-log does not exist</code></pre><p><strong>解决方式</strong>  </p><p>原因:由于spark的spark-defaults.conf配置文件中配置 eventLog 时指定的路径在本机不存在。  </p><p>解决办法：创建对应的文件夹，并赋予对应权限<br><code>sudo mkdir -p /data/spark_data/history/spark-events</code><br><code>sudo mkdir -p /data/spark_data/history/event-log</code><br><code>sudo chmod 777 -R /data</code>  </p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>首先，基于discuz搭建了论坛，针对论坛产生的日志，对其进行分析。主要的处理流程为log—&gt;flume—&gt;kafka—&gt;sparkstreaming—&gt;MySQL,最后将处理的结果写入MySQL共报表查询。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的状态后端(State Backends)</title>
      <link href="/2019/08/23/Flink%E7%9A%84%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF-State-Backends/"/>
      <url>/2019/08/23/Flink%E7%9A%84%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF-State-Backends/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp;当使用checkpoint时，状态(state)会被持久化到checkpoint上，以防止数据的丢失并确保发生故障时能够完全恢复。状态是通过什么方式在哪里持久化，取决于使用的状态后端。</p><a id="more"></a><h2 id="可用的状态后端"><a href="#可用的状态后端" class="headerlink" title="可用的状态后端"></a>可用的状态后端</h2><p><strong>MemoryStateBackend</strong><br><strong>FsStateBackend</strong><br><strong>FsStateBackend</strong>  </p><p>注意：如果什么都不配置，系统默认的是MemoryStateBackend</p><h2 id="MemoryStateBackend"><a href="#MemoryStateBackend" class="headerlink" title="MemoryStateBackend"></a>MemoryStateBackend</h2><p><img src="//jiamaoxiang.top/2019/08/23/Flink的状态后端-State-Backends/memorystatebackend.png" alt><br>&emsp;&emsp;<code>MemoryStateBackend</code> 是将状态维护在 Java 堆上的一个内部状态后端。键值状态和窗口算子使用哈希表来存储数据（values）和定时器（timers）。当应用程序 checkpoint 时，此后端会在将状态发给 JobManager 之前快照下状态，JobManager 也将状态存储在 Java 堆上。默认情况下，<code>MemoryStateBackend</code> 配置成支持异步快照。异步快照可以避免阻塞数据流的处理，从而避免反压的发生。当然，使用 <code>new MemoryStateBackend(MAX_MEM_STATE_SIZE, false)</code>也可以禁用该特点。</p><p><strong>缺点</strong>：</p><ul><li>默认情况下，每一个状态的大小限制为 5 MB。可以通过 <code>MemoryStateBackend</code> 的构造函数增加这个大小。状态大小受到 akka 帧大小的限制(maxStateSize &lt;= akka.framesize 默认 10 M)，所以无论怎么调整状态大小配置，都不能大于 akka 的帧大小。也可以通过 akka.framesize 调整 akka 帧大小。</li><li>状态的总大小不能超过 JobManager 的内存。</li></ul><p><strong>推荐使用的场景</strong>：</p><ul><li>本地测试、几乎无状态的作业，比如 ETL、JobManager 不容易挂，或挂掉影响不大的情况。</li><li>不推荐在生产场景使用。</li></ul><h2 id="FsStateBackend"><a href="#FsStateBackend" class="headerlink" title="FsStateBackend"></a>FsStateBackend</h2><p><img src="//jiamaoxiang.top/2019/08/23/Flink的状态后端-State-Backends/fsstatebackend.png" alt><br>&emsp;&emsp;<code>FsStateBackend</code>需要配置的主要是文件系统，如 URL（类型，地址，路径）。比如可以是：<br><code>“hdfs://namenode:40010/flink/checkpoints”</code> 或<code>“s3://flink/checkpoints”</code></p><p>&emsp;&emsp;当选择使用 <code>FsStateBackend</code>时，正在进行的数据会被存在TaskManager的内存中。在checkpoint时，此后端会将状态快照写入配置的文件系统和目录的文件中，同时会在JobManager的内存中（在高可用场景下会存在 Zookeeper 中）存储极少的元数据。容量限制上，单 TaskManager 上 State 总量不超过它的内存，总大小不超过配置的文件系统容量。</p><p>&emsp;&emsp;默认情况下，<code>FsStateBackend</code> 配置成提供异步快照，以避免在状态 checkpoint 时阻塞数据流的处理。该特性可以实例化 <code>FsStateBackend</code> 时传入false的布尔标志来禁用掉，例如：<code>new FsStateBackend(path, false)</code></p><p><strong>推荐使用的场景</strong>：</p><ul><li>处理大状态，长窗口，或大键值状态的有状态处理任务， 例如分钟级窗口聚合或 join。</li><li>适合用于高可用方案（需要开启HA的作业）。</li><li>可以在生产环境中使用</li></ul><h2 id="RocksDBStateBackend"><a href="#RocksDBStateBackend" class="headerlink" title="RocksDBStateBackend"></a>RocksDBStateBackend</h2><p><img src="//jiamaoxiang.top/2019/08/23/Flink的状态后端-State-Backends/rocksdbstatebackend.png" alt><br>&emsp;&emsp;<code>RocksDBStateBackend</code> 的配置也需要一个文件系统（类型，地址，路径），如下所示：<br>“hdfs://namenode:40010/flink/checkpoints” 或“s3://flink/checkpoints”<br>RocksDB 是一种嵌入式的本地数据库。RocksDBStateBackend 将处理中的数据使用 <a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a> 存储在本地磁盘上。在 checkpoint 时，整个 RocksDB 数据库会被存储到配置的文件系统中，或者在超大状态作业时可以将增量的数据存储到配置的文件系统中。同时 Flink 会将极少的元数据存储在 JobManager 的内存中，或者在 Zookeeper 中（对于高可用的情况）。<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a> 默认也是配置成异步快照的模式。</p><p>&emsp;&emsp;<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>是一个 key/value 的内存存储系统，和其他的 key/value 一样，先将状态放到内存中，如果内存快满时，则写入到磁盘中，但需要注意<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>不支持同步的 Checkpoint，构造方法中没有同步快照这个选项。不过<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>支持增量的 Checkpoint，也是目前唯一增量 Checkpoint 的 Backend，意味着并不需要把所有 sst 文件上传到 Checkpoint 目录，仅需要上传新生成的 sst 文件即可。它的 Checkpoint 存储在外部文件系统（本地或HDFS），其容量限制只要单个 TaskManager 上 State 总量不超过它的内存+磁盘，单Key最大2G，总大小不超过配置的文件系统容量即可。</p><p><strong>缺点</strong>：</p><ul><li><a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>支持的单key和单value的大小最大为每个 2^31 字节。这是因为 RocksDB 的 JNI API 是基于byte[]的。<br></li><li>对于使用具有合并操作的状态的应用程序，例如 ListState，随着时间可能会累积到超过 2^31 字节大小，这将会导致在接下来的查询中失败。</li></ul><p><strong>推荐使用的场景</strong>：</p><ul><li>最适合用于处理大状态，长窗口，或大键值状态的有状态处理任务。</li><li>非常适合用于高可用方案。</li><li>最好是对状态读写性能要求不高的作业</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;那如何选择状态的类型和存储方式？结合前面的内容，可以看到，首先是要分析清楚业务场景；比如想要做什么，状态到底大不大。比较各个方案的利弊，选择根据需求合适的状态类型和存储方式即可。</p><hr><p><strong>Reference</strong></p><p>[1]<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/state/state_backends.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/state/state_backends.html</a><br>[2]<a href="https://ververica.cn/developers/state-management/" target="_blank" rel="noopener">https://ververica.cn/developers/state-management/</a><br>[3]<a href="https://www.ververica.com/blog/stateful-stream-processing-apache-flink-state-backends" target="_blank" rel="noopener">https://www.ververica.com/blog/stateful-stream-processing-apache-flink-state-backends</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--运维与监控(七)</title>
      <link href="/2019/08/19/%E8%BF%90%E7%BB%B4%E4%B8%8E%E7%9B%91%E6%8E%A7/"/>
      <url>/2019/08/19/%E8%BF%90%E7%BB%B4%E4%B8%8E%E7%9B%91%E6%8E%A7/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--集群与部署(六)</title>
      <link href="/2019/08/19/%E9%9B%86%E7%BE%A4%E4%B8%8E%E9%83%A8%E7%BD%B2/"/>
      <url>/2019/08/19/%E9%9B%86%E7%BE%A4%E4%B8%8E%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--source/sink Connectors(五)</title>
      <link href="/2019/08/19/source-sink-Connectors/"/>
      <url>/2019/08/19/source-sink-Connectors/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--状态与容错（四）</title>
      <link href="/2019/08/19/%E7%8A%B6%E6%80%81%E4%B8%8E%E5%AE%B9%E9%94%99/"/>
      <url>/2019/08/19/%E7%8A%B6%E6%80%81%E4%B8%8E%E5%AE%B9%E9%94%99/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--基于时间的算子(三)</title>
      <link href="/2019/08/19/%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E7%9A%84%E7%AE%97%E5%AD%90/"/>
      <url>/2019/08/19/%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E7%9A%84%E7%AE%97%E5%AD%90/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--DataStream-API简介(二)</title>
      <link href="/2019/08/19/DataStream-API%E7%AE%80%E4%BB%8B/"/>
      <url>/2019/08/19/DataStream-API%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--Flink的几个重要概念(一)</title>
      <link href="/2019/08/19/Flink%E7%9A%84%E5%87%A0%E4%B8%AA%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5/"/>
      <url>/2019/08/19/Flink%E7%9A%84%E5%87%A0%E4%B8%AA%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p><h3 id="Event-time"><a href="#Event-time" class="headerlink" title="Event-time"></a>Event-time</h3><p>处理时间(process time)很好理解，指的是机器的本地时间，会产生不一致的、不可重复的结果。相反，事件时间(Event-time)能够产生一致的、可重复的结果。然而，相比基于处理时间的应用，基于事件时间的应用需要额外的配置。支持事件时间的流处理引擎的内部比仅仅支持处理时间的流处理引擎的内部更为复杂。</p><p>Flink不仅为常见的事件时间提供直观且易于使用的处理操作，而且也提供了API去自定义实现更高级的事件时间。 对于这样的高级应用，很好的理解Flink的内部时间处理通常是很有帮助的。Flink主要利用两个概念提供事件时间语义：记录时间戳(record timestamps)和watermarks。 接下来，我们将描述Flink内部如何实现和处理时间戳及watermark以支持流应用程序具有事件时间语义的。</p><h4 id="时间戳-timestamps"><a href="#时间戳-timestamps" class="headerlink" title="时间戳(timestamps)"></a>时间戳(timestamps)</h4><p>对于使用事件时间的应用，所处理的记录(record)必须携带时间戳。时间戳将记录与特定时间点相关联，代表事件发生的时间。当Flink以事件时间模式处理数据流时，比如窗口操作，内部会自动的按时间戳将事件发送到相对应的窗口。 Flink会将时间戳编码为16个字节的Long类型的值，并将它们作为元数据附加到记录中。Flink内置的算子会将Long型的值解析为精确到毫秒的Unix时间戳， 但是，自定义算子可以有自己的解析策略，例如，将精度调整为微秒。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浅析数据库缓冲池与SQL查询成本</title>
      <link href="/2019/08/14/%E6%B5%85%E6%9E%90%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%86%B2%E6%B1%A0%E4%B8%8ESQL%E6%9F%A5%E8%AF%A2%E6%88%90%E6%9C%AC/"/>
      <url>/2019/08/14/%E6%B5%85%E6%9E%90%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%86%B2%E6%B1%A0%E4%B8%8ESQL%E6%9F%A5%E8%AF%A2%E6%88%90%E6%9C%AC/</url>
      
        <content type="html"><![CDATA[<p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/background.jpg" alt><br>&emsp;&emsp;如果我们想要查找多行记录，查询时间是否会成倍地提升呢？其实数据库会采用缓冲池的方式提升页(page)的查找效率。数据库的缓冲池在数据库中起到了怎样的作用？如何查看一条 SQL 语句需要在缓冲池中进行加载的页的数量呢？</p><hr><h2 id="数据库缓冲池"><a href="#数据库缓冲池" class="headerlink" title="数据库缓冲池"></a>数据库缓冲池</h2><p>​        &emsp;&emsp;磁盘 I/O 需要消耗的时间很多，而在内存中进行操作，效率则会高很多，为了能让数据表或者索引中的数据随时被我们所用，DBMS 会申请占用内存来作为数据缓冲池，这样做的好处是可以让磁盘活动最小化，从而减少与磁盘直接进行 I/O 的时间。要知道，这种策略对提升 SQL 语句的查询性能来说至关重要。如果索引的数据在缓冲池里，那么访问的成本就会降低很多。<br>​       &emsp;&emsp;那么缓冲池如何读取数据呢？<br>​        &emsp;&emsp;缓冲池管理器会尽量将经常使用的数据保存起来，在数据库进行页面读操作的时候，首先会判断该页面是否在缓冲池中，如果存在就直接读取，如果不存在，就会通过内存或磁盘将页面存放到缓冲池中再进行读取。</p><h2 id="查看缓冲池大小"><a href="#查看缓冲池大小" class="headerlink" title="查看缓冲池大小"></a>查看缓冲池大小</h2><p>​         &emsp;&emsp;如果使用的是 MyISAM 存储引擎(只缓存索引，不缓存数据)，对应的键缓存参数为 key_buffer_size，可以用它进行查看。<br>​        &emsp;&emsp;如果使用的是 InnoDB 存储引擎，可以通过查看 innodb_buffer_pool_size 变量来查看缓冲池的大小，命令如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show variables like <span class="string">'innodb_buffer_pool_size'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/query_innodb_buffer_size.png" alt><br>​        &emsp;&emsp;此时 InnoDB 的缓冲池大小只有 8388608/1024/1024=8MB，我们可以修改缓冲池大小为 128MB，方法如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; <span class="built_in">set</span> global innodb_buffer_pool_size = 1073741824;</span><br></pre></td></tr></table></figure><p>​      &emsp;&emsp; 在 InnoDB 存储引擎中，可以同时开启多个缓冲池，查看缓冲池的个数，使用命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show variables like <span class="string">'innodb_buffer_pool_instances'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/innodb_buffer_pool_instance.png" alt><br>​        &emsp;&emsp;只有一个缓冲池。实际上innodb_buffer_pool_instances默认情况下为 8，为什么只显示只有一个呢？这里需要说明的是，如果想要开启多个缓冲池，你首先需要将innodb_buffer_pool_size参数设置为大于等于 1GB，这时innodb_buffer_pool_instances才会大于 1。你可以在 MySQL 的配置文件中对innodb_buffer_pool_size进行设置，大于等于 1GB，然后再针对innodb_buffer_pool_instances参数进行修改。</p><h2 id="查看SQL语句的查询成本"><a href="#查看SQL语句的查询成本" class="headerlink" title="查看SQL语句的查询成本"></a>查看SQL语句的查询成本</h2><p>​        &emsp;&emsp; 一条 SQL 查询语句在执行前需要确定查询计划，如果存在多种查询计划的话，MySQL 会计算每个查询计划所需要的成本，从中选择成本最小的一个作为最终执行的查询计划。</p><p>​          &emsp;&emsp;如果查看某条 SQL 语句的查询成本，可以在执行完这条 SQL 语句之后，通过查看当前会话中的 last_query_cost 变量值来得到当前查询的成本。这个查询成本对应的是 SQL 语句所需要读取的页(page)的数量。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show status like <span class="string">'last_query_cost'</span></span><br></pre></td></tr></table></figure><p><strong>example</strong>  </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; select userid,rating from movierating <span class="built_in">where</span> userid = 4169;</span><br></pre></td></tr></table></figure><p>结果：2313 rows in set (0.05 sec) </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show status like <span class="string">'last_query_cost'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/test1.png" alt></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; select userid,rating from movierating <span class="built_in">where</span> userid between 4168 and 4175;</span><br></pre></td></tr></table></figure><p>结果：2643 rows in set (0.01 sec) </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show status like <span class="string">'last_query_cost'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/test2.png" alt></p><p>&emsp;&emsp;你能看到页的数量是刚才的 1.4 倍，但是查询的效率并没有明显的变化，实际上这两个 SQL 查询的时间基本上一样，就是因为采用了顺序读取的方式将页面一次性加载到缓冲池中，然后再进行查找。虽然页数量（last_query_cost）增加了不少，但是通过缓冲池的机制，并没有增加多少查询时间。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程</title>
      <link href="/2019/08/13/Flink%E8%87%AA%E5%AD%A6%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B/"/>
      <url>/2019/08/13/Flink%E8%87%AA%E5%AD%A6%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p><img src="//jiamaoxiang.top/2019/08/13/Flink自学系列教程/logo.png" alt><br>&emsp;&emsp;Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p><hr><h4 id="1-Flink的几个重要概念"><a href="#1-Flink的几个重要概念" class="headerlink" title="1.Flink的几个重要概念"></a><a href="https://jiamaoxiang.top/2019/08/19/Flink%E7%9A%84%E5%87%A0%E4%B8%AA%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5/">1.Flink的几个重要概念</a></h4><h4 id="2-DataFrame-API"><a href="#2-DataFrame-API" class="headerlink" title="2. DataFrame API"></a><a href="https://jiamaoxiang.top/2019/08/19/DataFrame-API/">2. DataFrame API</a></h4><h4 id="3-基于时间的算子"><a href="#3-基于时间的算子" class="headerlink" title="3. 基于时间的算子"></a><a href="https://jiamaoxiang.top/2019/08/19/%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E7%9A%84%E7%AE%97%E5%AD%90/">3. 基于时间的算子</a></h4><h4 id="4-状态与容错"><a href="#4-状态与容错" class="headerlink" title="4. 状态与容错"></a><a href="https://jiamaoxiang.top/2019/08/19/%E7%8A%B6%E6%80%81%E4%B8%8E%E5%AE%B9%E9%94%99/">4. 状态与容错</a></h4><h4 id="5-source-sink-Connectors"><a href="#5-source-sink-Connectors" class="headerlink" title="5. source/sink Connectors"></a><a href="https://jiamaoxiang.top/2019/08/19/source-sink-Connectors/">5. source/sink Connectors</a></h4><h4 id="6-集群与部署"><a href="#6-集群与部署" class="headerlink" title="6. 集群与部署"></a><a href="https://jiamaoxiang.top/2019/08/19/%E9%9B%86%E7%BE%A4%E4%B8%8E%E9%83%A8%E7%BD%B2/">6. 集群与部署</a></h4><h4 id="7-运维与监控"><a href="#7-运维与监控" class="headerlink" title="7.运维与监控"></a><a href="https://jiamaoxiang.top/2019/08/19/%E8%BF%90%E7%BB%B4%E4%B8%8E%E7%9B%91%E6%8E%A7/">7.运维与监控</a></h4>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/08/12/hello-world/"/>
      <url>/2019/08/12/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p><p><img src="//jiamaoxiang.top/2019/08/12/hello-world/logo.png" alt></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
