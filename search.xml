<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2019/09/24/flink%E7%9A%84window%E5%88%86%E7%B1%BB/"/>
      <url>/2019/09/24/flink%E7%9A%84window%E5%88%86%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<h2 id="flink-的-window-分类"><a href="#flink-的-window-分类" class="headerlink" title="flink 的 window 分类"></a>flink 的 window 分类</h2><p><strong>flink中的窗口主要分为3大类共5种窗口</strong>:</p><p><img src="//jiamaoxiang.top/2019/09/24/flink的window分类/pictures/flink%E4%B8%ADwindow%E5%88%86%E7%B1%BB.png" alt></p><ul><li><p><strong>Time Window 时间窗口</strong></p><ul><li><p><strong>Tumbing Time Window 滚动时间窗口</strong></p><p>实现统计每一分钟(或其他长度)窗口内 计算的效果</p></li><li><p><strong>Sliding Time Window 滑动时间窗口</strong></p><p>实现每过xxx时间 统计 xxx时间窗口的效果. 比如，我们可以每30秒计算一次最近一分钟用户购买的商品总数。</p></li></ul></li><li><p><strong>Count Window 计数窗口</strong></p><ul><li><p><strong>Tumbing Count Window  滚动计数窗口</strong></p><p>当我们想要每100个用户购买行为事件统计购买总数，那么每当窗口中填满100个元素了，就会对窗口进行计算，这种窗口我们称之为翻滚计数窗口（Tumbling Count Window）</p></li><li><p><strong>Sliding Count Window   滑动计数窗口</strong></p><p>和Sliding Time Window含义是类似的，例如计算每10个元素计算一次最近100个元素的总和</p></li></ul></li><li><p><strong>Session Window  会话窗口</strong></p><p>在这种用户交互事件流中，我们首先想到的是将事件聚合到会话窗口中（一段用户持续活跃的周期），由非活跃的间隙分隔开。如上图所示，就是需要计算每个用户在活跃期间总共购买的商品数量，如果用户30秒没有活动则视为会话断开（假设raw data stream是单个用户的购买行为流）</p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E8%AF%B4%E4%B8%80%E4%B8%8Bzk%E7%9A%84%E9%80%9A%E7%9F%A5%E6%9C%BA%E5%88%B6/"/>
      <url>/2019/09/24/docs/%E8%AF%B4%E4%B8%80%E4%B8%8Bzk%E7%9A%84%E9%80%9A%E7%9F%A5%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h2 id="说一下zk的通知机制"><a href="#说一下zk的通知机制" class="headerlink" title="说一下zk的通知机制"></a>说一下zk的通知机制</h2><p>客户端端会对某个 znode 建立一个 watcher 事件，当该 znode 发生变化时，这些客户端会收到 zookeeper 的通知，然后客户端可以根据 znode 变化来做出业务上的改变</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8B%E5%AE%BD%E4%BE%9D%E8%B5%96%E5%92%8C%E7%AA%84%E4%BE%9D%E8%B5%96/"/>
      <url>/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8B%E5%AE%BD%E4%BE%9D%E8%B5%96%E5%92%8C%E7%AA%84%E4%BE%9D%E8%B5%96/</url>
      
        <content type="html"><![CDATA[<h2 id="讲一下宽依赖和窄依赖"><a href="#讲一下宽依赖和窄依赖" class="headerlink" title="讲一下宽依赖和窄依赖"></a>讲一下宽依赖和窄依赖</h2><p>区别宽窄依赖的核心点是 <strong>子RDD的partition与父RDD的partition是否是1对多的关系</strong>,如果是这样的关系的话,</p><p>说明多个父rdd的partition需要经过shuffle过程汇总到一个子rdd的partition,这样就是一次宽依赖,在DAGScheduler中会产生stage的切分.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bzookeeper%E5%9C%A8kafka%E4%B8%AD%E7%9A%84%E4%BD%9C%E7%94%A8/"/>
      <url>/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bzookeeper%E5%9C%A8kafka%E4%B8%AD%E7%9A%84%E4%BD%9C%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="讲一下zookeeper在kafka中的作用"><a href="#讲一下zookeeper在kafka中的作用" class="headerlink" title="讲一下zookeeper在kafka中的作用"></a>讲一下zookeeper在kafka中的作用</h2><p><img src="//jiamaoxiang.top/2019/09/24/docs/讲一下zookeeper在kafka中的作用/pictures/kafka%E5%9C%A8zk%E4%B8%AD%E7%9A%84%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84.png" alt></p><h4 id="zk的作用主要有如下几点"><a href="#zk的作用主要有如下几点" class="headerlink" title="zk的作用主要有如下几点:"></a>zk的作用主要有如下几点:</h4><ol><li>kafka的元数据都存放在zk上面,由zk来管理</li><li>0.8之前版本的kafka, consumer的消费状态，group的管理以及 offset的值都是由zk管理的,现在offset会保存在本地topic文件里</li><li>负责borker的lead选举和管理</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bspark%E7%9A%84%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84/"/>
      <url>/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bspark%E7%9A%84%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h2 id="讲一下spark-的运行架构"><a href="#讲一下spark-的运行架构" class="headerlink" title="讲一下spark 的运行架构"></a>讲一下spark 的运行架构</h2><p><img src="//jiamaoxiang.top/2019/09/24/docs/讲一下spark的运行架构/pictures/spark%E6%9E%B6%E6%9E%84%E5%9B%BE.jpg" alt></p><ul><li><p><strong>Cluster Manager(Master)</strong>：在standalone模式中即为Master主节点，控制整个集群，监控worker。在YARN模式中为资源管理器</p></li><li><p><strong>Worker节点</strong>：从节点，负责控制计算节点，启动Executor或者Driver。</p></li><li><p><strong>Driver</strong>： 运行Application 的main()函数</p></li><li><p><strong>Executor</strong>：执行器，是为某个Application运行在worker node上的一个进程</p></li></ul><p><a href="https://juejin.im/post/5a73c8386fb9a0635e3cafaa" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bspark%E7%9A%84%E5%87%A0%E7%A7%8D%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F/"/>
      <url>/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bspark%E7%9A%84%E5%87%A0%E7%A7%8D%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h2 id="讲一下spark的几种部署方式"><a href="#讲一下spark的几种部署方式" class="headerlink" title="讲一下spark的几种部署方式"></a>讲一下spark的几种部署方式</h2><p><strong>目前,除了local模式为本地调试模式以为, Spark支持三种分布式部署方式，分别是standalone、spark on mesos和 spark on YARN</strong></p><ul><li><p><strong>Standalone模式</strong></p><p>即独立模式，自带完整的服务，可单独部署到一个集群中，无需依赖任何其他资源管理系统。从一定程度上说，该模式是其他两种的基础。目前Spark在standalone模式下是没有任何单点故障问题的，这是借助zookeeper实现的，思想类似于Hbase master单点故障解决方案。将Spark standalone与MapReduce比较，会发现它们两个在架构上是完全一致的： </p><ul><li>都是由master/slaves服务组成的，且起初master均存在单点故障，后来均通过zookeeper解决（Apache MRv1的JobTracker仍存在单点问题，但CDH版本得到了解决）； </li><li>各个节点上的资源被抽象成粗粒度的slot，有多少slot就能同时运行多少task。不同的是，MapReduce将slot分为map slot和reduce slot，它们分别只能供Map Task和Reduce Task使用，而不能共享，这是MapReduce资源利率低效的原因之一，而Spark则更优化一些，它不区分slot类型，只有一种slot，可以供各种类型的Task使用，这种方式可以提高资源利用率，但是不够灵活，不能为不同类型的Task定制slot资源。总之，这两种方式各有优缺点。 </li></ul></li><li><p><strong>Spark On YARN模式</strong></p><p><strong>spark on yarn 的支持两种模式：</strong> </p><ul><li>yarn-cluster：适用于生产环境； </li><li>yarn-client：适用于交互、调试，希望立即看到app的输出 </li></ul><p>yarn-cluster和yarn-client的区别在于yarn appMaster，每个yarn app实例有一个appMaster进程，是为app启动的第一个container；负责从ResourceManager请求资源，获取到资源后，告诉NodeManager为其启动container。yarn-cluster和yarn-client模式内部实现还是有很大的区别。如果你需要用于生产环境，那么请选择yarn-cluster；而如果你仅仅是Debug程序，可以选择yarn-client。</p></li><li><p><strong>Spark On Mesos模式</strong></p><p>Spark运行在Mesos上会比运行在YARN上更加灵活，更加自然。目前在Spark On Mesos环境中，用户可选择两种调度模式之一运行自己的应用程序</p><ul><li><p>粗粒度模式（Coarse-grained Mode）：每个应用程序的运行环境由一个Dirver和若干个Executor组成，其中，每个Executor占用若干资源，内部可运行多个Task（对应多少个“slot”）。应用程序的各个任务正式运行之前，需要将运行环境中的资源全部申请好，且运行过程中要一直占用这些资源，即使不用，最后程序运行结束后，回收这些资源。</p></li><li><p>细粒度模式（Fine-grained Mode）：鉴于粗粒度模式会造成大量资源浪费，Spark On Mesos还提供了另外一种调度模式：细粒度模式，这种模式类似于现在的云计算，思想是按需分配。与粗粒度模式一样，应用程序启动时，先会启动executor，但每个executor占用资源仅仅是自己运行所需的资源，不需要考虑将来要运行的任务，之后，mesos会为每个executor动态分配资源，每分配一些，便可以运行一个新任务，单个Task运行完之后可以马上释放对应的资源。</p></li></ul></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bleader%20%E9%80%89%E4%B8%BE%E8%BF%87%E7%A8%8B/"/>
      <url>/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bleader%20%E9%80%89%E4%B8%BE%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="讲一下leader-选举过程"><a href="#讲一下leader-选举过程" class="headerlink" title="讲一下leader 选举过程"></a>讲一下leader 选举过程</h2><p>　　这里选取3台机器组成的服务器集群为例。在集群初始化阶段，当有一台服务器Server1启动时，其单独无法进行和完成Leader选举，当第二台服务器Server2启动时，此时两台机器可以相互通信，每台机器都试图找到Leader，于是进入Leader选举过程。选举过程如下</p><p>　　<strong>(1) 每个Server发出一个投票</strong>。由于是初始情况，Server1和Server2都会将自己作为Leader服务器来进行投票，每次投票会包含所推举的服务器的myid和ZXID，使用(myid, ZXID)来表示，此时Server1的投票为(1, 0)，Server2的投票为(2, 0)，然后各自将这个投票发给集群中其他机器。</p><p>　　<strong>(2) 接受来自各个服务器的投票</strong>。集群的每个服务器收到投票后，首先判断该投票的有效性，如检查是否是本轮投票、是否来自LOOKING状态的服务器。</p><p>　　<strong>(3) 处理投票</strong>。针对每一个投票，服务器都需要将别人的投票和自己的投票进行PK，PK规则如下</p><p>　　　　<strong>· 优先检查ZXID</strong>。ZXID比较大的服务器优先作为Leader。</p><p>　　　　<strong>· 如果ZXID相同，那么就比较myid</strong>。myid较大的服务器作为Leader服务器。</p><p>　　对于Server1而言，它的投票是(1, 0)，接收Server2的投票为(2, 0)，首先会比较两者的ZXID，均为0，再比较myid，此时Server2的myid最大，于是更新自己的投票为(2, 0)，然后重新投票，对于Server2而言，其无须更新自己的投票，只是再次向集群中所有机器发出上一次投票信息即可。</p><p>　　<strong>(4) 统计投票</strong>。每次投票后，服务器都会统计投票信息，判断是否已经有过半机器接受到相同的投票信息，对于Server1、Server2而言，都统计出集群中已经有两台机器接受了(2, 0)的投票信息，此时便认为已经选出了Leader。</p><p>　　<strong>(5) 改变服务器状态</strong>。一旦确定了Leader，每个服务器就会更新自己的状态，如果是Follower，那么就变更为FOLLOWING，如果是Leader，就变更为LEADING。</p><h3 id="Leader选举算法分析"><a href="#Leader选举算法分析" class="headerlink" title="Leader选举算法分析"></a>Leader选举算法分析</h3><p>在3.4.0后的Zookeeper的版本只保留了TCP版本的FastLeaderElection选举算法。当一台机器进入Leader选举时，当前集群可能会处于以下两种状态</p><p>　　　　· 集群中已经存在Leader。</p><p>　　　　· 集群中不存在Leader。</p><p>　　对于集群中已经存在Leader而言，此种情况一般都是某台机器启动得较晚，在其启动之前，集群已经在正常工作，对这种情况，该机器试图去选举Leader时，会被告知当前服务器的Leader信息，对于该机器而言，仅仅需要和Leader机器建立起连接，并进行状态同步即可。而在集群中不存在Leader情况下则会相对复杂，其步骤如下</p><p>　　(1) <strong>第一次投票</strong>。无论哪种导致进行Leader选举，集群的所有机器都处于试图选举出一个Leader的状态，即LOOKING状态，LOOKING机器会向所有其他机器发送消息，该消息称为投票。投票中包含了SID（服务器的唯一标识）和ZXID（事务ID），(SID, ZXID)形式来标识一次投票信息。假定Zookeeper由5台机器组成，SID分别为1、2、3、4、5，ZXID分别为9、9、9、8、8，并且此时SID为2的机器是Leader机器，某一时刻，1、2所在机器出现故障，因此集群开始进行Leader选举。在第一次投票时，每台机器都会将自己作为投票对象，于是SID为3、4、5的机器投票情况分别为(3, 9)，(4, 8)， (5, 8)。</p><p>　　(2) <strong>变更投票</strong>。每台机器发出投票后，也会收到其他机器的投票，每台机器会根据一定规则来处理收到的其他机器的投票，并以此来决定是否需要变更自己的投票，这个规则也是整个Leader选举算法的核心所在，其中术语描述如下</p><p>　　　　<strong>· vote_sid</strong>：接收到的投票中所推举Leader服务器的SID。</p><p>　　　　<strong>· vote_zxid</strong>：接收到的投票中所推举Leader服务器的ZXID。</p><p>　　　　<strong>· self_sid</strong>：当前服务器自己的SID。</p><p>　　　　<strong>· self_zxid</strong>：当前服务器自己的ZXID。</p><p>　　每次对收到的投票的处理，都是对(vote_sid, vote_zxid)和(self_sid, self_zxid)对比的过程。</p><p>　　　　规则一：如果vote_zxid大于self_zxid，就认可当前收到的投票，并再次将该投票发送出去。</p><p>　　　　规则二：如果vote_zxid小于self_zxid，那么坚持自己的投票，不做任何变更。</p><p>　　　　规则三：如果vote_zxid等于self_zxid，那么就对比两者的SID，如果vote_sid大于self_sid，那么就认可当前收到的投票，并再次将该投票发送出去。</p><p>　　　　规则四：如果vote_zxid等于self_zxid，并且vote_sid小于self_sid，那么坚持自己的投票，不做任何变更。</p><p>　　结合上面规则，给出下面的集群变更过程。</p><p><img src="//jiamaoxiang.top/2019/09/24/docs/讲一下leader 选举过程/pictures/leader%E9%80%89%E4%B8%BE%E7%AE%97%E6%B3%95.png" alt></p><p>​    (3) <strong>确定Leader</strong>。经过第二轮投票后，集群中的每台机器都会再次接收到其他机器的投票，然后开始统计投票，如果一台机器收到了超过半数的相同投票，那么这个投票对应的SID机器即为Leader。此时Server3将成为Leader。</p><p>　　由上面规则可知，通常那台服务器上的数据越新（ZXID会越大），其成为Leader的可能性越大，也就越能够保证数据的恢复。如果ZXID相同，则SID越大机会越大。</p><p><a href="https://www.cnblogs.com/leesf456/p/6107600.html" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bkafk%E7%9A%84%E6%9E%B6%E6%9E%84/"/>
      <url>/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bkafk%E7%9A%84%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h2 id="讲一下kafka-的架构"><a href="#讲一下kafka-的架构" class="headerlink" title="讲一下kafka 的架构"></a>讲一下kafka 的架构</h2><p><img src="//jiamaoxiang.top/2019/09/24/docs/讲一下kafk的架构/pictures/kafka%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt></p><ul><li><p><strong>Producer</strong>：消息生产者</p><ul><li><p>Producer可以发送消息到Topic</p></li><li><ul><li>Topic的消息存放在不同Partition中，不同Partition存放在不同Broker中</li><li>Producer只需要指定Topic的名字、要连接到的Broker，这样Kafka就可以自动地把消息数据路由到合适的Broker（不一定是指定连接的Broker）</li></ul></li><li><p>Producer发送消息后，可以选择是否要确认消息写入成功（ACK，Acknowledgment）</p></li><li><ul><li>ACK=0：Producer不会等待ACK（消息可能丢失）</li><li>ACK=1：Producer会等待Leader Partition的ACK（Follower Partition消息可能丢失）</li><li>ACK=all：Producer会等待Leader Partition和Follower Partition的ACK（消息不会丢失）</li></ul></li><li><p>消息key：Producer可以给消息加上key，带相同key的消息会被分发到同一个Partition，这样就可以保证带相同key的消息的消费是有序的</p></li></ul></li><li><p><strong>Broker</strong>：每个Broker里包含了不同Topic的不同Partition，Partition中包含了有序的消息</p><ul><li>一个Kafka集群由多个Broker（server）组成</li><li>每个Broker都有ID标识</li><li>每个Broker里保存一定数量的Partition</li><li>客户端只要连接上任意一个Broker，就可以连接上整个Kafka集群</li><li>大多数Kafka集群刚开始的时候建议使用至少3个Broker，集群大了可以有上百个Broker</li></ul></li><li><p><strong>Consumer</strong>：消息消费者</p><ul><li><p>Consumer可以从Topic读取消息进行消费</p></li><li><ul><li>Topic的消息存放在不同Partition中，不同Partition存放在不同Broker中</li><li>Consumer只需要指定Topic的名字、要连接到的Broker，这样Kafka就可以自动地把Consumer路由到合适的Broker拉取消息进行消费（不一定是指定连接的Broker）</li><li>每一个Partition中的消息都会被有序消费</li></ul></li><li><p>Consumer Group：</p></li><li><ul><li>Consumer Group由多个Consumer组成</li><li>Consumer Group里的每个Consumer都会从不同的Partition中读取消息</li><li>如果Consumer的数量大于Partition的数量，那么多出来的Consumer就会空闲下来（浪费资源）</li></ul></li><li><p>Consumer offset：</p></li><li><ul><li>Kafka会为Consumer Group要消费的每个Partion保存一个offset，这个offset标记了该Consumer Group最后消费消息的位置</li><li>这个offset保存在Kafka里一个名为“__consumer_offsets”的Topic中；当Consumer从Kafka拉取消息消费时，同时也要对这个offset提交修改更新操作。这样若一个Consumer消费消息时挂了，其他Consumer可以通过这个offset值重新找到上一个消息再进行处理</li></ul></li></ul></li></ul><p><a href="https://zhuanlan.zhihu.com/p/48896367" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bhbase%E8%AF%BB%E6%95%B0%E6%8D%AE%E7%9A%84%E6%B5%81%E7%A8%8B/"/>
      <url>/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bhbase%E8%AF%BB%E6%95%B0%E6%8D%AE%E7%9A%84%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="讲一下hbase读数据的流程"><a href="#讲一下hbase读数据的流程" class="headerlink" title="讲一下hbase读数据的流程"></a>讲一下hbase读数据的流程</h2><ol><li><p>首先，客户端需要获知其想要读取的信息的Region的位置，这个时候，Client访问hbase上数据时并不需要Hmaster参与（HMaster仅仅维护着table和Region的元数据信息，负载很低），只需要访问zookeeper，从meta表获取相应region信息(地址和端口等)。【Client请求ZK获取.META.所在的RegionServer的地址。】</p></li><li><p>客户端会将该保存着RegionServer的位置信息的元数据表.META.进行缓存。然后在表中确定待检索rowkey所在的RegionServer信息（得到持有对应行键的.META表的服务器名）。【获取访问数据所在的RegionServer地址】</p></li><li><p>根据数据所在RegionServer的访问信息，客户端会向该RegionServer发送真正的数据读取请求。服务器端接收到该请求之后需要进行复杂的处理。</p></li><li><p>先从MemStore找数据，如果没有，再到StoreFile上读(为了读取的效率)。</p></li></ol><p><a href="https://blog.csdn.net/HaixWang/article/details/79520141#%E8%AF%BB%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88" target="_blank" rel="noopener">参考文章1</a></p><p><a href="http://hbasefly.com/2016/12/21/hbase-getorscan/?rkfcfo=fy6gy1" target="_blank" rel="noopener">参考文章2</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bhbase%E7%9A%84%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84/"/>
      <url>/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bhbase%E7%9A%84%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h2 id="讲一下hbase的存储结构-这样的存储结构有什么优缺点"><a href="#讲一下hbase的存储结构-这样的存储结构有什么优缺点" class="headerlink" title="讲一下hbase的存储结构,这样的存储结构有什么优缺点"></a>讲一下hbase的存储结构,这样的存储结构有什么优缺点</h2><p><img src="//jiamaoxiang.top/2019/09/24/docs/讲一下hbase的存储结构/pictures%5Chbase%E9%80%BB%E8%BE%91%E7%BB%93%E6%9E%84.png" alt></p><p><strong>Hbase的优点及应用场景</strong>:</p><ol><li>半结构化或非结构化数据:<br>对于数据结构字段不够确定或杂乱无章非常难按一个概念去进行抽取的数据适合用HBase，因为HBase支持动态添加列。</li><li>记录很稀疏：<br>RDBMS的行有多少列是固定的。为null的列浪费了存储空间。HBase为null的Column不会被存储，这样既节省了空间又提高了读性能。</li><li>多版本号数据：<br>依据Row key和Column key定位到的Value能够有随意数量的版本号值，因此对于须要存储变动历史记录的数据，用HBase是很方便的。比方某个用户的Address变更，用户的Address变更记录也许也是具有研究意义的。</li><li>仅要求最终一致性：<br>对于数据存储事务的要求不像金融行业和财务系统这么高，只要保证最终一致性就行。（比如HBase+elasticsearch时，可能出现数据不一致）</li><li>高可用和海量数据以及很大的瞬间写入量：<br>WAL解决高可用，支持PB级数据，put性能高<br>适用于插入比查询操作更频繁的情况。比如，对于历史记录表和日志文件。（HBase的写操作更加高效）</li><li>业务场景简单：<br>不需要太多的关系型数据库特性，列入交叉列，交叉表，事务，连接等。</li></ol><p><strong>Hbase的缺点：</strong></p><ol><li>单一RowKey固有的局限性决定了它不可能有效地支持多条件查询</li><li>不适合于大范围扫描查询</li><li>不直接支持 SQL 的语句查询</li></ol><p><a href="https://www.iteye.com/blog/forlan-2364661" target="_blank" rel="noopener">参考文章1</a></p><p><a href="https://blog.csdn.net/liaynling/article/details/81199238" target="_blank" rel="noopener">参考文章2</a></p><p><a href="https://juejin.im/post/5c31cf486fb9a04a102f6f89#heading-2" target="_blank" rel="noopener">参考文章3</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bhbase%E7%9A%84%E5%86%99%E6%95%B0%E6%8D%AE%E7%9A%84%E6%B5%81%E7%A8%8B/"/>
      <url>/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bhbase%E7%9A%84%E5%86%99%E6%95%B0%E6%8D%AE%E7%9A%84%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="讲一下hbase的写数据的流程"><a href="#讲一下hbase的写数据的流程" class="headerlink" title="讲一下hbase的写数据的流程"></a>讲一下hbase的写数据的流程</h2><ol><li>Client先访问zookeeper，从.META.表获取相应region信息，然后从meta表获取相应region信息 </li><li>根据namespace、表名和rowkey根据meta表的数据找到写入数据对应的region信息 </li><li>找到对应的regionserver 把数据先写到WAL中，即HLog，然后写到MemStore上 </li><li>MemStore达到设置的阈值后则把数据刷成一个磁盘上的StoreFile文件。 </li><li>当多个StoreFile文件达到一定的大小后(这个可以称之为小合并，合并数据可以进行设置，必须大于等于2，小于10——hbase.hstore.compaction.max和hbase.hstore.compactionThreshold，默认为10和3)，会触发Compact合并操作，合并为一个StoreFile，（这里同时进行版本的合并和数据删除。） </li><li>当Storefile大小超过一定阈值后，会把当前的Region分割为两个（Split）【可称之为大合并，该阈值通过hbase.hregion.max.filesize设置，默认为10G】，并由Hmaster分配到相应的HRegionServer，实现负载均衡</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8BHbase%E6%9E%B6%E6%9E%84/"/>
      <url>/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8BHbase%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h2 id="讲一下-Hbase-架构"><a href="#讲一下-Hbase-架构" class="headerlink" title="讲一下 Hbase 架构"></a>讲一下 Hbase 架构</h2><p><img src="//jiamaoxiang.top/2019/09/24/docs/讲一下Hbase架构/pictures/hbase%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt></p><p><strong>Hbase主要包含HMaster/HRegionServer/Zookeeper</strong></p><ul><li><p><strong>HRegionServer 负责实际数据的读写. 当访问数据时, 客户端直接与RegionServer通信.</strong></p><p>HBase的表根据Row Key的区域分成多个Region, 一个Region包含这这个区域内所有数据. 而Region server负责管理多个Region, 负责在这个Region server上的所有region的读写操作. </p></li><li><p><strong>HMaster 负责管理Region的位置, DDL(新增和删除表结构)</strong></p><ul><li>协调RegionServer</li><li>在集群处于数据恢复或者动态调整负载时,分配Region到某一个RegionServer中</li><li>管控集群,监控所有Region Server的状态</li><li>提供DDL相关的API, 新建(create),删除(delete)和更新(update)表结构.</li></ul></li><li><p><strong>Zookeeper 负责维护和记录整个Hbase集群的状态</strong></p><p>zookeeper探测和记录Hbase集群中服务器的状态信息.如果zookeeper发现服务器宕机,它会通知Hbase的master节点.</p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bflink%E7%9A%84%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84/"/>
      <url>/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bflink%E7%9A%84%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h2 id="讲一下flink的运行架构"><a href="#讲一下flink的运行架构" class="headerlink" title="讲一下flink的运行架构"></a>讲一下flink的运行架构</h2><p><img src="//jiamaoxiang.top/2019/09/24/docs/讲一下flink的运行架构/pictures/flink%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt></p><p>当 Flink 集群启动后，首先会启动一个 JobManger 和一个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程。</p><ul><li><strong>Client</strong> 为提交 Job 的客户端，可以是运行在任何机器上（与 JobManager 环境连通即可）。提交 Job 后，Client 可以结束进程（Streaming的任务），也可以不结束并等待结果返回。</li><li><strong>JobManager</strong> 主要负责调度 Job 并协调 Task 做 checkpoint，职责上很像 Storm 的 Nimbus。从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。</li><li><strong>TaskManager</strong> 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自己的上游建立 Netty 连接，接收数据并处理。</li></ul><p><a href="http://wuchong.me/blog/2016/05/03/flink-internals-overview/" target="_blank" rel="noopener">参考文章1</a></p><p><a href="http://shiyanjun.cn/archives/1508.html" target="_blank" rel="noopener">参考文章2</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bflink%E7%9A%84%E4%BD%9C%E4%B8%9A%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/"/>
      <url>/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bflink%E7%9A%84%E4%BD%9C%E4%B8%9A%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="讲一下flink的作业执行流程"><a href="#讲一下flink的作业执行流程" class="headerlink" title="讲一下flink的作业执行流程"></a>讲一下flink的作业执行流程</h2><p> <img src="//jiamaoxiang.top/2019/09/24/docs/讲一下flink的作业执行流程/pictures/flinkRuntime.png" alt></p><p><strong>以yarn模式Per-job方式为例概述作业提交执行流程</strong></p><ol><li><p>当执行executor() 之后,会首先在本地client 中将代码转化为可以提交的 JobGraph</p><p>如果提交为Per-Job模式,则首先需要启动AM, client会首先向资源系统申请资源, 在yarn下即为申请container开启AM, 如果是Session模式的话则不需要这个步骤</p></li><li><p>Yarn分配资源, 开启AM</p></li><li><p>Client将Job提交给Dispatcher</p></li><li><p>Dispatcher 会开启一个新的 JobManager线程</p></li><li><p>JM 向Flink 自己的 Resourcemanager申请slot资源来执行任务</p></li><li><p>RM 向 Yarn申请资源来启动 TaskManger (Session模式跳过此步)</p></li><li><p>Yarn 分配 Container 来启动 taskManger (Session模式跳过此步)</p></li><li><p>Flink 的 RM 向 TM 申请 slot资源来启动 task</p></li><li><p>TM 将待分配的 slot 提供给 JM</p></li><li><p>JM 提交 task, TM 会启动新的线程来执行任务,开始启动后就可以通过 shuffle模块进行 task之间的数据交换</p></li></ol><p><a href="https://www.bilibili.com/video/av52394455?t=343" target="_blank" rel="noopener">参考视频</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bflinkonyarn%E7%9A%84%E9%83%A8%E7%BD%B2/"/>
      <url>/2019/09/24/docs/%E8%AE%B2%E4%B8%80%E4%B8%8Bflinkonyarn%E7%9A%84%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h2 id="讲一下flink-on-yarn的部署"><a href="#讲一下flink-on-yarn的部署" class="headerlink" title="讲一下flink on yarn的部署"></a>讲一下flink on yarn的部署</h2><p>Flink作业提交有两种类型:</p><ul><li><h4 id="yarn-session"><a href="#yarn-session" class="headerlink" title="yarn session"></a>yarn session</h4><p>需要先启动集群，然后在提交作业，接着会向yarn申请一块空间后，资源永远保持不变。如果资源满了，下一个作业就无法提交，只能等到yarn中的其中一个作业执行完成后，释放了资源，那下一个作业才会正常提交.</p><ul><li><p>客户端模式</p><p>对于客户端模式而言，你可以启动多个yarn session，一个yarn session模式对应一个JobManager,并按照需求提交作业，同一个Session中可以提交多个Flink作业。如果想要停止Flink Yarn Application，需要通过yarn application -kill命令来停止.</p></li><li><p>分离式模式</p><p>对于分离式模式，并不像客户端那样可以启动多个yarn session，如果启动多个，会出现下面的session一直处在等待状态。JobManager的个数只能是一个，同一个Session中可以提交多个Flink作业。如果想要停止Flink Yarn Application，需要通过yarn application -kill命令来停止</p></li></ul></li><li><h4 id="Flink-run-Per-Job"><a href="#Flink-run-Per-Job" class="headerlink" title="Flink run(Per-Job)"></a>Flink run(Per-Job)</h4><p>直接在YARN上提交运行Flink作业(Run a Flink job on YARN)，这种方式的好处是一个任务会对应一个job,即没提交一个作业会根据自身的情况，向yarn申请资源，直到作业执行完成，并不会影响下一个作业的正常运行，除非是yarn上面没有任何资源的情况下</p></li></ul><table><thead><tr><th>Session</th><th></th></tr></thead><tbody><tr><td>共享Dispatcher和Resource Manager</td><td>Dispatcher和Resource Manager</td></tr><tr><td>共享资源(即 TaskExecutor)</td><td>按需要申请资源 (即 TaskExecutor)</td></tr><tr><td>适合规模小,执行时间短的作业</td><td></td></tr></tbody></table><p><img src="//jiamaoxiang.top/2019/09/24/docs/讲一下flinkonyarn的部署/pictures/flinkOnYarn.png" alt></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E6%8C%89%E7%85%A7%E5%AD%A6%E7%94%9F%E7%A7%91%E7%9B%AE%E5%8F%96%E6%AF%8F%E4%B8%AA%E7%A7%91%E7%9B%AE%E7%9A%84TopN/"/>
      <url>/2019/09/24/docs/%E6%8C%89%E7%85%A7%E5%AD%A6%E7%94%9F%E7%A7%91%E7%9B%AE%E5%8F%96%E6%AF%8F%E4%B8%AA%E7%A7%91%E7%9B%AE%E7%9A%84TopN/</url>
      
        <content type="html"><![CDATA[<h2 id="Hive-SQL-按照学生科目取每个科目的TopN"><a href="#Hive-SQL-按照学生科目取每个科目的TopN" class="headerlink" title="Hive SQL : 按照学生科目取每个科目的TopN"></a>Hive SQL : 按照学生科目取每个科目的TopN</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">id,name,subject,score</span><br><span class="line">1,小明,语文,87</span><br><span class="line">2,张三,语文,27</span><br><span class="line">3,王五,语文,69</span><br><span class="line">4,李四,语文,99</span><br><span class="line">5,小明,数学,86</span><br><span class="line">6,马六,数学,33</span><br><span class="line">7,李四,数学,44</span><br><span class="line">8,小红,数学,50</span><br></pre></td></tr></table></figure><p><strong>按照各个科目的成绩排名 取 Top3</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select a.* from</span><br><span class="line">(select id,name,subject,score,row_number() over(partition by subject order by score desc) rank from student) a</span><br><span class="line">where a.rank &lt;= 3</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/WYpersist/article/details/80318305" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E6%80%8E%E6%A0%B7%E5%8E%BB%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/"/>
      <url>/2019/09/24/docs/%E6%80%8E%E6%A0%B7%E5%8E%BB%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是数据倾斜-怎样去处理数据倾斜"><a href="#什么是数据倾斜-怎样去处理数据倾斜" class="headerlink" title="什么是数据倾斜,怎样去处理数据倾斜"></a>什么是数据倾斜,怎样去处理数据倾斜</h2><p>数据倾斜是一种很常见的问题（依据二八定律），简单来说，比方WordCount中某个Key对应的数据量非常大的话，就会产生数据倾斜，导致两个后果：</p><ul><li>OOM（单或少数的节点）；</li><li>拖慢整个Job执行时间（其他已经完成的节点都在等这个还在做的节点）</li></ul><h4 id="数据倾斜主要分为两类-聚合倾斜-和-join倾斜"><a href="#数据倾斜主要分为两类-聚合倾斜-和-join倾斜" class="headerlink" title="数据倾斜主要分为两类: 聚合倾斜 和 join倾斜"></a>数据倾斜主要分为两类: 聚合倾斜 和 join倾斜</h4><ul><li><p><strong>聚合倾斜</strong></p><ul><li><p><strong>双重聚合（局部聚合+全局聚合）</strong></p><p><strong>场景</strong>: 对RDD进行reduceByKey等聚合类shuffle算子，SparkSQL的groupBy做分组聚合这两种情况<br> 思路：首先通过map给每个key打上n以内的随机数的前缀并进行局部聚合，即(hello, 1) (hello, 1) (hello, 1) (hello, 1)变为(1_hello, 1) (1_hello, 1) (2_hello, 1)，并进行reduceByKey的局部聚合，然后再次map将key的前缀随机数去掉再次进行全局聚合；<br> <strong>原理</strong>: 对原本相同的key进行随机数附加，变成不同key，让原本一个task处理的数据分摊到多个task做局部聚合，规避单task数据过量。之后再去随机前缀进行全局聚合；<br> 优点：效果非常好（对聚合类Shuffle操作的倾斜问题）；<br> 缺点：范围窄（仅适用于聚合类的Shuffle操作，join类的Shuffle还需其它方案）</p></li></ul></li><li><p><strong>join倾斜</strong></p><ul><li><p><strong>将reduce join转为map join</strong></p><p><strong>场景</strong>: 对RDD或Spark SQL使用join类操作或语句，且join操作的RDD或表比较小（百兆或1,2G）； 思路：使用broadcast和map类算子实现join的功能替代原本的join，彻底规避shuffle。对较小RDD直接collect到内存，并创建broadcast变量；并对另外一个RDD执行map类算子，在该算子的函数中，从broadcast变量（collect出的较小RDD）与当前RDD中的每条数据依次比对key，相同的key执行你需要方式的join；</p><p><strong>原理</strong>: 若RDD较小，可采用广播小的RDD，并对大的RDD进行map，来实现与join同样的效果。简而言之，用broadcast-map代替join，规避join带来的shuffle（无Shuffle无倾斜）； 优点：效果很好（对join操作导致的倾斜），根治； </p><p><strong>缺点</strong>：适用场景小（大表+小表），广播（driver和executor节点都会驻留小表数据）小表也耗内存</p></li><li><p><strong>采样倾斜key并分拆join操作</strong></p><p><strong>场景</strong>: 两个较大的（无法采用方案五）RDD/Hive表进行join时，且一个RDD/Hive表中少数key数据量过大，另一个RDD/Hive表的key分布较均匀（RDD中两者之一有一个更倾斜）；<br><strong>思路</strong>:</p><ol><li>对更倾斜rdd1进行采样（RDD.sample）并统计出数据量最大的几个key；</li><li>对这几个倾斜的key从原本rdd1中拆出形成一个单独的rdd1_1，并打上0~n的随机数前缀，被拆分的原rdd1的另一部分（不包含倾斜key）又形成一个新rdd1_2；</li><li>对rdd2过滤出rdd1倾斜的key，得到rdd2_1，并将其中每条数据扩n倍，对每条数据按顺序附加0~n的前缀，被拆分出key的rdd2也独立形成另一个rdd2_2； 【个人认为，这里扩了n倍，最后union完还需要将每个倾斜key对应的value减去(n-1)】</li><li>将加了随机前缀的rdd1_1和rdd2_1进行join（此时原本倾斜的key被打散n份并被分散到更多的task中进行join）； 【个人认为，这里应该做两次join，两次join中间有一个map去前缀】</li><li>另外两个普通的RDD（rdd1_2、rdd2_2）照常join；</li><li>最后将两次join的结果用union结合得到最终的join结果。 原理：对join导致的倾斜是因为某几个key，可将原本RDD中的倾斜key拆分出原RDD得到新RDD，并以加随机前缀的方式打散n份做join，将倾斜key对应的大量数据分摊到更多task上来规避倾斜；</li></ol><p><strong>优点</strong>: 前提是join导致的倾斜（某几个key倾斜），避免占用过多内存（只需对少数倾斜key扩容n倍）；<br><strong>缺点</strong>: 对过多倾斜key不适用。</p></li><li><p><strong>用随机前缀和扩容RDD进行join</strong></p><p><strong>场景</strong>: RDD中有大量key导致倾斜； 思路：与方案六类似。</p><ol><li>查看RDD/Hive表中数据分布并找到造成倾斜的RDD/表；</li><li>对倾斜RDD中的每条数据打上n以内的随机数前缀；</li><li>对另外一个正常RDD的每条数据扩容n倍，扩容出的每条数据依次打上0到n的前缀；</li><li>对处理后的两个RDD进行join。</li></ol><p><strong>原理</strong>: 与方案六只有唯一不同在于这里对不倾斜RDD中所有数据进行扩大n倍，而不是找出倾斜key进行扩容；<br><strong>优点</strong>: 对join类的数据倾斜都可处理，效果非常显著；<br><strong>缺点</strong>: 缓解，扩容需要大内存</p></li></ul></li></ul><p><a href="https://juejin.im/post/5ccd5cc7f265da03474e1249#heading-9" target="_blank" rel="noopener">参考文章1</a></p><p><a href="https://blog.csdn.net/qq_35394891/article/details/82260907" target="_blank" rel="noopener">参考文章2</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E5%B0%8F%E6%96%87%E4%BB%B6%E8%BF%87%E5%A4%9A%E4%BC%9A%E6%9C%89%E4%BB%80%E4%B9%88%E5%8D%B1%E5%AE%B3/"/>
      <url>/2019/09/24/docs/%E5%B0%8F%E6%96%87%E4%BB%B6%E8%BF%87%E5%A4%9A%E4%BC%9A%E6%9C%89%E4%BB%80%E4%B9%88%E5%8D%B1%E5%AE%B3/</url>
      
        <content type="html"><![CDATA[<h2 id="小文件过多会有什么危害-如何避免"><a href="#小文件过多会有什么危害-如何避免" class="headerlink" title="小文件过多会有什么危害,如何避免?"></a>小文件过多会有什么危害,如何避免?</h2><p>Hadoop上大量HDFS元数据信息存储在NameNode内存中,因此过多的小文件必定会压垮NameNode的内存.</p><p>每个元数据对象约占150byte，所以如果有1千万个小文件，每个文件占用一个block，则NameNode大约需要2G空间。如果存储1亿个文件，则NameNode需要20G空间.</p><p>显而易见的解决这个问题的方法就是合并小文件,可以选择在客户端上传时执行一定的策略先合并,或者是使用Hadoop的CombineFileInputFormat&lt;K,V&gt;实现小文件的合并</p><p><a href="https://blog.csdn.net/luofazha2012/article/details/80904791" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87offset%E5%AF%BB%E6%89%BE%E6%95%B0%E6%8D%AE/"/>
      <url>/2019/09/24/docs/%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87offset%E5%AF%BB%E6%89%BE%E6%95%B0%E6%8D%AE/</url>
      
        <content type="html"><![CDATA[<h2 id="如何通过offset寻找数据"><a href="#如何通过offset寻找数据" class="headerlink" title="如何通过offset寻找数据"></a>如何通过offset寻找数据</h2><p>如果consumer要找offset是1008的消息，那么，</p><p>1，按照二分法找到小于1008的segment，也就是00000000000000001000.log和00000000000000001000.index</p><p>2，用目标offset减去文件名中的offset得到消息在这个segment中的偏移量。也就是1008-1000=8，偏移量是8。</p><p>3，再次用二分法在index文件中找到对应的索引，也就是第三行6,45。</p><p>4，到log文件中，从偏移量45的位置开始（实际上这里的消息offset是1006），顺序查找，直到找到offset为1008的消息。查找期间kafka是按照log的存储格式来判断一条消息是否结束的。</p><p><a href="https://blog.csdn.net/lkforce/article/details/77854813" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E5%A6%82%E4%BD%95%E6%B8%85%E7%90%86%E8%BF%87%E6%9C%9F%E6%95%B0%E6%8D%AE/"/>
      <url>/2019/09/24/docs/%E5%A6%82%E4%BD%95%E6%B8%85%E7%90%86%E8%BF%87%E6%9C%9F%E6%95%B0%E6%8D%AE/</url>
      
        <content type="html"><![CDATA[<h2 id="如何清理过期数据"><a href="#如何清理过期数据" class="headerlink" title="如何清理过期数据"></a>如何清理过期数据</h2><ul><li><h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h4><p>log.cleanup.policy=delete启用删除策略</p><ul><li>直接删除，删除后的消息不可恢复。可配置以下两个策略：<br>清理超过指定时间清理：<br>log.retention.hours=16</li><li>超过指定大小后，删除旧的消息：<br>log.retention.bytes=1073741824<br>为了避免在删除时阻塞读操作，采用了copy-on-write形式的实现，删除操作进行时，读取操作的二分查找功能实际是在一个静态的快照副本上进行的，这类似于Java的CopyOnWriteArrayList。</li></ul></li><li><h4 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h4><p>将数据压缩，只保留每个key最后一个版本的数据。<br>首先在broker的配置中设置log.cleaner.enable=true启用cleaner，这个默认是关闭的。<br>在topic的配置中设置log.cleanup.policy=compact启用压缩策略。</p><p><img src="//jiamaoxiang.top/2019/09/24/docs/如何清理过期数据/pictures/kafka%E5%8E%8B%E7%BC%A9.png" alt></p><p>如上图，在整个数据流中，每个Key都有可能出现多次，压缩时将根据Key将消息聚合，只保留最后一次出现时的数据。这样，无论什么时候消费消息，都能拿到每个Key的最新版本的数据。<br>压缩后的offset可能是不连续的，比如上图中没有5和7，因为这些offset的消息被merge了，当从这些offset消费消息时，将会拿到比这个offset大的offset对应的消息，比如，当试图获取offset为5的消息时，实际上会拿到offset为6的消息，并从这个位置开始消费。<br>这种策略只适合特俗场景，比如消息的key是用户ID，消息体是用户的资料，通过这种压缩策略，整个消息集里就保存了所有用户最新的资料。<br>压缩策略支持删除，当某个Key的最新版本的消息没有内容时，这个Key将被删除，这也符合以上逻辑。</p></li></ul><p><a href="https://blog.csdn.net/honglei915/article/details/49683065" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81%E5%9C%88%E8%BF%98%E6%9C%89%E5%93%AA%E4%BA%9B%E9%80%89%E4%B8%BE%E5%8D%8F%E8%AE%AE/"/>
      <url>/2019/09/24/docs/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81%E5%9C%88%E8%BF%98%E6%9C%89%E5%93%AA%E4%BA%9B%E9%80%89%E4%B8%BE%E5%8D%8F%E8%AE%AE/</url>
      
        <content type="html"><![CDATA[<h2 id="zk-采用的哪种分布式一致性协议-还有哪些分布式一致性协议"><a href="#zk-采用的哪种分布式一致性协议-还有哪些分布式一致性协议" class="headerlink" title="zk 采用的哪种分布式一致性协议? 还有哪些分布式一致性协议"></a>zk 采用的哪种分布式一致性协议? 还有哪些分布式一致性协议</h2><p>常见的分布式一致性协议有: 两阶段提交协议，三阶段提交协议，向量时钟，RWN协议，paxos协议，Raft协议. zk采用的是paxos协议.</p><ul><li><h4 id="两阶段提交协议-2PC"><a href="#两阶段提交协议-2PC" class="headerlink" title="两阶段提交协议(2PC)"></a>两阶段提交协议(2PC)</h4><p>两阶段提交协议，简称2PC，是比较常用的解决分布式事务问题的方式，要么所有参与进程都提交事务，要么都取消事务，即实现ACID中的原子性(A)的常用手段。</p></li><li><h4 id="三阶段提交协议-3PC"><a href="#三阶段提交协议-3PC" class="headerlink" title="三阶段提交协议(3PC)"></a>三阶段提交协议(3PC)</h4><p>3PC就是在2PC基础上将2PC的提交阶段细分位两个阶段：预提交阶段和提交阶段</p></li><li><h4 id="向量时钟"><a href="#向量时钟" class="headerlink" title="向量时钟"></a>向量时钟</h4><p>通过向量空间祖先继承的关系比较, 使数据保持最终一致性,这就是向量时钟的基本定义。</p></li><li><h4 id="NWR协议"><a href="#NWR协议" class="headerlink" title="NWR协议"></a>NWR协议</h4><p>NWR是一种在分布式存储系统中用于控制一致性级别的一种策略。在Amazon的Dynamo云存储系统中，就应用NWR来控制一致性。<br>让我们先来看看这三个字母的含义：<br>N：在分布式存储系统中，有多少份备份数据<br>W：代表一次成功的更新操作要求至少有w份数据写入成功<br>R： 代表一次成功的读数据操作要求至少有R份数据成功读取<br>NWR值的不同组合会产生不同的一致性效果，当W+R&gt;N的时候，整个系统对于客户端来讲能保证强一致性。当W+R 以常见的N=3、W=2、R=2为例：<br>N=3，表示，任何一个对象都必须有三个副本（Replica），W=2表示，对数据的修改操作（Write）只需要在3个Replica中的2个上面完成就返回，R=2表示，从三个对象中要读取到2个数据对象，才能返回。<br>在分布式系统中，数据的单点是不允许存在的。即线上正常存在的Replica数量是1的情况是非常危险的，因为一旦这个Replica再次错误，就 可能发生数据的永久性错误。假如我们把N设置成为2，那么，只要有一个存储节点发生损坏，就会有单点的存在。所以N必须大于2。N约高，系统的维护和整体 成本就越高。工业界通常把N设置为3。<br>当W是2、R是2的时候，W+R&gt;N，这种情况对于客户端就是强一致性的。</p></li><li><h4 id="paxos协议"><a href="#paxos协议" class="headerlink" title="paxos协议"></a>paxos协议</h4><p><a href="http://chuansong.me/n/2189245" target="_blank" rel="noopener">架构师需要了解的Paxos原理，历程及实践</a></p></li><li><h4 id="Raft协议"><a href="#Raft协议" class="headerlink" title="Raft协议"></a>Raft协议</h4><p><a href="http://thesecretlivesofdata.com/raft/" target="_blank" rel="noopener">Raft协议的动画</a></p></li></ul><p><a href="https://blog.csdn.net/chdhust/article/details/52651741" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E5%90%AF%E5%8A%A8hadoop%E9%9B%86%E7%BE%A4%E4%BC%9A%E5%88%86%E5%88%AB%E5%90%AF%E5%8A%A8%E5%93%AA%E4%BA%9B%E8%BF%9B%E7%A8%8B/"/>
      <url>/2019/09/24/docs/%E5%90%AF%E5%8A%A8hadoop%E9%9B%86%E7%BE%A4%E4%BC%9A%E5%88%86%E5%88%AB%E5%90%AF%E5%8A%A8%E5%93%AA%E4%BA%9B%E8%BF%9B%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="启动hadoop集群会分别启动哪些进程-各自的作用"><a href="#启动hadoop集群会分别启动哪些进程-各自的作用" class="headerlink" title="启动hadoop集群会分别启动哪些进程,各自的作用"></a>启动hadoop集群会分别启动哪些进程,各自的作用</h2><ul><li><p><strong>NameNode：</strong></p><ul><li>维护文件系统树及整棵树内所有的文件和目录。这些信息永久保存在本地磁盘的两个文件中：命名空间镜像文件、编辑日志文件</li><li>记录每个文件中各个块所在的数据节点信息，这些信息在内存中保存，每次启动系统时重建这些信息</li><li>负责响应客户端的   数据块位置请求  。也就是客户端想存数据，应该往哪些节点的哪些块存；客户端想取数据，应该到哪些节点取</li><li>接受记录在数据存取过程中，datanode节点报告过来的故障、损坏信息</li></ul></li><li><p><strong>SecondaryNameNode(非HA模式)：</strong></p><ul><li>实现namenode容错的一种机制。定期合并编辑日志与命名空间镜像，当namenode挂掉时，可通过一定步骤进行上顶。(<strong>注意 并不是NameNode的备用节点</strong>)</li></ul></li><li><p><strong>DataNode：</strong></p><ul><li>根据需要存取并检索数据块</li><li>定期向namenode发送其存储的数据块列表</li></ul></li><li><p><strong>ResourceManager：</strong></p><ul><li>负责Job的调度,将一个任务与一个NodeManager相匹配。也就是将一个MapReduce之类的任务分配给一个从节点的NodeManager来执行。</li></ul></li><li><p><strong>NodeManager：</strong></p><ul><li>运行ResourceManager分配的任务，同时将任务进度向application master报告</li></ul></li><li><p><strong>JournalNode(HA下启用):</strong></p><ul><li>高可用情况下存放namenode的editlog文件</li></ul></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E5%88%86%E6%9E%90%E4%B8%80%E4%B8%8B%E4%B8%80%E6%AE%B5spark%E4%BB%A3%E7%A0%81%E4%B8%AD%E5%93%AA%E4%BA%9B%E9%83%A8%E5%88%86%E5%9C%A8Driver%E7%AB%AF%E6%89%A7%E8%A1%8C/"/>
      <url>/2019/09/24/docs/%E5%88%86%E6%9E%90%E4%B8%80%E4%B8%8B%E4%B8%80%E6%AE%B5spark%E4%BB%A3%E7%A0%81%E4%B8%AD%E5%93%AA%E4%BA%9B%E9%83%A8%E5%88%86%E5%9C%A8Driver%E7%AB%AF%E6%89%A7%E8%A1%8C/</url>
      
        <content type="html"><![CDATA[<h2 id="分析一下一段spark代码中哪些部分在Driver端执行-哪些部分在Worker端执行"><a href="#分析一下一段spark代码中哪些部分在Driver端执行-哪些部分在Worker端执行" class="headerlink" title="分析一下一段spark代码中哪些部分在Driver端执行,哪些部分在Worker端执行"></a>分析一下一段spark代码中哪些部分在Driver端执行,哪些部分在Worker端执行</h2><p>Driver Program是用户编写的提交给Spark集群执行的application，它包含两部分</p><ul><li><strong>作为驱动</strong>： Driver与Master、Worker协作完成application进程的启动、DAG划分、计算任务封装、计算任务分发到各个计算节点(Worker)、计算资源的分配等。</li><li><strong>计算逻辑本身</strong>，当计算任务在Worker执行时，执行计算逻辑完成application的计算任务</li></ul><p>一般来说transformation算子均是在worker上执行的,其他类型的代码在driver端执行</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/%E4%B8%80%E4%B8%AAspark%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/"/>
      <url>/2019/09/24/docs/%E4%B8%80%E4%B8%AAspark%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="一个spark程序的执行流程"><a href="#一个spark程序的执行流程" class="headerlink" title="一个spark程序的执行流程"></a>一个spark程序的执行流程</h2><p><img src="//jiamaoxiang.top/2019/09/24/docs/一个spark程序的执行流程/pictures/spark%E7%A8%8B%E5%BA%8F%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B.jpg" alt></p><ul><li><strong>A -&gt;</strong> 当 Driver 进程被启动之后,首先它将发送请求到Master节点上,进行Spark应用程序的注册</li><li><strong>B -&gt;</strong> Master在接受到Spark应用程序的注册申请之后,会发送给Worker,让其进行资源的调度和分配.</li><li><strong>C -&gt;</strong> Worker 在接受Master的请求之后,会为Spark应用程序启动Executor, 来分配资源</li><li><strong>D -&gt;</strong> Executor启动分配资源好后,就会想Driver进行反注册,这是Driver已经知道哪些Executor为他服务了</li><li><strong>E -&gt;</strong> 当Driver得到注册了Executor之后,就可以开始正式执行spark应用程序了. 首先第一步,就是创建初始RDD,读取数据源,再执行之后的一系列算子. HDFS文件内容被读取到多个worker节点上,形成内存中的分布式数据集,也就是初始RDD</li><li><strong>F -&gt;</strong> Driver就会根据 Job 任务任务中的算子形成对应的task,最后提交给 Executor, 来分配给task进行计算的线程</li><li><strong>G -&gt;</strong> task就会去调用对应的任务数据来计算,并task会对调用过来的RDD的partition数据执行指定的算子操作,形成新的RDD的partition,这时一个大的循环就结束了</li><li>后续的RDD的partition数据又通过Driver形成新的一批task提交给Executor执行,循环这个操作,直到所有的算子结束</li></ul><p><a href="https://zhuanlan.zhihu.com/p/35713084" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/zookeeper%E6%98%AF%E4%BB%80%E4%B9%88/"/>
      <url>/2019/09/24/docs/zookeeper%E6%98%AF%E4%BB%80%E4%B9%88/</url>
      
        <content type="html"><![CDATA[<h2 id="zookeeper是什么-有什么功能"><a href="#zookeeper是什么-有什么功能" class="headerlink" title="zookeeper是什么,有什么功能"></a>zookeeper是什么,有什么功能</h2><p>Zookeeper 是 一个典型的分布式数据一致性的解决方案.</p><p><strong>Zookeeper的典型应用场景</strong>:</p><ul><li>数据发布/订阅</li><li>负载均衡</li><li>命名服务</li><li>分布式协调/通知</li><li>集群管理</li><li>Master</li><li>分布式锁</li><li>分布式队列</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/zk%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F/"/>
      <url>/2019/09/24/docs/zk%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h2 id="zk的分布式锁实现方式"><a href="#zk的分布式锁实现方式" class="headerlink" title="zk的分布式锁实现方式"></a>zk的分布式锁实现方式</h2><p>使用zookeeper实现分布式锁的算法流程，假设锁空间的根节点为/lock：</p><ol><li>客户端连接zookeeper，并在/lock下创建<strong>临时的</strong>且<strong>有序的</strong>子节点，第一个客户端对应的子节点为/lock/lock-0000000000，第二个为/lock/lock-0000000001，以此类推。</li><li>客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中<strong>序号最小</strong>的子节点，如果是则认为获得锁，否则<strong>监听刚好在自己之前一位的子节点删除消息</strong>，获得子节点变更通知后重复此步骤直至获得锁；</li><li>执行业务代码；</li><li>完成业务流程后，删除对应的子节点释放锁。</li></ol><p><a href="http://www.dengshenyu.com/java/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2017/10/23/zookeeper-distributed-lock.html" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/zk%E6%9C%89%E5%87%A0%E7%A7%8D%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/"/>
      <url>/2019/09/24/docs/zk%E6%9C%89%E5%87%A0%E7%A7%8D%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h2 id="zk-有几种部署模式"><a href="#zk-有几种部署模式" class="headerlink" title="zk 有几种部署模式"></a>zk 有几种部署模式</h2><p>zookeeper有两种运行模式: 集群模式和单机模式,还有一种伪集群模式,在单机模式下模拟集群的zookeeper服务</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/zk%E6%98%AF%E6%80%8E%E6%A0%B7%E4%BF%9D%E8%AF%81%E4%B8%BB%E4%BB%8E%E8%8A%82%E7%82%B9%E7%9A%84%E7%8A%B6%E6%80%81%E5%90%8C%E6%AD%A5/"/>
      <url>/2019/09/24/docs/zk%E6%98%AF%E6%80%8E%E6%A0%B7%E4%BF%9D%E8%AF%81%E4%B8%BB%E4%BB%8E%E8%8A%82%E7%82%B9%E7%9A%84%E7%8A%B6%E6%80%81%E5%90%8C%E6%AD%A5/</url>
      
        <content type="html"><![CDATA[<h2 id="zk是怎样保证主从节点的状态同步"><a href="#zk是怎样保证主从节点的状态同步" class="headerlink" title="zk是怎样保证主从节点的状态同步"></a>zk是怎样保证主从节点的状态同步</h2><p>zookeeper 的核心是原子广播，这个机制保证了各个 server 之间的同步。实现这个机制的协议叫做 zab 协议。 zab 协议有两种模式，分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，zab 就进入了恢复模式，当领导者被选举出来，且大多数 server 完成了和 leader 的状态同步以后，恢复模式就结束了。状态同步保证了 leader 和 server 具有相同的系统状态。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/Yarn%E8%B0%83%E5%BA%A6MapReduce/"/>
      <url>/2019/09/24/docs/Yarn%E8%B0%83%E5%BA%A6MapReduce/</url>
      
        <content type="html"><![CDATA[<h2 id="Yarn-调度MapReduce过程"><a href="#Yarn-调度MapReduce过程" class="headerlink" title="Yarn 调度MapReduce过程"></a>Yarn 调度MapReduce过程</h2><p><img src="//jiamaoxiang.top/2019/09/24/docs/Yarn调度MapReduce/picturees/yarn%E8%B0%83%E5%BA%A6mr%E8%BF%87%E7%A8%8B.jpg" alt></p><ol><li>Mr程序提交到客户端所在的节点（MapReduce）</li><li>yarnrunner向Resourcemanager申请一个application。</li><li>rm将该应用程序的资源路径返回给yarnrunner</li><li>该程序将运行所需资源提交到HDFS上</li><li>程序资源提交完毕后，申请运行mrAppMaster</li><li>RM将用户的请求初始化成一个task</li><li>其中一个NodeManager领取到task任务。</li><li>该NodeManager创建容器Container，并产生MRAppmaster</li><li>Container从HDFS上拷贝资源到本地</li><li>MRAppmaster向RM申请运行maptask容器</li><li>RM将运行maptask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器.</li><li>MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动maptask，maptask对数据分区排序。</li><li>MRAppmaster向RM申请2个容器，运行reduce task。</li><li>reduce task向maptask获取相应分区的数据。</li><li>程序运行完毕后，MR会向RM注销自己。</li></ol><p><a href="https://blog.csdn.net/qq_26442553/article/details/78699759" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/Yarn%E6%9E%B6%E6%9E%84/"/>
      <url>/2019/09/24/docs/Yarn%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h2 id="Yarn架构"><a href="#Yarn架构" class="headerlink" title="Yarn架构"></a>Yarn架构</h2><p><img src="//jiamaoxiang.top/2019/09/24/docs/Yarn架构/pictures/yarn.gif" alt></p><h3 id="1-ResourceManager（RM）"><a href="#1-ResourceManager（RM）" class="headerlink" title="1. ResourceManager（RM）"></a>1. ResourceManager（RM）</h3><p>RM 是一个全局的资源管理器，负责整个系统的资源管理和分配，它主要有两个组件构成：</p><ol><li>调度器：Scheduler；</li><li>应用程序管理器：Applications Manager，ASM。</li></ol><h4 id="调度器"><a href="#调度器" class="headerlink" title="调度器"></a>调度器</h4><p>调度器根据容量、队列等限制条件（如某个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。要注意的是，该调度器是一个纯调度器，它不再从事任何与应用程序有关的工作，比如不负责重新启动（因应用程序失败或者硬件故障导致的失败），这些均交由应用程序相关的 ApplicationMaster 完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念 <strong>资源容器(Resource Container，也即 Container)</strong>，Container 是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需求设计新的调度器，YARN 提供了多种直接可用的调度器，比如 Fair Scheduler 和 Capacity Schedule 等。</p><h4 id="应用程序管理器"><a href="#应用程序管理器" class="headerlink" title="应用程序管理器"></a>应用程序管理器</h4><p>应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以 AM、监控 AM 运行状态并在失败时重新启动它等。</p><h3 id="2-NodeManager（NM）"><a href="#2-NodeManager（NM）" class="headerlink" title="2. NodeManager（NM）"></a>2. NodeManager（NM）</h3><p>NM 是每个节点上运行的资源和任务管理器，一方面，它会定时向 RM 汇报本节点上的资源使用情况和各个 Container 的运行状态；另一方面，它接收并处理来自 AM 的 Container 启动/停止等各种请求。</p><h3 id="3-ApplicationMaster（AM）"><a href="#3-ApplicationMaster（AM）" class="headerlink" title="3. ApplicationMaster（AM）"></a>3. ApplicationMaster（AM）</h3><p>提交的每个作业都会包含一个 AM，主要功能包括：</p><ol><li>与 RM 协商以获取资源（用 container 表示）；</li><li>将得到的任务进一步分配给内部的任务；</li><li>与 NM 通信以启动/停止任务；</li><li>监控所有任务的运行状态，当任务有失败时，重新为任务申请资源并重启任务。</li></ol><p>MapReduce 就是原生支持 ON YARN 的一种框架，可以在 YARN 上运行 MapReduce 作业。有很多分布式应用都开发了对应的应用程序框架，用于在 YARN 上运行任务，例如 Spark，Storm、Flink 等。</p><h3 id="4-Container"><a href="#4-Container" class="headerlink" title="4. Container"></a>4. Container</h3><p>Container 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。 YARN 会为每个任务分配一个 Container 且该任务只能使用该 Container 中描述的资源。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/spark%E7%9A%84%E5%90%84%E7%A7%8DHA/"/>
      <url>/2019/09/24/docs/spark%E7%9A%84%E5%90%84%E7%A7%8DHA/</url>
      
        <content type="html"><![CDATA[<h2 id="spark的各种HA-master-worker-executor的ha"><a href="#spark的各种HA-master-worker-executor的ha" class="headerlink" title="spark的各种HA,  master/worker/executor的ha"></a>spark的各种HA,  master/worker/executor的ha</h2><ul><li><h4 id="Master异常"><a href="#Master异常" class="headerlink" title="Master异常"></a>Master异常</h4><p>spark可以在集群运行时启动一个或多个standby Master,当 Master 出现异常时,会根据规则启动某个standby master接管,在standlone模式下有如下几种配置</p><ul><li><p>ZOOKEEPER</p><p>集群数据持久化到zk中,当master出现异常时,zk通过选举机制选出新的master,新的master接管是需要从zk获取持久化信息</p></li><li><p>FILESYSTEM</p><p>集群元数据信息持久化到本地文件系统, 当master出现异常时,只需要在该机器上重新启动master,启动后新的master获取持久化信息并根据这些信息恢复集群状态</p></li><li><p>CUSTOM</p><p>自定义恢复方式,对 standloneRecoveryModeFactory 抽象类 进行实现并把该类配置到系统中,当master出现异常时,会根据用户自定义行为恢复集群</p></li><li><p>None</p><p>不持久化集群的元数据, 当 master出现异常时, 新启动的Master 不进行恢复集群状态,而是直接接管集群</p></li></ul></li><li><h4 id="Worker异常"><a href="#Worker异常" class="headerlink" title="Worker异常"></a>Worker异常</h4><p>Worker 以定时发送心跳给 Master, 让 Master 知道 Worker 的实时状态,当worker出现超时时,Master 调用 timeOutDeadWorker 方法进行处理,在处理时根据 Worker 运行的是 Executor 和 Driver 分别进行处理</p><ul><li>如果是Executor, Master先把该 Worker 上运行的Executor 发送信息ExecutorUpdate给对应的Driver,告知Executor已经丢失,同时把这些Executor从其应用程序列表删除, 另外, 相关Executor的异常也需要处理</li><li>如果是Driver, 则判断是否设置重新启动,如果需要,则调用Master.shedule方法进行调度,分配合适节点重启Driver, 如果不需要重启, 则删除该应用程序</li></ul></li><li><h4 id="Executor异常"><a href="#Executor异常" class="headerlink" title="Executor异常"></a>Executor异常</h4><ol><li>Executor发生异常时由ExecutorRunner捕获该异常并发送ExecutorStateChanged信息给Worker</li><li>Worker接收到消息时, 在Worker的 handleExecutorStateChanged 方法中, 根据Executor状态进行信息更新,同时把Executor状态发送给Master</li><li>Master在接受Executor状态变化消息之后,如果发现其是异常退出,会尝试可用的Worker节点去启动Executor</li></ol></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/spark%E7%9A%84stage%E6%98%AF%E5%A6%82%E4%BD%95%E5%88%92%E5%88%86%E7%9A%84/"/>
      <url>/2019/09/24/docs/spark%E7%9A%84stage%E6%98%AF%E5%A6%82%E4%BD%95%E5%88%92%E5%88%86%E7%9A%84/</url>
      
        <content type="html"><![CDATA[<h2 id="spark的stage是如何划分的"><a href="#spark的stage是如何划分的" class="headerlink" title="spark的stage是如何划分的"></a>spark的stage是如何划分的</h2><p><strong>stage的划分依据就是看是否产生了shuflle(即宽依赖),遇到一个shuffle操作就划分为前后两个stage.</strong></p><p><img src="//jiamaoxiang.top/2019/09/24/docs/spark的stage是如何划分的/D:%5CNote%5Cbig-data-interview%5CBigData-Interview%5Cpictures%5CstageDivide.jpg" alt></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/Spark%E7%9A%84partitioner%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B/"/>
      <url>/2019/09/24/docs/Spark%E7%9A%84partitioner%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B/</url>
      
        <content type="html"><![CDATA[<h2 id="Spark的-partitioner-都有哪些"><a href="#Spark的-partitioner-都有哪些" class="headerlink" title="Spark的 partitioner 都有哪些?"></a>Spark的 partitioner 都有哪些?</h2><p><strong>Partitioner主要有两个实现类：HashPartitioner和RangePartitioner,HashPartitioner是大部分transformation的默认实现，sortBy、sortByKey使用RangePartitioner实现，也可以自定义Partitioner.</strong></p><ul><li><p><strong>HashPartitioner</strong></p><p>numPartitions方法返回传入的分区数，getPartition方法使用key的hashCode值对分区数取模得到PartitionId，写入到对应的bucket中。</p></li><li><p><strong>RangePartitioner</strong></p><p>RangePartitioner是先根据所有partition中数据的分布情况，尽可能均匀地构造出重分区的分隔符，再将数据的key值根据分隔符进行重新分区</p><ul><li>使用reservoir Sample方法对每个Partition进行分别抽样</li><li>对数据量大(大于sampleSizePerPartition)的分区进行重新抽样</li><li>由权重信息计算出分区分隔符rangeBounds</li><li>由rangeBounds计算分区数和key的所属分区</li></ul></li></ul><p><a href="https://blog.csdn.net/qq_34842671/article/details/83685179" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/spark%E6%9C%89%E5%93%AA%E5%87%A0%E7%A7%8Djoin/"/>
      <url>/2019/09/24/docs/spark%E6%9C%89%E5%93%AA%E5%87%A0%E7%A7%8Djoin/</url>
      
        <content type="html"><![CDATA[<h2 id="spark有哪几种join"><a href="#spark有哪几种join" class="headerlink" title="spark有哪几种join"></a>spark有哪几种join</h2><p><strong>Spark 中和 join 相关的算子有这几个</strong>：<code>join</code>、<code>fullOuterJoin</code>、<code>leftOuterJoin</code>、<code>rightOuterJoin</code></p><ul><li><p><strong>join</strong></p><p>join函数会输出两个RDD中key相同的所有项，并将它们的value联结起来，它联结的key要求在两个表中都存在，类似于SQL中的INNER JOIN。但它不满足交换律，a.join(b)与b.join(a)的结果不完全相同，值插入的顺序与调用关系有关。</p></li><li><p><strong>leftOuterJoin</strong></p><p>leftOuterJoin会保留对象的所有key，而用None填充在参数RDD other中缺失的值，因此调用顺序会使结果完全不同。如下面展示的结果，</p></li><li><p><strong>rightOuterJoin</strong></p><p>rightOuterJoin与leftOuterJoin基本一致，区别在于它的结果保留的是参数other这个RDD中所有的key。</p></li><li><p><strong>fullOuterJoin</strong></p><p>fullOuterJoin会保留两个RDD中所有的key，因此所有的值列都有可能出现缺失的情况，所有的值列都会转为Some对象。</p></li></ul><p><a href="http://www.neilron.xyz/join-in-spark/" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/spark%E4%BB%8E%E6%8F%90%E4%BA%A4%E4%B8%80%E4%B8%AAjar%E5%88%B0%E6%9C%80%E5%90%8E%E8%BF%94%E5%9B%9E%E7%BB%93%E6%9E%9C/"/>
      <url>/2019/09/24/docs/spark%E4%BB%8E%E6%8F%90%E4%BA%A4%E4%B8%80%E4%B8%AAjar%E5%88%B0%E6%9C%80%E5%90%8E%E8%BF%94%E5%9B%9E%E7%BB%93%E6%9E%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="spark运行原理-从提交一个jar到最后返回结果-整个过程"><a href="#spark运行原理-从提交一个jar到最后返回结果-整个过程" class="headerlink" title="spark运行原理,从提交一个jar到最后返回结果,整个过程"></a>spark运行原理,从提交一个jar到最后返回结果,整个过程</h2><ol><li><code>spark-submit</code> 提交代码，执行 <code>new SparkContext()</code>，在 SparkContext 里构造 <code>DAGScheduler</code> 和 <code>TaskScheduler</code>。</li><li>TaskScheduler 会通过后台的一个进程，连接 Master，向 Master 注册 Application。</li><li>Master 接收到 Application 请求后，会使用相应的资源调度算法，在 Worker 上为这个 Application 启动多个 Executer。</li><li>Executor 启动后，会自己反向注册到 TaskScheduler 中。 所有 Executor 都注册到 Driver 上之后，SparkContext 结束初始化，接下来往下执行我们自己的代码。</li><li>每执行到一个 Action，就会创建一个 Job。Job 会提交给 DAGScheduler。</li><li>DAGScheduler 会将 Job划分为多个 stage，然后每个 stage 创建一个 TaskSet。</li><li>TaskScheduler 会把每一个 TaskSet 里的 Task，提交到 Executor 上执行。</li><li>Executor 上有线程池，每接收到一个 Task，就用 TaskRunner 封装，然后从线程池里取出一个线程执行这个 task。(TaskRunner 将我们编写的代码，拷贝，反序列化，执行 Task，每个 Task 执行 RDD 里的一个 partition)</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/Spark%E4%B8%AD%E7%9A%84%E7%AE%97%E5%AD%90%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B/"/>
      <url>/2019/09/24/docs/Spark%E4%B8%AD%E7%9A%84%E7%AE%97%E5%AD%90%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B/</url>
      
        <content type="html"><![CDATA[<h2 id="Spark中的算子都有哪些"><a href="#Spark中的算子都有哪些" class="headerlink" title="Spark中的算子都有哪些"></a>Spark中的算子都有哪些</h2><p>总的来说,spark分为两大类算子:</p><ul><li><p><strong>Transformation 变换/转换算子：这种变换并不触发提交作业，完成作业中间过程处理</strong></p><p>Transformation 操作是延迟计算的，也就是说从一个RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Action 操作的时候才会真正触发运算</p></li><li><p><strong>Action 行动算子：这类算子会触发 SparkContext 提交 Job 作业</strong></p><p>Action 算子会触发 Spark 提交作业（Job），并将数据输出 Spark系统</p><hr></li></ul><h4 id="1-Value数据类型的Transformation算子"><a href="#1-Value数据类型的Transformation算子" class="headerlink" title="1. Value数据类型的Transformation算子"></a>1. Value数据类型的Transformation算子</h4><ul><li><p>输入分区与输出分区一对一型</p><ul><li>map算子</li><li>flatMap算子</li><li>mapPartitions算子</li><li>glom算子</li></ul></li><li><p>输入分区与输出分区多对一型</p><ul><li>union算子</li><li>cartesian算子</li></ul></li><li><p>输入分区与输出分区多对多型</p><ul><li>grouBy算子</li></ul></li><li><p>输出分区为输入分区子集型</p><ul><li>filter算子</li><li>distinct算子</li><li>subtract算子</li><li>sample算子</li><li>takeSample算子</li></ul></li><li><p>Cache型</p><ul><li>cache算子</li><li>persist算子</li></ul></li></ul><h4 id="2-Key-Value数据类型的Transfromation算子"><a href="#2-Key-Value数据类型的Transfromation算子" class="headerlink" title="2. Key-Value数据类型的Transfromation算子"></a>2. Key-Value数据类型的Transfromation算子</h4><ul><li><p>输入分区与输出分区一对一</p><ul><li>mapValues算子</li></ul></li><li><p>对单个RDD或两个RDD聚集</p><ul><li>combineByKey算子</li><li>reduceByKey算子</li><li>partitionBy算子</li><li>Cogroup算子</li></ul></li><li><p>连接</p><ul><li>join算子</li><li>leftOutJoin 和 rightOutJoin算子</li></ul></li></ul><h4 id="3-Action算子"><a href="#3-Action算子" class="headerlink" title="3. Action算子"></a>3. Action算子</h4><ul><li><p>无输出</p><ul><li>foreach算子</li></ul></li><li><p>HDFS算子</p><ul><li>saveAsTextFile算子</li><li>saveAsObjectFile算子</li></ul></li><li><p>Scala集合和数据类型</p><ul><li>collect算子</li><li>collectAsMap算子</li><li>reduceByKeyLocally算子</li><li>lookup算子</li><li>count算子</li><li>top算子</li><li>reduce算子</li><li>fold算子</li><li>aggregate算子</li></ul></li></ul><p><a href="https://www.cnblogs.com/kpsmile/p/10434390.html" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/spark%E4%B8%AD%E7%9A%84%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/"/>
      <url>/2019/09/24/docs/spark%E4%B8%AD%E7%9A%84%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/</url>
      
        <content type="html"><![CDATA[<h2 id="spark中的广播变量"><a href="#spark中的广播变量" class="headerlink" title="spark中的广播变量"></a>spark中的广播变量</h2><p><a href="https://www.jianshu.com/p/6ef7f0a44fbf" target="_blank" rel="noopener">图片来源</a> /<a href="https://github.com/JerryLead/SparkInternals/blob/master/markdown/7-Broadcast.md" target="_blank" rel="noopener">文字来源</a></p><p><img src="//jiamaoxiang.top/2019/09/24/docs/spark中的广播变量/pictures/spark%E4%B8%AD%E7%9A%84%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F.png" alt></p><p><strong>顾名思义，broadcast 就是将数据从一个节点发送到其他各个节点上去。这样的场景很多，比如 driver 上有一张表，其他节点上运行的 task 需要 lookup 这张表，那么 driver 可以先把这张表 copy 到这些节点，这样 task 就可以在本地查表了。如何实现一个可靠高效的 broadcast 机制是一个有挑战性的问题。先看看 Spark 官网上的一段话：</strong></p><p>Broadcast variables allow the programmer to keep a <strong>read-only</strong> variable cached on each <strong>machine</strong> rather than shipping a copy of it with <strong>tasks</strong>. They can be used, for example, to give every node a copy of a <strong>large input dataset</strong> in an efficient manner. Spark also attempts to distribute broadcast variables using <strong>efficient</strong> broadcast algorithms to reduce communication cost.</p><h3 id="问题：为什么只能-broadcast-只读的变量？"><a href="#问题：为什么只能-broadcast-只读的变量？" class="headerlink" title="问题：为什么只能 broadcast 只读的变量？"></a>问题：为什么只能 broadcast 只读的变量？</h3><p>这就涉及一致性的问题，如果变量可以被更新，那么一旦变量被某个节点更新，其他节点要不要一块更新？如果多个节点同时在更新，更新顺序是什么？怎么做同步？还会涉及 fault-tolerance 的问题。为了避免维护数据一致性问题，Spark 目前只支持 broadcast 只读变量。</p><h3 id="问题：broadcast-到节点而不是-broadcast-到每个-task？"><a href="#问题：broadcast-到节点而不是-broadcast-到每个-task？" class="headerlink" title="问题：broadcast 到节点而不是 broadcast 到每个 task？"></a>问题：broadcast 到节点而不是 broadcast 到每个 task？</h3><p>因为每个 task 是一个线程，而且同在一个进程运行 tasks 都属于同一个 application。因此每个节点（executor）上放一份就可以被所有 task 共享。</p><h3 id="问题：-具体怎么用-broadcast？"><a href="#问题：-具体怎么用-broadcast？" class="headerlink" title="问题： 具体怎么用 broadcast？"></a>问题： 具体怎么用 broadcast？</h3><p>driver program 例子：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val data = List(1, 2, 3, 4, 5, 6)</span><br><span class="line">val bdata = sc.broadcast(data)</span><br><span class="line"></span><br><span class="line">val rdd = sc.parallelize(1 to 6, 2)</span><br><span class="line">val observedSizes = rdd.map(_ =&gt; bdata.value.size)</span><br></pre></td></tr></table></figure><p>driver 使用 <code>sc.broadcast()</code> 声明要 broadcast 的 data，bdata 的类型是 Broadcast。</p><p>当 <code>rdd.transformation(func)</code> 需要用 bdata 时，直接在 func 中调用，比如上面的例子中的 map() 就使用了 bdata.value.size。</p><h3 id="问题：怎么实现-broadcast？"><a href="#问题：怎么实现-broadcast？" class="headerlink" title="问题：怎么实现 broadcast？"></a>问题：怎么实现 broadcast？</h3><p>broadcast 的实现机制很有意思：</p><h4 id="1-分发-task-的时候先分发-bdata-的元信息"><a href="#1-分发-task-的时候先分发-bdata-的元信息" class="headerlink" title="1. 分发 task 的时候先分发 bdata 的元信息"></a>1. 分发 task 的时候先分发 bdata 的元信息</h4><p>Driver 先建一个本地文件夹用以存放需要 broadcast 的 data，并启动一个可以访问该文件夹的 HttpServer。当调用<code>val bdata = sc.broadcast(data)</code>时就把 data 写入文件夹，同时写入 driver 自己的 blockManger 中（StorageLevel 为内存＋磁盘），获得一个 blockId，类型为 BroadcastBlockId。当调用<code>rdd.transformation(func)</code>时，如果 func 用到了 bdata，那么 driver submitTask() 的时候会将 bdata 一同 func 进行序列化得到 serialized task，<strong>注意序列化的时候不会序列化 bdata 中包含的 data。</strong>上一章讲到 serialized task 从 driverActor 传递到 executor 时使用 Akka 的传消息机制，消息不能太大，而实际的 data 可能很大，所以这时候还不能 broadcast data。</p><blockquote><p>driver 为什么会同时将 data 放到磁盘和 blockManager 里面？放到磁盘是为了让 HttpServer 访问到，放到 blockManager 是为了让 driver program 自身使用 bdata 时方便（其实我觉得不放到 blockManger 里面也行）。</p></blockquote><p><strong>那么什么时候传送真正的 data？</strong>在 executor 反序列化 task 的时候，会同时反序列化 task 中的 bdata 对象，这时候会调用 bdata 的 readObject() 方法。该方法先去本地 blockManager 那里询问 bdata 的 data 在不在 blockManager 里面，如果不在就使用下面的两种 fetch 方式之一去将 data fetch 过来。得到 data 后，将其存放到 blockManager 里面，这样后面运行的 task 如果需要 bdata 就不需要再去 fetch data 了。如果在，就直接拿来用了。</p><p>下面探讨 broadcast data 时候的两种实现方式：</p><h4 id="2-HttpBroadcast"><a href="#2-HttpBroadcast" class="headerlink" title="2. HttpBroadcast"></a>2. HttpBroadcast</h4><p>顾名思义，HttpBroadcast 就是每个 executor 通过的 http 协议连接 driver 并从 driver 那里 fetch data。</p><p>Driver 先准备好要 broadcast 的 data，调用<code>sc.broadcast(data)</code>后会调用工厂方法建立一个 HttpBroadcast 对象。该对象做的第一件事就是将 data 存到 driver 的 blockManager 里面，StorageLevel 为内存＋磁盘，blockId 类型为 BroadcastBlockId。</p><p>同时 driver 也会将 broadcast 的 data 写到本地磁盘，例如写入后得到 <code>/var/folders/87/grpn1_fn4xq5wdqmxk31v0l00000gp/T/spark-6233b09c-3c72-4a4d-832b-6c0791d0eb9c/broadcast_0</code>， 这个文件夹作为 HttpServer 的文件目录。</p><blockquote><p>Driver 和 executor 启动的时候，都会生成 broadcastManager 对象，调用 HttpBroadcast.initialize()，driver 会在本地建立一个临时目录用来存放 broadcast 的 data，并启动可以访问该目录的 httpServer。</p></blockquote><p><strong>Fetch data：</strong>在 executor 反序列化 task 的时候，会同时反序列化 task 中的 bdata 对象，这时候会调用 bdata 的 readObject() 方法。该方法先去本地 blockManager 那里询问 bdata 的 data 在不在 blockManager 里面，<strong>如果不在就使用 http 协议连接 driver 上的 httpServer，将 data fetch 过来。</strong>得到 data 后，将其存放到 blockManager 里面，这样后面运行的 task 如果需要 bdata 就不需要再去 fetch data 了。如果在，就直接拿来用了。</p><p>HttpBroadcast 最大的问题就是 <strong>driver 所在的节点可能会出现网络拥堵</strong>，因为 worker 上的 executor 都会去 driver 那里 fetch 数据。</p><h4 id="3-TorrentBroadcast"><a href="#3-TorrentBroadcast" class="headerlink" title="3. TorrentBroadcast"></a>3. TorrentBroadcast</h4><p>为了解决 HttpBroadast 中 driver 单点网络瓶颈的问题，Spark 又设计了一种 broadcast 的方法称为 TorrentBroadcast，<strong>这个类似于大家常用的 BitTorrent 技术。</strong>基本思想就是将 data 分块成 data blocks，然后假设有 executor fetch 到了一些 data blocks，那么这个 executor 就可以被当作 data server 了，随着 fetch 的 executor 越来越多，有更多的 data server 加入，data 就很快能传播到全部的 executor 那里去了。</p><p>HttpBroadcast 是通过传统的 http 协议和 httpServer 去传 data，在 TorrentBroadcast 里面使用在上一章介绍的 blockManager.getRemote() =&gt; NIO ConnectionManager 传数据的方法来传递，读取数据的过程与读取 cached rdd 的方式类似，可以参阅 <a href="https://github.com/JerryLead/SparkInternals/blob/master/markdown/6-CacheAndCheckpoint.md" target="_blank" rel="noopener">CacheAndCheckpoint</a> 中的最后一张图。</p><p>下面讨论 TorrentBroadcast 的一些细节：</p><p><a href="https://github.com/JerryLead/SparkInternals/blob/master/markdown/PNGfigures/TorrentBroadcast.png" target="_blank" rel="noopener"><img src="https://github.com/JerryLead/SparkInternals/raw/master/markdown/PNGfigures/TorrentBroadcast.png" alt="TorrentBroadcast"></a></p><h4 id="driver-端："><a href="#driver-端：" class="headerlink" title="driver 端："></a>driver 端：</h4><p>Driver 先把 data 序列化到 byteArray，然后切割成 BLOCK_SIZE（由 <code>spark.broadcast.blockSize = 4MB</code> 设置）大小的 data block，每个 data block 被 TorrentBlock 对象持有。切割完 byteArray 后，会将其回收，因此内存消耗虽然可以达到 2 * Size(data)，但这是暂时的。</p><p>完成分块切割后，就将分块信息（称为 meta 信息）存放到 driver 自己的 blockManager 里面，StorageLevel 为内存＋磁盘，同时会通知 driver 自己的 blockManagerMaster 说 meta 信息已经存放好。<strong>通知 blockManagerMaster 这一步很重要，因为 blockManagerMaster 可以被 driver 和所有 executor 访问到，信息被存放到 blockManagerMaster 就变成了全局信息。</strong></p><p>之后将每个分块 data block 存放到 driver 的 blockManager 里面，StorageLevel 为内存＋磁盘。存放后仍然通知 blockManagerMaster 说 blocks 已经存放好。到这一步，driver 的任务已经完成。</p><h4 id="Executor-端："><a href="#Executor-端：" class="headerlink" title="Executor 端："></a>Executor 端：</h4><p>executor 收到 serialized task 后，先反序列化 task，这时候会反序列化 serialized task 中包含的 bdata 类型是 TorrentBroadcast，也就是去调用 TorrentBroadcast.readObject()。这个方法首先得到 bdata 对象，<strong>然后发现 bdata 里面没有包含实际的 data。怎么办？</strong>先询问所在的 executor 里的 blockManager 是会否包含 data（通过查询 data 的 broadcastId），包含就直接从本地 blockManager 读取 data。否则，就通过本地 blockManager 去连接 driver 的 blockManagerMaster 获取 data 分块的 meta 信息，获取信息后，就开始了 BT 过程。</p><p><strong>BT 过程：</strong>task 先在本地开一个数组用于存放将要 fetch 过来的 data blocks <code>arrayOfBlocks = new Array[TorrentBlock](totalBlocks)</code>，TorrentBlock 是对 data block 的包装。然后打乱要 fetch 的 data blocks 的顺序，比如如果 data block 共有 5 个，那么打乱后的 fetch 顺序可能是 3-1-2-4-5。然后按照打乱后的顺序去 fetch 一个个 data block。fetch 的过程就是通过 “本地 blockManager －本地 connectionManager－driver/executor 的 connectionManager－driver/executor 的 blockManager－data” 得到 data，这个过程与 fetch cached rdd 类似。<strong>每 fetch 到一个 block 就将其存放到 executor 的 blockManager 里面，同时通知 driver 上的 blockManagerMaster 说该 data block 多了一个存储地址。</strong>这一步通知非常重要，意味着 blockManagerMaster 知道 data block 现在在 cluster 中有多份，下一个不同节点上的 task 再去 fetch 这个 data block 的时候，可以有两个选择了，而且会随机选择一个去 fetch。这个过程持续下去就是 BT 协议，随着下载的客户端越来越多，data block 服务器也越来越多，就变成 p2p下载了。关于 BT 协议，Wikipedia 上有一个<a href="http://zh.wikipedia.org/wiki/BitTorrent_(%E5%8D%8F%E8%AE%AE)" target="_blank" rel="noopener">动画</a>。</p><p>整个 fetch 过程结束后，task 会开一个大 Array[Byte]，大小为 data 的总大小，然后将 data block 都 copy 到这个 Array，然后对 Array 中 bytes 进行反序列化得到原始的 data，这个过程就是 driver 序列化 data 的反过程。</p><p>最后将 data 存放到 task 所在 executor 的 blockManager 里面，StorageLevel 为内存＋磁盘。显然，这时候 data 在 blockManager 里存了两份，不过等全部 executor 都 fetch 结束，存储 data blocks 那份可以删掉了。</p><h3 id="问题：broadcast-RDD-会怎样"><a href="#问题：broadcast-RDD-会怎样" class="headerlink" title="问题：broadcast RDD 会怎样?"></a>问题：broadcast RDD 会怎样?</h3><p><a href="http://weibo.com/u/1410938285" target="_blank" rel="noopener">@Andrew-Xia</a> 回答道：不会怎样，就是这个rdd在每个executor中实例化一份。</p><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>公共数据的 broadcast 是很实用的功能，在 Hadoop 中使用 DistributedCache，比如常用的<code>-libjars</code>就是使用 DistributedCache 来将 task 依赖的 jars 分发到每个 task 的工作目录。不过分发前 DistributedCache 要先将文件上传到 HDFS。这种方式的主要问题是<strong>资源浪费</strong>，如果某个节点上要运行来自同一 job 的 4 个 mapper，那么公共数据会在该节点上存在 4 份（每个 task 的工作目录会有一份）。但是通过 HDFS 进行 broadcast 的好处在于<strong>单点瓶颈不明显</strong>，因为公共 data 首先被分成多个 block，然后不同的 block 存放在不同的节点。这样，只要所有的 task 不是同时去同一个节点 fetch 同一个 block，网络拥塞不会很严重。</p><p>对于 Spark 来讲，broadcast 时考虑的不仅是如何将公共 data 分发下去的问题，还要考虑如何让同一节点上的 task 共享 data。</p><p>对于第一个问题，Spark 设计了两种 broadcast 的方式，传统存在单点瓶颈问题的 HttpBroadcast，和类似 BT 方式的 TorrentBroadcast。HttpBroadcast 使用传统的 client-server 形式的 HttpServer 来传递真正的 data，而 TorrentBroadcast 使用 blockManager 自带的 NIO 通信方式来传递 data。TorrentBroadcast 存在的问题是<strong>慢启动</strong>和<strong>占内存</strong>，慢启动指的是刚开始 data 只在 driver 上有，要等 executors fetch 很多轮 data block 后，data server 才会变得可观，后面的 fetch 速度才会变快。executor 所占内存的在 fetch 完 data blocks 后进行反序列化时需要将近两倍 data size 的内存消耗。不管哪一种方式，driver 在分块时会有两倍 data size 的内存消耗。</p><p>对于第二个问题，每个 executor 都包含一个 blockManager 用来管理存放在 executor 里的数据，将公共数据存放在 blockManager 中（StorageLevel 为内存＋磁盘），可以保证在 executor 执行的 tasks 能够共享 data。</p><p>其实 Spark 之前还尝试了一种称为 TreeBroadcast 的机制，详情可以见技术报告 <a href="http://www.cs.berkeley.edu/~agearh/cs267.sp10/files/mosharaf-spark-bc-report-spring10.pdf" target="_blank" rel="noopener">Performance and Scalability of Broadcast in Spark</a>。</p><p>更深入点，broadcast 可以用多播协议来做，不过多播使用 UDP，不是可靠的，仍然需要应用层的设计一些可靠性保障机制。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/spark%E4%B8%ADcluster%E6%A8%A1%E5%BC%8F%E5%92%8Cclient%E6%A8%A1%E5%BC%8F%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB/"/>
      <url>/2019/09/24/docs/spark%E4%B8%ADcluster%E6%A8%A1%E5%BC%8F%E5%92%8Cclient%E6%A8%A1%E5%BC%8F%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<h2 id="spark-on-yarn-模式下的-cluster模式和-client模式有什么区别"><a href="#spark-on-yarn-模式下的-cluster模式和-client模式有什么区别" class="headerlink" title="spark on yarn 模式下的 cluster模式和 client模式有什么区别"></a>spark on yarn 模式下的 cluster模式和 client模式有什么区别</h2><ol><li>yarn-cluster 适用于生产环境。而 yarn-client 适用于交互和调试，也就是希望快速地看到 application 的输出.</li><li>yarn-cluster 和 yarn-client 模式的区别其实就是 <strong>Application Master 进程</strong>的区别，yarn-cluster 模式下，driver 运行在 AM(Application Master)中，它负责向 YARN 申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉 Client，作业会继续在 YARN 上运行。然而 yarn-cluster 模式不适合运行交互类型的作业。而 yarn-client 模式下，Application Master 仅仅向 YARN 请求 executor，Client 会和请求的container 通信来调度他们工作，也就是说 Client 不能离开。</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/spark2.0%E4%B8%BA%E4%BB%80%E4%B9%88%E6%94%BE%E5%BC%83%E4%BA%86akka%E8%80%8C%E7%94%A8netty/"/>
      <url>/2019/09/24/docs/spark2.0%E4%B8%BA%E4%BB%80%E4%B9%88%E6%94%BE%E5%BC%83%E4%BA%86akka%E8%80%8C%E7%94%A8netty/</url>
      
        <content type="html"><![CDATA[<h2 id="spark2-0为什么放弃了akka-而用netty"><a href="#spark2-0为什么放弃了akka-而用netty" class="headerlink" title="spark2.0为什么放弃了akka 而用netty"></a>spark2.0为什么放弃了akka 而用netty</h2><ol><li>很多Spark用户也使用Akka，但是由于Akka不同版本之间无法互相通信，这就要求用户必须使用跟Spark完全一样的Akka版本，导致用户无法升级Akka。</li><li>Spark的Akka配置是针对Spark自身来调优的，可能跟用户自己代码中的Akka配置冲突。</li><li>Spark用的Akka特性很少，这部分特性很容易自己实现。同时，这部分代码量相比Akka来说少很多，debug比较容易。如果遇到什么bug，也可以自己马上fix，不需要等Akka上游发布新版本。而且，Spark升级Akka本身又因为第一点会强制要求用户升级他们使用的Akka，对于某些用户来说是不现实的。</li></ol><p><a href="https://www.zhihu.com/question/61638635" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/RDD%E7%9A%84%E7%BC%93%E5%AD%98%E7%BA%A7%E5%88%AB%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B/"/>
      <url>/2019/09/24/docs/RDD%E7%9A%84%E7%BC%93%E5%AD%98%E7%BA%A7%E5%88%AB%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B/</url>
      
        <content type="html"><![CDATA[<h2 id="RDD的缓存级别都有哪些"><a href="#RDD的缓存级别都有哪些" class="headerlink" title="RDD的缓存级别都有哪些"></a>RDD的缓存级别都有哪些</h2><p>NONE :什么类型都不是<br>DISK_ONLY：磁盘<br>DISK_ONLY_2：磁盘；双副本<br>MEMORY_ONLY： 内存；反序列化；把RDD作为反序列化的方式存储，假如RDD的内容存不下，剩余的分区在以后需要时会重新计算，不会刷到磁盘上。<br>MEMORY_ONLY_2：内存；反序列化；双副本<br>MEMORY_ONLY_SER：内存；序列化；这种序列化方式，每一个partition以字节数据存储，好处是能带来更好的空间存储，但CPU耗费高<br>MEMORY_ONLY_SER_2 : 内存；序列化；双副本<br>MEMORY_AND_DISK：内存 + 磁盘；反序列化；双副本；RDD以反序列化的方式存内存，假如RDD的内容存不下，剩余的会存到磁盘<br>MEMORY_AND_DISK_2 : 内存 + 磁盘；反序列化；双副本<br>MEMORY_AND_DISK_SER：内存 + 磁盘；序列化<br>MEMORY_AND_DISK_SER_2：内存 + 磁盘；序列化；双副本</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/RDD%E6%9C%89%E5%93%AA%E4%BA%9B%E7%89%B9%E7%82%B9/"/>
      <url>/2019/09/24/docs/RDD%E6%9C%89%E5%93%AA%E4%BA%9B%E7%89%B9%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<h2 id="RDD有哪些特点"><a href="#RDD有哪些特点" class="headerlink" title="RDD有哪些特点"></a>RDD有哪些特点</h2><ol><li><p><strong>A list of partitions</strong><br>RDD是一个由多个partition（某个节点里的某一片连续的数据）组成的的list；将数据加载为RDD时，一般会遵循数据的本地性（一般一个hdfs里的block会加载为一个partition）。</p></li><li><p><strong>A function for computing each split</strong><br>RDD的每个partition上面都会有function，也就是函数应用，其作用是实现RDD之间partition的转换。</p></li><li><p><strong>A list of dependencies on other RDDs</strong><br>RDD会记录它的依赖 ，为了容错（重算，cache，checkpoint），也就是说在内存中的RDD操作时出错或丢失会进行重算。</p></li><li><p><strong>Optionally,a Partitioner for Key-value RDDs</strong><br>  可选项，如果RDD里面存的数据是key-value形式，则可以传递一个自定义的Partitioner进行重新分区，例如这里自定义的Partitioner是基于key进行分区，那则会将不同RDD里面的相同key的数据放到同一个partition里面</p></li><li><p><strong>Optionally, a list of preferred locations to compute each split on</strong></p><p>最优的位置去计算，也就是数据的本地性。</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/RDD%E6%87%92%E5%8A%A0%E8%BD%BD%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D/"/>
      <url>/2019/09/24/docs/RDD%E6%87%92%E5%8A%A0%E8%BD%BD%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D/</url>
      
        <content type="html"><![CDATA[<h2 id="RDD懒加载是什么意思"><a href="#RDD懒加载是什么意思" class="headerlink" title="RDD懒加载是什么意思"></a>RDD懒加载是什么意思</h2><p>Transformation 操作是延迟计算的，也就是说从一个RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Acion 操作的时候才会真正触发运算,这也就是懒加载.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/MapReduce%E8%BF%87%E7%A8%8B/"/>
      <url>/2019/09/24/docs/MapReduce%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="MapReduce过程"><a href="#MapReduce过程" class="headerlink" title="MapReduce过程"></a>MapReduce过程</h2><p>MapReduce分为两个阶段: <strong>Map</strong> 和  <strong>Ruduce</strong>.</p><p><strong>Map阶段:</strong></p><ol><li><p><strong>input</strong>. 在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务</p></li><li><p><strong>map</strong>. 就是程序员编写好的map函数了，因此map函数效率相对好控制，而且一般map操作都是本地化操作也就是在数据存储节点上进行</p></li><li><p><strong>Partition</strong>. 需要计算每一个map的结果需要发到哪个reduce端,partition数等于reducer数.默认采用HashPartition.</p></li><li><p><strong>spill</strong>.此阶段分为sort和combine.首先分区过得数据会经过排序之后写入环形内存缓冲区.在达到阈值之后守护线程将数据溢出分区文件.</p><ul><li><strong>sort</strong>. 在写入环形缓冲区前,对数据排序.&lt;key,value,partition&gt;格式排序</li><li><strong>combine</strong>(可选). 在溢出文件之前,提前开始combine,相当于本地化的reduce操作</li></ul></li><li><p><strong>merge.</strong> spill结果会有很多个文件,但最终输出只有一个,故有一个merge操作会合并所有的本地文件,并且该文件会有一个对应的索引文件.</p></li></ol><p><strong>Reduce阶段:</strong></p><ol><li><strong>copy</strong>. 拉取数据,reduce启动数据copy线程(默认5个),通过Http请求对应节点的map task输出文件,copy的数据也会先放到内部缓冲区.之后再溢写,类似map端操作.</li><li><strong>merge</strong>. 合并多个copy的多个map端的数据.在一个reduce端先将多个map端的数据溢写到本地磁盘,之后再将多个文件合并成一个文件.  数据经过 <strong>内存-&gt;磁盘 , 磁盘-&gt;磁盘</strong>的过程.</li><li><strong>output</strong>.merge阶段最后会生成一个文件,将此文件转移到内存中,shuffle阶段结束</li><li><strong>reduce</strong>. 开始执行reduce任务,最后结果保留在hdfs上.</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/kafka%E6%9C%89%E5%87%A0%E7%A7%8D%E6%95%B0%E6%8D%AE%E4%BF%9D%E7%95%99%E7%AD%96%E7%95%A5/"/>
      <url>/2019/09/24/docs/kafka%E6%9C%89%E5%87%A0%E7%A7%8D%E6%95%B0%E6%8D%AE%E4%BF%9D%E7%95%99%E7%AD%96%E7%95%A5/</url>
      
        <content type="html"><![CDATA[<h2 id="kafka-有几种数据保留策略"><a href="#kafka-有几种数据保留策略" class="headerlink" title="kafka 有几种数据保留策略"></a>kafka 有几种数据保留策略</h2><p>kafka 有两种数据保存策略：按照过期时间保留和按照存储的消息大小保留。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/kafka%E6%80%8E%E6%A0%B7%E4%BF%9D%E8%AF%81%E4%B8%8D%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9/"/>
      <url>/2019/09/24/docs/kafka%E6%80%8E%E6%A0%B7%E4%BF%9D%E8%AF%81%E4%B8%8D%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9/</url>
      
        <content type="html"><![CDATA[<h2 id="kafka怎样保证不重复消费"><a href="#kafka怎样保证不重复消费" class="headerlink" title="kafka怎样保证不重复消费"></a>kafka怎样保证不重复消费</h2><p>此问题其实等价于保证消息队列消费的幂等性</p><p>主要需要结合实际业务来操作:</p><ul><li>比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。</li><li>比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。</li><li>比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。</li><li>比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。</li></ul><p><a href="https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/how-to-ensure-that-messages-are-not-repeatedly-consumed.md" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/kafka%E6%80%8E%E6%A0%B7%E4%BF%9D%E8%AF%81%E4%B8%8D%E4%B8%A2%E5%A4%B1%E6%B6%88%E6%81%AF/"/>
      <url>/2019/09/24/docs/kafka%E6%80%8E%E6%A0%B7%E4%BF%9D%E8%AF%81%E4%B8%8D%E4%B8%A2%E5%A4%B1%E6%B6%88%E6%81%AF/</url>
      
        <content type="html"><![CDATA[<h2 id="kafka怎样保证不丢失消息"><a href="#kafka怎样保证不丢失消息" class="headerlink" title="kafka怎样保证不丢失消息"></a>kafka怎样保证不丢失消息</h2><h4 id="消费端弄丢了数据"><a href="#消费端弄丢了数据" class="headerlink" title="消费端弄丢了数据"></a>消费端弄丢了数据</h4><p>唯一可能导致消费者弄丢数据的情况，就是说，你消费到了这个消息，然后消费者那边<strong>自动提交了 offset</strong>，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。</p><p>这不是跟 RabbitMQ 差不多吗，大家都知道 Kafka 会自动提交 offset，那么只要<strong>关闭自动提交</strong> offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢。但是此时确实还是<strong>可能会有重复消费</strong>，比如你刚处理完，还没提交 offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。</p><p>生产环境碰到的一个问题，就是说我们的 Kafka 消费者消费到了数据之后是写到一个内存的 queue 里先缓冲一下，结果有的时候，你刚把消息写入内存 queue，然后消费者会自动提交 offset。然后此时我们重启了系统，就会导致内存 queue 里还没来得及处理的数据就丢失了。</p><h4 id="Kafka-弄丢了数据"><a href="#Kafka-弄丢了数据" class="headerlink" title="Kafka 弄丢了数据"></a>Kafka 弄丢了数据</h4><p>这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader。大家想想，要是此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？这就丢了一些数据啊。</p><p>生产环境也遇到过，我们也是，之前 Kafka 的 leader 机器宕机了，将 follower 切换为 leader 之后，就会发现说这个数据就丢了。</p><p>所以此时一般是要求起码设置如下 4 个参数：</p><ul><li>给 topic 设置 <code>replication.factor</code> 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。</li><li>在 Kafka 服务端设置 <code>min.insync.replicas</code> 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。</li><li>在 producer 端设置 <code>acks=all</code>：这个是要求每条数据，必须是<strong>写入所有 replica 之后，才能认为是写成功了</strong>。</li><li>在 producer 端设置 <code>retries=MAX</code>（很大很大很大的一个值，无限次重试的意思）：这个是<strong>要求一旦写入失败，就无限重试</strong>，卡在这里了。</li></ul><p>我们生产环境就是按照上述要求配置的，这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。</p><h4 id="生产者会不会弄丢数据？"><a href="#生产者会不会弄丢数据？" class="headerlink" title="生产者会不会弄丢数据？"></a>生产者会不会弄丢数据？</h4><p>如果按照上述的思路设置了 <code>acks=all</code>，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。</p><p><a href="https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/how-to-ensure-the-reliable-transmission-of-messages.md" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/kafka%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%90%9E%E5%90%90%E7%9A%84%E5%8E%9F%E7%90%86/"/>
      <url>/2019/09/24/docs/kafka%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%90%9E%E5%90%90%E7%9A%84%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h2 id="kafka-实现高吞吐的原理"><a href="#kafka-实现高吞吐的原理" class="headerlink" title="kafka 实现高吞吐的原理"></a>kafka 实现高吞吐的原理</h2><ul><li>读写文件依赖OS文件系统的页缓存，而不是在JVM内部缓存数据，利用OS来缓存，内存利用率高</li><li>sendfile技术（零拷贝），避免了传统网络IO四步流程</li><li>支持End-to-End的压缩</li><li>顺序IO以及常量时间get、put消息</li><li>Partition 可以很好的横向扩展和提供高并发处理</li></ul><p><a href="https://www.jianshu.com/p/d6a73be9d803" target="_blank" rel="noopener">参考文章1</a></p><p><a href="https://blog.csdn.net/stark_summer/article/details/50144591" target="_blank" rel="noopener">参考文章2</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/kafka%E5%90%8C%E6%97%B6%E8%AE%BE%E7%BD%AE%E4%BA%867%E5%A4%A9%E5%92%8C10G%E6%B8%85%E9%99%A4%E6%95%B0%E6%8D%AE/"/>
      <url>/2019/09/24/docs/kafka%E5%90%8C%E6%97%B6%E8%AE%BE%E7%BD%AE%E4%BA%867%E5%A4%A9%E5%92%8C10G%E6%B8%85%E9%99%A4%E6%95%B0%E6%8D%AE/</url>
      
        <content type="html"><![CDATA[<h2 id="kafka同时设置了7天和10G清除数据-到第5天的时候消息到达了10G-这个时候kafka如何处理"><a href="#kafka同时设置了7天和10G清除数据-到第5天的时候消息到达了10G-这个时候kafka如何处理" class="headerlink" title="kafka同时设置了7天和10G清除数据,到第5天的时候消息到达了10G,这个时候kafka如何处理?"></a>kafka同时设置了7天和10G清除数据,到第5天的时候消息到达了10G,这个时候kafka如何处理?</h2><p>这个时候 kafka 会执行数据清除工作，时间和大小不论那个满足条件，都会清空数据。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/kafka%E5%8F%AF%E4%BB%A5%E8%84%B1%E7%A6%BBzookeeper%E5%8D%95%E7%8B%AC%E4%BD%BF%E7%94%A8%E5%90%97/"/>
      <url>/2019/09/24/docs/kafka%E5%8F%AF%E4%BB%A5%E8%84%B1%E7%A6%BBzookeeper%E5%8D%95%E7%8B%AC%E4%BD%BF%E7%94%A8%E5%90%97/</url>
      
        <content type="html"><![CDATA[<h2 id="kafka-可以脱离-zookeeper-单独使用吗"><a href="#kafka-可以脱离-zookeeper-单独使用吗" class="headerlink" title="kafka 可以脱离 zookeeper 单独使用吗"></a>kafka 可以脱离 zookeeper 单独使用吗</h2><p>kafka 不能脱离 zookeeper 单独使用，因为 kafka 使用 zookeeper 管理和协调 kafka 的节点服务器。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/kafka%E4%B8%8E%E5%85%B6%E4%BB%96%E6%B6%88%E6%81%AF%E7%BB%84%E4%BB%B6%E5%AF%B9%E6%AF%94/"/>
      <url>/2019/09/24/docs/kafka%E4%B8%8E%E5%85%B6%E4%BB%96%E6%B6%88%E6%81%AF%E7%BB%84%E4%BB%B6%E5%AF%B9%E6%AF%94/</url>
      
        <content type="html"><![CDATA[<h2 id="kafka-与其他消息组件对比"><a href="#kafka-与其他消息组件对比" class="headerlink" title="kafka 与其他消息组件对比"></a>kafka 与其他消息组件对比</h2><p><a href="https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/why-mq.md" target="_blank" rel="noopener">推荐阅读文章</a></p><table><thead><tr><th>特性</th><th>ActiveMQ</th><th>RabbitMQ</th><th>RocketMQ</th><th>Kafka</th></tr></thead><tbody><tr><td>单机吞吐量</td><td>万级，比 RocketMQ、Kafka 低一个数量级</td><td>同 ActiveMQ</td><td>10 万级，支撑高吞吐</td><td>10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景</td></tr><tr><td>topic 数量对吞吐量的影响</td><td></td><td></td><td>topic 可以达到几百/几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic</td><td>topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源</td></tr><tr><td>时效性</td><td>ms 级</td><td>微秒级，这是 RabbitMQ 的一大特点，延迟最低</td><td>ms 级</td><td>延迟在 ms 级以内</td></tr><tr><td>可用性</td><td>高，基于主从架构实现高可用</td><td>同 ActiveMQ</td><td>非常高，分布式架构</td><td>非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用</td></tr><tr><td>消息可靠性</td><td>有较低的概率丢失数据</td><td>基本不丢</td><td>经过参数优化配置，可以做到 0 丢失</td><td>同 RocketMQ</td></tr><tr><td>功能支持</td><td>MQ 领域的功能极其完备</td><td>基于 erlang 开发，并发能力很强，性能极好，延时很低</td><td>MQ 功能较为完善，还是分布式的，扩展性好</td><td>功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用</td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/kafka%E4%B8%8Esparkstreaming%E9%9B%86%E6%88%90/"/>
      <url>/2019/09/24/docs/kafka%E4%B8%8Esparkstreaming%E9%9B%86%E6%88%90/</url>
      
        <content type="html"><![CDATA[<h2 id="kafka-与-spark-streaming-集成-如何保证-exactly-once-语义"><a href="#kafka-与-spark-streaming-集成-如何保证-exactly-once-语义" class="headerlink" title="kafka 与 spark streaming 集成,如何保证 exactly once 语义"></a>kafka 与 spark streaming 集成,如何保证 exactly once 语义</h2><ul><li><h3 id="Spark-Streaming上游对接kafka时保证Exactly-Once"><a href="#Spark-Streaming上游对接kafka时保证Exactly-Once" class="headerlink" title="Spark Streaming上游对接kafka时保证Exactly Once"></a>Spark Streaming上游对接kafka时保证Exactly Once</h3><p>Spark Streaming使用Direct模式对接上游kafka。无论kafka有多少个partition， 使用Direct模式总能保证SS中有相同数量的partition与之相对， 也就是说SS中的KafkaRDD的并发数量在Direct模式下是由上游kafka决定的。 在这个模式下，kafka的offset是作为KafkaRDD的一部分存在，会存储在checkpoints中， 由于checkpoints只存储offset内容，而不存储数据，这就使得checkpoints是相对轻的操作。 这就使得SS在遇到故障时，可以从checkpoint中恢复上游kafka的offset，从而保证exactly once</p></li><li><h3 id="Spark-Streaming输出下游保证Exactly-once"><a href="#Spark-Streaming输出下游保证Exactly-once" class="headerlink" title="Spark Streaming输出下游保证Exactly once"></a>Spark Streaming输出下游保证Exactly once</h3><ul><li><p>第一种“鸵鸟做法”，就是期望下游（数据）具有幂等特性。</p><p>多次尝试总是写入相同的数据，例如，saveAs***Files 总是将相同的数据写入生成的文件</p></li></ul></li><li><ul><li><p>使用事务更新</p><p>所有更新都是事务性的，以便更新完全按原子进行。这样做的一个方法如下： 使用批处理时间(在foreachRDD中可用)和RDD的partitionIndex（分区索引）来创建identifier（标识符)。 该标识符唯一地标识streaming application 中的blob数据。 使用该identifier，blob 事务地更新到外部系统中。也就是说，如果identifier尚未提交，则以 (atomicall)原子方式提交分区数据和identifier。否则，如果已经提交，请跳过更新。</p></li></ul></li></ul><p><a href="http://www.aihacks.life/post/spark-streaming%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81exactly-once%E8%AF%AD%E4%B9%89/" target="_blank" rel="noopener">参考文章1</a></p><p><a href="https://www.jianshu.com/p/10de8f3b1be8" target="_blank" rel="noopener">参考文章2</a></p><p><a href="https://blog.csdn.net/cymvp/article/details/52605987" target="_blank" rel="noopener">参考文章3</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/Impala%E5%92%8Chive%E7%9A%84%E6%9F%A5%E8%AF%A2%E6%9C%89%E5%93%AA%E4%BA%9B%E5%8C%BA%E5%88%AB/"/>
      <url>/2019/09/24/docs/Impala%E5%92%8Chive%E7%9A%84%E6%9F%A5%E8%AF%A2%E6%9C%89%E5%93%AA%E4%BA%9B%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<h2 id="Impala-和-hive-的查询有哪些区别"><a href="#Impala-和-hive-的查询有哪些区别" class="headerlink" title="Impala 和 hive 的查询有哪些区别"></a>Impala 和 hive 的查询有哪些区别</h2><p><strong>Impala是基于Hive的大数据实时分析查询引擎</strong>，直接使用Hive的元数据库Metadata,意味着impala元数据都存储在Hive的metastore中。并且impala兼容Hive的sql解析，实现了Hive的SQL语义的子集，功能还在不断的完善中。</p><h4 id="Impala相对于Hive所使用的优化技术"><a href="#Impala相对于Hive所使用的优化技术" class="headerlink" title="Impala相对于Hive所使用的优化技术"></a>Impala相对于Hive所使用的优化技术</h4><ul><li>1、没有使用 MapReduce进行并行计算，虽然MapReduce是非常好的并行计算框架，但它更多的面向批处理模式，而不是面向交互式的SQL执行。与 MapReduce相比：Impala把整个查询分成一执行计划树，而不是一连串的MapReduce任务，在分发执行计划后，Impala使用拉式获取 数据的方式获取结果，把结果数据组成按执行树流式传递汇集，减少的了把中间结果写入磁盘的步骤，再从磁盘读取数据的开销。Impala使用服务的方式避免 每次执行查询都需要启动的开销，即相比Hive没了MapReduce启动时间。</li><li>2、使用LLVM产生运行代码，针对特定查询生成特定代码，同时使用Inline的方式减少函数调用的开销，加快执行效率。</li><li>3、充分利用可用的硬件指令（SSE4.2）。</li><li>4、更好的IO调度，Impala知道数据块所在的磁盘位置能够更好的利用多磁盘的优势，同时Impala支持直接数据块读取和本地代码计算checksum。</li><li>5、通过选择合适的数据存储格式可以得到最好的性能（Impala支持多种存储格式）。</li><li>6、最大使用内存，中间结果不写磁盘，及时通过网络以stream的方式传递。</li></ul><p><a href="https://cloud.tencent.com/developer/article/1175527" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/HMaster%E5%AE%95%E6%9C%BA/"/>
      <url>/2019/09/24/docs/HMaster%E5%AE%95%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="HMaster宕机的时候-哪些操作还能正常工作"><a href="#HMaster宕机的时候-哪些操作还能正常工作" class="headerlink" title="HMaster宕机的时候,哪些操作还能正常工作"></a>HMaster宕机的时候,哪些操作还能正常工作</h2><p>对表内数据的增删查改是可以正常进行的,因为hbase client 访问数据只需要通过 zookeeper 来找到 rowkey 的具体 region 位置即可. 但是对于创建表/删除表等的操作就无法进行了,因为这时候是需要HMaster介入, 并且region的拆分,合并,迁移等操作也都无法进行了</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/hive%E7%9A%84metastore%E7%9A%84%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%BC%8F/"/>
      <url>/2019/09/24/docs/hive%E7%9A%84metastore%E7%9A%84%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h2 id="hive的metastore的三种模式"><a href="#hive的metastore的三种模式" class="headerlink" title="hive的metastore的三种模式"></a>hive的metastore的三种模式</h2><ul><li><p><strong>内嵌Derby方式</strong></p><p>这个是Hive默认的启动模式，一般用于单元测试，这种存储方式有一个缺点：在同一时间只能有一个进程连接使用数据库。</p></li><li><p><strong>Local方式</strong></p><p>本地MySQL</p></li><li><p><strong>Remote方式</strong></p><p>远程MySQL,一般常用此种方式</p></li></ul><p><a href="https://blog.csdn.net/baolibin528/article/details/46710025" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/hive%E5%9B%9B%E7%A7%8D%E6%8E%92%E5%BA%8F%E6%96%B9%E5%BC%8F%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2019/09/24/docs/hive%E5%9B%9B%E7%A7%8D%E6%8E%92%E5%BA%8F%E6%96%B9%E5%BC%8F%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<h2 id="hive四种排序方式的区别"><a href="#hive四种排序方式的区别" class="headerlink" title="hive四种排序方式的区别"></a>hive四种排序方式的区别</h2><ul><li><p><strong>order by</strong> </p><pre><code>order by 是要对输出的结果进行全局排序，这就意味着**只有一个reducer**才能实现（多个reducer无法保证全局有序）但是当数据量过大的时候，效率就很低。如果在严格模式下（hive.mapred.mode=strict）,则必须配合limit使用</code></pre></li><li><p><strong>sort by</strong></p><pre><code>sort by 不是全局排序，只是在进入到reducer之前完成排序，只保证了每个reducer中数据按照指定字段的有序性，是局部排序。配置mapred.reduce.tasks=[nums]可以对输出的数据执行归并排序。可以配合limit使用，提高性能</code></pre></li><li><p><strong>distribute by</strong> </p><pre><code>distribute by 指的是按照指定的字段划分到不同的输出reduce文件中，和sort by一起使用时需要注意，</code></pre><p>distribute by必须放在前面</p></li><li><p><strong>cluster by</strong></p><p>cluster by 可以看做是一个特殊的distribute by+sort by，它具备二者的功能，但是只能实现倒序排序的方式,不能指定排序规则为asc 或者desc</p></li></ul><p><a href="https://blog.csdn.net/high2011/article/details/78012317" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/hive%E5%86%85%E9%83%A8%E8%A1%A8%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2019/09/24/docs/hive%E5%86%85%E9%83%A8%E8%A1%A8%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<h2 id="hive-内部表和外部表的区别"><a href="#hive-内部表和外部表的区别" class="headerlink" title="hive 内部表和外部表的区别"></a>hive 内部表和外部表的区别</h2><ul><li>建表时带有external关键字为外部表，否则为内部表</li><li>内部表和外部表建表时都可以自己指定location</li><li>删除表时，外部表不会删除对应的数据，只会删除元数据信息，内部表则会删除</li><li>其他用法是一样的</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/Hive%E4%B8%AD%E5%A4%A7%E8%A1%A8join%E5%B0%8F%E8%A1%A8%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/"/>
      <url>/2019/09/24/docs/Hive%E4%B8%AD%E5%A4%A7%E8%A1%A8join%E5%B0%8F%E8%A1%A8%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="Hive中大表join小表的优化方法"><a href="#Hive中大表join小表的优化方法" class="headerlink" title="Hive中大表join小表的优化方法"></a>Hive中大表join小表的优化方法</h2><p>在小表和大表进行join时，将<strong>小表放在前边</strong>，效率会高，hive会将小表进行缓存</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/hive%E4%B8%ADjoin%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B/"/>
      <url>/2019/09/24/docs/hive%E4%B8%ADjoin%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B/</url>
      
        <content type="html"><![CDATA[<h2 id="hive中join都有哪些"><a href="#hive中join都有哪些" class="headerlink" title="hive中join都有哪些"></a>hive中join都有哪些</h2><p>Hive中除了支持和传统数据库中一样的内关联（JOIN）、左关联（LEFT JOIN）、右关联（RIGHT JOIN）、全关联（FULL JOIN），还支持左半关联（LEFT SEMI JOIN）</p><ul><li><p><strong>内关联（JOIN）</strong></p><p>只返回能关联上的结果。</p></li><li><p><strong>左外关联（LEFT [OUTER] JOIN）</strong></p><p>以LEFT [OUTER] JOIN关键字前面的表作为主表，和其他表进行关联，返回记录和主表的记录数一致，关联不上的字段置为NULL。</p></li><li><p><strong>右外关联（RIGHT [OUTER] JOIN）</strong></p><p>和左外关联相反，以RIGTH [OUTER] JOIN关键词后面的表作为主表，和前面的表做关联，返回记录数和主表一致，关联不上的字段为NULL。</p></li><li><p><strong>全外关联（FULL [OUTER] JOIN）</strong></p><p>以两个表的记录为基准，返回两个表的记录去重之和，关联不上的字段为NULL。</p></li><li><p><strong>LEFT SEMI JOIN</strong></p><p>以LEFT SEMI JOIN关键字前面的表为主表，返回主表的KEY也在副表中的记录</p></li><li><p><strong>笛卡尔积关联（CROSS JOIN）</strong></p><p>返回两个表的笛卡尔积结果，不需要指定关联键。</p></li></ul><p><a href="http://lxw1234.com/archives/2015/06/315.htm" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/HiveUDF%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/09/24/docs/HiveUDF%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<h2 id="Hive-UDF简单介绍"><a href="#Hive-UDF简单介绍" class="headerlink" title="Hive UDF简单介绍"></a>Hive UDF简单介绍</h2><p>在Hive中，用户可以自定义一些函数，用于扩展HiveQL的功能，而这类函数叫做UDF（用户自定义函数）。UDF分为两大类：UDAF（用户自定义聚合函数）和UDTF（用户自定义表生成函数）。</p><p><strong>Hive有两个不同的接口编写UDF程序。一个是基础的UDF接口，一个是复杂的GenericUDF接口。</strong></p><ol><li>org.apache.hadoop.hive.ql. exec.UDF 基础UDF的函数读取和返回基本类型，即Hadoop和Hive的基本类型。如，Text、IntWritable、LongWritable、DoubleWritable等。</li><li>org.apache.hadoop.hive.ql.udf.generic.GenericUDF 复杂的GenericUDF可以处理Map、List、Set类型。</li></ol><p><a href="http://www.voidcn.com/article/p-suceexsl-vb.html" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/HiveToMR/"/>
      <url>/2019/09/24/docs/HiveToMR/</url>
      
        <content type="html"><![CDATA[<h2 id="Hive-Sql-是怎样解析成MR-job的"><a href="#Hive-Sql-是怎样解析成MR-job的" class="headerlink" title="Hive Sql 是怎样解析成MR job的?"></a>Hive Sql 是怎样解析成MR job的?</h2><p><strong>主要分为6个阶段:</strong></p><ol><li><p><strong>Hive使用Antlr实现语法解析</strong>.根据Antlr制定的SQL语法解析规则,完成SQL语句的词法/语法解析,将SQL转为抽象语法树AST.</p></li><li><p><strong>遍历AST,生成基本查询单元QueryBlock</strong>.QueryBlock是一条SQL最基本的组成单元，包括三个部分：输入源，计算过程，输出.</p></li><li><p><strong>遍历QueryBlock,生成OperatorTree</strong>.Hive最终生成的MapReduce任务，Map阶段和Reduce阶段均由OperatorTree组成。Operator就是在Map阶段或者Reduce阶段完成单一特定的操作。QueryBlock生成Operator Tree就是遍历上一个过程中生成的QB和QBParseInfo对象的保存语法的属性.</p></li><li><p><strong>优化OperatorTree.</strong>大部分逻辑层优化器通过变换OperatorTree，合并操作符，达到减少MapReduce Job，减少shuffle数据量的目的</p></li><li><p><strong>OperatorTree生成MapReduce Job</strong>.遍历OperatorTree,翻译成MR任务.</p><ul><li>对输出表生成MoveTask</li><li>从OperatorTree的其中一个根节点向下深度优先遍历</li><li>ReduceSinkOperator标示Map/Reduce的界限，多个Job间的界限</li><li>遍历其他根节点，遇过碰到JoinOperator合并MapReduceTask</li><li>生成StatTask更新元数据</li><li>剪断Map与Reduce间的Operator的关系</li></ul></li><li><p><strong>优化任务.</strong> 使用物理优化器对MR任务进行优化,生成最终执行任务</p></li></ol><p><a href="https://www.cnblogs.com/Dhouse/p/7132476.html" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/hdfs%E8%AF%BB%E6%B5%81%E7%A8%8B/"/>
      <url>/2019/09/24/docs/hdfs%E8%AF%BB%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="hdfs读流程"><a href="#hdfs读流程" class="headerlink" title="hdfs读流程"></a>hdfs读流程</h2><p><img src="//jiamaoxiang.top/2019/09/24/docs/hdfs读流程/pictures/hdfs%E8%AF%BB%E6%B5%81%E7%A8%8B.png" alt></p><ol><li>Client 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，获取文件 block 位置信息；</li><li>NameNode 返回存储的每个块的 DataNode 列表；</li><li>Client 将连接到列表中最近的 DataNode；</li><li>Client 开始从 DataNode 并行读取数据；</li><li>一旦 Client 获得了所有必须的 block，它就会将这些 block 组合起来形成一个文件。</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/hdfs%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6%E7%9A%84%E6%B5%81%E7%A8%8B/"/>
      <url>/2019/09/24/docs/hdfs%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6%E7%9A%84%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="hdfs创建一个文件的流程"><a href="#hdfs创建一个文件的流程" class="headerlink" title="hdfs创建一个文件的流程"></a>hdfs创建一个文件的流程</h2><ol><li>客户端通过ClientProtocol协议向RpcServer发起创建文件的RPC请求。</li><li>FSNamesystem封装了各种HDFS操作的实现细节，RpcServer调用FSNamesystem中的相关方法以创建目录。</li><li>进一步的，FSDirectory封装了各种目录树操作的实现细节，FSNamesystem调用FSDirectory中的相关方法在目录树中创建目标文件，并通过日志系统备份文件系统的修改。</li><li>最后，RpcServer将RPC响应返回给客户端。</li></ol><p><a href="https://monkeysayhi.github.io/2018/02/07/%E6%BA%90%E7%A0%81%7CHDFS%E4%B9%8BNameNode%EF%BC%9A%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6%EF%BC%881%EF%BC%89/" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/hdfs%E5%86%99%E6%B5%81%E7%A8%8B/"/>
      <url>/2019/09/24/docs/hdfs%E5%86%99%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="hdfs写流程"><a href="#hdfs写流程" class="headerlink" title="hdfs写流程"></a>hdfs写流程</h2><p><img src="//jiamaoxiang.top/2019/09/24/docs/hdfs写流程/pictures/hdfs%E5%86%99%E6%B5%81%E7%A8%8B.png" alt></p><ol><li>Client 调用 DistributedFileSystem 对象的 <code>create</code> 方法，创建一个文件输出流（FSDataOutputStream）对象；</li><li>通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，在 HDFS 的 Namespace 中创建一个文件条目（Entry），此时该条目没有任何的 Block，NameNode 会返回该数据每个块需要拷贝的 DataNode 地址信息；</li><li>通过 FSDataOutputStream 对象，开始向 DataNode 写入数据，数据首先被写入 FSDataOutputStream 对象内部的数据队列中，数据队列由 DataStreamer 使用，它通过选择合适的 DataNode 列表来存储副本，从而要求 NameNode 分配新的 block；</li><li>DataStreamer 将数据包以流式传输的方式传输到分配的第一个 DataNode 中，该数据流将数据包存储到第一个 DataNode 中并将其转发到第二个 DataNode 中，接着第二个 DataNode 节点会将数据包转发到第三个 DataNode 节点；</li><li>DataNode 确认数据传输完成，最后由第一个 DataNode 通知 client 数据写入成功；</li><li>完成向文件写入数据，Client 在文件输出流（FSDataOutputStream）对象上调用 <code>close</code> 方法，完成文件写入；</li><li>调用 DistributedFileSystem 对象的 complete 方法，通知 NameNode 文件写入成功，NameNode 会将相关结果记录到 editlog 中。</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/hbase%E7%9A%84HA%E5%AE%9E%E7%8E%B0/"/>
      <url>/2019/09/24/docs/hbase%E7%9A%84HA%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="hbase的HA实现-zookeeper在其中的作用"><a href="#hbase的HA实现-zookeeper在其中的作用" class="headerlink" title="hbase的HA实现,zookeeper在其中的作用"></a>hbase的HA实现,zookeeper在其中的作用</h2><p> HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行。<br>配置HBase高可用，只需要启动两个HMaster，让Zookeeper自己去选择一个Master Acitve即可</p><p>zk的在这里起到的作用就是用来管理master节点,以及帮助hbase做master选举</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/hbase%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1rowkey/"/>
      <url>/2019/09/24/docs/hbase%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1rowkey/</url>
      
        <content type="html"><![CDATA[<h2 id="hbase-如何设计rowkey"><a href="#hbase-如何设计rowkey" class="headerlink" title="hbase 如何设计rowkey"></a>hbase 如何设计rowkey</h2><ul><li><p><strong>RowKey长度原则</strong></p><p>Rowkey是一个二进制码流，Rowkey的长度被很多开发者建议说设计在10~100个字节，不过建议是越短越好，不要超过16个字节。</p><p>原因如下：</p><ul><li><p>数据的持久化文件HFile中是按照KeyValue存储的，如果Rowkey过长比如100个字节，1000万列数据光Rowkey就要占用100*1000万=10亿个字节，将近1G数据，这会极大影响HFile的存储效率；</p></li><li><p>MemStore将缓存部分数据到内存，如果Rowkey字段过长内存的有效利用率会降低，系统将无法缓存更多的数据，这会降低检索效率。因此Rowkey的字节长度越短越好。</p></li><li><p>目前操作系统是都是64位系统，内存8字节对齐。控制在16个字节，8字节的整数倍利用操作系统的最佳特性。</p></li></ul></li><li><p><strong>RowKey散列原则</strong></p><p>如果Rowkey是按时间戳的方式递增，不要将时间放在二进制码的前面，建议将Rowkey的高位作为散列字段，由程序循环生成，低位放时间字段，这样将提高数据均衡分布在每个Regionserver实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息将产生所有新数据都在一个RegionServer上堆积的热点现象，这样在做数据检索的时候负载将会集中在个别RegionServer，降低查询效率。</p></li><li><p><strong>RowKey唯一原则</strong></p><p>必须在设计上保证其唯一性。</p></li></ul><p><a href="https://zhuanlan.zhihu.com/p/30074408" target="_blank" rel="noopener">参考文章1</a></p><p><a href="http://www.nosqlnotes.com/technotes/hbase/hbase-rowkey-design/" target="_blank" rel="noopener">参考文章2</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/hadoop%E7%9A%84%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E6%9C%89%E5%93%AA%E4%BA%9B/"/>
      <url>/2019/09/24/docs/hadoop%E7%9A%84%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E6%9C%89%E5%93%AA%E4%BA%9B/</url>
      
        <content type="html"><![CDATA[<h2 id="hadoop的常用配置文件有哪些"><a href="#hadoop的常用配置文件有哪些" class="headerlink" title="hadoop的常用配置文件有哪些"></a>hadoop的常用配置文件有哪些</h2><ul><li><p><strong>hadoop-env.sh</strong>: 用于定义hadoop运行环境相关的配置信息，比如配置JAVA_HOME环境变量、为hadoop的JVM指定特定的选项、指定日志文件所在的目录路径以及master和slave文件的位置等；</p></li><li><p><strong>core-site.xml</strong>: 用于定义系统级别的参数，如HDFS URL、Hadoop的临时目录以及用于rack-aware集群中的配置文件的配置等，此中的参数定义会覆盖core-default.xml文件中的默认配置；</p></li><li><p><strong>hdfs-site.xml</strong>: HDFS的相关设定，如文件副本的个数、块大小及是否使用强制权限等，此中的参数定义会覆盖hdfs-default.xml文件中的默认配置；</p></li><li><p><strong>mapred-site.xml</strong>：HDFS的相关设定，如reduce任务的默认个数、任务所能够使用内存的默认上下限等，此中的参数定义会覆盖mapred-default.xml文件中的默认配置；</p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/hadoopHA%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/09/24/docs/hadoopHA%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<h2 id="hadoop-HA介绍"><a href="#hadoop-HA介绍" class="headerlink" title="hadoop HA介绍"></a>hadoop HA介绍</h2><p><img src="//jiamaoxiang.top/2019/09/24/docs/hadoopHA介绍/pictures/hdfs-ha.png" alt></p><ol><li><strong>Active NameNode 和 Standby NameNode</strong>：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务；</li><li><strong>ZKFailoverController（主备切换控制器，FC）</strong>：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换（当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换）；</li><li><strong>Zookeeper 集群</strong>：为主备切换控制器提供主备选举支持；</li><li><strong>共享存储系统</strong>：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在<strong>确认元数据完全同步之后才能继续对外提供服务</strong>。</li><li><strong>DataNode 节点</strong>：因为主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/hadoop1.x%E7%9A%84%E7%BC%BA%E7%82%B9/"/>
      <url>/2019/09/24/docs/hadoop1.x%E7%9A%84%E7%BC%BA%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<h2 id="hadoop1-x的缺点"><a href="#hadoop1-x的缺点" class="headerlink" title="hadoop1.x的缺点"></a>hadoop1.x的缺点</h2><ol><li>JobTracker存在单点故障的隐患</li><li>任务调度和资源管理全部是JobTracker来完成,单点负担过重</li><li>TaskTracker以Map/Reduce数量表示资源太过简单</li><li>TaskTracker 分Map Slot 和 Reduce Slot, 如果任务只需要map任务可能会造成资源浪费</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/hadoop1.x%E5%92%8Chadoop2.x%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2019/09/24/docs/hadoop1.x%E5%92%8Chadoop2.x%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<h2 id="hadoop1-x-和hadoop-2-x-的区别"><a href="#hadoop1-x-和hadoop-2-x-的区别" class="headerlink" title="hadoop1.x 和hadoop 2.x 的区别"></a>hadoop1.x 和hadoop 2.x 的区别</h2><ol><li><p><strong>资源调度方式的改变</strong></p><p>在1.x, 使用Jobtracker负责任务调度和资源管理,单点负担过重,在2.x中,新增了yarn作为集群的调度工具.在yarn中,使用ResourceManager进行 资源管理, 单独开启一个Container作为ApplicationMaster来进行任务管理.</p></li><li><p><strong>HA模式</strong></p><p>在1.x中没有HA模式,集群中只有一个NameNode,而在2.x中可以启用HA模式,存在一个Active NameNode 和Standby NameNode.</p></li><li><p><strong>HDFS Federation</strong></p><p>Hadoop 2.0中对HDFS进行了改进，使NameNode可以横向扩展成多个，每个NameNode分管一部分目录，进而产生了HDFS Federation，该机制的引入不仅增强了HDFS的扩展性，也使HDFS具备了隔离性</p></li></ol><p><img src="//jiamaoxiang.top/2019/09/24/docs/hadoop1.x和hadoop2.x的区别/pictures/hadoop1.jpg" alt></p><p><img src="//jiamaoxiang.top/2019/09/24/docs/hadoop1.x和hadoop2.x的区别/pictures/hadoop2.jpg" alt></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/flink%E7%9A%84%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B/"/>
      <url>/2019/09/24/docs/flink%E7%9A%84%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B/</url>
      
        <content type="html"><![CDATA[<h2 id="flink的部署模式都有哪些"><a href="#flink的部署模式都有哪些" class="headerlink" title="flink的部署模式都有哪些"></a>flink的部署模式都有哪些</h2><p><strong>flink可以以多种方式部署,包括standlone模式/yarn/Mesos/Kubernetes/Docker/AWS/Google Compute Engine/MAPR等</strong></p><p>一般公司中主要采用 on yarn模式</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/flink%E7%9A%84window%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6/"/>
      <url>/2019/09/24/docs/flink%E7%9A%84window%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h2 id="flink-的-window-实现机制"><a href="#flink-的-window-实现机制" class="headerlink" title="flink 的 window 实现机制"></a>flink 的 window 实现机制</h2><p>Flink 中定义一个窗口主要需要以下三个组件。</p><ul><li><p><strong>Window Assigner：</strong>用来决定某个元素被分配到哪个/哪些窗口中去。</p></li><li><p><strong>Trigger：</strong>触发器。决定了一个窗口何时能够被计算或清除，每个窗口都会拥有一个自己的Trigger。</p></li><li><p><strong>Evictor：</strong>可以译为“驱逐者”。在Trigger触发之后，在窗口被处理之前，Evictor（如果有Evictor的话）会用来剔除窗口中不需要的元素，相当于一个filter。</p></li></ul><h4 id="Window-的实现"><a href="#Window-的实现" class="headerlink" title="Window 的实现"></a>Window 的实现</h4><p><img src="//jiamaoxiang.top/2019/09/24/docs/flink的window实现机制/pictures/flink%E4%B8%ADwindow%E7%9A%84%E5%AE%9E%E7%8E%B0.png" alt></p><p>首先上图中的组件都位于一个算子（window operator）中，数据流源源不断地进入算子，每一个到达的元素都会被交给 WindowAssigner。WindowAssigner 会决定元素被放到哪个或哪些窗口（window），可能会创建新窗口。因为一个元素可以被放入多个窗口中，所以同时存在多个窗口是可能的。注意，<code>Window</code>本身只是一个ID标识符，其内部可能存储了一些元数据，如<code>TimeWindow</code>中有开始和结束时间，但是并不会存储窗口中的元素。窗口中的元素实际存储在 Key/Value State 中，key为<code>Window</code>，value为元素集合（或聚合值）。为了保证窗口的容错性，该实现依赖了 Flink 的 State 机制（参见 <a href="https://ci.apache.org/projects/flink/flink-docs-master/apis/streaming/state.html" target="_blank" rel="noopener">state 文档</a>）。</p><p>每一个窗口都拥有一个属于自己的 Trigger，Trigger上会有定时器，用来决定一个窗口何时能够被计算或清除。每当有元素加入到该窗口，或者之前注册的定时器超时了，那么Trigger都会被调用。Trigger的返回结果可以是 continue（不做任何操作），fire（处理窗口数据），purge（移除窗口和窗口中的数据），或者 fire + purge。一个Trigger的调用结果只是fire的话，那么会计算窗口并保留窗口原样，也就是说窗口中的数据仍然保留不变，等待下次Trigger fire的时候再次执行计算。一个窗口可以被重复计算多次知道它被 purge 了。在purge之前，窗口会一直占用着内存。</p><p>当Trigger fire了，窗口中的元素集合就会交给<code>Evictor</code>（如果指定了的话）。Evictor 主要用来遍历窗口中的元素列表，并决定最先进入窗口的多少个元素需要被移除。剩余的元素会交给用户指定的函数进行窗口的计算。如果没有 Evictor 的话，窗口中的所有元素会一起交给函数进行计算。</p><p>计算函数收到了窗口的元素（可能经过了 Evictor 的过滤），并计算出窗口的结果值，并发送给下游。窗口的结果值可以是一个也可以是多个。DataStream API 上可以接收不同类型的计算函数，包括预定义的<code>sum()</code>,<code>min()</code>,<code>max()</code>，还有 <code>ReduceFunction</code>，<code>FoldFunction</code>，还有<code>WindowFunction</code>。WindowFunction 是最通用的计算函数，其他的预定义的函数基本都是基于该函数实现的。</p><p>Flink 对于一些聚合类的窗口计算（如sum,min）做了优化，因为聚合类的计算不需要将窗口中的所有数据都保存下来，只需要保存一个result值就可以了。每个进入窗口的元素都会执行一次聚合函数并修改result值。这样可以大大降低内存的消耗并提升性能。但是如果用户定义了 Evictor，则不会启用对聚合窗口的优化，因为 Evictor 需要遍历窗口中的所有元素，必须要将窗口中所有元素都存下来。</p><p><a href="http://wuchong.me/blog/2016/05/25/flink-internals-window-mechanism/" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/flink%E7%9A%84window%E5%88%86%E7%B1%BB/"/>
      <url>/2019/09/24/docs/flink%E7%9A%84window%E5%88%86%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<h2 id="flink-的-window-分类"><a href="#flink-的-window-分类" class="headerlink" title="flink 的 window 分类"></a>flink 的 window 分类</h2><p><strong>flink中的窗口主要分为3大类共5种窗口</strong>:</p><p><img src="//jiamaoxiang.top/2019/09/24/docs/flink的window分类/pictures/flink%E4%B8%ADwindow%E5%88%86%E7%B1%BB.png" alt></p><ul><li><p><strong>Time Window 时间窗口</strong></p><ul><li><p><strong>Tumbing Time Window 滚动时间窗口</strong></p><p>实现统计每一分钟(或其他长度)窗口内 计算的效果</p></li><li><p><strong>Sliding Time Window 滑动时间窗口</strong></p><p>实现每过xxx时间 统计 xxx时间窗口的效果. 比如，我们可以每30秒计算一次最近一分钟用户购买的商品总数。</p></li></ul></li><li><p><strong>Count Window 计数窗口</strong></p><ul><li><p><strong>Tumbing Count Window  滚动计数窗口</strong></p><p>当我们想要每100个用户购买行为事件统计购买总数，那么每当窗口中填满100个元素了，就会对窗口进行计算，这种窗口我们称之为翻滚计数窗口（Tumbling Count Window）</p></li><li><p><strong>Sliding Count Window   滑动计数窗口</strong></p><p>和Sliding Time Window含义是类似的，例如计算每10个元素计算一次最近100个元素的总和</p></li></ul></li><li><p><strong>Session Window  会话窗口</strong></p><p>在这种用户交互事件流中，我们首先想到的是将事件聚合到会话窗口中（一段用户持续活跃的周期），由非活跃的间隙分隔开。如上图所示，就是需要计算每个用户在活跃期间总共购买的商品数量，如果用户30秒没有活动则视为会话断开（假设raw data stream是单个用户的购买行为流）</p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/flink%E7%9A%84state%E6%98%AF%E5%AD%98%E5%82%A8%E5%9C%A8%E5%93%AA%E9%87%8C%E7%9A%84/"/>
      <url>/2019/09/24/docs/flink%E7%9A%84state%E6%98%AF%E5%AD%98%E5%82%A8%E5%9C%A8%E5%93%AA%E9%87%8C%E7%9A%84/</url>
      
        <content type="html"><![CDATA[<h2 id="flink-的-state-是存储在哪里的"><a href="#flink-的-state-是存储在哪里的" class="headerlink" title="flink 的 state 是存储在哪里的"></a>flink 的 state 是存储在哪里的</h2><p>Apache Flink内部有四种state的存储实现，具体如下：</p><ul><li><strong>基于内存的HeapStateBackend</strong> - 在debug模式使用，不 建议在生产模式下应用；</li><li><strong>基于HDFS的FsStateBackend</strong> - 分布式文件持久化，每次读写都产生网络IO，整体性能不佳；</li><li><strong>基于RocksDB的RocksDBStateBackend</strong> - 本地文件+异步HDFS持久化；</li><li><strong>基于Niagara(Alibaba内部实现)NiagaraStateBackend</strong> - 分布式持久化- 在Alibaba生产环境应用；</li></ul><p><a href="https://juejin.im/post/5c87dbdbe51d45494c77d607" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/flink%E6%98%AF%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%8F%8D%E5%8E%8B%E7%9A%84/"/>
      <url>/2019/09/24/docs/flink%E6%98%AF%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%8F%8D%E5%8E%8B%E7%9A%84/</url>
      
        <content type="html"><![CDATA[<h2 id="flink是如何实现反压的"><a href="#flink是如何实现反压的" class="headerlink" title="flink是如何实现反压的"></a>flink是如何实现反压的</h2><p>flink的反压经历了两个发展阶段,分别是基于TCP的反压(&lt;1.5)和基于credit的反压(&gt;1.5)</p><ul><li><h4 id="基于-TCP-的反压"><a href="#基于-TCP-的反压" class="headerlink" title="基于 TCP 的反压"></a>基于 TCP 的反压</h4><p>flink中的消息发送通过RS(ResultPartition),消息接收通过IC(InputGate),两者的数据都是以 LocalBufferPool的形式来存储和提取,进一步的依托于Netty的NetworkBufferPool,之后更底层的便是依托于TCP的滑动窗口机制,当IC端的buffer池满了之后,两个task之间的滑动窗口大小便为0,此时RS端便无法再发送数据</p><p>基于TCP的反压最大的问题是会造成整个TaskManager端的反压,所有的task都会受到影响</p></li><li><h4 id="基于-Credit-的反压"><a href="#基于-Credit-的反压" class="headerlink" title="基于 Credit 的反压"></a>基于 Credit 的反压</h4><p>RS与IC之间通过backlog和credit来确定双方可以发送和接受的数据量的大小以提前感知,而不是通过TCP滑动窗口的形式来确定buffer的大小之后再进行反压</p><p><img src="//jiamaoxiang.top/2019/09/24/docs/flink是如何实现反压的/D:%5CNote%5Cbig-data-interview%5CBigData-Interview%5Cpictures%5Cflink%E5%9F%BA%E4%BA%8Ecredit%E7%9A%84%E5%8F%8D%E5%8E%8B.png" alt></p></li></ul><p><a href="https://www.bilibili.com/video/av55487329" target="_blank" rel="noopener">参考视频</a></p><p><a href="https://blog.csdn.net/u010376788/article/details/92086752" target="_blank" rel="noopener">参考文章1</a></p><p><a href="https://blog.csdn.net/u010376788/article/details/95047250" target="_blank" rel="noopener">参考文章2</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/flink%E5%85%B7%E4%BD%93%E6%98%AF%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0exactlyonce%E8%AF%AD%E4%B9%89/"/>
      <url>/2019/09/24/docs/flink%E5%85%B7%E4%BD%93%E6%98%AF%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0exactlyonce%E8%AF%AD%E4%B9%89/</url>
      
        <content type="html"><![CDATA[<h2 id="flink具体是如何实现exactly-once-语义"><a href="#flink具体是如何实现exactly-once-语义" class="headerlink" title="flink具体是如何实现exactly once 语义"></a>flink具体是如何实现exactly once 语义</h2><p>在谈到 flink 所实现的 exactly-once语义时,主要是2个层面上的,首先 flink在0.9版本以后已经实现了基于state的内部一致性语义, 在1.4版本以后也可以实现端到端 Exactly-Once语义</p><ul><li><h4 id="状态-Exactly-Once"><a href="#状态-Exactly-Once" class="headerlink" title="状态 Exactly-Once"></a>状态 Exactly-Once</h4><p>Flink 提供 exactly-once 的状态（state）投递语义，这为有状态的（stateful）计算提供了准确性保证。也就是状态是不会重复使用的,有且仅有一次消费</p></li></ul><p><img src="//jiamaoxiang.top/2019/09/24/docs/flink具体是如何实现exactlyonce语义/pictures/flink%E6%95%85%E9%9A%9C%E6%81%A2%E5%A4%8D.png" alt></p><p>​    这里需要注意的一点是如何理解state语义的exactly-once,并不是说在flink中的所有事件均只会处理一次,而是所有的事件所影响生成的state只有作用一次.</p><p>​    在上图中, 假设每两条消息后出发一次checkPoint操作,持久化一次state. TaskManager 在 处理完 event c 之后被shutdown, 这时候当 JobManager重启task之后, TaskManager  会从 checkpoint 1 处恢复状态,重新执行流处理,也就是说 此时 event c 事件 的的确确是会被再一次处理的. 那么 这里所说的一致性语义是何意思呢? 本身,flink每处理完一条数据都会记录当前进度到 state中, 也就是说在 故障前, 处理完 event c 这件事情已经记录到了state中,但是,由于在checkPoint 2 之前, 就已经发生了宕机,那么 event c 对于state的影响并没有被记录下来,对于整个flink内部系统来说就好像没有发生过一样, 在 故障恢复后, 当触发 checkpoint 2 时, event c 的 state才最终被保存下来. <strong>所以说,可以这样理解, 进入flink 系统中的 事件 永远只会被 一次state记录并checkpoint下来,而state是永远不会发生重复被消费的, 这也就是 flink内部的一致性语义,就叫做 状态 Exactly once.</strong></p><ul><li><h4 id="端到端（end-to-end）Exactly-Once"><a href="#端到端（end-to-end）Exactly-Once" class="headerlink" title="端到端（end-to-end）Exactly-Once"></a>端到端（end-to-end）Exactly-Once</h4></li></ul><p>2017年12月份发布的Apache Flink 1.4版本，引进了一个重要的特性：TwoPhaseCommitSinkFunction.，它抽取了两阶段提交协议的公共部分，使得构建端到端Excatly-Once的Flink程序变为了可能。这些外部系统包括Kafka0.11及以上的版本，以及一些其他的数据输入（data sources）和数据接收(data sink)。它提供了一个抽象层，需要用户自己手动去实现Exactly-Once语义.</p><p>为了提供端到端Exactly-Once语义，除了Flink应用程序本身的状态，Flink写入的外部存储也需要满足这个语义。也就是说，这些外部系统必须提供提交或者回滚的方法，然后通过Flink的checkpoint来协调</p><p><a href="https://www.whitewood.me/2018/10/16/Flink-Exactly-Once-%E6%8A%95%E9%80%92%E5%AE%9E%E7%8E%B0%E6%B5%85%E6%9E%90/" target="_blank" rel="noopener">参考文章1</a></p><p><a href="https://my.oschina.net/u/992559/blog/1819948" target="_blank" rel="noopener">参考文章2</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/flink%E4%B8%AD%E7%9A%84%E6%97%B6%E9%97%B4%E6%A6%82%E5%BF%B5/"/>
      <url>/2019/09/24/docs/flink%E4%B8%AD%E7%9A%84%E6%97%B6%E9%97%B4%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<h2 id="flink中的时间概念-eventTime-和-processTime的区别"><a href="#flink中的时间概念-eventTime-和-processTime的区别" class="headerlink" title="flink中的时间概念 , eventTime 和 processTime的区别"></a>flink中的时间概念 , eventTime 和 processTime的区别</h2><p>Flink中有三种时间概念,分别是 Processing Time、Event Time 和 Ingestion Time</p><ul><li><h4 id="Processing-Time"><a href="#Processing-Time" class="headerlink" title="Processing Time"></a>Processing Time</h4><p>Processing Time 是指事件被处理时机器的系统时间。</p><p>当流程序在 Processing Time 上运行时，所有基于时间的操作(如时间窗口)将使用当时机器的系统时间。每小时 Processing Time 窗口将包括在系统时钟指示整个小时之间到达特定操作的所有事件</p></li><li><h4 id="Event-Time"><a href="#Event-Time" class="headerlink" title="Event Time"></a>Event Time</h4><p>Event Time 是事件发生的时间，一般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。在 Event Time 中，时间取决于数据，而跟其他没什么关系。Event Time 程序必须指定如何生成 Event Time 水印，这是表示 Event Time 进度的机制</p></li><li><h4 id="Ingestion-Time"><a href="#Ingestion-Time" class="headerlink" title="Ingestion Time"></a>Ingestion Time</h4><p>Ingestion Time 是事件进入 Flink 的时间。 在源操作处，每个事件将源的当前时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳</p><p>Ingestion Time 在概念上位于 Event Time 和 Processing Time 之间。 与 Processing Time 相比，它稍微贵一些，但结果更可预测。因为 Ingestion Time 使用稳定的时间戳（在源处分配一次），所以对事件的不同窗口操作将引用相同的时间戳，而在 Processing Time 中，每个窗口操作符可以将事件分配给不同的窗口（基于机器系统时间和到达延迟）</p><p>与 Event Time 相比，Ingestion Time 程序无法处理任何无序事件或延迟数据，但程序不必指定如何生成水印</p></li></ul><p><a href="https://zhuanlan.zhihu.com/p/55322400" target="_blank" rel="noopener">参考文章</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/flink%E4%B8%AD%E7%9A%84sessionWindow%E6%80%8E%E6%A0%B7%E4%BD%BF%E7%94%A8/"/>
      <url>/2019/09/24/docs/flink%E4%B8%AD%E7%9A%84sessionWindow%E6%80%8E%E6%A0%B7%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="flink中的session-Window怎样使用"><a href="#flink中的session-Window怎样使用" class="headerlink" title="flink中的session Window怎样使用"></a>flink中的session Window怎样使用</h2><p>会话窗口主要是将某段时间内活跃度较高的数据聚合成一个窗口进行计算,窗口的触发条件是 Session Gap, 是指在规定的时间内如果没有数据活跃接入,则认为窗口结束,然后触发窗口结果</p><p>Session Windows窗口类型比较适合非连续性数据处理或周期性产生数据的场景,根据用户在线上某段时间内的活跃度对用户行为进行数据统计</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val sessionWindowStream = inputStream</span><br><span class="line">.keyBy(_.id)</span><br><span class="line">//使用EventTimeSessionWindow 定义 Event Time 滚动窗口</span><br><span class="line">.window(EventTimeSessionWindow.withGap(Time.milliseconds(10)))</span><br><span class="line">.process(......)</span><br></pre></td></tr></table></figure><p>Session Window 本质上没有固定的起止时间点,因此底层计算逻辑和Tumbling窗口及Sliding 窗口有一定的区别,</p><p>Session Window 为每个进入的数据都创建了一个窗口,最后再将距离窗口Session Gap 最近的窗口进行合并,然后计算窗口结果</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/ack%E6%9C%89%E5%93%AA%E5%87%A0%E7%A7%8D/"/>
      <url>/2019/09/24/docs/ack%E6%9C%89%E5%93%AA%E5%87%A0%E7%A7%8D/</url>
      
        <content type="html"><![CDATA[<h2 id="Ack-有哪几种-生产中怎样选择"><a href="#Ack-有哪几种-生产中怎样选择" class="headerlink" title="Ack 有哪几种, 生产中怎样选择?"></a>Ack 有哪几种, 生产中怎样选择?</h2><p>ack=0/1/-1的不同情况：</p><ul><li><p>Ack = 0</p><p>producer不等待broker的ack，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据；</p></li><li><p>Ack = 1</p><p>producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据；</p></li><li><p>Ack = -1</p><p>producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack，数据一般不会丢失，延迟时间长但是可靠性高。</p></li></ul><p><strong>生产中主要以 Ack=-1为主,如果压力过大,可切换为Ack=1. Ack=0的情况只能在测试中使用.</strong></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/09/24/docs/1%E6%9D%A1message%E4%B8%AD%E5%8C%85%E5%90%AB%E5%93%AA%E4%BA%9B%E4%BF%A1%E6%81%AF/"/>
      <url>/2019/09/24/docs/1%E6%9D%A1message%E4%B8%AD%E5%8C%85%E5%90%AB%E5%93%AA%E4%BA%9B%E4%BF%A1%E6%81%AF/</url>
      
        <content type="html"><![CDATA[<h2 id="1条message中包含哪些信息"><a href="#1条message中包含哪些信息" class="headerlink" title="1条message中包含哪些信息"></a>1条message中包含哪些信息</h2><table><thead><tr><th><strong>Field</strong></th><th>Description</th></tr></thead><tbody><tr><td>Attributes</td><td>该字节包含有关消息的元数据属性。 最低的2位包含用于消息的压缩编解码器。 其他位应设置为0。</td></tr><tr><td>Crc</td><td>CRC是消息字节的其余部分的CRC32。 这用于检查代理和使用者上的消息的完整性。</td></tr><tr><td></td><td>key是用于分区分配的可选参数。 key可以为null。</td></tr><tr><td>MagicByte</td><td>这是用于允许向后兼容的消息二进制格式演变的版本ID。 当前值为0。</td></tr><tr><td>Offset</td><td>这是kafka中用作日志序列号的偏移量。 当producer发送消息时，它实际上并不知道偏移量，并且可以填写它喜欢的任何值。</td></tr><tr><td>Value</td><td>该值是实际的消息内容，作为不透明的字节数组。 Kafka支持递归消息，在这种情况下，它本身可能包含消息集。 消息可以为null。</td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Impala使用的端口</title>
      <link href="/2019/08/29/Impala%E4%BD%BF%E7%94%A8%E7%9A%84%E7%AB%AF%E5%8F%A3/"/>
      <url>/2019/08/29/Impala%E4%BD%BF%E7%94%A8%E7%9A%84%E7%AB%AF%E5%8F%A3/</url>
      
        <content type="html"><![CDATA[<p>本文主要介绍了Impala所使用的端口号，在部署Impala的时候，确保下面列出的端口是开启的。</p><a id="more"></a><table><thead><tr><th align="center">组件</th><th>服务</th><th>端口</th><th align="center"><span style="white-space:nowrap;">访问需求&emsp;&emsp;</span></th><th>备注</th></tr></thead><tbody><tr><td align="center">Impala Daemon</td><td>Impala Daemon Frontend Port</td><td>21000</td><td align="center">外部</td><td>被 impala-shell, Beeswax, Cloudera ODBC 1.2 驱动 用于传递命令和接收结果</td></tr><tr><td align="center">Impala Daemon</td><td>Impala Daemon Frontend Port</td><td>21050</td><td align="center">外部</td><td>被使用 JDBC 或 Cloudera ODBC 2.0 及以上驱动的诸如 BI 工具之类的应用用来传递命令和接收结果</td></tr><tr><td align="center">Impala Daemon</td><td>Impala Daemon Backend Port</td><td>22000</td><td align="center">内部</td><td>仅内部使用。Impala</td></tr><tr><td align="center">Impala Daemon</td><td>StateStoreSubscriber Service Port</td><td>23000</td><td align="center">内部</td><td>仅内部使用。Impala 守护进程监听该端口接收来源于 state store 的更新</td></tr><tr><td align="center">Catalog Daemon</td><td>StateStoreSubscriber Service Port</td><td>23020</td><td align="center">内部</td><td>仅内部使用，catalog daemon监听该端口接收来源于 state store 的更新</td></tr><tr><td align="center">Impala Daemon</td><td>Impala Daemon HTTP Server Port</td><td>25000</td><td align="center">外部</td><td>Impala Daemon的Web端口，用于管理员监控和线上故障排查</td></tr><tr><td align="center">Impala StateStore Daemon</td><td>StateStore HTTP Server Port</td><td>25010</td><td align="center">外部</td><td>StateStore的Web端口，用于管理员监控和线上故障排查</td></tr><tr><td align="center">Impala Catalog Daemon</td><td>Catalog HTTP Server Port</td><td>25020</td><td align="center">外部</td><td>Catalog的Web端口，用于管理员监控和线上故障排查，从Impala1.2开始加入</td></tr><tr><td align="center">Impala StateStore Daemon</td><td>StateStore Service Port</td><td>24000</td><td align="center">内部</td><td>仅内部使用，statestore daemon监听的端口，用于registration/unregistration请求</td></tr><tr><td align="center">Impala Catalog Daemon</td><td>Catalog Service Port</td><td>26000</td><td align="center">内部</td><td>仅内部使用，catalog服务使用此端口与Impala Daemon进行通信，从Impala1.2开始加入</td></tr><tr><td align="center">Impala Daemon</td><td>KRPC Port</td><td>27000</td><td align="center">内部</td><td>仅内部使用，Impala daemon使用此端口进行基于krpc的相互通信。</td></tr></tbody></table><hr><p>Refrence:<a href="https://www.cloudera.com/documentation/enterprise/6/latest/topics/impala_ports.html#ports" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/6/latest/topics/impala_ports.html#ports</a></p>]]></content>
      
      
      <categories>
          
          <category> Impala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Impala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban安装部署</title>
      <link href="/2019/08/28/Azkaban%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
      <url>/2019/08/28/Azkaban%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<p>Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。</p><a id="more"></a><h1 id="安装前准备"><a href="#安装前准备" class="headerlink" title="安装前准备"></a>安装前准备</h1><p>(1)将Azkaban Web服务器、Azkaban执行服务器要安装机器的/opt/software目录下：  </p><ul><li>azkaban-web-server-2.5.0.tar.gz  </li><li>azkaban-executor-server-2.5.0.tar.gz  </li><li>azkaban-sql-script-2.5.0.tar.gz  </li><li>mysql-libs.zip  </li></ul><p>(2)目前azkaban只支持 mysql作为元数据库，需安装mysql，本文档中默认已安装好mysql服务器</p><h1 id="安装Azkaban"><a href="#安装Azkaban" class="headerlink" title="安装Azkaban"></a>安装Azkaban</h1><p>(1)在/opt/module/目录下创建azkaban目录<br>(2)解压azkaban-web-server-2.5.0.tar.gz、azkaban-executor-server-2.5.0.tar.gz、azkaban-sql-script-2.5.0.tar.gz到/opt/module/azkaban目录下<br>解压完成后的文件夹如下图所示：<br><img src="//jiamaoxiang.top/2019/08/28/Azkaban安装部署/1.png" alt><br>(3)初始化Azkaban的元数据库<br>登录mysql，创建azkaban的数据库，并执行脚本create-all-sql-2.5.0.sql，如下所示：  </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; create database azkaban;  </span><br><span class="line">Query OK, 1 row affected (0.00 sec)  </span><br><span class="line">mysql&gt; use azkaban;  </span><br><span class="line">Database changed  </span><br><span class="line">mysql&gt; <span class="built_in">source</span> /opt/module/azkaban/azkaban-2.5.0/create-all-sql-2.5.0.sql</span><br></pre></td></tr></table></figure><h2 id="创建SSL配置"><a href="#创建SSL配置" class="headerlink" title="创建SSL配置"></a>创建SSL配置</h2><p>(1)生成 keystore的密码及相应信息  </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban]$ keytool -keystore keystore -<span class="built_in">alias</span> jetty -genkey -keyalg RSA  </span><br><span class="line">输入keystore密码：   </span><br><span class="line">再次输入新密码:    </span><br><span class="line">您的名字与姓氏是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您的组织单位名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您的组织名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您所在的城市或区域名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您所在的州或省份名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">该单位的两字母国家代码是什么    </span><br><span class="line">[Unknown]：  CN    </span><br><span class="line">CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN 正确吗？  </span><br><span class="line">[否]：  y    </span><br><span class="line"> </span><br><span class="line">输入&lt;jetty&gt;的主密码    </span><br><span class="line">（如果和 keystore 密码相同，按回车）  ：</span><br></pre></td></tr></table></figure><p>(2)将keystore 考贝到 azkaban web服务器根目录中</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban]<span class="comment"># mv keystore  azkaban-web-2.5.0/</span></span><br></pre></td></tr></table></figure><h2 id="Web服务器配置"><a href="#Web服务器配置" class="headerlink" title="Web服务器配置"></a>Web服务器配置</h2><p>(1)进入azkaban web服务器安装目录 conf目录，修改azkaban.properties文件<br>(2)按照如下配置修改azkaban.properties文件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Azkaban Personalization Settings      </span></span><br><span class="line"><span class="comment">#服务器UI名称,用于服务器上方显示的名字      </span></span><br><span class="line">azkaban.name=Test      </span><br><span class="line"><span class="comment">#描述                            </span></span><br><span class="line">azkaban.label=My Local Azkaban     </span><br><span class="line"><span class="comment">#UI颜色                         </span></span><br><span class="line">azkaban.color=<span class="comment">#FF3601                                         </span></span><br><span class="line">azkaban.default.servlet.path=/index  </span><br><span class="line"><span class="comment">#根web目录,配置绝对路径，即web的目录  </span></span><br><span class="line">web.resource.dir=/opt/module/azkaban/azkaban-web-2.5.0/web   </span><br><span class="line"><span class="comment">#默认时区,已改为亚洲/上海 默认为美国                                            </span></span><br><span class="line">default.timezone.id=Asia/Shanghai                           </span><br><span class="line"><span class="comment">#用户权限管理默认类  </span></span><br><span class="line">user.manager.class=azkaban.user.XmlUserManager    </span><br><span class="line"><span class="comment">#用户配置,配置绝对路径，即azkaban-users.xml的路径     </span></span><br><span class="line">user.manager.xml.file=/opt/module/azkaban/azkaban-web-2.5.0/conf/azkaban-users.xml             </span><br><span class="line"><span class="comment">#Loader for projects .global配置文件所在位置,即global.properties绝对路径    </span></span><br><span class="line">executor.global.properties=/opt/module/azkaban/azkaban-executor-2.5.0/conf/global.properties    </span><br><span class="line">azkaban.project.dir=projects                                                 </span><br><span class="line"><span class="comment">#数据库类型  </span></span><br><span class="line">database.type=mysql                                                            </span><br><span class="line">mysql.port=3306                                                                  </span><br><span class="line">mysql.host=cdh01                                                    </span><br><span class="line">mysql.database=azkaban                                                      </span><br><span class="line">mysql.user=root  </span><br><span class="line"><span class="comment">#数据库密码                                                               </span></span><br><span class="line">mysql.password=123qwe                                                     </span><br><span class="line">mysql.numconnections=100                                                </span><br><span class="line"><span class="comment"># Velocity dev mode   </span></span><br><span class="line">velocity.dev.mode=<span class="literal">false</span>  </span><br><span class="line"><span class="comment"># Jetty服务器属性.  </span></span><br><span class="line"><span class="comment">#最大线程数     </span></span><br><span class="line">jetty.maxThreads=25   </span><br><span class="line"><span class="comment">#Jetty SSL端口                                                                 </span></span><br><span class="line">jetty.ssl.port=8443  </span><br><span class="line"><span class="comment">#Jetty端口                                                                      </span></span><br><span class="line">jetty.port=8081    </span><br><span class="line"><span class="comment">#SSL文件名,即keystore绝对路径                                                                              </span></span><br><span class="line">jetty.keystore=/opt/module/azkaban/azkaban-web-2.5.0/keystore    </span><br><span class="line"><span class="comment">#SSL文件密码,本配置与keystore密码相同                                                           </span></span><br><span class="line">jetty.password=123qwe  </span><br><span class="line"><span class="comment">#Jetty主密码 与 keystore文件相同                                                          </span></span><br><span class="line">jetty.keypassword=123qwe  </span><br><span class="line"><span class="comment">#SSL文件名,即keystore绝对路径                                                            </span></span><br><span class="line">jetty.truststore=/opt/module/azkaban/azkaban-web-2.5.0/keystore    </span><br><span class="line"><span class="comment"># SSL文件密码                                                             </span></span><br><span class="line">jetty.trustpassword=123qwe                                                    </span><br><span class="line"><span class="comment"># 执行服务器属性, 执行服务器端口  </span></span><br><span class="line">executor.port=12321                                                                </span><br><span class="line"><span class="comment"># 邮件设置,发送邮箱    </span></span><br><span class="line">mail.sender=xxxxxxxx@163.com    </span><br><span class="line"><span class="comment">#发送邮箱smtp地址                                           </span></span><br><span class="line">mail.host=smtp.163.com     </span><br><span class="line"><span class="comment">#发送邮件时显示的名称                                                            </span></span><br><span class="line">mail.user=xxxxxxxx  </span><br><span class="line"><span class="comment">#邮箱密码                                            </span></span><br><span class="line">mail.password=**********   </span><br><span class="line"><span class="comment">#任务失败时发送邮件的地址                                                        </span></span><br><span class="line">job.failure.email=xxxxxxxx@163.com   </span><br><span class="line"><span class="comment">#任务成功时发送邮件的地址                                 </span></span><br><span class="line">job.success.email=xxxxxxxx@163.com                            </span><br><span class="line">lockdown.create.projects=<span class="literal">false</span>    </span><br><span class="line"><span class="comment">#缓存目录                                            </span></span><br><span class="line">cache.directory=cache</span><br></pre></td></tr></table></figure><p>(3)web服务器用户配置<br>在azkaban web服务器安装目录 conf目录，按照如下配置修改azkaban-users.xml 文件，增加管理员用户。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;azkaban-users&gt;  </span><br><span class="line">       &lt;user username=<span class="string">"azkaban"</span> password=<span class="string">"azkaban"</span> roles=<span class="string">"admin"</span> groups=<span class="string">"azkaban"</span> /&gt;    </span><br><span class="line">       &lt;user username=<span class="string">"metrics"</span> password=<span class="string">"metrics"</span> roles=<span class="string">"metrics"</span>/&gt;  </span><br><span class="line">       &lt;user username=<span class="string">"admin"</span> password=<span class="string">"admin"</span> roles=<span class="string">"admin,metrics"</span> /&gt;  </span><br><span class="line">       &lt;role name=<span class="string">"admin"</span> permissions=<span class="string">"ADMIN"</span> /&gt;  </span><br><span class="line">       &lt;role name=<span class="string">"metrics"</span> permissions=<span class="string">"METRICS"</span>/&gt;    </span><br><span class="line">&lt;/azkaban-users&gt;</span><br></pre></td></tr></table></figure><h2 id="executor服务器配置"><a href="#executor服务器配置" class="headerlink" title="executor服务器配置"></a>executor服务器配置</h2><p>(1)进入executor安装目录，修改azkaban.properties</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Azkaban  </span></span><br><span class="line">default.timezone.id=Asia/Shanghai  </span><br><span class="line"><span class="comment"># Azkaban JobTypes Plugins  </span></span><br><span class="line">azkaban.jobtype.plugin.dir=./../plugins/jobtypes  </span><br><span class="line"><span class="comment">#Loader for projects  </span></span><br><span class="line">executor.global.properties=/opt/module/azkaban/azkaban-executor-2.5.0/conf/global.properties  </span><br><span class="line">azkaban.project.dir=projects  </span><br><span class="line">database.type=mysql  </span><br><span class="line">mysql.port=3306  </span><br><span class="line">mysql.host=cdh01  </span><br><span class="line">mysql.database=azkaban  </span><br><span class="line">mysql.user=root  </span><br><span class="line">mysql.password=123qwe    </span><br><span class="line">mysql.numconnections=100    </span><br><span class="line"><span class="comment"># Azkaban Executor settings  </span></span><br><span class="line">executor.maxThreads=50  </span><br><span class="line">executor.port=12321  </span><br><span class="line">executor.flow.threads=30</span><br></pre></td></tr></table></figure><h1 id="启动web服务器"><a href="#启动web服务器" class="headerlink" title="启动web服务器"></a>启动web服务器</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban-web-2.5.0]<span class="comment"># bin/azkaban-web-start.sh  &amp;</span></span><br></pre></td></tr></table></figure><h1 id="启动executor服务器"><a href="#启动executor服务器" class="headerlink" title="启动executor服务器"></a>启动executor服务器</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban-executor-2.5.0]<span class="comment"># bin/azkaban-executor-start.sh  &amp;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的数据类型</title>
      <link href="/2019/08/27/Flink%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
      <url>/2019/08/27/Flink%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>Flink使用type information来代表数据类型，Flink还具有一个类型提取系统，该系统分析函数的输入和返回类型，以自动获取类型信息(type information)，从而获得序列化程序和反序列化程序。但是，在某些情况下，例如lambda函数或泛型类型，需要显式地提供类型信息((type information)),从而提高其性能。本文主要讨论包括：(1)Flink支持的数据类型,(2)如何为数据类型创建type information，（3）如果无法自动推断函数的返回类型，如何使用提示(hints)来帮助Flink的类型系统识别类型信息。</p><a id="more"></a><h2 id="支持的数据类型"><a href="#支持的数据类型" class="headerlink" title="支持的数据类型"></a>支持的数据类型</h2><p>Flink支持Java和Scala中所有常见的数据类型，使用比较广泛的类型主要包括以下五种：</p><ul><li>原始类型  </li><li>Java和Scala的tuple类型  </li><li>Scala样例类  </li><li>POJO类型  </li><li>一些特殊的类型  </li></ul><p><strong>NOTE：</strong>不能被处理的类型将会被视为普通的数据类型，通过Kyro序列化框架进行序列化。</p><h3 id="原始类型"><a href="#原始类型" class="headerlink" title="原始类型"></a>原始类型</h3><p>Flink支持所有Java和Scala的原始类型，比如Int(Java中的Integer)，String、Double等。下面的例子是处理一个Long类型的数据流，处理每个元素+1  </p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> numbers: <span class="type">DataStream</span>[<span class="type">Long</span>] = env.fromElements(<span class="number">1</span>L, <span class="number">2</span>L,<span class="number">3</span>L, <span class="number">4</span>L)  </span><br><span class="line">numbers.map( n =&gt; n + <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="Java和Scala的tuple类型"><a href="#Java和Scala的tuple类型" class="headerlink" title="Java和Scala的tuple类型"></a>Java和Scala的tuple类型</h3><p>基于Scala的DataStream API使用的Scala的tuple。下面的例子是过滤一个具有两个字段的tuple类型的数据流.  </p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// DataStream of Tuple2[String, Integer] for Person(name,age)  </span></span><br><span class="line"><span class="keyword">val</span> persons: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Integer</span>)] = env.fromElements((<span class="string">"Adam"</span>, <span class="number">17</span>),(<span class="string">"Sarah"</span>, <span class="number">23</span>))  </span><br><span class="line"><span class="comment">// filter for persons of age &gt; 18  </span></span><br><span class="line">persons.filter(p =&gt; p._2 &gt; <span class="number">18</span>)</span><br></pre></td></tr></table></figure><p>Flink提供了有效的Java tuple实现，Flink的Java tuple最多包括25个字段，分别为tuple1，tuple2，直到tuple25，tuple类型是强类型的。使用Java DataStream API重写上面的例子:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// DataStream of Tuple2&lt;String, Integer&gt; for Person(name,age)  </span></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; persons =env.fromElements(Tuple2.of(<span class="string">"Adam"</span>, <span class="number">17</span>),Tuple2.of(<span class="string">"Sarah"</span>,<span class="number">23</span>));  </span><br><span class="line"><span class="comment">// filter for persons of age &gt; 18  </span></span><br><span class="line">persons.filter(p -&gt; p.f1 &gt; <span class="number">18</span>);</span><br></pre></td></tr></table></figure><p>Tuple字段可以通过使用f0，f1，f2的形式访问，也可以通过getField(int pos)方法访问，参数的索引起始值为0，比如:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Tuple2&lt;String, Integer&gt; personTuple = Tuple2.of(<span class="string">"Alex"</span>,<span class="string">"42"</span>);  </span><br><span class="line">Integer age = personTuple.getField(<span class="number">1</span>); <span class="comment">// age = 42</span></span><br></pre></td></tr></table></figure><p>与Scala相比，Flink的Java tuple是可变的，所以tuple的元素值是可以被重新复制的。Function可以重用Java tuple,从而减小垃圾回收的压力。下面的例子展示了如何更新一个tuple字段值</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">personTuple.f1 = <span class="number">42</span>; <span class="comment">// set the 2nd field to 42     </span></span><br><span class="line">personTuple.setField(<span class="number">43</span>, <span class="number">1</span>); <span class="comment">// set the 2nd field to 43</span></span><br></pre></td></tr></table></figure><h3 id="Scala的样例类"><a href="#Scala的样例类" class="headerlink" title="Scala的样例类"></a>Scala的样例类</h3><p>Flink支持Scala的样例类，可以通过字段名称来访问样例类的字段，下面的例子定义了一个<code>Person</code>样例类，该样例类有两个字段：<code>name</code>和<code>age</code>,按<code>age</code>过滤DataStream，如下所示</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)  </span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">persons</span></span>: <span class="type">DataStream</span>[<span class="type">Person</span>] = env.fromElements(<span class="type">Person</span>(<span class="string">"Adam"</span>, <span class="number">17</span>),<span class="type">Person</span>(<span class="string">"Sarah"</span>, <span class="number">23</span>))  </span><br><span class="line"><span class="comment">// filter for persons with age &gt; 18  </span></span><br><span class="line">persons.filter(p =&gt; p.age &gt; <span class="number">18</span>)</span><br></pre></td></tr></table></figure><h3 id="POJO"><a href="#POJO" class="headerlink" title="POJO"></a>POJO</h3><p>Flink接受的POJO类型需满足以下条件：</p><ul><li>public 类  </li><li>无参的共有构造方法  </li><li>所有字段都是public的，可以通过getter和setter方法访问  </li><li>所有字段类型必须是Flink能够支持的<br>下面的例子定义一个<code>Person</code>POJO</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;  </span><br><span class="line"><span class="comment">// both fields are public  </span></span><br><span class="line"><span class="keyword">public</span> String name;  </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">int</span> age;  </span><br><span class="line"><span class="comment">// default constructor is present  </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">()</span> </span>&#123;&#125;  </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(String name, <span class="keyword">int</span> age)</span> </span>&#123;  </span><br><span class="line"><span class="keyword">this</span>.name = name;  </span><br><span class="line"><span class="keyword">this</span>.age = age;  </span><br><span class="line">&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">DataStream&lt;Person&gt; persons = env.fromElements(   </span><br><span class="line"><span class="keyword">new</span> Person(<span class="string">"Alex"</span>, <span class="number">42</span>),  </span><br><span class="line"><span class="keyword">new</span> Person(<span class="string">"Wendy"</span>, <span class="number">23</span>));</span><br></pre></td></tr></table></figure><h3 id="一些特殊的类型"><a href="#一些特殊的类型" class="headerlink" title="一些特殊的类型"></a>一些特殊的类型</h3><p>Flink支持一些有特殊作用的数据类型，比如Array，Java中的ArrayList、HashMap和Enum等，也支持Hadoop的Writable类型。  </p><h2 id="为数据类型创建类型信息-type-information"><a href="#为数据类型创建类型信息-type-information" class="headerlink" title="为数据类型创建类型信息(type information)"></a>为数据类型创建类型信息(type information)</h2><h2 id="显示地指定类型信息-type-information"><a href="#显示地指定类型信息-type-information" class="headerlink" title="显示地指定类型信息(type information)"></a>显示地指定类型信息(type information)</h2>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于SparkStreaming的日志分析项目</title>
      <link href="/2019/08/26/%E5%9F%BA%E4%BA%8ESparkStreaming%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E9%A1%B9%E7%9B%AE/"/>
      <url>/2019/08/26/%E5%9F%BA%E4%BA%8ESparkStreaming%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E9%A1%B9%E7%9B%AE/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp;基于SparkStreaming实现实时的日志分析，首先基于discuz搭建一个论坛平台，然后将该论坛的日志写入到指定文件，最后通过SparkStreaming实时对日志进行分析。</p><a id="more"></a><h1 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h1><ul><li>统计指定时间段的热门文章</li></ul><ul><li>统计指定时间段内的最受欢迎的用户（以 ip 为单位）</li></ul><ul><li>统计指定时间段内的不同模块的访问量  </li></ul><h1 id="项目架构"><a href="#项目架构" class="headerlink" title="项目架构"></a>项目架构</h1><p><img src="//jiamaoxiang.top/2019/08/26/基于SparkStreaming的日志分析项目/%E9%A1%B9%E7%9B%AE%E6%9E%B6%E6%9E%84.png" alt></p><h1 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h1><p>resources<br>&emsp;&emsp;&emsp;&emsp;access_log.txt:日志样例<br>&emsp;&emsp;&emsp;&emsp;log_sta.conf ：配置文件<br>scala.com.jmx.analysis<br>&emsp;&emsp;&emsp;&emsp;AccessLogParser.scala :日志解析<br>&emsp;&emsp;&emsp;&emsp;logAnalysis：日志分析<br>scala.com.jmx.util<br>&emsp;&emsp;&emsp;&emsp;Utility.scala:工具类<br>scala<br>&emsp;&emsp;&emsp;&emsp;Run：驱动程序(main)<br>具体代码详见<a href="https://github.com/jiamx/log_analysis" target="_blank" rel="noopener">github</a></p><h1 id="搭建discuz论坛"><a href="#搭建discuz论坛" class="headerlink" title="搭建discuz论坛"></a>搭建discuz论坛</h1><h2 id="安装XAMPP"><a href="#安装XAMPP" class="headerlink" title="安装XAMPP"></a>安装XAMPP</h2><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><p><code>wget https://www.apachefriends.org/xampp-files/5.6.33/xampp-linux-x64-5.6.33-0-installer.run</code></p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><code># 赋予文件执行权限</code><br><code>chmod u+x xampp-linux-x64-5.6.33-0-installer.run</code><br><code># 运行安装文件</code><br>`./xampp-linux-x64-5.6.33-0-installer.run``</p><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><p>将以下内容加入到 ~/.bash_profile<br><code>export XAMPP=/opt/lampp/</code><br><code>export PATH=$PATH:$XAMPP:$XAMPP/bin</code>   </p><h3 id="刷新环境变量"><a href="#刷新环境变量" class="headerlink" title="刷新环境变量"></a>刷新环境变量</h3><p><code>source ~/.bash_profile</code></p><h3 id="启动XAMPP"><a href="#启动XAMPP" class="headerlink" title="启动XAMPP"></a>启动XAMPP</h3><p><code>xampp restart</code></p><h2 id="root用户密码和权限修改"><a href="#root用户密码和权限修改" class="headerlink" title="root用户密码和权限修改"></a>root用户密码和权限修改</h2><p><code>#修改root用户密码为123</code><br><code>update mysql.user set password=PASSWORD(&#39;123&#39;) where user=&#39;root&#39;;</code><br><code>flush privileges;</code><br><code>#赋予root用户远程登录权限</code><br><code>grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;123&#39; with grant option;</code><br><code>flush privileges;</code>  </p><h2 id="安装Discuz"><a href="#安装Discuz" class="headerlink" title="安装Discuz"></a>安装Discuz</h2><h3 id="下载discuz"><a href="#下载discuz" class="headerlink" title="下载discuz"></a>下载discuz</h3><p><code>wget http://download.comsenz.com/DiscuzX/3.2/Discuz_X3.2_SC_UTF8.zip</code>  </p><h3 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h3><p><code>#删除原有的web应用</code><br><code>rm -rf /opt/lampp/htdocs/*</code><br><code>unzip Discuz_X3.2_SC_UTF8.zip –d /opt/lampp/htdocs/</code><br><code>cd /opt/lampp/htdocs/</code><br><code>mv upload/*</code><br><code>#修改目录权限</code><br><code>chmod 777 -R /opt/lampp/htdocs/config/</code><br><code>chmod 777 -R /opt/lampp/htdocs/data/</code><br><code>chmod 777 -R /opt/lampp/htdocs/uc_client/</code><br><code>chmod 777 -R /opt/lampp/htdocs/uc_server/</code>  </p><h2 id="Discuz基本操作"><a href="#Discuz基本操作" class="headerlink" title="Discuz基本操作"></a>Discuz基本操作</h2><h3 id="自定义版块"><a href="#自定义版块" class="headerlink" title="自定义版块"></a>自定义版块</h3><ul><li>进入discuz后台：<a href="http://slave1/admin.php" target="_blank" rel="noopener">http://slave1/admin.php</a>  </li><li>点击顶部的“论坛”菜单  </li><li>按照页面提示创建所需版本，可以创建父子版块  </li></ul><h3 id="查看访问日志"><a href="#查看访问日志" class="headerlink" title="查看访问日志"></a>查看访问日志</h3><p>日志默认地址<br><code>/opt/lampp/logs/access_log</code><br>实时查看日志命令<br><code>tail –f /opt/lampp/logs/access_log</code>  </p><h2 id="Discuz帖子-版块存储简介"><a href="#Discuz帖子-版块存储简介" class="headerlink" title="Discuz帖子/版块存储简介"></a>Discuz帖子/版块存储简介</h2><p><code>mysql -uroot -p123 ultrax # 登录ultrax数据库</code><br><code>查看包含帖子id及标题对应关系的表</code><br><code>#tid, subject（文章id、标题）</code><br><code>select tid, subject from pre_forum_post limit 10;</code><br><code>#fid, name（版块id、标题）</code><br><code>select fid, name from pre_forum_forum limit 40;</code>  </p><h2 id="修改日志格式"><a href="#修改日志格式" class="headerlink" title="修改日志格式"></a>修改日志格式</h2><h3 id="找到Apache配置文件"><a href="#找到Apache配置文件" class="headerlink" title="找到Apache配置文件"></a>找到Apache配置文件</h3><p>Apache配置文件名称为httpd.conf，所在目录为 /opt/lampp/etc/ ，完整路径为 /opt/lampp/etc/httpd.conf</p><h3 id="修改日志格式-1"><a href="#修改日志格式-1" class="headerlink" title="修改日志格式"></a>修改日志格式</h3><p>关闭通用日志文件的使用<br><code>CustomLog &quot;logs/access_log&quot; common</code><br>启用组合日志文件<br><code>CustomLog &quot;logs/access_log&quot; combined</code><br>重新加载配置文件<br><code>xampp reload</code><br>检查访问日志<br><code>tail -f /opt/lampp/logs/access_log</code>  </p><h3 id="Flume与Kafka配置"><a href="#Flume与Kafka配置" class="headerlink" title="Flume与Kafka配置"></a>Flume与Kafka配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#agent的名称为a1  </span><br><span class="line">a1.sources = source1  </span><br><span class="line">a1.channels = channel1  </span><br><span class="line">a1.sinks = sink1</span><br><span class="line">#set source</span><br><span class="line">a1.sources.source1.type = TAILDIR  </span><br><span class="line">a1.sources.source1.filegroups = f1  </span><br><span class="line">a1.sources.source1.filegroups.f1 = /opt/lampp/logs/access_log  </span><br><span class="line">a1sources.source1.fileHeader = flase  </span><br><span class="line">#set sink</span><br><span class="line">a1.sinks.sink1.type = org.apache.flume.sink.kafka.KafkaSink  </span><br><span class="line">a1.sinks.sink1.brokerList=kms-2.apache.com:9092,kms-3.apache.com:9092,kms-4.apache.com:9092    </span><br><span class="line">​a1.sinks.sink1.topic= discuzlog  </span><br><span class="line">​a1.sinks.sink1.kafka.flumeBatchSize = 20  </span><br><span class="line">​a1.sinks.sink1.kafka.producer.acks = 1  </span><br><span class="line">​a1.sinks.sink1.kafka.producer.linger.ms = 1  </span><br><span class="line">​a1.sinks.sink1.kafka.producer.compression.type = snappy  </span><br><span class="line">#set channel</span><br><span class="line">​a1.channels.channel1.type = file  </span><br><span class="line">​a1.channels.channel1.checkpointDir = /home/kms/data/flume_data/checkpoint  </span><br><span class="line">​a1.channels.channel1.dataDirs= /home/kms/data/flume_data/data  </span><br><span class="line">#bind</span><br><span class="line">​a1.sources.source1.channels = channel1  </span><br><span class="line">​a1.sinks.sink1.channel = channel1</span><br></pre></td></tr></table></figure><h2 id="创建MySQL数据库和所需要的表"><a href="#创建MySQL数据库和所需要的表" class="headerlink" title="创建MySQL数据库和所需要的表"></a>创建MySQL数据库和所需要的表</h2><p><strong>创建数据库</strong>  </p><p><code>CREATE DATABASE</code>statistics<code>CHARACTER SET &#39;utf8&#39; COLLATE &#39;utf8_general_ci&#39;;</code>  </p><p><strong>创建表:</strong><br>&emsp;&emsp;特定时间段内不同ip的访问次数：client_ip_access<br>CREATE TABLE client_ip_access (<br>&emsp;&emsp;&emsp;&emsp;client_ip text COMMENT ‘客户端ip’,<br>&emsp;&emsp;&emsp;&emsp;sum BIGINT ( 20 ) NOT NULL COMMENT ‘访问次数’,<br>&emsp;&emsp;&emsp;&emsp;time text NOT NULL COMMENT ‘统计时间’<br>) ENGINE = INNODB DEFAULT CHARSET = utf8;  </p><p>&emsp;&emsp;特定时间段内不同文章的访问次数：hot_article<br>CREATE TABLE hot_article (<br>&emsp;&emsp;&emsp;&emsp;article_id text COMMENT ‘文章id’,<br>&emsp;&emsp;&emsp;&emsp;subject text NOT NULL COMMENT ‘文章标题’,<br>&emsp;&emsp;&emsp;&emsp;sum BIGINT ( 20 ) NOT NULL COMMENT ‘访问次数’,<br>&emsp;&emsp;&emsp;&emsp;time text NOT NULL COMMENT ‘统计时间’<br>) ENGINE = INNODB DEFAULT CHARSET = utf8;  </p><p>&emsp;&emsp;特定时间段内不同版块的访问次数：hot_section<br>CREATE TABLE hot_section (<br>&emsp;&emsp;&emsp;&emsp;section_id text COMMENT ‘版块id’,<br>&emsp;&emsp;&emsp;&emsp;name text NOT NULL COMMENT ‘版块标题’,<br>&emsp;&emsp;&emsp;&emsp;sum BIGINT ( 20 ) NOT NULL COMMENT ‘访问次数’,<br>&emsp;&emsp;&emsp;&emsp;time text NOT NULL COMMENT ‘统计时间’<br>) ENGINE = INNODB DEFAULT CHARSET = utf8; </p><h2 id="打包部署存在的问题"><a href="#打包部署存在的问题" class="headerlink" title="打包部署存在的问题"></a>打包部署存在的问题</h2><p><strong>问题1</strong> </p><pre><code>Exception in thread &quot;main&quot; java.lang.SecurityException: Invalid signature file digest for Manifest main attributes</code></pre><p><strong>解决方式</strong>  </p><p>原因:使用sbt打包的时候导致某些包的重复引用，所以打包之后的META-INF的目录下多出了一些<em>.SF,</em>.DSA,*.RSA文件  </p><p>解决办法：删除掉多于的<em>.SF,</em>.DSA,*.RSA文件  </p><p><code>zip -d log_analysis-1.0-SNAPSHOT.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF</code>  </p><p><strong>问题2</strong>    </p><pre><code>Exception in thread &quot;main&quot; java.io.FileNotFoundException: File file:/data/spark_data/history/event-log does not exist</code></pre><p><strong>解决方式</strong>  </p><p>原因:由于spark的spark-defaults.conf配置文件中配置 eventLog 时指定的路径在本机不存在。  </p><p>解决办法：创建对应的文件夹，并赋予对应权限<br><code>sudo mkdir -p /data/spark_data/history/spark-events</code><br><code>sudo mkdir -p /data/spark_data/history/event-log</code><br><code>sudo chmod 777 -R /data</code>  </p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>首先，基于discuz搭建了论坛，针对论坛产生的日志，对其进行分析。主要的处理流程为log—&gt;flume—&gt;kafka—&gt;sparkstreaming—&gt;MySQL,最后将处理的结果写入MySQL共报表查询。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的状态后端(State Backends)</title>
      <link href="/2019/08/23/Flink%E7%9A%84%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF-State-Backends/"/>
      <url>/2019/08/23/Flink%E7%9A%84%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF-State-Backends/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp;当使用checkpoint时，状态(state)会被持久化到checkpoint上，以防止数据的丢失并确保发生故障时能够完全恢复。状态是通过什么方式在哪里持久化，取决于使用的状态后端。</p><a id="more"></a><h2 id="可用的状态后端"><a href="#可用的状态后端" class="headerlink" title="可用的状态后端"></a>可用的状态后端</h2><p><strong>MemoryStateBackend</strong><br><strong>FsStateBackend</strong><br><strong>FsStateBackend</strong>  </p><p>注意：如果什么都不配置，系统默认的是MemoryStateBackend</p><h2 id="MemoryStateBackend"><a href="#MemoryStateBackend" class="headerlink" title="MemoryStateBackend"></a>MemoryStateBackend</h2><p><img src="//jiamaoxiang.top/2019/08/23/Flink的状态后端-State-Backends/memorystatebackend.png" alt><br>&emsp;&emsp;<code>MemoryStateBackend</code> 是将状态维护在 Java 堆上的一个内部状态后端。键值状态和窗口算子使用哈希表来存储数据（values）和定时器（timers）。当应用程序 checkpoint 时，此后端会在将状态发给 JobManager 之前快照下状态，JobManager 也将状态存储在 Java 堆上。默认情况下，<code>MemoryStateBackend</code> 配置成支持异步快照。异步快照可以避免阻塞数据流的处理，从而避免反压的发生。当然，使用 <code>new MemoryStateBackend(MAX_MEM_STATE_SIZE, false)</code>也可以禁用该特点。</p><p><strong>缺点</strong>：</p><ul><li>默认情况下，每一个状态的大小限制为 5 MB。可以通过 <code>MemoryStateBackend</code> 的构造函数增加这个大小。状态大小受到 akka 帧大小的限制(maxStateSize &lt;= akka.framesize 默认 10 M)，所以无论怎么调整状态大小配置，都不能大于 akka 的帧大小。也可以通过 akka.framesize 调整 akka 帧大小。</li><li>状态的总大小不能超过 JobManager 的内存。</li></ul><p><strong>推荐使用的场景</strong>：</p><ul><li>本地测试、几乎无状态的作业，比如 ETL、JobManager 不容易挂，或挂掉影响不大的情况。</li><li>不推荐在生产场景使用。</li></ul><h2 id="FsStateBackend"><a href="#FsStateBackend" class="headerlink" title="FsStateBackend"></a>FsStateBackend</h2><p><img src="//jiamaoxiang.top/2019/08/23/Flink的状态后端-State-Backends/fsstatebackend.png" alt><br>&emsp;&emsp;<code>FsStateBackend</code>需要配置的主要是文件系统，如 URL（类型，地址，路径）。比如可以是：<br><code>“hdfs://namenode:40010/flink/checkpoints”</code> 或<code>“s3://flink/checkpoints”</code></p><p>&emsp;&emsp;当选择使用 <code>FsStateBackend</code>时，正在进行的数据会被存在TaskManager的内存中。在checkpoint时，此后端会将状态快照写入配置的文件系统和目录的文件中，同时会在JobManager的内存中（在高可用场景下会存在 Zookeeper 中）存储极少的元数据。容量限制上，单 TaskManager 上 State 总量不超过它的内存，总大小不超过配置的文件系统容量。</p><p>&emsp;&emsp;默认情况下，<code>FsStateBackend</code> 配置成提供异步快照，以避免在状态 checkpoint 时阻塞数据流的处理。该特性可以实例化 <code>FsStateBackend</code> 时传入false的布尔标志来禁用掉，例如：<code>new FsStateBackend(path, false)</code></p><p><strong>推荐使用的场景</strong>：</p><ul><li>处理大状态，长窗口，或大键值状态的有状态处理任务， 例如分钟级窗口聚合或 join。</li><li>适合用于高可用方案（需要开启HA的作业）。</li><li>可以在生产环境中使用</li></ul><h2 id="RocksDBStateBackend"><a href="#RocksDBStateBackend" class="headerlink" title="RocksDBStateBackend"></a>RocksDBStateBackend</h2><p><img src="//jiamaoxiang.top/2019/08/23/Flink的状态后端-State-Backends/rocksdbstatebackend.png" alt><br>&emsp;&emsp;<code>RocksDBStateBackend</code> 的配置也需要一个文件系统（类型，地址，路径），如下所示：<br>“hdfs://namenode:40010/flink/checkpoints” 或“s3://flink/checkpoints”<br>RocksDB 是一种嵌入式的本地数据库。RocksDBStateBackend 将处理中的数据使用 <a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a> 存储在本地磁盘上。在 checkpoint 时，整个 RocksDB 数据库会被存储到配置的文件系统中，或者在超大状态作业时可以将增量的数据存储到配置的文件系统中。同时 Flink 会将极少的元数据存储在 JobManager 的内存中，或者在 Zookeeper 中（对于高可用的情况）。<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a> 默认也是配置成异步快照的模式。</p><p>&emsp;&emsp;<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>是一个 key/value 的内存存储系统，和其他的 key/value 一样，先将状态放到内存中，如果内存快满时，则写入到磁盘中，但需要注意<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>不支持同步的 Checkpoint，构造方法中没有同步快照这个选项。不过<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>支持增量的 Checkpoint，也是目前唯一增量 Checkpoint 的 Backend，意味着并不需要把所有 sst 文件上传到 Checkpoint 目录，仅需要上传新生成的 sst 文件即可。它的 Checkpoint 存储在外部文件系统（本地或HDFS），其容量限制只要单个 TaskManager 上 State 总量不超过它的内存+磁盘，单Key最大2G，总大小不超过配置的文件系统容量即可。</p><p><strong>缺点</strong>：</p><ul><li><a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>支持的单key和单value的大小最大为每个 2^31 字节。这是因为 RocksDB 的 JNI API 是基于byte[]的。<br></li><li>对于使用具有合并操作的状态的应用程序，例如 ListState，随着时间可能会累积到超过 2^31 字节大小，这将会导致在接下来的查询中失败。</li></ul><p><strong>推荐使用的场景</strong>：</p><ul><li>最适合用于处理大状态，长窗口，或大键值状态的有状态处理任务。</li><li>非常适合用于高可用方案。</li><li>最好是对状态读写性能要求不高的作业</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;那如何选择状态的类型和存储方式？结合前面的内容，可以看到，首先是要分析清楚业务场景；比如想要做什么，状态到底大不大。比较各个方案的利弊，选择根据需求合适的状态类型和存储方式即可。</p><hr><p><strong>Reference</strong></p><p>[1]<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/state/state_backends.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/state/state_backends.html</a><br>[2]<a href="https://ververica.cn/developers/state-management/" target="_blank" rel="noopener">https://ververica.cn/developers/state-management/</a><br>[3]<a href="https://www.ververica.com/blog/stateful-stream-processing-apache-flink-state-backends" target="_blank" rel="noopener">https://www.ververica.com/blog/stateful-stream-processing-apache-flink-state-backends</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--运维与监控(七)</title>
      <link href="/2019/08/19/%E8%BF%90%E7%BB%B4%E4%B8%8E%E7%9B%91%E6%8E%A7/"/>
      <url>/2019/08/19/%E8%BF%90%E7%BB%B4%E4%B8%8E%E7%9B%91%E6%8E%A7/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--集群与部署(六)</title>
      <link href="/2019/08/19/%E9%9B%86%E7%BE%A4%E4%B8%8E%E9%83%A8%E7%BD%B2/"/>
      <url>/2019/08/19/%E9%9B%86%E7%BE%A4%E4%B8%8E%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--source/sink Connectors(五)</title>
      <link href="/2019/08/19/source-sink-Connectors/"/>
      <url>/2019/08/19/source-sink-Connectors/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--状态与容错（四）</title>
      <link href="/2019/08/19/%E7%8A%B6%E6%80%81%E4%B8%8E%E5%AE%B9%E9%94%99/"/>
      <url>/2019/08/19/%E7%8A%B6%E6%80%81%E4%B8%8E%E5%AE%B9%E9%94%99/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--基于时间的算子(三)</title>
      <link href="/2019/08/19/%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E7%9A%84%E7%AE%97%E5%AD%90/"/>
      <url>/2019/08/19/%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E7%9A%84%E7%AE%97%E5%AD%90/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--DataStream-API简介(二)</title>
      <link href="/2019/08/19/DataStream-API%E7%AE%80%E4%BB%8B/"/>
      <url>/2019/08/19/DataStream-API%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--Flink的几个重要概念(一)</title>
      <link href="/2019/08/19/Flink%E7%9A%84%E5%87%A0%E4%B8%AA%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5/"/>
      <url>/2019/08/19/Flink%E7%9A%84%E5%87%A0%E4%B8%AA%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p><h3 id="Event-time"><a href="#Event-time" class="headerlink" title="Event-time"></a>Event-time</h3><p>处理时间(process time)很好理解，指的是机器的本地时间，会产生不一致的、不可重复的结果。相反，事件时间(Event-time)能够产生一致的、可重复的结果。然而，相比基于处理时间的应用，基于事件时间的应用需要额外的配置。支持事件时间的流处理引擎的内部比仅仅支持处理时间的流处理引擎的内部更为复杂。</p><p>Flink不仅为常见的事件时间提供直观且易于使用的处理操作，而且也提供了API去自定义实现更高级的事件时间。 对于这样的高级应用，很好的理解Flink的内部时间处理通常是很有帮助的。Flink主要利用两个概念提供事件时间语义：记录时间戳(record timestamps)和watermarks。 接下来，我们将描述Flink内部如何实现和处理时间戳及watermark以支持流应用程序具有事件时间语义的。</p><h4 id="时间戳-timestamps"><a href="#时间戳-timestamps" class="headerlink" title="时间戳(timestamps)"></a>时间戳(timestamps)</h4><p>对于使用事件时间的应用，所处理的记录(record)必须携带时间戳。时间戳将记录与特定时间点相关联，代表事件发生的时间。当Flink以事件时间模式处理数据流时，比如窗口操作，内部会自动的按时间戳将事件发送到相对应的窗口。 Flink会将时间戳编码为16个字节的Long类型的值，并将它们作为元数据附加到记录中。Flink内置的算子会将Long型的值解析为精确到毫秒的Unix时间戳， 但是，自定义算子可以有自己的解析策略，例如，将精度调整为微秒。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浅析数据库缓冲池与SQL查询成本</title>
      <link href="/2019/08/14/%E6%B5%85%E6%9E%90%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%86%B2%E6%B1%A0%E4%B8%8ESQL%E6%9F%A5%E8%AF%A2%E6%88%90%E6%9C%AC/"/>
      <url>/2019/08/14/%E6%B5%85%E6%9E%90%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%86%B2%E6%B1%A0%E4%B8%8ESQL%E6%9F%A5%E8%AF%A2%E6%88%90%E6%9C%AC/</url>
      
        <content type="html"><![CDATA[<p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/background.jpg" alt><br>&emsp;&emsp;如果我们想要查找多行记录，查询时间是否会成倍地提升呢？其实数据库会采用缓冲池的方式提升页(page)的查找效率。数据库的缓冲池在数据库中起到了怎样的作用？如何查看一条 SQL 语句需要在缓冲池中进行加载的页的数量呢？</p><hr><h2 id="数据库缓冲池"><a href="#数据库缓冲池" class="headerlink" title="数据库缓冲池"></a>数据库缓冲池</h2><p>​        &emsp;&emsp;磁盘 I/O 需要消耗的时间很多，而在内存中进行操作，效率则会高很多，为了能让数据表或者索引中的数据随时被我们所用，DBMS 会申请占用内存来作为数据缓冲池，这样做的好处是可以让磁盘活动最小化，从而减少与磁盘直接进行 I/O 的时间。要知道，这种策略对提升 SQL 语句的查询性能来说至关重要。如果索引的数据在缓冲池里，那么访问的成本就会降低很多。<br>​       &emsp;&emsp;那么缓冲池如何读取数据呢？<br>​        &emsp;&emsp;缓冲池管理器会尽量将经常使用的数据保存起来，在数据库进行页面读操作的时候，首先会判断该页面是否在缓冲池中，如果存在就直接读取，如果不存在，就会通过内存或磁盘将页面存放到缓冲池中再进行读取。</p><h2 id="查看缓冲池大小"><a href="#查看缓冲池大小" class="headerlink" title="查看缓冲池大小"></a>查看缓冲池大小</h2><p>​         &emsp;&emsp;如果使用的是 MyISAM 存储引擎(只缓存索引，不缓存数据)，对应的键缓存参数为 key_buffer_size，可以用它进行查看。<br>​        &emsp;&emsp;如果使用的是 InnoDB 存储引擎，可以通过查看 innodb_buffer_pool_size 变量来查看缓冲池的大小，命令如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show variables like <span class="string">'innodb_buffer_pool_size'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/query_innodb_buffer_size.png" alt><br>​        &emsp;&emsp;此时 InnoDB 的缓冲池大小只有 8388608/1024/1024=8MB，我们可以修改缓冲池大小为 128MB，方法如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; <span class="built_in">set</span> global innodb_buffer_pool_size = 1073741824;</span><br></pre></td></tr></table></figure><p>​      &emsp;&emsp; 在 InnoDB 存储引擎中，可以同时开启多个缓冲池，查看缓冲池的个数，使用命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show variables like <span class="string">'innodb_buffer_pool_instances'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/innodb_buffer_pool_instance.png" alt><br>​        &emsp;&emsp;只有一个缓冲池。实际上innodb_buffer_pool_instances默认情况下为 8，为什么只显示只有一个呢？这里需要说明的是，如果想要开启多个缓冲池，你首先需要将innodb_buffer_pool_size参数设置为大于等于 1GB，这时innodb_buffer_pool_instances才会大于 1。你可以在 MySQL 的配置文件中对innodb_buffer_pool_size进行设置，大于等于 1GB，然后再针对innodb_buffer_pool_instances参数进行修改。</p><h2 id="查看SQL语句的查询成本"><a href="#查看SQL语句的查询成本" class="headerlink" title="查看SQL语句的查询成本"></a>查看SQL语句的查询成本</h2><p>​        &emsp;&emsp; 一条 SQL 查询语句在执行前需要确定查询计划，如果存在多种查询计划的话，MySQL 会计算每个查询计划所需要的成本，从中选择成本最小的一个作为最终执行的查询计划。</p><p>​          &emsp;&emsp;如果查看某条 SQL 语句的查询成本，可以在执行完这条 SQL 语句之后，通过查看当前会话中的 last_query_cost 变量值来得到当前查询的成本。这个查询成本对应的是 SQL 语句所需要读取的页(page)的数量。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show status like <span class="string">'last_query_cost'</span></span><br></pre></td></tr></table></figure><p><strong>example</strong>  </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; select userid,rating from movierating <span class="built_in">where</span> userid = 4169;</span><br></pre></td></tr></table></figure><p>结果：2313 rows in set (0.05 sec) </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show status like <span class="string">'last_query_cost'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/test1.png" alt></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; select userid,rating from movierating <span class="built_in">where</span> userid between 4168 and 4175;</span><br></pre></td></tr></table></figure><p>结果：2643 rows in set (0.01 sec) </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show status like <span class="string">'last_query_cost'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/test2.png" alt></p><p>&emsp;&emsp;你能看到页的数量是刚才的 1.4 倍，但是查询的效率并没有明显的变化，实际上这两个 SQL 查询的时间基本上一样，就是因为采用了顺序读取的方式将页面一次性加载到缓冲池中，然后再进行查找。虽然页数量（last_query_cost）增加了不少，但是通过缓冲池的机制，并没有增加多少查询时间。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程</title>
      <link href="/2019/08/13/Flink%E8%87%AA%E5%AD%A6%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B/"/>
      <url>/2019/08/13/Flink%E8%87%AA%E5%AD%A6%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p><img src="//jiamaoxiang.top/2019/08/13/Flink自学系列教程/logo.png" alt><br>&emsp;&emsp;Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p><hr><h4 id="1-Flink的几个重要概念"><a href="#1-Flink的几个重要概念" class="headerlink" title="1.Flink的几个重要概念"></a><a href="https://jiamaoxiang.top/2019/08/19/Flink%E7%9A%84%E5%87%A0%E4%B8%AA%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5/">1.Flink的几个重要概念</a></h4><h4 id="2-DataFrame-API"><a href="#2-DataFrame-API" class="headerlink" title="2. DataFrame API"></a><a href="https://jiamaoxiang.top/2019/08/19/DataFrame-API/">2. DataFrame API</a></h4><h4 id="3-基于时间的算子"><a href="#3-基于时间的算子" class="headerlink" title="3. 基于时间的算子"></a><a href="https://jiamaoxiang.top/2019/08/19/%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E7%9A%84%E7%AE%97%E5%AD%90/">3. 基于时间的算子</a></h4><h4 id="4-状态与容错"><a href="#4-状态与容错" class="headerlink" title="4. 状态与容错"></a><a href="https://jiamaoxiang.top/2019/08/19/%E7%8A%B6%E6%80%81%E4%B8%8E%E5%AE%B9%E9%94%99/">4. 状态与容错</a></h4><h4 id="5-source-sink-Connectors"><a href="#5-source-sink-Connectors" class="headerlink" title="5. source/sink Connectors"></a><a href="https://jiamaoxiang.top/2019/08/19/source-sink-Connectors/">5. source/sink Connectors</a></h4><h4 id="6-集群与部署"><a href="#6-集群与部署" class="headerlink" title="6. 集群与部署"></a><a href="https://jiamaoxiang.top/2019/08/19/%E9%9B%86%E7%BE%A4%E4%B8%8E%E9%83%A8%E7%BD%B2/">6. 集群与部署</a></h4><h4 id="7-运维与监控"><a href="#7-运维与监控" class="headerlink" title="7.运维与监控"></a><a href="https://jiamaoxiang.top/2019/08/19/%E8%BF%90%E7%BB%B4%E4%B8%8E%E7%9B%91%E6%8E%A7/">7.运维与监控</a></h4>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/08/12/hello-world/"/>
      <url>/2019/08/12/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p><p><img src="//jiamaoxiang.top/2019/08/12/hello-world/logo.png" alt></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
