<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>如何使用Hive进行OLAP分析</title>
      <link href="/2020/04/09/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Hive%E8%BF%9B%E8%A1%8COLAP%E5%88%86%E6%9E%90/"/>
      <url>/2020/04/09/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Hive%E8%BF%9B%E8%A1%8COLAP%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>在线分析处理(OLAP,Online Analytical Processing)是通过带层次的维度和跨维度进行多维分析的，简单理解为一种多维数据分析的方式，通过OLAP可以展示数据仓库中数据的多维逻辑视图。在多维分析中，数据是按照维度(观察数据的角度)来表示的，比如商品、城市、客户。而维通常按层次(层次维度)组织的，如城市、省、国家，再比如时间也是有层次的，如天、周、月、季度和年。不同的管理者可以从不同的维度(视角)去观察这些数据，这些在多个不同维度上对数据进行综合考察的手段就是通常所说的数据仓库多维查询，最常见的就如上卷(roll-up)和下钻(drill-down)了,所谓上卷，指的是选定特定的数据范围之后，对其进行汇总统计以获取更高层次的信息。所谓下钻，指的是选定特定的数据范围之后，需要进一步查看细节的数据。从另一种意义上说，钻取就是针对多维展现的数据，进一步探究其内部的组成和来源。值得注意的是，上卷和下钻要求维度具有层级结构，即数仓中所说的层次维度。</p><h2 id="如何实现数据的多维分析"><a href="#如何实现数据的多维分析" class="headerlink" title="如何实现数据的多维分析"></a>如何实现数据的多维分析</h2><p>Hive提供了多维数据分析的函数，如<figure class="highlight plain"><figcaption><span>SETS```,```GROUPING__ID```,```CUBE```,```ROLLUP```,通过这些分析函数，可以轻而易举的实现多维数据分析。下面将会通过一个案例来了解这些函数的具体含义以及该如何使用这些函数。</span></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">### 简单介绍</span><br><span class="line"></span><br><span class="line">- GROUPING SETS</span><br><span class="line"></span><br><span class="line">在一个group by查询中，通过该子句可以对不同维度或同一维度的不同层次进行聚合，简单理解为一条sql可以实现多种不同的分组规则，用户可以在该函数中传入自己定义的多种分组字段，本质上等价于多个group by语句进行UNION，对于GROUPING SETS子句中的空白集&apos;（）&apos;表示对总体进行聚集。</span><br><span class="line"></span><br><span class="line">**示例模板**</span><br><span class="line"></span><br><span class="line">```sql</span><br><span class="line">-- 使用GROUPING SETS查询</span><br><span class="line">SELECT a,</span><br><span class="line">       b,</span><br><span class="line">       SUM(c)</span><br><span class="line">FROM tab1</span><br><span class="line">GROUP BY a,b</span><br><span class="line">GROUPING SETS ((a,b), a,b, ());</span><br><span class="line">-- 与GROUP BY等价关系</span><br><span class="line">SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, b</span><br><span class="line">UNION</span><br><span class="line">SELECT a, null, SUM( c ) FROM tab1 GROUP BY a, null</span><br><span class="line">UNION</span><br><span class="line">SELECT null, b, SUM( c ) FROM tab1 GROUP BY null, b</span><br><span class="line">UNION</span><br><span class="line">SELECT null, null, SUM( c ) FROM tab1;</span><br></pre></td></tr></table></figure></p><ul><li>GROUPING__ID</li></ul><p>当使用聚合时，有时候会出现数据本身为null值，很难区分究竟是数据列本身为null值还是聚合数据行为null，即无法区分查询结果中的null值是属于列本身的还是聚合的结果行，因此需要一种方法识别出列中的null值。grouping__id函数就是此场景下的解决方案。注意该函数是有两个下划线。这个函数为每种聚合数据行生成唯一的组id。它的返回值看起来像整型数值，其实是字符串类型，这个值使用了位图策略（bitvector，位向量），即它的二进制形式中的每一位表示对应列是否参与分组，如果某一列参与了分组，对应位就被置为1，否则为0。通过这种方式可以区分出数据本身中的null值。看到这是不是还是一头雾水，没关系，来看下面的示例：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_grouping__id(<span class="keyword">id</span> <span class="built_in">int</span>,amount <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>));</span><br><span class="line"><span class="comment">-- 插入数据</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> test_grouping__id <span class="keyword">values</span>(<span class="number">1</span>,<span class="literal">null</span>),(<span class="number">1</span>,<span class="number">1</span>),(<span class="number">2</span>,<span class="number">2</span>),(<span class="number">3</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="literal">null</span>),(<span class="number">4</span>,<span class="number">5</span>);</span><br><span class="line"><span class="comment">--执行查询</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">id</span>,</span><br><span class="line">       amount,</span><br><span class="line">       grouping__id,</span><br><span class="line">       <span class="keyword">count</span>(*) cnt</span><br><span class="line"><span class="keyword">FROM</span> test_grouping__id</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">id</span>,</span><br><span class="line">         amount</span><br><span class="line"><span class="keyword">GROUPING</span> <span class="keyword">sets</span>(<span class="keyword">id</span>,(<span class="keyword">id</span>,amount),())</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> grouping__id</span><br></pre></td></tr></table></figure><p><strong>查询结果分析</strong></p><p>查询结果如下图所示：绿色框表示未进行分组，即进行全局聚合，<figure class="highlight plain"><figcaption><span>+ 2^1  </span></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">![](F:\npm\mywebsite\source\_posts\如何使用Hive进行OLAP分析\grouping__id.png)</span><br><span class="line"></span><br><span class="line">以上面为例，分组字段为id、amount，转成二进制表示形式为：</span><br><span class="line"></span><br><span class="line">![](F:\npm\mywebsite\source\_posts\如何使用Hive进行OLAP分析\二进制.png)</span><br><span class="line"></span><br><span class="line">- ROLLUP</span><br><span class="line"></span><br><span class="line">通用的语法为```WITH ROLLUP```,需要与group by一起用于在维的层次结构级别上计算聚合。功能为可以按照group by的分组字段进行组合，计算出不同分组的结果。注意对于分组字段的组合会与最左边的字段为主。使用ROLLUP的GROUP BY a，b，c假定层次结构是“ a”向下钻取到“ b”，“ b”向下钻取到“ c”。则可以通过```GROUP BY a，b，c，WITH ROLLUP```进行实现，该语句等价于```GROUP BY a，b，c GROUPING SETS（（a，b，c），（a，b），（a），（））```。即使用WITH ROLLUP，首先会对全局聚合(不分组)，然后会按GROUP BY字段组合，进行聚合，但是最左侧的分组字段必须参与分组，比如a字段是最左侧的字段，则a必定参与分组组合。</span><br><span class="line"></span><br><span class="line">**示例模板**</span><br><span class="line"></span><br><span class="line">```sql</span><br><span class="line">-- 使用WITH ROLLUP查询</span><br><span class="line">SELECT a,</span><br><span class="line">       b,</span><br><span class="line">       c</span><br><span class="line">       SUM(d)</span><br><span class="line">FROM tab1</span><br><span class="line">GROUP BY a,b,c</span><br><span class="line">WITH ROLLUP</span><br><span class="line">-- 等价于下面的方式</span><br><span class="line">SELECT a,</span><br><span class="line">       b,</span><br><span class="line">       c,</span><br><span class="line">       SUM(d)</span><br><span class="line">FROM tab1</span><br><span class="line">GROUP BY a,b,c</span><br><span class="line">GROUPING SETS ((a,b,c), (a,b), (a),());</span><br></pre></td></tr></table></figure></p><ul><li>CUBE</li></ul><p>CUBE表示一个立方体，apache的kylin使用就是这种预计算方式。即会对给定的维度(分组字段)进行多种组合之后，形成不同分组规则的数据结果。一旦我们在一组维度上计算出CUBE，就可以得到这些维度上所有可能的聚合聚合结果。比如：<figure class="highlight plain"><figcaption><span>BY a，b，c WITH CUBE```，等价于```GROUP BY a，b，c GROUPING SETS（（a，b，c），（a，b），（b，c）， （a，c），（a），（b），（c），（））```。</span></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">其实，可以将上面的情况抽象成排列组合的问题，即从分组字段集合(假设有n个字段)中随意取出0~n个字段，那么会有多少中组合方式，如下面公式所示：</span><br><span class="line"></span><br><span class="line">![](F:\npm\mywebsite\source\_posts\如何使用Hive进行OLAP分析\组合1.png)</span><br><span class="line"></span><br><span class="line">结合上面的例子，```GROUP BY a，b，c WITH CUBE```，那么所有的组合方式有：（a，b，c），（a，b），（b，c）， （a，c），（a），（b），（c），（）,一共有8种组合，即2^3 = 8。</span><br><span class="line"></span><br><span class="line">**示例模板**</span><br><span class="line"></span><br><span class="line">```sql</span><br><span class="line">-- 使用WITH CUBE查询</span><br><span class="line">SELECT a,</span><br><span class="line">       b,</span><br><span class="line">       c</span><br><span class="line">       SUM(d)</span><br><span class="line">FROM tab1</span><br><span class="line">GROUP BY a,b,c</span><br><span class="line">WITH CUBE</span><br><span class="line">-- 等价于下面的方式</span><br><span class="line">SELECT a,</span><br><span class="line">       b,</span><br><span class="line">       c,</span><br><span class="line">       SUM(d)</span><br><span class="line">FROM tab1</span><br><span class="line">GROUP BY a,b,c</span><br><span class="line">GROUPING SETS ((a,b,c),(a,b),(b,c), (a,c),(a),(b),(c),());</span><br></pre></td></tr></table></figure></p><h3 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h3><h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><p>有一份用户行为数据集，包括用户的所有行为（包括pv点击、buy购买、cart加购、fav收藏），具体如下表所示：</p><table><thead><tr><th>字段名</th><th align="left">列名称</th><th align="left">说明</th></tr></thead><tbody><tr><td>user_id</td><td align="left">用户ID</td><td align="left">整数类型，用户ID</td></tr><tr><td>item_id</td><td align="left">商品ID</td><td align="left">整数类型，商品ID</td></tr><tr><td>category_id</td><td align="left">商品类目ID</td><td align="left">整数类型，商品所属类目ID</td></tr><tr><td>behavior</td><td align="left">行为类型</td><td align="left">字符串，枚举类型，包括(‘pv’, ‘buy’, ‘cart’, ‘fav’)</td></tr><tr><td>access_time</td><td align="left">时间戳</td><td align="left">行为发生的时间戳，单位秒</td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_behavior</span><br><span class="line">             (</span><br><span class="line">                user_id <span class="built_in">int</span> ,</span><br><span class="line">                item_id <span class="built_in">int</span>,</span><br><span class="line">                category_id <span class="built_in">int</span>,</span><br><span class="line">                behavior <span class="keyword">string</span>,</span><br><span class="line">               access_time <span class="keyword">string</span></span><br><span class="line">               )</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span>;</span><br><span class="line"><span class="comment">-- 装载数据</span></span><br><span class="line">1,101,1,pv,1511658000</span><br><span class="line">2,102,1,pv,1511658000</span><br><span class="line">3,103,1,pv,1511658000</span><br><span class="line">4,104,2,cart,1511659329</span><br><span class="line">5,105,2,buy,1511659326</span><br><span class="line">6,106,3,fav,1511659323</span><br><span class="line">7,101,1,pv,1511658010</span><br><span class="line">8,102,1,buy,1511658200</span><br><span class="line">9,103,1,cart,1511658030</span><br><span class="line">10,107,3,fav,1511659332</span><br></pre></td></tr></table></figure><h4 id="GROUPING-SETS使用"><a href="#GROUPING-SETS使用" class="headerlink" title="GROUPING SETS使用"></a>GROUPING SETS使用</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询每种商品品类、每种用户行为的访问次数</span></span><br><span class="line"><span class="comment">-- 查询每种用户行为的访问次数</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">item_id,</span><br><span class="line">category_id,</span><br><span class="line">behavior,</span><br><span class="line"><span class="keyword">COUNT</span>(*) <span class="keyword">AS</span> cnt,</span><br><span class="line">GROUPING__ID </span><br><span class="line"><span class="keyword">FROM</span> user_behavior </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> item_id,category_id,behavior </span><br><span class="line"><span class="keyword">GROUPING</span> <span class="keyword">SETS</span> ((category_id,behavior),behavior)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br></pre></td></tr></table></figure><p><strong>结果如下：</strong></p><p><img src="//jiamaoxiang.top/2020/04/09/如何使用Hive进行OLAP分析/F:%5Cnpm%5Cmywebsite%5Csource_posts%5C%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Hive%E8%BF%9B%E8%A1%8COLAP%E5%88%86%E6%9E%90%5Cgrouping_set.png" alt></p><h4 id="ROLLUP使用"><a href="#ROLLUP使用" class="headerlink" title="ROLLUP使用"></a>ROLLUP使用</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询每种商品品类的访问次数</span></span><br><span class="line"><span class="comment">-- 查询每种商品品类、每种用户行为的次数</span></span><br><span class="line"><span class="comment">-- 查询用户的总访问次数</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">category_id,</span><br><span class="line">behavior,</span><br><span class="line"><span class="keyword">COUNT</span>(*) <span class="keyword">AS</span> cnt,</span><br><span class="line">GROUPING__ID </span><br><span class="line"><span class="keyword">FROM</span> user_behavior </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> category_id,behavior </span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">ROLLUP</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br></pre></td></tr></table></figure><p><strong>结果如下：</strong></p><p><img src="//jiamaoxiang.top/2020/04/09/如何使用Hive进行OLAP分析/F:%5Cnpm%5Cmywebsite%5Csource_posts%5C%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Hive%E8%BF%9B%E8%A1%8COLAP%E5%88%86%E6%9E%90%5Crollup.png" alt></p><h4 id="CUBE使用"><a href="#CUBE使用" class="headerlink" title="CUBE使用"></a>CUBE使用</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询每种商品品类的访问次数</span></span><br><span class="line"><span class="comment">-- 查询每种用户行为的次数</span></span><br><span class="line"><span class="comment">-- 查询每种商品品类、每种用户行为的次数</span></span><br><span class="line"><span class="comment">-- 查询用户的总访问次数</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">category_id,</span><br><span class="line">behavior,</span><br><span class="line"><span class="keyword">COUNT</span>(*) <span class="keyword">AS</span> cnt,</span><br><span class="line">GROUPING__ID </span><br><span class="line"><span class="keyword">FROM</span> user_behavior </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> category_id,behavior </span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">CUBE</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br></pre></td></tr></table></figure><p><strong>结果如下：</strong></p><p><img src="//jiamaoxiang.top/2020/04/09/如何使用Hive进行OLAP分析/F:%5Cnpm%5Cmywebsite%5Csource_posts%5C%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Hive%E8%BF%9B%E8%A1%8COLAP%E5%88%86%E6%9E%90%5Ccube.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先介绍了什么是OLAP，接着介绍Hive中提供的几种OLAP分析的函数，并对每一种函数进行了详细说明，并给出了相关的图示解释，最后以一个案例说明了这几种函数的使用方式，可以进一步加深理解。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>你真的了解Flink Kafka source吗？</title>
      <link href="/2020/04/02/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3Flink-Kafka-connector%E5%90%97%EF%BC%9F/"/>
      <url>/2020/04/02/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3Flink-Kafka-connector%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>Flink 提供了专门的 Kafka 连接器，向 Kafka topic 中读取或者写入数据。Flink Kafka Consumer 集成了 Flink 的 Checkpoint 机制，可提供 exactly-once 的处理语义。为此，Flink 并不完全依赖于跟踪 Kafka 消费组的偏移量，而是在内部跟踪和检查偏移量。</p><a id="more"></a><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>当我们在使用Spark Streaming、Flink等计算框架进行数据实时处理时，使用Kafka作为一款发布与订阅的消息系统成为了标配。Spark Streaming与Flink都提供了相对应的Kafka Consumer，使用起来非常的方便，只需要设置一下Kafka的参数，然后添加kafka的source就万事大吉了。如果你真的觉得事情就是如此的so easy，感觉妈妈再也不用担心你的学习了，那就真的是too young too simple sometimes naive了。本文以Flink 的Kafka Source为讨论对象，首先从基本的使用入手，然后深入源码逐一剖析，一并为你拨开Flink Kafka connector的神秘面纱。值得注意的是，本文假定读者具备了Kafka的相关知识，关于Kafka的相关细节问题，不在本文的讨论范围之内。</p><h2 id="Flink-Kafka-Consumer介绍"><a href="#Flink-Kafka-Consumer介绍" class="headerlink" title="Flink Kafka Consumer介绍"></a>Flink Kafka Consumer介绍</h2><p>Flink Kafka Connector有很多个版本，可以根据你的kafka和Flink的版本选择相应的包（maven artifact id）和类名。本文所涉及的Flink版本为1.10，Kafka的版本为2.3.4。Flink所提供的Maven依赖于类名如下表所示：</p><table><thead><tr><th align="left">Maven 依赖</th><th align="left">自从哪个版本 开始支持</th><th align="left">类名</th><th align="left">Kafka 版本</th><th align="left">注意</th></tr></thead><tbody><tr><td align="left">flink-connector-kafka-0.8_2.11</td><td align="left">1.0.0</td><td align="left">FlinkKafkaConsumer08 FlinkKafkaProducer08</td><td align="left">0.8.x</td><td align="left">这个连接器在内部使用 Kafka 的 <a href="https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+SimpleConsumer+Example" target="_blank" rel="noopener">SimpleConsumer</a> API。偏移量由 Flink 提交给 ZK。</td></tr><tr><td align="left">flink-connector-kafka-0.9_2.11</td><td align="left">1.0.0</td><td align="left">FlinkKafkaConsumer09 FlinkKafkaProducer09</td><td align="left">0.9.x</td><td align="left">这个连接器使用新的 Kafka <a href="http://kafka.apache.org/documentation.html#newconsumerapi" target="_blank" rel="noopener">Consumer API</a></td></tr><tr><td align="left">flink-connector-kafka-0.10_2.11</td><td align="left">1.2.0</td><td align="left">FlinkKafkaConsumer010 FlinkKafkaProducer010</td><td align="left">0.10.x</td><td align="left">这个连接器支持 <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message" target="_blank" rel="noopener">带有时间戳的 Kafka 消息</a>，用于生产和消费。</td></tr><tr><td align="left">flink-connector-kafka-0.11_2.11</td><td align="left">1.4.0</td><td align="left">FlinkKafkaConsumer011 FlinkKafkaProducer011</td><td align="left">&gt;=  0.11.x</td><td align="left">Kafka 从 0.11.x 版本开始不支持 Scala 2.10。此连接器支持了 <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging" target="_blank" rel="noopener">Kafka 事务性的消息传递</a>来为生产者提供 Exactly once 语义。</td></tr><tr><td align="left">flink-connector-kafka_2.11</td><td align="left">1.7.0</td><td align="left">FlinkKafkaConsumer FlinkKafkaProducer</td><td align="left">&gt;= 1.0.0</td><td align="left">这个通用的 Kafka 连接器尽力与 Kafka client 的最新版本保持同步。该连接器使用的 Kafka client 版本可能会在 Flink 版本之间发生变化。从 Flink 1.9 版本开始，它使用 Kafka 2.2.0 client。当前 Kafka 客户端向后兼容 0.10.0 或更高版本的 Kafka broker。 但是对于 Kafka 0.11.x 和 0.10.x 版本，我们建议你分别使用专用的 flink-connector-kafka-0.11_2.11 和 flink-connector-kafka-0.10_2.11 连接器。</td></tr></tbody></table><h2 id="Demo示例"><a href="#Demo示例" class="headerlink" title="Demo示例"></a>Demo示例</h2><h3 id="添加Maven依赖"><a href="#添加Maven依赖" class="headerlink" title="添加Maven依赖"></a>添加Maven依赖</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;!--本文使用的是通用型的connector--&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="简单代码案例"><a href="#简单代码案例" class="headerlink" title="简单代码案例"></a>简单代码案例</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConnector</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment senv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">// 开启checkpoint，时间间隔为毫秒</span></span><br><span class="line">        senv.enableCheckpointing(<span class="number">5000L</span>);</span><br><span class="line">        <span class="comment">// 选择状态后端</span></span><br><span class="line">        senv.setStateBackend((StateBackend) <span class="keyword">new</span> FsStateBackend(<span class="string">"file:///E://checkpoint"</span>));</span><br><span class="line">        <span class="comment">//senv.setStateBackend((StateBackend) new FsStateBackend("hdfs://kms-1:8020/checkpoint"));</span></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// kafka broker地址</span></span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"kms-2:9092,kms-3:9092,kms-4:9092"</span>);</span><br><span class="line">        <span class="comment">// 仅kafka0.8版本需要配置</span></span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"kms-2:2181,kms-3:2181,kms-4:2181"</span>);</span><br><span class="line">        <span class="comment">// 消费者组</span></span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line">        <span class="comment">// 自动偏移量提交</span></span><br><span class="line">        props.put(<span class="string">"enable.auto.commit"</span>, <span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">// 偏移量提交的时间间隔，毫秒</span></span><br><span class="line">        props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="number">5000</span>);</span><br><span class="line">        <span class="comment">// kafka 消息的key序列化器</span></span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        <span class="comment">// kafka 消息的value序列化器</span></span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        <span class="comment">// 指定kafka的消费者从哪里开始消费数据</span></span><br><span class="line">        <span class="comment">// 共有三种方式，</span></span><br><span class="line">        <span class="comment">// #earliest</span></span><br><span class="line">        <span class="comment">// 当各分区下有已提交的offset时，从提交的offset开始消费；</span></span><br><span class="line">        <span class="comment">// 无提交的offset时，从头开始消费</span></span><br><span class="line">        <span class="comment">// #latest</span></span><br><span class="line">        <span class="comment">// 当各分区下有已提交的offset时，从提交的offset开始消费；</span></span><br><span class="line">        <span class="comment">// 无提交的offset时，消费新产生的该分区下的数据</span></span><br><span class="line">        <span class="comment">// #none</span></span><br><span class="line">        <span class="comment">// topic各分区都存在已提交的offset时，</span></span><br><span class="line">        <span class="comment">// 从offset后开始消费；</span></span><br><span class="line">        <span class="comment">// 只要有一个分区不存在已提交的offset，则抛出异常</span></span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; consumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(</span><br><span class="line">                <span class="string">"qfbap_ods.code_city"</span>,</span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props);</span><br><span class="line">        <span class="comment">//设置checkpoint后在提交offset，即oncheckpoint模式</span></span><br><span class="line">        <span class="comment">// 该值默认为true，</span></span><br><span class="line">        consumer.setCommitOffsetsOnCheckpoints(<span class="keyword">true</span>);</span><br><span class="line">     </span><br><span class="line">        <span class="comment">// 最早的数据开始消费</span></span><br><span class="line">        <span class="comment">// 该模式下，Kafka 中的 committed offset 将被忽略，不会用作起始位置。</span></span><br><span class="line">        <span class="comment">//consumer.setStartFromEarliest();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 消费者组最近一次提交的偏移量，默认。</span></span><br><span class="line">        <span class="comment">// 如果找不到分区的偏移量，那么将会使用配置中的 auto.offset.reset 设置</span></span><br><span class="line">        <span class="comment">//consumer.setStartFromGroupOffsets();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 最新的数据开始消费</span></span><br><span class="line">        <span class="comment">// 该模式下，Kafka 中的 committed offset 将被忽略，不会用作起始位置。</span></span><br><span class="line">        <span class="comment">//consumer.setStartFromLatest();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定具体的偏移量时间戳,毫秒</span></span><br><span class="line">        <span class="comment">// 对于每个分区，其时间戳大于或等于指定时间戳的记录将用作起始位置。</span></span><br><span class="line">        <span class="comment">// 如果一个分区的最新记录早于指定的时间戳，则只从最新记录读取该分区数据。</span></span><br><span class="line">        <span class="comment">// 在这种模式下，Kafka 中的已提交 offset 将被忽略，不会用作起始位置。</span></span><br><span class="line">        <span class="comment">//consumer.setStartFromTimestamp(1585047859000L);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 为每个分区指定偏移量</span></span><br><span class="line">        <span class="comment">/*Map&lt;KafkaTopicPartition, Long&gt; specificStartOffsets = new HashMap&lt;&gt;();</span></span><br><span class="line"><span class="comment">        specificStartOffsets.put(new KafkaTopicPartition("qfbap_ods.code_city", 0), 23L);</span></span><br><span class="line"><span class="comment">        specificStartOffsets.put(new KafkaTopicPartition("qfbap_ods.code_city", 1), 31L);</span></span><br><span class="line"><span class="comment">        specificStartOffsets.put(new KafkaTopicPartition("qfbap_ods.code_city", 2), 43L);</span></span><br><span class="line"><span class="comment">        consumer1.setStartFromSpecificOffsets(specificStartOffsets);*/</span></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 请注意：当 Job 从故障中自动恢复或使用 savepoint 手动恢复时，</span></span><br><span class="line"><span class="comment">         * 这些起始位置配置方法不会影响消费的起始位置。</span></span><br><span class="line"><span class="comment">         * 在恢复时，每个 Kafka 分区的起始位置由存储在 savepoint 或 checkpoint 中的 offset 确定</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; source = senv.addSource(consumer);</span><br><span class="line">        <span class="comment">// TODO</span></span><br><span class="line">        source.print();</span><br><span class="line">        senv.execute(<span class="string">"test kafka connector"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="参数配置解读"><a href="#参数配置解读" class="headerlink" title="参数配置解读"></a>参数配置解读</h3><p>在Demo示例中，给出了详细的配置信息，下面将对上面的参数配置进行逐一分析。</p><h4 id="kakfa的properties参数配置"><a href="#kakfa的properties参数配置" class="headerlink" title="kakfa的properties参数配置"></a>kakfa的properties参数配置</h4><ul><li><p>bootstrap.servers：kafka broker地址</p></li><li><p>zookeeper.connect：仅kafka0.8版本需要配置</p></li><li><p>group.id：消费者组</p></li><li><p>enable.auto.commit：</p><p>自动偏移量提交，该值的配置不是最终的偏移量提交模式，需要考虑用户是否开启了checkpoint，</p><p>在下面的源码分析中会进行解读</p></li><li><p>auto.commit.interval.ms：偏移量提交的时间间隔，毫秒</p></li><li><p>key.deserializer：</p><p>kafka 消息的key序列化器，如果不指定会使用ByteArrayDeserializer序列化器</p></li><li><p>value.deserializer：</p></li></ul><p>kafka 消息的value序列化器，如果不指定会使用ByteArrayDeserializer序列化器</p><ul><li><p>auto.offset.reset：</p><p>指定kafka的消费者从哪里开始消费数据，共有三种方式，</p><ul><li>第一种：earliest<br>当各分区下有已提交的offset时，从提交的offset开始消费； 无提交的offset时，从头开始消费</li><li>第二种：latest<br>当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据</li><li>第三种：none<br>topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常</li></ul><p>注意：上面的指定消费模式并不是最终的消费模式，取决于用户在Flink程序中配置的消费模式</p></li></ul><h4 id="Flink程序用户配置的参数"><a href="#Flink程序用户配置的参数" class="headerlink" title="Flink程序用户配置的参数"></a>Flink程序用户配置的参数</h4><ul><li>consumer.setCommitOffsetsOnCheckpoints(true)</li></ul><p>​    解释：设置checkpoint后在提交offset，即oncheckpoint模式，该值默认为true，该参数会影响偏移量的提交方式，下面的源码中会进行分析</p><ul><li><p>consumer.setStartFromEarliest()</p><p>解释： 最早的数据开始消费 ，该模式下，Kafka 中的 committed offset 将被忽略，不会用作起始位置。该方法为继承父类FlinkKafkaConsumerBase的方法。</p></li><li><p>consumer.setStartFromGroupOffsets()</p><p>解释：消费者组最近一次提交的偏移量，默认。 如果找不到分区的偏移量，那么将会使用配置中的 auto.offset.reset 设置，该方法为继承父类FlinkKafkaConsumerBase的方法。</p></li><li><p>consumer.setStartFromLatest()</p><p>解释：最新的数据开始消费，该模式下，Kafka 中的 committed offset 将被忽略，不会用作起始位置。该方法为继承父类FlinkKafkaConsumerBase的方法。</p></li><li><p>consumer.setStartFromTimestamp(1585047859000L)</p><p>解释：指定具体的偏移量时间戳,毫秒。对于每个分区，其时间戳大于或等于指定时间戳的记录将用作起始位置。 如果一个分区的最新记录早于指定的时间戳，则只从最新记录读取该分区数据。在这种模式下，Kafka 中的已提交 offset 将被忽略，不会用作起始位置。</p></li><li><p>consumer.setStartFromSpecificOffsets(specificStartOffsets)</p></li></ul><p>解释：为每个分区指定偏移量，该方法为继承父类FlinkKafkaConsumerBase的方法。</p><p>请注意：当 Job 从故障中自动恢复或使用 savepoint 手动恢复时，这些起始位置配置方法不会影响消费的起始位置。在恢复时，每个 Kafka 分区的起始位置由存储在 savepoint 或 checkpoint 中的 offset 确定。</p><h2 id="Flink-Kafka-Consumer源码解读"><a href="#Flink-Kafka-Consumer源码解读" class="headerlink" title="Flink Kafka Consumer源码解读"></a>Flink Kafka Consumer源码解读</h2><h3 id="继承关系"><a href="#继承关系" class="headerlink" title="继承关系"></a>继承关系</h3><p>Flink Kafka Consumer继承了FlinkKafkaConsumerBase抽象类，而FlinkKafkaConsumerBase抽象类又继承了RichParallelSourceFunction，所以要实现一个自定义的source时，有两种实现方式：一种是通过实现SourceFunction接口来自定义并行度为1的数据源；另一种是通过实现ParallelSourceFunction接口或者继承RichParallelSourceFunction来自定义具有并行度的数据源。FlinkKafkaConsumer的继承关系如下图所示。</p><p><img src="//jiamaoxiang.top/2020/04/02/你真的了解Flink-Kafka-connector吗？/F:%5Cnpm%5Cmywebsite%5Csource_posts%5C%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3Flink-Kafka-connector%E5%90%97%EF%BC%9F%5C%E7%BB%A7%E6%89%BF%E5%9B%BE.png" alt></p><h3 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h3><h4 id="FlinkKafkaConsumer源码"><a href="#FlinkKafkaConsumer源码" class="headerlink" title="FlinkKafkaConsumer源码"></a>FlinkKafkaConsumer源码</h4><p>先看一下FlinkKafkaConsumer的源码，为了方面阅读，本文将尽量给出本比较完整的源代码片段，具体如下所示：代码较长，在这里可以先有有一个总体的印象，下面会对重要的代码片段详细进行分析。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkKafkaConsumer</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">FlinkKafkaConsumerBase</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 配置轮询超时超时时间，使用flink.poll-timeout参数在properties进行配置</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String KEY_POLL_TIMEOUT = <span class="string">"flink.poll-timeout"</span>;</span><br><span class="line"><span class="comment">// 如果没有可用数据，则等待轮询所需的时间（以毫秒为单位）。 如果为0，则立即返回所有可用的记录</span></span><br><span class="line"><span class="comment">//默认轮询超时时间</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> DEFAULT_POLL_TIMEOUT = <span class="number">100L</span>;</span><br><span class="line"><span class="comment">// 用户提供的kafka 参数配置</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> Properties properties;</span><br><span class="line"><span class="comment">// 如果没有可用数据，则等待轮询所需的时间（以毫秒为单位）。 如果为0，则立即返回所有可用的记录</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">long</span> pollTimeout;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topic                   消费的主题名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> valueDeserializer       反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props                   用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(String topic, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(Collections.singletonList(topic), valueDeserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * 该构造方法允许传入KafkaDeserializationSchema，该反序列化类支持访问kafka消费的额外信息</span></span><br><span class="line"><span class="comment"> * 比如：key/value对，offsets(偏移量)，topic(主题名称)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topic                消费的主题名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer         反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props                用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(String topic, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(Collections.singletonList(topic), deserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * 该构造方法允许传入多个topic(主题)，支持消费多个主题</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topics          消费的主题名称，多个主题为List集合</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer    反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props           用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(List&lt;String&gt; topics, DeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(topics, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(deserializer), props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * 该构造方法允许传入多个topic(主题)，支持消费多个主题,</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topics         消费的主题名称，多个主题为List集合</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer   反序列化类型，用于将kafka的字节消息转换为Flink的对象,支持获取额外信息</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props          用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(List&lt;String&gt; topics, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(topics, <span class="keyword">null</span>, deserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于正则表达式订阅多个topic</span></span><br><span class="line"><span class="comment"> * 如果开启了分区发现，即FlinkKafkaConsumer.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值为非负数</span></span><br><span class="line"><span class="comment"> * 只要是能够正则匹配上，主题一旦被创建就会立即被订阅</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> subscriptionPattern   主题的正则表达式</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> valueDeserializer   反序列化类型，用于将kafka的字节消息转换为Flink的对象,支持获取额外信息</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props               用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(Pattern subscriptionPattern, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(<span class="keyword">null</span>, subscriptionPattern, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(valueDeserializer), props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于正则表达式订阅多个topic</span></span><br><span class="line"><span class="comment"> * 如果开启了分区发现，即FlinkKafkaConsumer.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值为非负数</span></span><br><span class="line"><span class="comment"> * 只要是能够正则匹配上，主题一旦被创建就会立即被订阅</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> subscriptionPattern   主题的正则表达式</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer          该反序列化类支持访问kafka消费的额外信息,比如：key/value对，offsets(偏移量)，topic(主题名称)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props                 用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(Pattern subscriptionPattern, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(<span class="keyword">null</span>, subscriptionPattern, deserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">List&lt;String&gt; topics,</span></span></span><br><span class="line"><span class="function"><span class="params">Pattern subscriptionPattern,</span></span></span><br><span class="line"><span class="function"><span class="params">KafkaDeserializationSchema&lt;T&gt; deserializer,</span></span></span><br><span class="line"><span class="function"><span class="params">Properties props)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 调用父类(FlinkKafkaConsumerBase)构造方法，PropertiesUtil.getLong方法第一个参数为Properties，第二个参数为key，第三个参数为value默认值</span></span><br><span class="line"><span class="keyword">super</span>(</span><br><span class="line">topics,</span><br><span class="line">subscriptionPattern,</span><br><span class="line">deserializer,</span><br><span class="line">getLong(</span><br><span class="line">checkNotNull(props, <span class="string">"props"</span>),</span><br><span class="line">KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS, PARTITION_DISCOVERY_DISABLED),</span><br><span class="line">!getBoolean(props, KEY_DISABLE_METRICS, <span class="keyword">false</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">this</span>.properties = props;</span><br><span class="line">setDeserializer(<span class="keyword">this</span>.properties);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 配置轮询超时时间，如果在properties中配置了KEY_POLL_TIMEOUT参数，则返回具体的配置值，否则返回默认值DEFAULT_POLL_TIMEOUT</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (properties.containsKey(KEY_POLL_TIMEOUT)) &#123;</span><br><span class="line"><span class="keyword">this</span>.pollTimeout = Long.parseLong(properties.getProperty(KEY_POLL_TIMEOUT));</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">this</span>.pollTimeout = DEFAULT_POLL_TIMEOUT;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Cannot parse poll timeout for '"</span> + KEY_POLL_TIMEOUT + <span class="string">'\''</span>, e);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">   <span class="comment">// 父类(FlinkKafkaConsumerBase)方法重写，该方法的作用是返回一个fetcher实例，</span></span><br><span class="line"><span class="comment">// fetcher的作用是连接kafka的broker，拉去数据并进行反序列化，然后将数据输出为数据流(data stream)</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">protected</span> AbstractFetcher&lt;T, ?&gt; createFetcher(</span><br><span class="line">SourceContext&lt;T&gt; sourceContext,</span><br><span class="line">Map&lt;KafkaTopicPartition, Long&gt; assignedPartitionsWithInitialOffsets,</span><br><span class="line">SerializedValue&lt;AssignerWithPeriodicWatermarks&lt;T&gt;&gt; watermarksPeriodic,</span><br><span class="line">SerializedValue&lt;AssignerWithPunctuatedWatermarks&lt;T&gt;&gt; watermarksPunctuated,</span><br><span class="line">StreamingRuntimeContext runtimeContext,</span><br><span class="line">OffsetCommitMode offsetCommitMode,</span><br><span class="line">MetricGroup consumerMetricGroup,</span><br><span class="line"><span class="keyword">boolean</span> useMetrics) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 确保当偏移量的提交模式为ON_CHECKPOINTS(条件1：开启checkpoint，条件2：consumer.setCommitOffsetsOnCheckpoints(true))时，禁用自动提交</span></span><br><span class="line"><span class="comment">// 该方法为父类(FlinkKafkaConsumerBase)的静态方法</span></span><br><span class="line"><span class="comment">// 这将覆盖用户在properties中配置的任何设置</span></span><br><span class="line"><span class="comment">// 当offset的模式为ON_CHECKPOINTS，或者为DISABLED时，会将用户配置的properties属性进行覆盖</span></span><br><span class="line"><span class="comment">// 具体是将ENABLE_AUTO_COMMIT_CONFIG = "enable.auto.commit"的值重置为"false</span></span><br><span class="line">        <span class="comment">// 可以理解为：如果开启了checkpoint，并且设置了consumer.setCommitOffsetsOnCheckpoints(true)，默认为true，</span></span><br><span class="line"><span class="comment">// 就会将kafka properties的enable.auto.commit强制置为false</span></span><br><span class="line">adjustAutoCommitConfig(properties, offsetCommitMode);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> KafkaFetcher&lt;&gt;(</span><br><span class="line">sourceContext,</span><br><span class="line">assignedPartitionsWithInitialOffsets,</span><br><span class="line">watermarksPeriodic,</span><br><span class="line">watermarksPunctuated,</span><br><span class="line">runtimeContext.getProcessingTimeService(),</span><br><span class="line">runtimeContext.getExecutionConfig().getAutoWatermarkInterval(),</span><br><span class="line">runtimeContext.getUserCodeClassLoader(),</span><br><span class="line">runtimeContext.getTaskNameWithSubtasks(),</span><br><span class="line">deserializer,</span><br><span class="line">properties,</span><br><span class="line">pollTimeout,</span><br><span class="line">runtimeContext.getMetricGroup(),</span><br><span class="line">consumerMetricGroup,</span><br><span class="line">useMetrics);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//父类(FlinkKafkaConsumerBase)方法重写</span></span><br><span class="line"><span class="comment">// 返回一个分区发现类，分区发现可以使用kafka broker的高级consumer API发现topic和partition的元数据</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> AbstractPartitionDiscoverer <span class="title">createPartitionDiscoverer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">KafkaTopicsDescriptor topicsDescriptor,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> indexOfThisSubtask,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> numParallelSubtasks)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> KafkaPartitionDiscoverer(topicsDescriptor, indexOfThisSubtask, numParallelSubtasks, properties);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *判断是否在kafka的参数开启了自动提交，即enable.auto.commit=true，</span></span><br><span class="line"><span class="comment"> * 并且auto.commit.interval.ms&gt;0,</span></span><br><span class="line"><span class="comment"> * 注意：如果没有没有设置enable.auto.commit的参数，则默认为true</span></span><br><span class="line"><span class="comment"> *       如果没有设置auto.commit.interval.ms的参数，则默认为5000毫秒</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">getIsAutoCommitEnabled</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="keyword">return</span> getBoolean(properties, ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">true</span>) &amp;&amp;</span><br><span class="line">PropertiesUtil.getLong(properties, ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="number">5000</span>) &gt; <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 确保配置了kafka消息的key与value的反序列化方式，</span></span><br><span class="line"><span class="comment"> * 如果没有配置，则使用ByteArrayDeserializer序列化器，</span></span><br><span class="line"><span class="comment"> * 该类的deserialize方法是直接将数据进行return，未做任何处理</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">setDeserializer</span><span class="params">(Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">final</span> String deSerName = ByteArrayDeserializer.class.getName();</span><br><span class="line"></span><br><span class="line">Object keyDeSer = props.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">Object valDeSer = props.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (keyDeSer != <span class="keyword">null</span> &amp;&amp; !keyDeSer.equals(deSerName)) &#123;</span><br><span class="line">LOG.warn(<span class="string">"Ignoring configured key DeSerializer (&#123;&#125;)"</span>, ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (valDeSer != <span class="keyword">null</span> &amp;&amp; !valDeSer.equals(deSerName)) &#123;</span><br><span class="line">LOG.warn(<span class="string">"Ignoring configured value DeSerializer (&#123;&#125;)"</span>, ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">&#125;</span><br><span class="line">props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, deSerName);</span><br><span class="line">props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, deSerName);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>上面的代码已经给出了非常详细的注释，下面将对比较关键的部分进行分析。</p><ul><li><p>构造方法分析</p><p><img src="//jiamaoxiang.top/2020/04/02/你真的了解Flink-Kafka-connector吗？/F:%5Cnpm%5Cmywebsite%5Csource_posts%5C%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3Flink-Kafka-connector%E5%90%97%EF%BC%9F%5C%E6%9E%84%E9%80%A0%E6%96%B9%E6%B3%95%E9%87%8D%E5%86%99.png" alt></p></li></ul><p>FlinkKakfaConsumer提供了7种构造方法，如上图所示。不同的构造方法分别具有不同的功能，通过传递的参数也可以大致分析出每种构造方法特有的功能，为了方便理解，本文将对其进行分组讨论，具体如下：</p><p><strong>单topic</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topic                   消费的主题名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> valueDeserializer       反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props                   用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(String topic, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(Collections.singletonList(topic), valueDeserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * 该构造方法允许传入KafkaDeserializationSchema，该反序列化类支持访问kafka消费的额外信息</span></span><br><span class="line"><span class="comment"> * 比如：key/value对，offsets(偏移量)，topic(主题名称)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topic                消费的主题名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer         反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props                用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(String topic, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(Collections.singletonList(topic), deserializer, props);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面两种构造方法只支持单个topic，区别在于反序列化的方式不一样。第一种使用的是DeserializationSchema，第二种使用的是KafkaDeserializationSchema，其中使用带有KafkaDeserializationSchema参数的构造方法可以获取更多的附属信息，比如在某些场景下需要获取key/value对，offsets(偏移量)，topic(主题名称)等信息，可以选择使用此方式的构造方法。以上两种方法都调用了私有的构造方法，私有构造方法的分析见下面。</p><p><strong>多topic</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * 该构造方法允许传入多个topic(主题)，支持消费多个主题</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topics          消费的主题名称，多个主题为List集合</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer    反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props           用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(List&lt;String&gt; topics, DeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(topics, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(deserializer), props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * 该构造方法允许传入多个topic(主题)，支持消费多个主题,</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topics         消费的主题名称，多个主题为List集合</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer   反序列化类型，用于将kafka的字节消息转换为Flink的对象,支持获取额外信息</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props          用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(List&lt;String&gt; topics, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(topics, <span class="keyword">null</span>, deserializer, props);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的两种多topic的构造方法，可以使用一个list集合接收多个topic进行消费，区别在于反序列化的方式不一样。第一种使用的是DeserializationSchema，第二种使用的是KafkaDeserializationSchema，其中使用带有KafkaDeserializationSchema参数的构造方法可以获取更多的附属信息，比如在某些场景下需要获取key/value对，offsets(偏移量)，topic(主题名称)等信息，可以选择使用此方式的构造方法。以上两种方法都调用了私有的构造方法，私有构造方法的分析见下面。</p><p><strong>正则匹配topic</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于正则表达式订阅多个topic</span></span><br><span class="line"><span class="comment"> * 如果开启了分区发现，即FlinkKafkaConsumer.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值为非负数</span></span><br><span class="line"><span class="comment"> * 只要是能够正则匹配上，主题一旦被创建就会立即被订阅</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> subscriptionPattern   主题的正则表达式</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> valueDeserializer   反序列化类型，用于将kafka的字节消息转换为Flink的对象,支持获取额外信息</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props               用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(Pattern subscriptionPattern, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(<span class="keyword">null</span>, subscriptionPattern, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(valueDeserializer), props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于正则表达式订阅多个topic</span></span><br><span class="line"><span class="comment"> * 如果开启了分区发现，即FlinkKafkaConsumer.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值为非负数</span></span><br><span class="line"><span class="comment"> * 只要是能够正则匹配上，主题一旦被创建就会立即被订阅</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> subscriptionPattern   主题的正则表达式</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer          该反序列化类支持访问kafka消费的额外信息,比如：key/value对，offsets(偏移量)，topic(主题名称)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props                 用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(Pattern subscriptionPattern, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(<span class="keyword">null</span>, subscriptionPattern, deserializer, props);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实际的生产环境中可能有这样一些需求，比如有一个flink作业需要将多种不同的数据聚合到一起，而这些数据对应着不同的kafka topic，随着业务增长，新增一类数据，同时新增了一个kafka topic，如何在不重启作业的情况下作业自动感知新的topic。首先需要在构建FlinkKafkaConsumer时的properties中设置flink.partition-discovery.interval-millis参数为非负值，表示开启动态发现的开关，以及设置的时间间隔。此时FLinkKafkaConsumer内部会启动一个单独的线程定期去kafka获取最新的meta信息。具体的调用执行信息，参见下面的私有构造方法</p><p><strong>私有构造方法</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">List&lt;String&gt; topics,</span></span></span><br><span class="line"><span class="function"><span class="params">Pattern subscriptionPattern,</span></span></span><br><span class="line"><span class="function"><span class="params">KafkaDeserializationSchema&lt;T&gt; deserializer,</span></span></span><br><span class="line"><span class="function"><span class="params">Properties props)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用父类(FlinkKafkaConsumerBase)构造方法，PropertiesUtil.getLong方法第一个参数为Properties，第二个参数为key，第三个参数为value默认值。KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值是开启分区发现的配置参数，在properties里面配置flink.partition-discovery.interval-millis=5000(大于0的数),如果没有配置则使用PARTITION_DISCOVERY_DISABLED=Long.MIN_VALUE(表示禁用分区发现)</span></span><br><span class="line"><span class="keyword">super</span>(</span><br><span class="line">topics,</span><br><span class="line">subscriptionPattern,</span><br><span class="line">deserializer,</span><br><span class="line">getLong(</span><br><span class="line">checkNotNull(props, <span class="string">"props"</span>),</span><br><span class="line">KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS, PARTITION_DISCOVERY_DISABLED),</span><br><span class="line">!getBoolean(props, KEY_DISABLE_METRICS, <span class="keyword">false</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">this</span>.properties = props;</span><br><span class="line">setDeserializer(<span class="keyword">this</span>.properties);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 配置轮询超时时间，如果在properties中配置了KEY_POLL_TIMEOUT参数，则返回具体的配置值，否则返回默认值DEFAULT_POLL_TIMEOUT</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (properties.containsKey(KEY_POLL_TIMEOUT)) &#123;</span><br><span class="line"><span class="keyword">this</span>.pollTimeout = Long.parseLong(properties.getProperty(KEY_POLL_TIMEOUT));</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">this</span>.pollTimeout = DEFAULT_POLL_TIMEOUT;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Cannot parse poll timeout for '"</span> + KEY_POLL_TIMEOUT + <span class="string">'\''</span>, e);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>其他方法分析</li></ul><p><strong>KafkaFetcher对象创建</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">// 父类(FlinkKafkaConsumerBase)方法重写，该方法的作用是返回一个fetcher实例，</span></span><br><span class="line"><span class="comment">// fetcher的作用是连接kafka的broker，拉去数据并进行反序列化，然后将数据输出为数据流(data stream)</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">protected</span> AbstractFetcher&lt;T, ?&gt; createFetcher(</span><br><span class="line">SourceContext&lt;T&gt; sourceContext,</span><br><span class="line">Map&lt;KafkaTopicPartition, Long&gt; assignedPartitionsWithInitialOffsets,</span><br><span class="line">SerializedValue&lt;AssignerWithPeriodicWatermarks&lt;T&gt;&gt; watermarksPeriodic,</span><br><span class="line">SerializedValue&lt;AssignerWithPunctuatedWatermarks&lt;T&gt;&gt; watermarksPunctuated,</span><br><span class="line">StreamingRuntimeContext runtimeContext,</span><br><span class="line">OffsetCommitMode offsetCommitMode,</span><br><span class="line">MetricGroup consumerMetricGroup,</span><br><span class="line"><span class="keyword">boolean</span> useMetrics) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">       <span class="comment">// 确保当偏移量的提交模式为ON_CHECKPOINTS(条件1：开启checkpoint，条件2：consumer.setCommitOffsetsOnCheckpoints(true))时，禁用自动提交</span></span><br><span class="line"><span class="comment">// 该方法为父类(FlinkKafkaConsumerBase)的静态方法</span></span><br><span class="line"><span class="comment">// 这将覆盖用户在properties中配置的任何设置</span></span><br><span class="line"><span class="comment">// 当offset的模式为ON_CHECKPOINTS，或者为DISABLED时，会将用户配置的properties属性进行覆盖</span></span><br><span class="line"><span class="comment">// 具体是将ENABLE_AUTO_COMMIT_CONFIG = "enable.auto.commit"的值重置为"false</span></span><br><span class="line">       <span class="comment">// 可以理解为：如果开启了checkpoint，并且设置了consumer.setCommitOffsetsOnCheckpoints(true)，默认为true，</span></span><br><span class="line"><span class="comment">// 就会将kafka properties的enable.auto.commit强制置为false</span></span><br><span class="line">adjustAutoCommitConfig(properties, offsetCommitMode);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> KafkaFetcher&lt;&gt;(</span><br><span class="line">sourceContext,</span><br><span class="line">assignedPartitionsWithInitialOffsets,</span><br><span class="line">watermarksPeriodic,</span><br><span class="line">watermarksPunctuated,</span><br><span class="line">runtimeContext.getProcessingTimeService(),</span><br><span class="line">runtimeContext.getExecutionConfig().getAutoWatermarkInterval(),</span><br><span class="line">runtimeContext.getUserCodeClassLoader(),</span><br><span class="line">runtimeContext.getTaskNameWithSubtasks(),</span><br><span class="line">deserializer,</span><br><span class="line">properties,</span><br><span class="line">pollTimeout,</span><br><span class="line">runtimeContext.getMetricGroup(),</span><br><span class="line">consumerMetricGroup,</span><br><span class="line">useMetrics);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该方法的作用是返回一个fetcher实例，fetcher的作用是连接kafka的broker，拉去数据并进行反序列化，然后将数据输出为数据流(data stream)，在这里对自动偏移量提交模式进行了强制调整，即确保当偏移量的提交模式为ON_CHECKPOINTS(条件1：开启checkpoint，条件2：consumer.setCommitOffsetsOnCheckpoints(true))时，禁用自动提交。这将覆盖用户在properties中配置的任何设置，简单可以理解为：如果开启了checkpoint，并且设置了consumer.setCommitOffsetsOnCheckpoints(true)，默认为true，就会将kafka properties的enable.auto.commit强制置为false。关于offset的提交模式，见下文的偏移量提交模式分析。</p><p><strong>判断是否设置了自动提交</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">getIsAutoCommitEnabled</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="keyword">return</span> getBoolean(properties, ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">true</span>) &amp;&amp;</span><br><span class="line">PropertiesUtil.getLong(properties, ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="number">5000</span>) &gt; <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>判断是否在kafka的参数开启了自动提交，即enable.auto.commit=true，并且auto.commit.interval.ms&gt;0, 注意：如果没有没有设置enable.auto.commit的参数，则默认为true, 如果没有设置auto.commit.interval.ms的参数，则默认为5000毫秒。该方法会在FlinkKafkaConsumerBase的open方法进行初始化的时候调用。</p><p><strong>反序列化</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">setDeserializer</span><span class="params">(Properties props)</span> </span>&#123;</span><br><span class="line">         <span class="comment">// 默认的反序列化方式 </span></span><br><span class="line"><span class="keyword">final</span> String deSerName = ByteArrayDeserializer.class.getName();</span><br><span class="line">         <span class="comment">//获取用户配置的properties关于key与value的反序列化模式</span></span><br><span class="line">Object keyDeSer = props.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">Object valDeSer = props.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">         <span class="comment">// 如果配置了，则使用用户配置的值</span></span><br><span class="line"><span class="keyword">if</span> (keyDeSer != <span class="keyword">null</span> &amp;&amp; !keyDeSer.equals(deSerName)) &#123;</span><br><span class="line">LOG.warn(<span class="string">"Ignoring configured key DeSerializer (&#123;&#125;)"</span>, ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (valDeSer != <span class="keyword">null</span> &amp;&amp; !valDeSer.equals(deSerName)) &#123;</span><br><span class="line">LOG.warn(<span class="string">"Ignoring configured value DeSerializer (&#123;&#125;)"</span>, ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">&#125;</span><br><span class="line">        <span class="comment">// 没有配置，则使用ByteArrayDeserializer进行反序列化</span></span><br><span class="line">props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, deSerName);</span><br><span class="line">props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, deSerName);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>确保配置了kafka消息的key与value的反序列化方式，如果没有配置，则使用ByteArrayDeserializer序列化器，<br>ByteArrayDeserializer类的deserialize方法是直接将数据进行return，未做任何处理。</p><h4 id="FlinkKafkaConsumerBase源码"><a href="#FlinkKafkaConsumerBase源码" class="headerlink" title="FlinkKafkaConsumerBase源码"></a>FlinkKafkaConsumerBase源码</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkKafkaConsumerBase</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span></span></span><br><span class="line"><span class="class"><span class="title">CheckpointListener</span>,</span></span><br><span class="line"><span class="class"><span class="title">ResultTypeQueryable</span>&lt;<span class="title">T</span>&gt;,</span></span><br><span class="line"><span class="class"><span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAX_NUM_PENDING_CHECKPOINTS = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> PARTITION_DISCOVERY_DISABLED = Long.MIN_VALUE;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String KEY_DISABLE_METRICS = <span class="string">"flink.disable-metrics"</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS = <span class="string">"flink.partition-discovery.interval-millis"</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String OFFSETS_STATE_NAME = <span class="string">"topic-partition-offset-states"</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">boolean</span> enableCommitOnCheckpoints = <span class="keyword">true</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 偏移量的提交模式，仅能通过在FlinkKafkaConsumerBase#open(Configuration)进行配置</span></span><br><span class="line"><span class="comment"> * 该值取决于用户是否开启了checkpoint</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> OffsetCommitMode offsetCommitMode;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 配置从哪个位置开始消费kafka的消息，</span></span><br><span class="line"><span class="comment"> * 默认为StartupMode#GROUP_OFFSETS，即从当前提交的偏移量开始消费</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> StartupMode startupMode = StartupMode.GROUP_OFFSETS;</span><br><span class="line"><span class="keyword">private</span> Map&lt;KafkaTopicPartition, Long&gt; specificStartupOffsets;</span><br><span class="line"><span class="keyword">private</span> Long startupOffsetsTimestamp;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 确保当偏移量的提交模式为ON_CHECKPOINTS时，禁用自动提交，</span></span><br><span class="line"><span class="comment"> * 这将覆盖用户在properties中配置的任何设置。</span></span><br><span class="line"><span class="comment"> * 当offset的模式为ON_CHECKPOINTS，或者为DISABLED时，会将用户配置的properties属性进行覆盖</span></span><br><span class="line"><span class="comment"> * 具体是将ENABLE_AUTO_COMMIT_CONFIG = "enable.auto.commit"的值重置为"false，即禁用自动提交</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> properties       kafka配置的properties，会通过该方法进行覆盖</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> offsetCommitMode    offset提交模式</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">adjustAutoCommitConfig</span><span class="params">(Properties properties, OffsetCommitMode offsetCommitMode)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS || offsetCommitMode == OffsetCommitMode.DISABLED) &#123;</span><br><span class="line">properties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="string">"false"</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 决定是否在开启checkpoint时，在checkpoin之后提交偏移量，</span></span><br><span class="line"><span class="comment"> * 只有用户配置了启用checkpoint，该参数才会其作用</span></span><br><span class="line"><span class="comment"> * 如果没有开启checkpoint，则使用kafka的配置参数：enable.auto.commit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> commitOnCheckpoints</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setCommitOffsetsOnCheckpoints</span><span class="params">(<span class="keyword">boolean</span> commitOnCheckpoints)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.enableCommitOnCheckpoints = commitOnCheckpoints;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 从最早的偏移量开始消费，</span></span><br><span class="line"><span class="comment"> *该模式下，Kafka 中的已经提交的偏移量将被忽略，不会用作起始位置。</span></span><br><span class="line"><span class="comment"> *可以通过consumer1.setStartFromEarliest()进行设置</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromEarliest</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.startupMode = StartupMode.EARLIEST;</span><br><span class="line"><span class="keyword">this</span>.startupOffsetsTimestamp = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">this</span>.specificStartupOffsets = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 从最新的数据开始消费,</span></span><br><span class="line"><span class="comment"> *  该模式下，Kafka 中的 已提交的偏移量将被忽略，不会用作起始位置。</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromLatest</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.startupMode = StartupMode.LATEST;</span><br><span class="line"><span class="keyword">this</span>.startupOffsetsTimestamp = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">this</span>.specificStartupOffsets = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *指定具体的偏移量时间戳,毫秒</span></span><br><span class="line"><span class="comment"> *对于每个分区，其时间戳大于或等于指定时间戳的记录将用作起始位置。</span></span><br><span class="line"><span class="comment"> * 如果一个分区的最新记录早于指定的时间戳，则只从最新记录读取该分区数据。</span></span><br><span class="line"><span class="comment"> * 在这种模式下，Kafka 中的已提交 offset 将被忽略，不会用作起始位置。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromTimestamp</span><span class="params">(<span class="keyword">long</span> startupOffsetsTimestamp)</span> </span>&#123;</span><br><span class="line">checkArgument(startupOffsetsTimestamp &gt;= <span class="number">0</span>, <span class="string">"The provided value for the startup offsets timestamp is invalid."</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">long</span> currentTimestamp = System.currentTimeMillis();</span><br><span class="line">checkArgument(startupOffsetsTimestamp &lt;= currentTimestamp,</span><br><span class="line"><span class="string">"Startup time[%s] must be before current time[%s]."</span>, startupOffsetsTimestamp, currentTimestamp);</span><br><span class="line"></span><br><span class="line"><span class="keyword">this</span>.startupMode = StartupMode.TIMESTAMP;</span><br><span class="line"><span class="keyword">this</span>.startupOffsetsTimestamp = startupOffsetsTimestamp;</span><br><span class="line"><span class="keyword">this</span>.specificStartupOffsets = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 从具体的消费者组最近提交的偏移量开始消费，为默认方式</span></span><br><span class="line"><span class="comment"> * 如果没有发现分区的偏移量，使用auto.offset.reset参数配置的值</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromGroupOffsets</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.startupMode = StartupMode.GROUP_OFFSETS;</span><br><span class="line"><span class="keyword">this</span>.startupOffsetsTimestamp = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">this</span>.specificStartupOffsets = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *为每个分区指定偏移量进行消费</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromSpecificOffsets</span><span class="params">(Map&lt;KafkaTopicPartition, Long&gt; specificStartupOffsets)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.startupMode = StartupMode.SPECIFIC_OFFSETS;</span><br><span class="line"><span class="keyword">this</span>.startupOffsetsTimestamp = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">this</span>.specificStartupOffsets = checkNotNull(specificStartupOffsets);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration configuration)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// determine the offset commit mode</span></span><br><span class="line"><span class="comment">// 决定偏移量的提交模式，</span></span><br><span class="line"><span class="comment">// 第一个参数为是否开启了自动提交，</span></span><br><span class="line"><span class="comment">// 第二个参数为是否开启了CommitOnCheckpoint模式</span></span><br><span class="line"><span class="comment">// 第三个参数为是否开启了checkpoint</span></span><br><span class="line"><span class="keyword">this</span>.offsetCommitMode = OffsetCommitModes.fromConfiguration(</span><br><span class="line">getIsAutoCommitEnabled(),</span><br><span class="line">enableCommitOnCheckpoints,</span><br><span class="line">((StreamingRuntimeContext) getRuntimeContext()).isCheckpointingEnabled());</span><br><span class="line">       </span><br><span class="line">   <span class="comment">// 省略的代码</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 省略的代码</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个fetcher用于连接kafka的broker，拉去数据并进行反序列化，然后将数据输出为数据流(data stream)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> sourceContext   数据输出的上下文</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> subscribedPartitionsToStartOffsets  当前sub task需要处理的topic分区集合，即topic的partition与offset的Map集合</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> watermarksPeriodic    可选,一个序列化的时间戳提取器，生成periodic类型的 watermark</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> watermarksPunctuated  可选,一个序列化的时间戳提取器，生成punctuated类型的 watermark</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> runtimeContext        task的runtime context上下文</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> offsetCommitMode      offset的提交模式,有三种，分别为：DISABLED(禁用偏移量自动提交),ON_CHECKPOINTS(仅仅当checkpoints完成之后，才提交偏移量给kafka)</span></span><br><span class="line"><span class="comment"> * KAFKA_PERIODIC(使用kafka自动提交函数，周期性自动提交偏移量)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> kafkaMetricGroup   Flink的Metric</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> useMetrics         是否使用Metric</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span>                   返回一个fetcher实例</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">abstract</span> AbstractFetcher&lt;T, ?&gt; createFetcher(</span><br><span class="line">SourceContext&lt;T&gt; sourceContext,</span><br><span class="line">Map&lt;KafkaTopicPartition, Long&gt; subscribedPartitionsToStartOffsets,</span><br><span class="line">SerializedValue&lt;AssignerWithPeriodicWatermarks&lt;T&gt;&gt; watermarksPeriodic,</span><br><span class="line">SerializedValue&lt;AssignerWithPunctuatedWatermarks&lt;T&gt;&gt; watermarksPunctuated,</span><br><span class="line">StreamingRuntimeContext runtimeContext,</span><br><span class="line">OffsetCommitMode offsetCommitMode,</span><br><span class="line">MetricGroup kafkaMetricGroup,</span><br><span class="line"><span class="keyword">boolean</span> useMetrics) <span class="keyword">throws</span> Exception;</span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">getIsAutoCommitEnabled</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">// 省略的代码</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述代码是FlinkKafkaConsumerBase的部分代码片段，基本上对其做了详细注释，里面的有些方法是FlinkKafkaConsumer继承的，有些是重写的。之所以在这里给出，可以对照FlinkKafkaConsumer的源码，从而方便理解。</p><h3 id="偏移量提交模式分析"><a href="#偏移量提交模式分析" class="headerlink" title="偏移量提交模式分析"></a>偏移量提交模式分析</h3><p>Flink Kafka Consumer 允许有配置如何将 offset 提交回 Kafka broker（或 0.8 版本的 Zookeeper）的行为。请注意：Flink Kafka Consumer 不依赖于提交的 offset 来实现容错保证。提交的 offset 只是一种方法，用于公开 consumer 的进度以便进行监控。</p><p>配置 offset 提交行为的方法是否相同，取决于是否为 job 启用了 checkpointing。在这里先给出提交模式的具体结论，下面会对两种方式进行具体的分析。基本的结论为：</p><ul><li><p>开启checkpoint</p><ul><li><p>情况1：用户通过调用 consumer 上的 setCommitOffsetsOnCheckpoints(true) 方法来启用 offset 的提交(默认情况下为 true )<br>那么当 checkpointing 完成时，Flink Kafka Consumer 将提交的 offset 存储在 checkpoint 状态中。<br>这确保 Kafka broker 中提交的 offset 与 checkpoint 状态中的 offset 一致。<br>注意，在这个场景中，Properties 中的自动定期 offset 提交设置会被完全忽略。<br>此情况使用的是ON_CHECKPOINTS</p></li><li><p>情况2：用户通过调用 consumer 上的 setCommitOffsetsOnCheckpoints(“false”) 方法来禁用 offset 的提交，则使用DISABLED模式提交offset</p></li></ul></li><li><p>未开启checkpoint<br>Flink Kafka Consumer 依赖于内部使用的 Kafka client 自动定期 offset 提交功能，因此，要禁用或启用 offset 的提交</p></li><li><p>情况1：配置了Kafka properties的参数配置了”enable.auto.commit” = “true”或者 Kafka 0.8 的 auto.commit.enable=true，使用KAFKA_PERIODIC模式提交offset，即自动提交offset</p><ul><li>情况2：没有配置enable.auto.commit参数，使用DISABLED模式提交offset，这意味着kafka不知道当前的消费者组的消费者每次消费的偏移量。</li></ul></li></ul><h4 id="提交模式源码分析"><a href="#提交模式源码分析" class="headerlink" title="提交模式源码分析"></a>提交模式源码分析</h4><ul><li>offset的提交模式</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> OffsetCommitMode &#123;</span><br><span class="line"><span class="comment">// 禁用偏移量自动提交</span></span><br><span class="line">DISABLED,</span><br><span class="line"><span class="comment">// 仅仅当checkpoints完成之后，才提交偏移量给kafka</span></span><br><span class="line">ON_CHECKPOINTS,</span><br><span class="line"><span class="comment">// 使用kafka自动提交函数，周期性自动提交偏移量</span></span><br><span class="line">KAFKA_PERIODIC;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>提交模式的调用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OffsetCommitModes</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> OffsetCommitMode <span class="title">fromConfiguration</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">boolean</span> enableAutoCommit,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">boolean</span> enableCommitOnCheckpoint,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">boolean</span> enableCheckpointing)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 如果开启了checkinpoint，执行下面判断</span></span><br><span class="line"><span class="keyword">if</span> (enableCheckpointing) &#123;</span><br><span class="line"><span class="comment">// 如果开启了checkpoint，进一步判断是否在checkpoin启用时提交(setCommitOffsetsOnCheckpoints(true))，如果是则使用ON_CHECKPOINTS模式</span></span><br><span class="line"><span class="comment">// 否则使用DISABLED模式</span></span><br><span class="line"><span class="keyword">return</span> (enableCommitOnCheckpoint) ? OffsetCommitMode.ON_CHECKPOINTS : OffsetCommitMode.DISABLED;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// 若Kafka properties的参数配置了"enable.auto.commit" = "true"，则使用KAFKA_PERIODIC模式提交offset</span></span><br><span class="line"><span class="comment">// 否则使用DISABLED模式</span></span><br><span class="line"><span class="keyword">return</span> (enableAutoCommit) ? OffsetCommitMode.KAFKA_PERIODIC : OffsetCommitMode.DISABLED;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了Flink Kafka Consumer，首先对FlinkKafkaConsumer的不同版本进行了对比，然后给出了一个完整的Demo案例，并对案例的配置参数进行了详细解释，接着分析了FlinkKafkaConsumer的继承关系，并分别对FlinkKafkaConsumer以及其父类FlinkKafkaConsumerBase的源码进行了解读，最后从源码层面分析了Flink Kafka Consumer的偏移量提交模式，并对每一种提交模式进行了梳理。</p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink1.10集成Hive快速入门</title>
      <link href="/2020/03/31/Flink1-10%E9%9B%86%E6%88%90Hive%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"/>
      <url>/2020/03/31/Flink1-10%E9%9B%86%E6%88%90Hive%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<p>Hive 是大数据领域最早出现的 SQL 引擎，发展至今有着丰富的功能和广泛的用户基础。之后出现的 SQL 引擎，如 Spark SQL、Impala 等，都在一定程度上提供了与 Hive 集成的功能，从而方便用户使用现有的数据仓库、进行作业迁移等。</p><p>Flink从1.9开始支持集成Hive，不过1.9版本为beta版，不推荐在生产环境中使用。在最新版Flink1.10版本，标志着对 Blink的整合宣告完成，随着对 Hive 的生产级别集成，Hive作为数据仓库系统的绝对核心，承担着绝大多数的离线数据ETL计算和数据管理，期待Flink未来对Hive的完美支持。</p><p>而 HiveCatalog 会与一个 Hive Metastore 的实例连接，提供元数据持久化的能力。要使用 Flink 与 Hive 进行交互，用户需要配置一个 HiveCatalog，并通过 HiveCatalog 访问 Hive 中的元数据。</p><h2 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h2><p>要与Hive集成，需要在Flink的lib目录下添加额外的依赖jar包，以使集成在Table API程序或SQL Client中的SQL中起作用。或者，可以将这些依赖项放在文件夹中，并分别使用Table API程序或SQL Client 的<code>-C</code> 或<code>-l</code>选项将它们添加到classpath中。本文使用第一种方式，即将jar包直接复制到$FLINK_HOME/lib目录下。本文使用的Hive版本为2.3.4(对于不同版本的Hive，可以参照官网选择不同的jar包依赖)，总共需要3个jar包，如下：</p><ul><li>flink-connector-hive_2.11-1.10.0.jar</li><li>flink-shaded-hadoop-2-uber-2.7.5-8.0.jar</li><li>hive-exec-2.3.4.jar</li></ul><p>其中hive-exec-2.3.4.jar在hive的lib文件夹下，另外两个需要自行下载，下载地址：<a href="https://repo1.maven.org/maven2/org/apache/flink/flink-connector-hive_2.11/1.10.0/" target="_blank" rel="noopener">flink-connector-hive_2.11-1.10.0.jar</a>，<a href="//https://maven.aliyun.com/mvn/search">flink-shaded-hadoop-2-uber-2.7.5-8.0.jar</a></p><p><img src="//jiamaoxiang.top/2020/03/31/Flink1-10集成Hive快速入门/F:%5Cnpm%5Cmywebsite%5Csource_posts%5CFlink1-10%E9%9B%86%E6%88%90Hive%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%5Cdemo.png" alt></p><p><strong>切莫拔剑四顾心茫然，话不多说，直接上代码。</strong></p><h2 id="构建程序"><a href="#构建程序" class="headerlink" title="构建程序"></a>构建程序</h2><h3 id="添加Maven依赖"><a href="#添加Maven依赖" class="headerlink" title="添加Maven依赖"></a>添加Maven依赖</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Flink Dependency --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-java-bridge_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Hive Dependency --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="实例代码"><a href="#实例代码" class="headerlink" title="实例代码"></a>实例代码</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.flink.sql.hiveintegration;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.catalog.hive.HiveCatalog;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Created</span> with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@author</span> : jmx</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Date</span>: 2020/3/31</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Time</span>: 13:22</span></span><br><span class="line"><span class="comment"> *  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkHiveIntegration</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">                .newInstance()</span><br><span class="line">                .useBlinkPlanner() <span class="comment">// 使用BlinkPlanner</span></span><br><span class="line">                .inBatchMode() <span class="comment">// Batch模式，默认为StreamingMode</span></span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用StreamingMode</span></span><br><span class="line">       <span class="comment">/* EnvironmentSettings settings = EnvironmentSettings</span></span><br><span class="line"><span class="comment">                .newInstance()</span></span><br><span class="line"><span class="comment">                .useBlinkPlanner() // 使用BlinkPlanner</span></span><br><span class="line"><span class="comment">                .inStreamingMode() // StreamingMode</span></span><br><span class="line"><span class="comment">                .build();*/</span></span><br><span class="line"></span><br><span class="line">        TableEnvironment tableEnv = TableEnvironment.create(settings);</span><br><span class="line"></span><br><span class="line">        String name = <span class="string">"myhive"</span>;      <span class="comment">// Catalog名称，定义一个唯一的名称表示</span></span><br><span class="line">        String defaultDatabase = <span class="string">"qfbap_ods"</span>;  <span class="comment">// 默认数据库名称</span></span><br><span class="line">        String hiveConfDir = <span class="string">"/opt/modules/apache-hive-2.3.4-bin/conf"</span>;  <span class="comment">// hive-site.xml路径</span></span><br><span class="line">        String version = <span class="string">"2.3.4"</span>;       <span class="comment">// Hive版本号</span></span><br><span class="line"></span><br><span class="line">        HiveCatalog hive = <span class="keyword">new</span> HiveCatalog(name, defaultDatabase, hiveConfDir, version);</span><br><span class="line"></span><br><span class="line">        tableEnv.registerCatalog(<span class="string">"myhive"</span>, hive);</span><br><span class="line">        tableEnv.useCatalog(<span class="string">"myhive"</span>);</span><br><span class="line">        <span class="comment">// 创建数据库，目前不支持创建hive表</span></span><br><span class="line">        String createDbSql = <span class="string">"CREATE DATABASE IF NOT EXISTS myhive.test123"</span>;</span><br><span class="line"></span><br><span class="line">        tableEnv.sqlUpdate(createDbSql);  </span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Flink-SQL-Client集成Hive"><a href="#Flink-SQL-Client集成Hive" class="headerlink" title="Flink SQL Client集成Hive"></a>Flink SQL Client集成Hive</h2><p>Flink的表和SQL API可以处理用SQL语言编写的查询，但是这些查询需要嵌入到用Java或Scala编写的程序中。此外，这些程序在提交到集群之前需要与构建工具打包。这或多或少地限制了Java/Scala程序员对Flink的使用。</p><p>SQL客户端旨在提供一种简单的方式，无需一行Java或Scala代码，即可将表程序编写、调试和提交到Flink集群。Flink SQL客户端CLI允许通过命令行的形式运行分布式程序。使用Flink SQL cli访问Hive，需要配置sql-client-defaults.yaml文件。</p><h3 id="sql-client-defaults-yaml配置"><a href="#sql-client-defaults-yaml配置" class="headerlink" title="sql-client-defaults.yaml配置"></a>sql-client-defaults.yaml配置</h3><p>目前 HiveTableSink 不支持流式写入（未实现 AppendStreamTableSink）。需要将执行模式改成 batch<br>模式，否则会报如下错误：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">org.apache.flink.table.api.TableException: Stream Tables can only be emitted by AppendStreamTableSink, RetractStreamTableSink, or UpsertStreamTableSink.</span><br></pre></td></tr></table></figure><p>需要修改的配置内容如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#...省略的配置项...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line"><span class="comment"># Catalogs</span></span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line"><span class="comment"># 配置catalogs,可以配置多个.</span></span><br><span class="line">catalogs: <span class="comment"># empty list</span></span><br><span class="line">  - name: myhive</span><br><span class="line">    <span class="built_in">type</span>: hive</span><br><span class="line">    hive-conf-dir: /opt/modules/apache-hive-2.3.4-bin/conf</span><br><span class="line">    hive-version: 2.3.4</span><br><span class="line">    default-database: qfbap_ods</span><br><span class="line"></span><br><span class="line"><span class="comment">#...省略的配置项...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line"><span class="comment"># Execution properties</span></span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Properties that change the fundamental execution behavior of a table program.</span></span><br><span class="line"></span><br><span class="line">execution:</span><br><span class="line">  <span class="comment"># select the implementation responsible for planning table programs</span></span><br><span class="line">  <span class="comment"># possible values are 'blink' (used by default) or 'old'</span></span><br><span class="line">  planner: blink</span><br><span class="line">  <span class="comment"># 'batch' or 'streaming' execution</span></span><br><span class="line">  <span class="built_in">type</span>: batch</span><br></pre></td></tr></table></figure><h3 id="启动Flink-SQL-Cli"><a href="#启动Flink-SQL-Cli" class="headerlink" title="启动Flink SQL Cli"></a>启动Flink SQL Cli</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/sql-client.sh  embedded</span><br></pre></td></tr></table></figure><p>启动之后，就可以在此Cli下执行SQL命令访问Hive的表了，基本的操作如下：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 命令行帮助</span></span><br><span class="line">Flink SQL&gt; help</span><br><span class="line"><span class="comment">-- 查看当前会话的catalog，其中myhive为自己配置的，default_catalog为默认的</span></span><br><span class="line">Flink SQL&gt; show catalogs;</span><br><span class="line">default_catalog</span><br><span class="line">myhive</span><br><span class="line"><span class="comment">-- 使用catalog</span></span><br><span class="line">Flink SQL&gt; use catalog myhive;</span><br><span class="line"><span class="comment">-- 查看当前catalog的数据库</span></span><br><span class="line">Flink SQL&gt; show databases;</span><br><span class="line"><span class="comment">-- 创建数据库</span></span><br><span class="line">Flink SQL&gt; create database testdb;</span><br><span class="line"><span class="comment">-- 删除数据库</span></span><br><span class="line">Flink SQL&gt; drop database testdb;</span><br><span class="line"><span class="comment">-- 创建表</span></span><br><span class="line">Flink SQL&gt; create table tbl(id int,name string);</span><br><span class="line"><span class="comment">-- 删除表</span></span><br><span class="line">Flink SQL&gt; drop table tbl;</span><br><span class="line"><span class="comment">-- 查询表</span></span><br><span class="line">Flink SQL&gt; select * from  code_city;</span><br><span class="line"><span class="comment">-- 插入数据</span></span><br><span class="line">Flink SQL&gt; insert overwrite code_city select id,city,province,event_time from code_city_delta ;</span><br><span class="line">Flink SQL&gt; INSERT into code_city values(1,'南京','江苏','');</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文以最新版本的Flink为例，对Flink集成Hive进行了实操。首先通过代码的方式与Hive进行集成，然后介绍了如何使用Flink SQL 客户端访问Hive，并对其中会遇到的坑进行了描述，最后给出了Flink SQL Cli的详细使用。相信在未来的版本中Flink SQL会越来越完善，期待Flink未来对Hive的完美支持。</p><p>欢迎添加我的公众号，随时随地了解更多精彩内容。</p><p><img src="//jiamaoxiang.top/2020/03/31/Flink1-10集成Hive快速入门/F:%5Cnpm%5Cmywebsite%5Csource_posts%5CFlink1-10%E9%9B%86%E6%88%90Hive%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%5C%E4%B8%89%E7%BB%B4%E7%A0%81.png" alt></p><p><img src="//jiamaoxiang.top/2020/03/31/Flink1-10集成Hive快速入门/F:%5Cnpm%5Cmywebsite%5Csource_posts%5CFlink1-10%E9%9B%86%E6%88%90Hive%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%5C%E4%BA%8C%E7%BB%B4%E7%A0%81%E6%B5%B7%E6%8A%A5.jpg" alt></p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的八种分区策略源码解读</title>
      <link href="/2020/03/30/Flink%E7%9A%84%E5%85%AB%E7%A7%8D%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/"/>
      <url>/2020/03/30/Flink%E7%9A%84%E5%85%AB%E7%A7%8D%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p>Flink包含8中分区策略，这8中分区策略(分区器)分别如下面所示，本文将从源码的角度一一解读每个分区器的实现方式。</p><ul><li><strong>GlobalPartitioner</strong></li><li><strong>ShufflePartitioner</strong></li><li><strong>RebalancePartitioner</strong></li><li><strong>RescalePartitioner</strong></li><li><strong>BroadcastPartitioner</strong></li><li><strong>ForwardPartitioner</strong></li><li><strong>KeyGroupStreamPartitioner</strong></li><li><strong>CustomPartitionerWrapper</strong></li></ul><h2 id="继承关系图"><a href="#继承关系图" class="headerlink" title="继承关系图"></a>继承关系图</h2><h3 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h3><h4 id="名称"><a href="#名称" class="headerlink" title="名称"></a>名称</h4><p>**<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">#### 实现</span><br><span class="line"></span><br><span class="line">```java</span><br><span class="line">public interface ChannelSelector&lt;T extends IOReadableWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 初始化channels数量，channel可以理解为下游Operator的某个实例(并行算子的某个subtask).</span><br><span class="line"> */</span><br><span class="line">void setup(int numberOfChannels);</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> *根据当前的record以及Channel总数，</span><br><span class="line"> *决定应将record发送到下游哪个Channel。</span><br><span class="line"> *不同的分区策略会实现不同的该方法。</span><br><span class="line"> */</span><br><span class="line">int selectChannel(T record);</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">*是否以广播的形式发送到下游所有的算子实例</span><br><span class="line"> */</span><br><span class="line">boolean isBroadcast();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="抽象类"><a href="#抽象类" class="headerlink" title="抽象类"></a>抽象类</h3><h4 id="名称-1"><a href="#名称-1" class="headerlink" title="名称"></a>名称</h4><p>**<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">#### 实现</span><br><span class="line"></span><br><span class="line">```java</span><br><span class="line">public abstract class StreamPartitioner&lt;T&gt; implements</span><br><span class="line">ChannelSelector&lt;SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt;&gt;, Serializable &#123;</span><br><span class="line">private static final long serialVersionUID = 1L;</span><br><span class="line"></span><br><span class="line">protected int numberOfChannels;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void setup(int numberOfChannels) &#123;</span><br><span class="line">this.numberOfChannels = numberOfChannels;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public boolean isBroadcast() &#123;</span><br><span class="line">return false;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public abstract StreamPartitioner&lt;T&gt; copy();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="继承关系图-1"><a href="#继承关系图-1" class="headerlink" title="继承关系图"></a>继承关系图</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/F:%5Cnpm%5Cmywebsite%5Csource_posts%5CFlink%E7%9A%84%E5%85%AB%E7%A7%8D%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%5CFlink%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E7%B1%BB%E7%BB%A7%E6%89%BF%E5%85%B3%E7%B3%BB%E5%9B%BE.png" alt></p><h2 id="GlobalPartitioner"><a href="#GlobalPartitioner" class="headerlink" title="GlobalPartitioner"></a>GlobalPartitioner</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>该分区器会将所有的数据都发送到下游的某个算子实例(subtask id = 0)</p><h3 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 发送所有的数据到下游算子的第一个task(ID = 0)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GlobalPartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="comment">//只返回0，即只发送给下游算子的第一个task</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"GLOBAL"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解"><a href="#图解" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/F:%5Cnpm%5Cmywebsite%5Csource_posts%5CFlink%E7%9A%84%E5%85%AB%E7%A7%8D%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%5Cgloba.png" alt></p><h2 id="ShufflePartitioner"><a href="#ShufflePartitioner" class="headerlink" title="ShufflePartitioner"></a>ShufflePartitioner</h2><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p>随机选择一个下游算子实例进行发送</p><h3 id="源码解读-1"><a href="#源码解读-1" class="headerlink" title="源码解读"></a>源码解读</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 随机的选择一个channel进行发送</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ShufflePartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Random random = <span class="keyword">new</span> Random();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="comment">//产生[0,numberOfChannels)伪随机数，随机发送到下游的某个task</span></span><br><span class="line"><span class="keyword">return</span> random.nextInt(numberOfChannels);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> ShufflePartitioner&lt;T&gt;();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"SHUFFLE"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解-1"><a href="#图解-1" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/F:%5Cnpm%5Cmywebsite%5Csource_posts%5CFlink%E7%9A%84%E5%85%AB%E7%A7%8D%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%5Cshuffle.png" alt></p><h2 id="BroadcastPartitioner"><a href="#BroadcastPartitioner" class="headerlink" title="BroadcastPartitioner"></a>BroadcastPartitioner</h2><h3 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h3><p>发送到下游所有的算子实例</p><h3 id="源码解读-2"><a href="#源码解读-2" class="headerlink" title="源码解读"></a>源码解读</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 发送到所有的channel</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BroadcastPartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Broadcast模式是直接发送到下游的所有task，所以不需要通过下面的方法选择发送的通道</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">"Broadcast partitioner does not support select channels."</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isBroadcast</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"BROADCAST"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解-2"><a href="#图解-2" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/F:%5Cnpm%5Cmywebsite%5Csource_posts%5CFlink%E7%9A%84%E5%85%AB%E7%A7%8D%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%5Cbroadcast.png" alt></p><h2 id="RebalancePartitioner"><a href="#RebalancePartitioner" class="headerlink" title="RebalancePartitioner"></a>RebalancePartitioner</h2><h3 id="简介-3"><a href="#简介-3" class="headerlink" title="简介"></a>简介</h3><p>通过循环的方式依次发送到下游的task</p><h3 id="源码解读-3"><a href="#源码解读-3" class="headerlink" title="源码解读"></a>源码解读</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *通过循环的方式依次发送到下游的task</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RebalancePartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> nextChannelToSendTo;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(<span class="keyword">int</span> numberOfChannels)</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>.setup(numberOfChannels);</span><br><span class="line"><span class="comment">//初始化channel的id，返回[0,numberOfChannels)的伪随机数</span></span><br><span class="line">nextChannelToSendTo = ThreadLocalRandom.current().nextInt(numberOfChannels);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="comment">//循环依次发送到下游的task，比如：nextChannelToSendTo初始值为0，numberOfChannels(下游算子的实例个数，并行度)值为2</span></span><br><span class="line"><span class="comment">//则第一次发送到ID = 1的task，第二次发送到ID = 0的task，第三次发送到ID = 1的task上...依次类推</span></span><br><span class="line">nextChannelToSendTo = (nextChannelToSendTo + <span class="number">1</span>) % numberOfChannels;</span><br><span class="line"><span class="keyword">return</span> nextChannelToSendTo;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"REBALANCE"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解-3"><a href="#图解-3" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/F:%5Cnpm%5Cmywebsite%5Csource_posts%5CFlink%E7%9A%84%E5%85%AB%E7%A7%8D%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%5Crebalance.png" alt></p><h2 id="RescalePartitioner"><a href="#RescalePartitioner" class="headerlink" title="RescalePartitioner"></a>RescalePartitioner</h2><h3 id="简介-4"><a href="#简介-4" class="headerlink" title="简介"></a>简介</h3><p>基于上下游Operator的并行度，将记录以循环的方式输出到下游Operator的每个实例。<br>  举例: 上游并行度是2，下游是4，则上游一个并行度以循环的方式将记录输出到下游的两个并行度上;上游另一个并行度以循环的方式将记录输出到下游另两个并行度上。<br> 若上游并行度是4，下游并行度是2，则上游两个并行度将记录输出到下游一个并行度上；上游另两个并行度将记录输出到下游另一个并行度上。</p><h3 id="源码解读-4"><a href="#源码解读-4" class="headerlink" title="源码解读"></a>源码解读</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RescalePartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> nextChannelToSendTo = -<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (++nextChannelToSendTo &gt;= numberOfChannels) &#123;</span><br><span class="line">nextChannelToSendTo = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> nextChannelToSendTo;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"RESCALE"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解-4"><a href="#图解-4" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/F:%5Cnpm%5Cmywebsite%5Csource_posts%5CFlink%E7%9A%84%E5%85%AB%E7%A7%8D%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%5Crescale.png" alt></p><h4 id="尖叫提示"><a href="#尖叫提示" class="headerlink" title="尖叫提示"></a>尖叫提示</h4><p>Flink 中的执行图可以分成四层：StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图。</p><p>**<figure class="highlight plain"><figcaption><span>Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构。</span></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">**```JobGraph```**：StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。</span><br><span class="line"></span><br><span class="line">**```ExecutionGraph```**：JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。</span><br><span class="line"></span><br><span class="line">物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。</span><br><span class="line"></span><br><span class="line"> 而StreamingJobGraphGenerator就是StreamGraph转换为JobGraph。在这个类中，把ForwardPartitioner和RescalePartitioner列为POINTWISE分配模式，其他的为ALL_TO_ALL分配模式。代码如下：</span><br><span class="line"></span><br><span class="line">```java</span><br><span class="line">if (partitioner instanceof ForwardPartitioner || partitioner instanceof RescalePartitioner) &#123;</span><br><span class="line">jobEdge = downStreamVertex.connectNewDataSetAsInput(</span><br><span class="line">headVertex,</span><br><span class="line"></span><br><span class="line">   // 上游算子(生产端)的实例(subtask)连接下游算子(消费端)的一个或者多个实例(subtask)</span><br><span class="line">DistributionPattern.POINTWISE,</span><br><span class="line">resultPartitionType);</span><br><span class="line">&#125; else &#123;</span><br><span class="line">jobEdge = downStreamVertex.connectNewDataSetAsInput(</span><br><span class="line">headVertex,</span><br><span class="line">// 上游算子(生产端)的实例(subtask)连接下游算子(消费端)的所有实例(subtask)</span><br><span class="line">DistributionPattern.ALL_TO_ALL,</span><br><span class="line">resultPartitionType);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="ForwardPartitioner"><a href="#ForwardPartitioner" class="headerlink" title="ForwardPartitioner"></a>ForwardPartitioner</h2><h3 id="简介-5"><a href="#简介-5" class="headerlink" title="简介"></a>简介</h3><p>发送到下游对应的第一个task，保证上下游算子并行度一致，即上有算子与下游算子是1:1的关系</p><h3 id="源码解读-5"><a href="#源码解读-5" class="headerlink" title="源码解读"></a>源码解读</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 发送到下游对应的第一个task</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ForwardPartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"FORWARD"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解-5"><a href="#图解-5" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/F:%5Cnpm%5Cmywebsite%5Csource_posts%5CFlink%E7%9A%84%E5%85%AB%E7%A7%8D%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%5Cforward.png" alt></p><h4 id="尖叫提示-1"><a href="#尖叫提示-1" class="headerlink" title="尖叫提示"></a>尖叫提示</h4><p>在上下游的算子没有指定分区器的情况下，如果上下游的算子并行度一致，则使用ForwardPartitioner，否则使用RebalancePartitioner，对于ForwardPartitioner，必须保证上下游算子并行度一致，否则会抛出异常</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//在上下游的算子没有指定分区器的情况下，如果上下游的算子并行度一致，则使用ForwardPartitioner，否则使用RebalancePartitioner</span></span><br><span class="line"><span class="keyword">if</span> (partitioner == <span class="keyword">null</span> &amp;&amp; upstreamNode.getParallelism() == downstreamNode.getParallelism()) &#123;</span><br><span class="line">partitioner = <span class="keyword">new</span> ForwardPartitioner&lt;Object&gt;();</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (partitioner == <span class="keyword">null</span>) &#123;</span><br><span class="line">partitioner = <span class="keyword">new</span> RebalancePartitioner&lt;Object&gt;();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (partitioner <span class="keyword">instanceof</span> ForwardPartitioner) &#123;</span><br><span class="line"><span class="comment">//如果上下游的并行度不一致，会抛出异常</span></span><br><span class="line"><span class="keyword">if</span> (upstreamNode.getParallelism() != downstreamNode.getParallelism()) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">"Forward partitioning does not allow "</span> +</span><br><span class="line"><span class="string">"change of parallelism. Upstream operation: "</span> + upstreamNode + <span class="string">" parallelism: "</span> + upstreamNode.getParallelism() +</span><br><span class="line"><span class="string">", downstream operation: "</span> + downstreamNode + <span class="string">" parallelism: "</span> + downstreamNode.getParallelism() +</span><br><span class="line"><span class="string">" You must use another partitioning strategy, such as broadcast, rebalance, shuffle or global."</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="KeyGroupStreamPartitioner"><a href="#KeyGroupStreamPartitioner" class="headerlink" title="KeyGroupStreamPartitioner"></a>KeyGroupStreamPartitioner</h2><h3 id="简介-6"><a href="#简介-6" class="headerlink" title="简介"></a>简介</h3><p>根据key的分组索引选择发送到相对应的下游subtask</p><h3 id="源码解读-6"><a href="#源码解读-6" class="headerlink" title="源码解读"></a>源码解读</h3><ul><li>org.apache.flink.streaming.runtime.partitioner.KeyGroupStreamPartitioner</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据key的分组索引选择发送到相对应的下游subtask</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;K&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyGroupStreamPartitioner</span>&lt;<span class="title">T</span>, <span class="title">K</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span> <span class="title">ConfigurableStreamPartitioner</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line">K key;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">key = keySelector.getKey(record.getInstance().getValue());</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Could not extract key from "</span> + record.getInstance().getValue(), e);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//调用KeyGroupRangeAssignment类的assignKeyToParallelOperator方法,代码如下所示</span></span><br><span class="line"><span class="keyword">return</span> KeyGroupRangeAssignment.assignKeyToParallelOperator(key, maxParallelism, numberOfChannels);</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>org.apache.flink.runtime.state.KeyGroupRangeAssignment</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyGroupRangeAssignment</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据key分配一个并行算子实例的索引，该索引即为该key要发送的下游算子实例的路由信息，</span></span><br><span class="line"><span class="comment"> * 即该key发送到哪一个task</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">assignKeyToParallelOperator</span><span class="params">(Object key, <span class="keyword">int</span> maxParallelism, <span class="keyword">int</span> parallelism)</span> </span>&#123;</span><br><span class="line">Preconditions.checkNotNull(key, <span class="string">"Assigned key must not be null!"</span>);</span><br><span class="line"><span class="keyword">return</span> computeOperatorIndexForKeyGroup(maxParallelism, parallelism, assignToKeyGroup(key, maxParallelism));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *根据key分配一个分组id(keyGroupId)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">assignToKeyGroup</span><span class="params">(Object key, <span class="keyword">int</span> maxParallelism)</span> </span>&#123;</span><br><span class="line">Preconditions.checkNotNull(key, <span class="string">"Assigned key must not be null!"</span>);</span><br><span class="line"><span class="comment">//获取key的hashcode</span></span><br><span class="line"><span class="keyword">return</span> computeKeyGroupForKeyHash(key.hashCode(), maxParallelism);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据key分配一个分组id(keyGroupId),</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">computeKeyGroupForKeyHash</span><span class="params">(<span class="keyword">int</span> keyHash, <span class="keyword">int</span> maxParallelism)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//与maxParallelism取余，获取keyGroupId</span></span><br><span class="line"><span class="keyword">return</span> MathUtils.murmurHash(keyHash) % maxParallelism;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//计算分区index，即该key group应该发送到下游的哪一个算子实例</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">computeOperatorIndexForKeyGroup</span><span class="params">(<span class="keyword">int</span> maxParallelism, <span class="keyword">int</span> parallelism, <span class="keyword">int</span> keyGroupId)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> keyGroupId * parallelism / maxParallelism;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="图解-6"><a href="#图解-6" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/F:%5Cnpm%5Cmywebsite%5Csource_posts%5CFlink%E7%9A%84%E5%85%AB%E7%A7%8D%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%5Ckey.png" alt></p><h2 id="CustomPartitionerWrapper"><a href="#CustomPartitionerWrapper" class="headerlink" title="CustomPartitionerWrapper"></a>CustomPartitionerWrapper</h2><h3 id="简介-7"><a href="#简介-7" class="headerlink" title="简介"></a>简介</h3><p>通过<code>Partitioner</code>实例的<code>partition</code>方法(自定义的)将记录输出到下游。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomPartitionerWrapper</span>&lt;<span class="title">K</span>, <span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">Partitioner&lt;K&gt; partitioner;</span><br><span class="line">KeySelector&lt;T, K&gt; keySelector;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">CustomPartitionerWrapper</span><span class="params">(Partitioner&lt;K&gt; partitioner, KeySelector&lt;T, K&gt; keySelector)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.partitioner = partitioner;</span><br><span class="line"><span class="keyword">this</span>.keySelector = keySelector;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line">K key;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">key = keySelector.getKey(record.getInstance().getValue());</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Could not extract key from "</span> + record.getInstance(), e);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//实现Partitioner接口，重写partition方法</span></span><br><span class="line"><span class="keyword">return</span> partitioner.partition(key, numberOfChannels);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"CUSTOM"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>比如：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">      <span class="comment">// key: 根据key的值来分区</span></span><br><span class="line">      <span class="comment">// numPartitions: 下游算子并行度</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String key, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> key.length() % numPartitions;<span class="comment">//在此处定义分区策略</span></span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于Canal与Flink实现数据实时增量同步(二)</title>
      <link href="/2020/03/24/%E5%9F%BA%E4%BA%8ECanal%E4%B8%8EFlink%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%A2%9E%E9%87%8F%E5%90%8C%E6%AD%A5-%E4%BA%8C/"/>
      <url>/2020/03/24/%E5%9F%BA%E4%BA%8ECanal%E4%B8%8EFlink%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%A2%9E%E9%87%8F%E5%90%8C%E6%AD%A5-%E4%BA%8C/</url>
      
        <content type="html"><![CDATA[<p>本文主要从Binlog实时采集和离线处理Binlog还原业务数据两个方面，来介绍如何实现DB数据准确、高效地进入Hive数仓。</p><a id="more"></a><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在数据仓库建模中，未经任何加工处理的原始业务层数据，我们称之为ODS(Operational Data Store)数据。在互联网企业中，常见的ODS数据有业务日志数据（Log）和业务DB数据（DB）两类。对于业务DB数据来说，从MySQL等关系型数据库的业务数据进行采集，然后导入到Hive中，是进行数据仓库生产的重要环节。如何准确、高效地把MySQL数据同步到Hive中？一般常用的解决方案是批量取数并Load：直连MySQL去Select表中的数据，然后存到本地文件作为中间存储，最后把文件Load到Hive表中。这种方案的优点是实现简单，但是随着业务的发展，缺点也逐渐暴露出来：</p><ul><li><p>性能瓶颈：随着业务规模的增长，Select From MySQL -&gt; Save to Localfile -&gt; Load to Hive这种数据流花费的时间越来越长，无法满足下游数仓生产的时间要求。</p></li><li><p>直接从MySQL中Select大量数据，对MySQL的影响非常大，容易造成慢查询，影响业务线上的正常服务。</p></li><li><p>由于Hive本身的语法不支持更新、删除等SQL原语(高版本Hive支持，但是需要分桶+ORC存储格式)，对于MySQL中发生Update/Delete的数据无法很好地进行支持。</p></li></ul><p>为了彻底解决这些问题，我们逐步转向CDC (Change Data Capture) + Merge的技术方案，即实时Binlog采集 + 离线处理Binlog还原业务数据这样一套解决方案。Binlog是MySQL的二进制日志，记录了MySQL中发生的所有数据变更，MySQL集群自身的主从同步就是基于Binlog做的。</p><h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><p>首先，采用Flink负责把Kafka上的Binlog数据拉取到HDFS上。</p><p>然后，对每张ODS表，首先需要一次性制作快照（Snapshot），把MySQL里的存量数据读取到Hive上，这一过程底层采用直连MySQL去Select数据的方式，可以使用Sqoop进行一次性全量导入。</p><p>最后，对每张ODS表，每天基于存量数据和当天增量产生的Binlog做Merge，从而还原出业务数据。</p><p>Binlog是流式产生的，通过对Binlog的实时采集，把部分数据处理需求由每天一次的批处理分摊到实时流上。无论从性能上还是对MySQL的访问压力上，都会有明显地改善。Binlog本身记录了数据变更的类型（Insert/Update/Delete），通过一些语义方面的处理，完全能够做到精准的数据还原。</p><h2 id="实现方案"><a href="#实现方案" class="headerlink" title="实现方案"></a>实现方案</h2><h3 id="Flink处理Kafka的binlog日志"><a href="#Flink处理Kafka的binlog日志" class="headerlink" title="Flink处理Kafka的binlog日志"></a>Flink处理Kafka的binlog日志</h3><p>使用kafka source，对读取的数据进行JSON解析，将解析的字段拼接成字符串，符合Hive的schema格式，具体代码如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.etl.kafka2hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONArray;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.parser.Feature;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FilterFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringEncoder;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.core.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.StateBackend;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.filesystem.FsStateBackend;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.CheckpointConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.RollingPolicy;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Created</span> with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@author</span> : jmx</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Date</span>: 2020/3/27</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Time</span>: 12:52</span></span><br><span class="line"><span class="comment"> *  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsSink</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String fieldDelimiter = <span class="string">","</span>;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// checkpoint</span></span><br><span class="line">        env.enableCheckpointing(<span class="number">10_000</span>);</span><br><span class="line">        <span class="comment">//env.setStateBackend((StateBackend) new FsStateBackend("file:///E://checkpoint"));</span></span><br><span class="line">        env.setStateBackend((StateBackend) <span class="keyword">new</span> FsStateBackend(<span class="string">"hdfs://kms-1:8020/checkpoint"</span>));</span><br><span class="line">        CheckpointConfig config = env.getCheckpointConfig();</span><br><span class="line">        config.enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// source</span></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.setProperty(<span class="string">"bootstrap.servers"</span>, <span class="string">"kms-2:9092,kms-3:9092,kms-4:9092"</span>);</span><br><span class="line">        <span class="comment">// only required for Kafka 0.8</span></span><br><span class="line">        props.setProperty(<span class="string">"zookeeper.connect"</span>, <span class="string">"kms-2:2181,kms-3:2181,kms-4:2181"</span>);</span><br><span class="line">        props.setProperty(<span class="string">"group.id"</span>, <span class="string">"test123"</span>);</span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; consumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(</span><br><span class="line">                <span class="string">"qfbap_ods.code_city"</span>, <span class="keyword">new</span> SimpleStringSchema(), props);</span><br><span class="line">        consumer.setStartFromEarliest();</span><br><span class="line">        DataStream&lt;String&gt; stream = env.addSource(consumer);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// transform</span></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; cityDS = stream</span><br><span class="line">                .filter(<span class="keyword">new</span> FilterFunction&lt;String&gt;() &#123;</span><br><span class="line">                    <span class="comment">// 过滤掉DDL操作</span></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(String jsonVal)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        JSONObject record = JSON.parseObject(jsonVal, Feature.OrderedField);</span><br><span class="line">                        <span class="keyword">return</span> record.getString(<span class="string">"isDdl"</span>).equals(<span class="string">"false"</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        StringBuilder fieldsBuilder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">                        <span class="comment">// 解析JSON数据</span></span><br><span class="line">                        JSONObject record = JSON.parseObject(value, Feature.OrderedField);</span><br><span class="line">                        <span class="comment">// 获取最新的字段值</span></span><br><span class="line">                        JSONArray data = record.getJSONArray(<span class="string">"data"</span>);</span><br><span class="line">                        <span class="comment">// 遍历，字段值的JSON数组，只有一个元素</span></span><br><span class="line">                        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; data.size(); i++) &#123;</span><br><span class="line">                            <span class="comment">// 获取到JSON数组的第i个元素</span></span><br><span class="line">                            JSONObject obj = data.getJSONObject(i);</span><br><span class="line">                            <span class="keyword">if</span> (obj != <span class="keyword">null</span>) &#123;</span><br><span class="line">                                fieldsBuilder.append(record.getLong(<span class="string">"id"</span>)); <span class="comment">// 序号id</span></span><br><span class="line">                                fieldsBuilder.append(fieldDelimiter); <span class="comment">// 字段分隔符</span></span><br><span class="line">                                fieldsBuilder.append(record.getLong(<span class="string">"es"</span>)); <span class="comment">//业务时间戳</span></span><br><span class="line">                                fieldsBuilder.append(fieldDelimiter);</span><br><span class="line">                                fieldsBuilder.append(record.getLong(<span class="string">"ts"</span>)); <span class="comment">// 日志时间戳</span></span><br><span class="line">                                fieldsBuilder.append(fieldDelimiter);</span><br><span class="line">                                fieldsBuilder.append(record.getString(<span class="string">"type"</span>)); <span class="comment">// 操作类型</span></span><br><span class="line">                                <span class="keyword">for</span> (Map.Entry&lt;String, Object&gt; entry : obj.entrySet()) &#123;</span><br><span class="line"></span><br><span class="line">                                    fieldsBuilder.append(fieldDelimiter);</span><br><span class="line">                                    fieldsBuilder.append(entry.getValue()); <span class="comment">// 表字段数据</span></span><br><span class="line">                                &#125;</span><br><span class="line"></span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">return</span> fieldsBuilder.toString();</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//cityDS.print();</span></span><br><span class="line">        <span class="comment">//stream.print();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// sink</span></span><br><span class="line">        <span class="comment">// 以下条件满足其中之一就会滚动生成新的文件</span></span><br><span class="line">        RollingPolicy&lt;String, String&gt; rollingPolicy = DefaultRollingPolicy.create()</span><br><span class="line">                .withRolloverInterval(<span class="number">60L</span> * <span class="number">1000L</span>) <span class="comment">//滚动写入新文件的时间，默认60s。根据具体情况调节</span></span><br><span class="line">                .withMaxPartSize(<span class="number">1024</span> * <span class="number">1024</span> * <span class="number">128L</span>) <span class="comment">//设置每个文件的最大大小 ,默认是128M，这里设置为128M</span></span><br><span class="line">                .withInactivityInterval(<span class="number">60L</span> * <span class="number">1000L</span>) <span class="comment">//默认60秒,未写入数据处于不活跃状态超时会滚动新文件</span></span><br><span class="line">                .build();</span><br><span class="line">        </span><br><span class="line">        StreamingFileSink&lt;String&gt; sink = StreamingFileSink</span><br><span class="line">                <span class="comment">//.forRowFormat(new Path("file:///E://binlog_db/city"), new SimpleStringEncoder&lt;String&gt;())</span></span><br><span class="line">                .forRowFormat(<span class="keyword">new</span> Path(<span class="string">"hdfs://kms-1:8020/binlog_db/code_city_delta"</span>), <span class="keyword">new</span> SimpleStringEncoder&lt;String&gt;())</span><br><span class="line">                .withBucketAssigner(<span class="keyword">new</span> EventTimeBucketAssigner())</span><br><span class="line">                .withRollingPolicy(rollingPolicy)</span><br><span class="line">                .withBucketCheckInterval(<span class="number">1000</span>)  <span class="comment">// 桶检查间隔，这里设置1S</span></span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        cityDS.addSink(sink);</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于Flink Sink到HDFS，<code>StreamingFileSink</code> 替代了先前的 <code>BucketingSink</code>，用来将上游数据存储到 HDFS 的不同目录中。它的核心逻辑是分桶，默认的分桶方式是 <code>DateTimeBucketAssigner</code>，即按照处理时间分桶。处理时间指的是消息到达 Flink 程序的时间，这点并不符合我们的需求。因此，我们需要自己编写代码将事件时间从消息体中解析出来，按规则生成分桶的名称，具体代码如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.etl.kafka2hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.core.io.SimpleVersionedSerializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.BucketAssigner;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.SimpleVersionedStringSerializer;</span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Created</span> with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@author</span> : jmx</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Date</span>: 2020/3/27</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Time</span>: 12:49</span></span><br><span class="line"><span class="comment"> *  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EventTimeBucketAssigner</span> <span class="keyword">implements</span> <span class="title">BucketAssigner</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getBucketId</span><span class="params">(String element, Context context)</span> </span>&#123;</span><br><span class="line">        String partitionValue;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            partitionValue = getPartitionValue(element);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            partitionValue = <span class="string">"00000000"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"dt="</span> + partitionValue;<span class="comment">//分区目录名称</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SimpleVersionedSerializer&lt;String&gt; <span class="title">getSerializer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> SimpleVersionedStringSerializer.INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> String <span class="title">getPartitionValue</span><span class="params">(String element)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 取出最后拼接字符串的es字段值，该值为业务时间</span></span><br><span class="line">        <span class="keyword">long</span> eventTime = Long.parseLong(element.split(<span class="string">","</span>)[<span class="number">1</span>]);</span><br><span class="line">        Date eventDate = <span class="keyword">new</span> Date(eventTime);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> SimpleDateFormat(<span class="string">"yyyyMMdd"</span>).format(eventDate);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="离线还原MySQL数据"><a href="#离线还原MySQL数据" class="headerlink" title="离线还原MySQL数据"></a>离线还原MySQL数据</h3><p>经过上述步骤，即可将Binlog日志记录写入到HDFS的对应的分区中，接下来就需要根据增量的数据和存量的数据还原最新的数据。Hive 表保存在 HDFS 上，该文件系统不支持修改，因此我们需要一些额外工作来写入数据变更。常用的方式包括：JOIN、Hive 事务、或改用 HBase、kudu。</p><p>如昨日的存量数据code_city,今日增量的数据为code_city_delta，可以通过 <code>FULL OUTER JOIN</code>，将存量和增量数据合并成一张最新的数据表，并作为明天的存量数据：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> code_city</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">        <span class="keyword">COALESCE</span>( t2.id, t1.id ) <span class="keyword">AS</span> <span class="keyword">id</span>,</span><br><span class="line">        <span class="keyword">COALESCE</span> ( t2.city, t1.city ) <span class="keyword">AS</span> city,</span><br><span class="line">        <span class="keyword">COALESCE</span> ( t2.province, t1.province ) <span class="keyword">AS</span> province,</span><br><span class="line">        <span class="keyword">COALESCE</span> ( t2.event_time, t1.event_time ) <span class="keyword">AS</span> event_time </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">        code_city t1</span><br><span class="line">        <span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> (</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">        <span class="keyword">id</span>,</span><br><span class="line">        city,</span><br><span class="line">        province,</span><br><span class="line">        event_time </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">        (<span class="comment">-- 取最后一条状态数据</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">        <span class="keyword">id</span>,</span><br><span class="line">        city,</span><br><span class="line">        province,</span><br><span class="line">        dml_type,</span><br><span class="line">        event_time,</span><br><span class="line">        row_number ( ) <span class="keyword">over</span> ( <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">id</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> event_time <span class="keyword">DESC</span> ) <span class="keyword">AS</span> <span class="keyword">rank</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">        code_city_delta </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">        dt = <span class="string">'20200324'</span> <span class="comment">-- 分区数据</span></span><br><span class="line">        ) temp </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">        <span class="keyword">rank</span> = <span class="number">1</span> </span><br><span class="line">        ) t2 <span class="keyword">ON</span> t1.id = t2.id;</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要从Binlog流式采集和基于Binlog的ODS数据还原两方面，介绍了通过Flink实现实时的ETL，此外还可以将binlog日志写入kudu、HBase等支持事务操作的NoSQL中，这样就可以省去数据表还原的步骤。本文是《基于Canal与Flink实现数据实时增量同步》的第二篇，关于canal解析Binlog日志写入kafka的实现步骤，参见《基于Canal与Flink实现数据实时增量同步一》。</p><p><strong>refrence：</strong></p><p>[1]<a href="https://tech.meituan.com/2018/12/06/binlog-dw.html" target="_blank" rel="noopener">https://tech.meituan.com/2018/12/06/binlog-dw.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式数据集成框架gobblin快速入门</title>
      <link href="/2020/03/22/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90%E6%A1%86%E6%9E%B6gobblin%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"/>
      <url>/2020/03/22/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90%E6%A1%86%E6%9E%B6gobblin%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/apache/incubator-gobblin" target="_blank" rel="noopener">Apache Gobblin </a>是一个通用的分布式数据集成框架，用于从各种数据源（数据库，REST API，FTP / SFTP服务器，文件管理器等）提取，转换和加载大量数据到Hadoop上。使得大数据集成变得更加简单，例如<strong>流</strong>和<strong>批处理</strong>数据生态系统的数据摄取，复制，组织和生命周期管理。gobblin由LinkedIn开源，现为Apache的孵化项目。</p><hr><h2 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h2><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><p>下载gobblin的发布包，下载地址为：<a href="https://github.com/apache/incubator-gobblin/releases" target="_blank" rel="noopener">https://github.com/apache/incubator-gobblin/releases</a>,本文档下载的版本为<a href="https://github.com/apache/incubator-gobblin/releases/tag/release-0.14.0" target="_blank" rel="noopener">release-0.14.0</a>，包名称为：incubator-gobblin-release-0.14.0.tar.gz</p><h3 id="解压编译"><a href="#解压编译" class="headerlink" title="解压编译"></a>解压编译</h3><ul><li>解压源码tar包，进入解压文件夹</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar -xzvf incubator-gobblin-release-0.14.0.tar.gz -C /opt/module/</span></span><br><span class="line"><span class="comment"># cd /opt/module/incubator-gobblin-release-0.14.0</span></span><br></pre></td></tr></table></figure><ul><li>编译</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./gradlew :gobblin-distribution:buildDistributionTar</span><br></pre></td></tr></table></figure><p>编译过程大概几分钟，编译完成后会生成一个build文件夹，进入该文件夹会看到生成的tar包名称为：apache-gobblin-incubating-bin-0.14.0.tar.gz</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cd /opt/module/incubator-gobblin-release-0.14.0/build/gobblin-distribution/distributions</span></span><br></pre></td></tr></table></figure><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>将上面编译好的tar包解压，解压后的文件名称为gobblin-dist</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar -xzvf gobblin-incubating-bin-0.14.0.tar.gz  -C /opt/module/</span></span><br><span class="line"><span class="comment"># ll</span></span><br><span class="line">drwxr-xr-x 2 root root  4096 3月  22 17:35 bin</span><br><span class="line">drwxr-xr-x 6 root root  4096 3月  22 17:35 conf</span><br><span class="line">drwxr-xr-x 2 root root 16384 3月  22 17:35 lib</span><br></pre></td></tr></table></figure><h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2>]]></content>
      
      
      <categories>
          
          <category> gobblin </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gobblin </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于Canal与Flink实现数据实时增量同步(一)</title>
      <link href="/2020/03/05/%E5%9F%BA%E4%BA%8ECanal%E4%B8%8EFlink%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%A2%9E%E9%87%8F%E5%90%8C%E6%AD%A5-%E4%B8%80/"/>
      <url>/2020/03/05/%E5%9F%BA%E4%BA%8ECanal%E4%B8%8EFlink%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%A2%9E%E9%87%8F%E5%90%8C%E6%AD%A5-%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<p>canal是阿里巴巴旗下的一款开源项目，纯Java开发。基于数据库增量日志解析，提供增量数据订阅&amp;消费，目前主要支持了MySQL（也支持mariaDB）。</p><a id="more"></a><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><h3 id="常见的binlog命令"><a href="#常见的binlog命令" class="headerlink" title="常见的binlog命令"></a>常见的binlog命令</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 是否启用binlog日志</span></span><br><span class="line">show variables like <span class="string">'log_bin'</span>;</span><br><span class="line"><span class="comment"># 查看binlog类型</span></span><br><span class="line">show global variables like <span class="string">'binlog_format'</span>;</span><br><span class="line"><span class="comment"># 查看详细的日志配置信息</span></span><br><span class="line">show global variables like <span class="string">'%log%'</span>;</span><br><span class="line"><span class="comment"># mysql数据存储目录</span></span><br><span class="line">show variables like <span class="string">'%dir%'</span>;</span><br><span class="line"><span class="comment"># 查看binlog的目录</span></span><br><span class="line">show global variables like <span class="string">"%log_bin%"</span>;</span><br><span class="line"><span class="comment"># 查看当前服务器使用的biglog文件及大小</span></span><br><span class="line">show binary logs;</span><br><span class="line"><span class="comment"># 查看最新一个binlog日志文件名称和Position</span></span><br><span class="line">show master status;</span><br></pre></td></tr></table></figure><h3 id="配置MySQL的binlog"><a href="#配置MySQL的binlog" class="headerlink" title="配置MySQL的binlog"></a>配置MySQL的binlog</h3><p>对于自建 MySQL , 需要先开启 Binlog 写入功能，配置 binlog-format 为 ROW 模式，my.cnf 中配置如下</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line"><span class="built_in">log</span>-bin=mysql-bin <span class="comment"># 开启 binlog</span></span><br><span class="line">binlog-format=ROW <span class="comment"># 选择 ROW 模式</span></span><br><span class="line">server_id=1 <span class="comment"># 配置 MySQL replaction 需要定义，不要和 canal 的 slaveId 重复</span></span><br></pre></td></tr></table></figure><h3 id="授权"><a href="#授权" class="headerlink" title="授权"></a>授权</h3><p>授权 canal 链接 MySQL 账号具有作为 MySQL slave 的权限, 如果已有账户可直接 grant</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">USER</span> canal <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'canal'</span>;  </span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span>, <span class="keyword">REPLICATION</span> <span class="keyword">SLAVE</span>, <span class="keyword">REPLICATION</span> <span class="keyword">CLIENT</span> <span class="keyword">ON</span> *.* <span class="keyword">TO</span> <span class="string">'canal'</span>@<span class="string">'%'</span>;</span><br><span class="line"><span class="comment">-- GRANT ALL PRIVILEGES ON *.* TO 'canal'@'%' ;</span></span><br><span class="line"><span class="keyword">FLUSH</span> <span class="keyword">PRIVILEGES</span>;</span><br></pre></td></tr></table></figure><h2 id="部署canal"><a href="#部署canal" class="headerlink" title="部署canal"></a>部署canal</h2><h3 id="安装canal"><a href="#安装canal" class="headerlink" title="安装canal"></a>安装canal</h3><ul><li>下载：<a href="https://github.com/alibaba/canal/releases" target="_blank" rel="noopener">点此下载</a></li><li>解压缩</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[kms@kms-1 softwares]$ tar -xzvf canal.deployer-1.1.4.tar.gz  -C /opt/modules/canal/</span><br></pre></td></tr></table></figure><ul><li>目录结构</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">drwxr-xr-x 2 root root 4096 Mar  5 14:19 bin</span><br><span class="line">drwxr-xr-x 5 root root 4096 Mar  5 13:54 conf</span><br><span class="line">drwxr-xr-x 2 root root 4096 Mar  5 13:04 lib</span><br><span class="line">drwxrwxrwx 4 root root 4096 Mar  5 14:19 logs</span><br></pre></td></tr></table></figure><h3 id="配置修改"><a href="#配置修改" class="headerlink" title="配置修改"></a>配置修改</h3><ul><li>修改conf/example/instance.properties，修改内容如下：</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## mysql serverId</span></span><br><span class="line">canal.instance.mysql.slaveId = 1234</span><br><span class="line"><span class="comment">#position info，需要改成自己的数据库信息</span></span><br><span class="line">canal.instance.master.address = kms-1.apache.com:3306 </span><br><span class="line"><span class="comment">#username/password，需要改成自己的数据库信息</span></span><br><span class="line">canal.instance.dbUsername = canal  </span><br><span class="line">canal.instance.dbPassword = canal</span><br><span class="line"><span class="comment"># mq config，kafka topic名称</span></span><br><span class="line">canal.mq.topic=<span class="built_in">test</span></span><br></pre></td></tr></table></figure><ul><li>修改conf/canal.properties，修改内容如下：</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 配置zookeeper地址</span></span><br><span class="line">canal.zkServers =kms-2:2181,kms-3:2181,kms-4:2181</span><br><span class="line"><span class="comment"># 可选项: tcp(默认), kafka, RocketMQ，</span></span><br><span class="line">canal.serverMode = kafka</span><br><span class="line"><span class="comment"># 配置kafka地址</span></span><br><span class="line">canal.mq.servers = kms-2:9092,kms-3:9092,kms-4:9092</span><br></pre></td></tr></table></figure><h3 id="启动canal"><a href="#启动canal" class="headerlink" title="启动canal"></a>启动canal</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh bin/startup.sh</span><br></pre></td></tr></table></figure><h3 id="关闭canal"><a href="#关闭canal" class="headerlink" title="关闭canal"></a>关闭canal</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh bin/stop.sh</span><br></pre></td></tr></table></figure><h2 id="部署Canal-Admin-可选"><a href="#部署Canal-Admin-可选" class="headerlink" title="部署Canal Admin(可选)"></a>部署Canal Admin(可选)</h2><p>canal-admin设计上是为canal提供整体配置管理、节点运维等面向运维的功能，提供相对友好的WebUI操作界面，方便更多用户快速和安全的操作。</p><h3 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h3><p>canal-admin的限定依赖：</p><ul><li>MySQL，用于存储配置和节点等相关数据</li><li>canal版本，要求&gt;=1.1.4 (需要依赖canal-server提供面向admin的动态运维管理接口)</li></ul><h3 id="安装canal-admin"><a href="#安装canal-admin" class="headerlink" title="安装canal-admin"></a>安装canal-admin</h3><ul><li><p>下载</p><p><a href="https://github.com/alibaba/canal/releases" target="_blank" rel="noopener">点此下载</a></p></li><li><p>解压缩</p></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[kms@kms-1 softwares]$ tar -xzvf canal.admin-1.1.4.tar.gz  -C /opt/modules/canal-admin/</span><br></pre></td></tr></table></figure><ul><li>目录结构</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">drwxrwxr-x 2 kms kms 4096 Mar  6 11:25 bin</span><br><span class="line">drwxrwxr-x 3 kms kms 4096 Mar  6 11:25 conf</span><br><span class="line">drwxrwxr-x 2 kms kms 4096 Mar  6 11:25 lib</span><br><span class="line">drwxrwxr-x 2 kms kms 4096 Sep  2  2019 logs</span><br></pre></td></tr></table></figure><ul><li>配置修改</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi conf/application.yml</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">server:</span><br><span class="line">  port: 8089</span><br><span class="line">spring:</span><br><span class="line">  jackson:</span><br><span class="line">    date-format: yyyy-MM-dd HH:mm:ss</span><br><span class="line">    time-zone: GMT+8</span><br><span class="line"></span><br><span class="line">spring.datasource:</span><br><span class="line">  address: kms-1:3306</span><br><span class="line">  database: canal_manager</span><br><span class="line">  username: canal</span><br><span class="line">  password: canal</span><br><span class="line">  driver-class-name: com.mysql.jdbc.Driver</span><br><span class="line">  url: jdbc:mysql://<span class="variable">$&#123;spring.datasource.address&#125;</span>/<span class="variable">$&#123;spring.datasource.database&#125;</span>?useUnicode=<span class="literal">true</span>&amp;characterEncoding=UTF-8&amp;useSSL=<span class="literal">false</span></span><br><span class="line">  hikari:</span><br><span class="line">    maximum-pool-size: 30</span><br><span class="line">    minimum-idle: 1</span><br><span class="line"></span><br><span class="line">canal:</span><br><span class="line">  adminUser: admin</span><br><span class="line">  adminPasswd: admin</span><br></pre></td></tr></table></figure><ul><li>初始化原数据库</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql -uroot -p</span><br><span class="line"><span class="comment"># 导入初始化SQL</span></span><br><span class="line"><span class="comment">#注：(1)初始化SQL脚本里会默认创建canal_manager的数据库，建议使用root等有超级权限的账号进行初始化 </span></span><br><span class="line"><span class="comment">#    (2)canal_manager.sql默认会在conf目录下</span></span><br><span class="line">&gt; mysql&gt; <span class="built_in">source</span> /opt/modules/canal-admin/conf/canal_manager.sql</span><br></pre></td></tr></table></figure><ul><li>启动canal-admin</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh bin/startup.sh</span><br></pre></td></tr></table></figure><ul><li>访问</li></ul><p>可以通过 <a href="http://kms-1:8089/" target="_blank" rel="noopener">http://kms-1:8089/</a> 访问，默认密码：admin/123456 </p><ul><li>canal-server端配置</li></ul><p>使用canal_local.properties的配置覆盖canal.properties,将下面配置内容配置在canal_local.properties文件里面，就可以了。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># register ip</span></span><br><span class="line">canal.register.ip =</span><br><span class="line"><span class="comment"># canal admin config</span></span><br><span class="line">canal.admin.manager = 127.0.0.1:8089</span><br><span class="line">canal.admin.port = 11110</span><br><span class="line">canal.admin.user = admin</span><br><span class="line">canal.admin.passwd = 4ACFE3202A5FF5CF467898FC58AAB1D615029441</span><br><span class="line"><span class="comment"># admin auto register</span></span><br><span class="line">canal.admin.register.auto = <span class="literal">true</span></span><br><span class="line">canal.admin.register.cluster =</span><br></pre></td></tr></table></figure><ul><li>启动canal-serve</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh bin/startup.sh  <span class="built_in">local</span></span><br></pre></td></tr></table></figure><p>注意：先启canal-server,然后再启动canal-admin，之后登陆canal-admin就可以添加serve和instance了。</p><h2 id="启动kafka控制台消费者测试"><a href="#启动kafka控制台消费者测试" class="headerlink" title="启动kafka控制台消费者测试"></a>启动kafka控制台消费者测试</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server kms-2:9092,kms-3:9092,kms-4:9092  --topic <span class="built_in">test</span> --from-beginning</span><br></pre></td></tr></table></figure><p>此时MySQL数据表若有变化，会将row类型的log写进Kakfa，具体格式为JSON：</p><ul><li>insert操作</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"data"</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"id"</span>:<span class="string">"338"</span>,</span><br><span class="line">            <span class="string">"city"</span>:<span class="string">"成都"</span>,</span><br><span class="line">            <span class="string">"province"</span>:<span class="string">"四川省"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"database"</span>:<span class="string">"qfbap_ods"</span>,</span><br><span class="line">    <span class="string">"es"</span>:1583394964000,</span><br><span class="line">    <span class="string">"id"</span>:2,</span><br><span class="line">    <span class="string">"isDdl"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="string">"mysqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:<span class="string">"int(11)"</span>,</span><br><span class="line">        <span class="string">"city"</span>:<span class="string">"varchar(256)"</span>,</span><br><span class="line">        <span class="string">"province"</span>:<span class="string">"varchar(256)"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"old"</span>:null,</span><br><span class="line">    <span class="string">"pkNames"</span>:[</span><br><span class="line">        <span class="string">"id"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"sql"</span>:<span class="string">""</span>,</span><br><span class="line">    <span class="string">"sqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:4,</span><br><span class="line">        <span class="string">"city"</span>:12,</span><br><span class="line">        <span class="string">"province"</span>:12</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"table"</span>:<span class="string">"code_city"</span>,</span><br><span class="line">    <span class="string">"ts"</span>:1583394964361,</span><br><span class="line">    <span class="string">"type"</span>:<span class="string">"INSERT"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>update操作</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"data"</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"id"</span>:<span class="string">"338"</span>,</span><br><span class="line">            <span class="string">"city"</span>:<span class="string">"绵阳市"</span>,</span><br><span class="line">            <span class="string">"province"</span>:<span class="string">"四川省"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"database"</span>:<span class="string">"qfbap_ods"</span>,</span><br><span class="line">    <span class="string">"es"</span>:1583395177000,</span><br><span class="line">    <span class="string">"id"</span>:3,</span><br><span class="line">    <span class="string">"isDdl"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="string">"mysqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:<span class="string">"int(11)"</span>,</span><br><span class="line">        <span class="string">"city"</span>:<span class="string">"varchar(256)"</span>,</span><br><span class="line">        <span class="string">"province"</span>:<span class="string">"varchar(256)"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"old"</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"city"</span>:<span class="string">"成都"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"pkNames"</span>:[</span><br><span class="line">        <span class="string">"id"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"sql"</span>:<span class="string">""</span>,</span><br><span class="line">    <span class="string">"sqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:4,</span><br><span class="line">        <span class="string">"city"</span>:12,</span><br><span class="line">        <span class="string">"province"</span>:12</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"table"</span>:<span class="string">"code_city"</span>,</span><br><span class="line">    <span class="string">"ts"</span>:1583395177408,</span><br><span class="line">    <span class="string">"type"</span>:<span class="string">"UPDATE"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>delete操作</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"data"</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"id"</span>:<span class="string">"338"</span>,</span><br><span class="line">            <span class="string">"city"</span>:<span class="string">"绵阳市"</span>,</span><br><span class="line">            <span class="string">"province"</span>:<span class="string">"四川省"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"database"</span>:<span class="string">"qfbap_ods"</span>,</span><br><span class="line">    <span class="string">"es"</span>:1583395333000,</span><br><span class="line">    <span class="string">"id"</span>:4,</span><br><span class="line">    <span class="string">"isDdl"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="string">"mysqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:<span class="string">"int(11)"</span>,</span><br><span class="line">        <span class="string">"city"</span>:<span class="string">"varchar(256)"</span>,</span><br><span class="line">        <span class="string">"province"</span>:<span class="string">"varchar(256)"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"old"</span>:null,</span><br><span class="line">    <span class="string">"pkNames"</span>:[</span><br><span class="line">        <span class="string">"id"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"sql"</span>:<span class="string">""</span>,</span><br><span class="line">    <span class="string">"sqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:4,</span><br><span class="line">        <span class="string">"city"</span>:12,</span><br><span class="line">        <span class="string">"province"</span>:12</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"table"</span>:<span class="string">"code_city"</span>,</span><br><span class="line">    <span class="string">"ts"</span>:1583395333208,</span><br><span class="line">    <span class="string">"type"</span>:<span class="string">"DELETE"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="JSON日志格式解释"><a href="#JSON日志格式解释" class="headerlink" title="JSON日志格式解释"></a>JSON日志格式解释</h3><ul><li>data：最新的数据，为JSON数组，如果是插入则表示最新插入的数据，如果是更新，则表示更新后的最新数据，如果是删除，则表示被删除的数据</li><li>database：数据库名称</li><li>es：事件时间，13位的时间戳</li><li>id：事件操作的序列号，1,2,3…</li><li>isDdl：是否是DDL操作</li><li>mysqlType：字段类型</li><li>old：旧数据</li><li>pkNames：主键名称</li><li>sql：SQL语句</li><li>sqlType：是经过canal转换处理的，比如unsigned int会被转化为Long，unsigned long会被转换为BigDecimal</li><li>table：表名</li><li>ts：日志时间</li><li>type：操作类型，比如DELETE，UPDATE，INSERT</li></ul><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>本文首先介绍了MySQL binlog日志的配置以及Canal的搭建，然后描述了通过canal数据传输到Kafka的配置，最后对canal解析之后的JSON数据进行了详细解释。本文是基于Canal与Flink实现数据实时增量同步的第一篇，在下一篇介绍如何使用Flink实现实时增量数据同步。</p><hr>]]></content>
      
      
      <categories>
          
          <category> canal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> canal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQL中的相关子查询解析</title>
      <link href="/2019/12/10/SQL%E4%B8%AD%E7%9A%84%E7%9B%B8%E5%85%B3%E5%AD%90%E6%9F%A5%E8%AF%A2%E8%A7%A3%E6%9E%90/"/>
      <url>/2019/12/10/SQL%E4%B8%AD%E7%9A%84%E7%9B%B8%E5%85%B3%E5%AD%90%E6%9F%A5%E8%AF%A2%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>分步骤解析SQL的相关子查询</p><a id="more"></a><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>在SQL语言中，一个SELECT-FROM-WHERE语句称为一个查询块。讲一个查询块嵌套在另一个查询块的WHERE子句或者HAVing短语的条件中的查询称为嵌套查询。其中上层查询块称为外层查询或者父查询，下层查询称为内查询或者子查询。</p><p>根据子查询是否依赖于父查询，可以分为不相关子查询和相关子查询。其中子查询的查询条件不依赖于父查询，称为不相关子查询，如果子查询的查询条件依赖于父查询，则这类子查询称之为相关子查询。</p><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><table><thead><tr><th align="center">学号</th><th align="center">课程号</th><th align="center">成绩</th></tr></thead><tbody><tr><td align="center">201215121</td><td align="center">1</td><td align="center">92</td></tr><tr><td align="center">201215121</td><td align="center">2</td><td align="center">85</td></tr><tr><td align="center">201215121</td><td align="center">3</td><td align="center">88</td></tr><tr><td align="center">201215122</td><td align="center">2</td><td align="center">90</td></tr><tr><td align="center">201215122</td><td align="center">3</td><td align="center">80</td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> SC (Sno <span class="built_in">char</span>(<span class="number">9</span>), Cno <span class="built_in">char</span>(<span class="number">4</span>),Grade <span class="built_in">SMALLINT</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> SC;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> SC <span class="keyword">values</span> (<span class="string">'201215121'</span>,<span class="string">'1'</span>, <span class="number">92</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> SC <span class="keyword">values</span> (<span class="string">'201215121'</span>,<span class="string">'2'</span>, <span class="number">85</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> SC <span class="keyword">values</span> (<span class="string">'201215121'</span>,<span class="string">'3'</span>, <span class="number">88</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> SC <span class="keyword">values</span> (<span class="string">'201215122'</span>,<span class="string">'2'</span>,<span class="number">90</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> SC <span class="keyword">values</span> (<span class="string">'201215122'</span>,<span class="string">'3'</span>,<span class="number">80</span>);</span><br></pre></td></tr></table></figure><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><ul><li>找出每个学生超过他自己选修课程平均分的课程号</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> Sno,Cno </span><br><span class="line"><span class="keyword">FROM</span> SC x </span><br><span class="line"><span class="keyword">WHERE</span> Grade &gt;= ( <span class="keyword">SELECT</span> <span class="keyword">avg</span>( Grade )</span><br><span class="line">                 <span class="keyword">FROM</span> SC y </span><br><span class="line"><span class="keyword">WHERE</span> y.Sno = x.Sno )</span><br></pre></td></tr></table></figure><p>x是SC的别名，又称为元祖变量，可以用来表示SC的一个元祖。内层查询时求一个学生所有选修课程的平均成绩的，至于是哪一个学生的平均成绩要看参数<code>x.Sno</code>的值，而该值是与父查询相关的，因此这类查询称之为相关子查询。</p><p>这个语句的一种可能执行过程采用以下三个步骤。</p><p>1.从外层查询中取出SC的一个元祖x，将元祖x的Sno值(201215121)传送给内层查询。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">AVG</span>(Grade)</span><br><span class="line"><span class="keyword">FROM</span>  SC y</span><br><span class="line"><span class="keyword">WHERE</span> y.Sno=<span class="string">'201215121'</span>;</span><br></pre></td></tr></table></figure><p>2.执行内层查询，得到值88(近似值)，用该值代替内层查询，得到外层查询：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> Sno,Cno</span><br><span class="line"><span class="keyword">FROM</span>  SC x</span><br><span class="line"><span class="keyword">WHERE</span> Grade &gt; <span class="number">88</span>;</span><br></pre></td></tr></table></figure><p>3.执行这个查询，得到 (201215121,1)</p><p>然后外层查询取出下一个元祖重复上述1至2步骤的处理，直到外层的SC元祖全部处理完毕。</p><hr><ul><li><p><strong>reference</strong></p><p>[1]王珊, 萨师煊. 数据库系统概论(第5版)</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeeCode数据库部分题目汇总</title>
      <link href="/2019/12/09/LeeCode%E6%95%B0%E6%8D%AE%E5%BA%93%E9%83%A8%E5%88%86%E9%A2%98%E7%9B%AE%E6%B1%87%E6%80%BB/"/>
      <url>/2019/12/09/LeeCode%E6%95%B0%E6%8D%AE%E5%BA%93%E9%83%A8%E5%88%86%E9%A2%98%E7%9B%AE%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<p>LeeCode数据库部分SQL题目总结</p><a id="more"></a><h2 id="176-第二高的薪水"><a href="#176-第二高的薪水" class="headerlink" title="176. 第二高的薪水"></a>176. 第二高的薪水</h2><h3 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h3><p>编写一个 SQL 查询，获取 Employee 表中第二高的薪水（Salary）</p><table><thead><tr><th align="center">Id</th><th align="center">Salary</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">100</td></tr><tr><td align="center">2</td><td align="center">200</td></tr><tr><td align="center">3</td><td align="center">300</td></tr></tbody></table><p>例如上述 Employee 表，SQL查询应该返回 200 作为第二高的薪水。如果不存在第二高的薪水，那么查询应返回 null</p><table><thead><tr><th align="center">SecondHighestSalary</th></tr></thead><tbody><tr><td align="center">200</td></tr></tbody></table><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Employee (<span class="keyword">Id</span> <span class="built_in">int</span>, Salary <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Employee;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, Salary) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'100'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, Salary) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'200'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, Salary) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'300'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句"><a href="#SQL语句" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">MAX</span>(Salary) SecondHighestSalary</span><br><span class="line"><span class="keyword">FROM</span> Employee</span><br><span class="line"><span class="keyword">WHERE</span> Salary &lt;</span><br><span class="line">(<span class="keyword">SELECT</span> <span class="keyword">MAX</span>(Salary) <span class="keyword">FROM</span> Employee)</span><br></pre></td></tr></table></figure><h2 id="178-分数排名"><a href="#178-分数排名" class="headerlink" title="178.分数排名"></a>178.分数排名</h2><h3 id="描述-1"><a href="#描述-1" class="headerlink" title="描述"></a>描述</h3><p>编写一个 SQL 查询来实现分数排名。如果两个分数相同，则两个分数排名（Rank）相同。请注意，平分后的下一个名次应该是下一个连续的整数值。换句话说，名次之间不应该有“间隔”。</p><table><thead><tr><th align="center">Id</th><th align="center">Score</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">3.50</td></tr><tr><td align="center">2</td><td align="center">3.65</td></tr><tr><td align="center">3</td><td align="center">4.00</td></tr><tr><td align="center">4</td><td align="center">3.85</td></tr><tr><td align="center">5</td><td align="center">4.00</td></tr><tr><td align="center">6</td><td align="center">3.65</td></tr></tbody></table><p>例如，根据上述给定的 Scores 表，你的查询应该返回（按分数从高到低排列）：</p><table><thead><tr><th align="center">Score</th><th align="center">Rank</th></tr></thead><tbody><tr><td align="center">4.00</td><td align="center">1</td></tr><tr><td align="center">4.00</td><td align="center">1</td></tr><tr><td align="center">3.85</td><td align="center">2</td></tr><tr><td align="center">3.65</td><td align="center">3</td></tr><tr><td align="center">3.65</td><td align="center">3</td></tr><tr><td align="center">3.50</td><td align="center">4</td></tr></tbody></table><h3 id="数据准备-1"><a href="#数据准备-1" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Scores (<span class="keyword">Id</span> <span class="built_in">int</span>, Score <span class="built_in">DECIMAL</span>(<span class="number">3</span>,<span class="number">2</span>));</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Scores;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Scores (<span class="keyword">Id</span>, Score) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'3.5'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Scores (<span class="keyword">Id</span>, Score) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'3.65'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Scores (<span class="keyword">Id</span>, Score) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'4.0'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Scores (<span class="keyword">Id</span>, Score) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'3.85'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Scores (<span class="keyword">Id</span>, Score) <span class="keyword">values</span> (<span class="string">'5'</span>, <span class="string">'4.0'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Scores (<span class="keyword">Id</span>, Score) <span class="keyword">values</span> (<span class="string">'6'</span>, <span class="string">'3.65'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-1"><a href="#SQL语句-1" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">Score,</span><br><span class="line">@<span class="keyword">rank</span> := @<span class="keyword">rank</span> + (@prev &lt;&gt; (@prev := Score)) <span class="keyword">Rank</span></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Scores,</span><br><span class="line">(<span class="keyword">SELECT</span> @<span class="keyword">rank</span> := <span class="number">0</span>, @prev := <span class="number">-1</span>) init</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> Score <span class="keyword">desc</span></span><br></pre></td></tr></table></figure><h2 id="180-连续出现的数字"><a href="#180-连续出现的数字" class="headerlink" title="180. 连续出现的数字"></a>180. 连续出现的数字</h2><h3 id="描述-2"><a href="#描述-2" class="headerlink" title="描述"></a>描述</h3><p>编写一个 SQL 查询，查找所有至少连续出现三次的数字。</p><table><thead><tr><th align="center">Id</th><th align="center">Num</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">1</td></tr><tr><td align="center">2</td><td align="center">1</td></tr><tr><td align="center">3</td><td align="center">1</td></tr><tr><td align="center">4</td><td align="center">2</td></tr><tr><td align="center">5</td><td align="center">1</td></tr><tr><td align="center">6</td><td align="center">2</td></tr><tr><td align="center">7</td><td align="center">2</td></tr></tbody></table><p>例如，给定上面的 Logs 表， 1 是唯一连续出现至少三次的数字。</p><table><thead><tr><th align="center">ConsecutiveNums</th></tr></thead><tbody><tr><td align="center">1</td></tr></tbody></table><h3 id="数据准备-2"><a href="#数据准备-2" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Num</span> <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> <span class="keyword">Logs</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'5'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'6'</span>, <span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'7'</span>, <span class="string">'2'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-2"><a href="#SQL语句-2" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> l1.Num ConsecutiveNums</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">Logs</span> l1,</span><br><span class="line"><span class="keyword">Logs</span> l2,</span><br><span class="line"><span class="keyword">Logs</span> l3</span><br><span class="line"><span class="keyword">WHERE</span> l1.Id=l2.Id<span class="number">-1</span></span><br><span class="line"><span class="keyword">AND</span> l2.Id =l3.Id<span class="number">-1</span></span><br><span class="line"><span class="keyword">AND</span> l1.Num =l2.Num</span><br><span class="line"><span class="keyword">AND</span> l2.Num =l3.Num</span><br></pre></td></tr></table></figure><h2 id="181-超过经理收入的员工"><a href="#181-超过经理收入的员工" class="headerlink" title="181. 超过经理收入的员工"></a>181. 超过经理收入的员工</h2><h3 id="描述-3"><a href="#描述-3" class="headerlink" title="描述"></a>描述</h3><p>Employee 表包含所有员工，他们的经理也属于员工。每个员工都有一个 Id，此外还有一列对应员工的经理的 Id。</p><table><thead><tr><th align="center">Id</th><th align="center">Name</th><th align="center">Salary</th><th align="center">ManagerId</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">Joe</td><td align="center">70000</td><td align="center">3</td></tr><tr><td align="center">2</td><td align="center">Henry</td><td align="center">80000</td><td align="center">4</td></tr><tr><td align="center">3</td><td align="center">Sam</td><td align="center">60000</td><td align="center">null</td></tr><tr><td align="center">4</td><td align="center">Max</td><td align="center">90000</td><td align="center">null</td></tr></tbody></table><p>给定 Employee 表，编写一个 SQL 查询，该查询可以获取收入超过他们经理的员工的姓名。在上面的表格中，Joe 是唯一一个收入超过他的经理的员工</p><table><thead><tr><th align="center">Employee</th></tr></thead><tbody><tr><td align="center">Joe</td></tr></tbody></table><h3 id="数据准备-3"><a href="#数据准备-3" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Employee (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), Salary <span class="built_in">int</span>, ManagerId <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Employee;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, ManagerId) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'Joe'</span>, <span class="string">'70000'</span>, <span class="string">'3'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, ManagerId) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Henry'</span>, <span class="string">'80000'</span>, <span class="string">'4'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, ManagerId) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'Sam'</span>, <span class="string">'60000'</span>, <span class="string">'None'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, ManagerId) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'Max'</span>, <span class="string">'90000'</span>, <span class="string">'None'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-3"><a href="#SQL语句-3" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">     a.NAME <span class="keyword">AS</span> Employee</span><br><span class="line"><span class="keyword">FROM</span> Employee <span class="keyword">AS</span> a <span class="keyword">JOIN</span> Employee <span class="keyword">AS</span> b</span><br><span class="line">     <span class="keyword">ON</span> a.ManagerId = b.Id</span><br><span class="line">     <span class="keyword">AND</span> a.Salary &gt; b.Salary</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="182-查找重复的电子邮箱"><a href="#182-查找重复的电子邮箱" class="headerlink" title="182. 查找重复的电子邮箱"></a>182. 查找重复的电子邮箱</h2><h3 id="描述-4"><a href="#描述-4" class="headerlink" title="描述"></a>描述</h3><p>编写一个 SQL 查询，查找 Person 表中所有重复的电子邮箱。示例：</p><table><thead><tr><th align="center">Id</th><th align="center">Email</th></tr></thead><tbody><tr><td align="center">1</td><td align="center"><a href="mailto:a@b.com" target="_blank" rel="noopener">a@b.com</a></td></tr><tr><td align="center">2</td><td align="center"><a href="mailto:c@d.com" target="_blank" rel="noopener">c@d.com</a></td></tr><tr><td align="center">3</td><td align="center"><a href="mailto:a@b.com" target="_blank" rel="noopener">a@b.com</a></td></tr></tbody></table><p>根据以上输入，你的查询应返回以下结果：</p><table><thead><tr><th align="center">Email</th></tr></thead><tbody><tr><td align="center"><a href="mailto:a@b.com" target="_blank" rel="noopener">a@b.com</a></td></tr></tbody></table><p>说明：所有电子邮箱都是小写字母。</p><h3 id="数据准备-4"><a href="#数据准备-4" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Person (<span class="keyword">Id</span> <span class="built_in">int</span>, Email <span class="built_in">varchar</span>(<span class="number">255</span>));</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Person;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Person (<span class="keyword">Id</span>, Email) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'a@b.com'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Person (<span class="keyword">Id</span>, Email) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'c@d.com'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Person (<span class="keyword">Id</span>, Email) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'a@b.com'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-4"><a href="#SQL语句-4" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 方法1：</span></span><br><span class="line"><span class="keyword">select</span> Email <span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">  <span class="keyword">select</span> Email, <span class="keyword">count</span>(Email) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line">  <span class="keyword">from</span> Person</span><br><span class="line">  <span class="keyword">group</span> <span class="keyword">by</span> Email</span><br><span class="line">) <span class="keyword">as</span> statistic</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">num</span> &gt; <span class="number">1</span></span><br><span class="line">;</span><br><span class="line"><span class="comment">-- 方法2</span></span><br><span class="line"><span class="keyword">select</span> Email</span><br><span class="line"><span class="keyword">from</span> Person</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> Email</span><br><span class="line"><span class="keyword">having</span> <span class="keyword">count</span>(Email) &gt; <span class="number">1</span>;</span><br><span class="line"><span class="comment">-- 方法3</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">     <span class="keyword">distinct</span>(P1.Email) <span class="string">'Email'</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">      Person P1,</span><br><span class="line">      Person P2</span><br><span class="line"><span class="keyword">where</span> P1.Id &lt;&gt; P2.Id <span class="keyword">and</span> P1.Email = P2.Email</span><br></pre></td></tr></table></figure><h2 id="183-从不订购的客户"><a href="#183-从不订购的客户" class="headerlink" title="183. 从不订购的客户"></a>183. 从不订购的客户</h2><h3 id="描述-5"><a href="#描述-5" class="headerlink" title="描述"></a>描述</h3><p>某网站包含两个表，Customers 表和 Orders 表。编写一个 SQL 查询，找出所有从不订购任何东西的客户。</p><p>Customers 表：</p><table><thead><tr><th align="center">Id</th><th align="center">Name</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">Joe</td></tr><tr><td align="center">2</td><td align="center">Henry</td></tr><tr><td align="center">3</td><td align="center">Sam</td></tr><tr><td align="center">4</td><td align="center">Max</td></tr></tbody></table><p>Orders 表：</p><table><thead><tr><th align="center">Id</th><th align="center">CustomerId</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">3</td></tr><tr><td align="center">2</td><td align="center">1</td></tr></tbody></table><p>例如给定上述表格，你的查询应返回：</p><table><thead><tr><th align="center">Customers</th></tr></thead><tbody><tr><td align="center">Henry</td></tr><tr><td align="center">Max</td></tr></tbody></table><h3 id="数据准备-5"><a href="#数据准备-5" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Customers (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Name</span> <span class="built_in">varchar</span>(<span class="number">255</span>));</span><br><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Orders (<span class="keyword">Id</span> <span class="built_in">int</span>, CustomerId <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Customers;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Customers (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'Joe'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Customers (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Henry'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Customers (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'Sam'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Customers (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'Max'</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Orders;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Orders (<span class="keyword">Id</span>, CustomerId) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'3'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Orders (<span class="keyword">Id</span>, CustomerId) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'1'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-5"><a href="#SQL语句-5" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 方法1：</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">a.NAME <span class="string">'Customers'</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Customers a</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> Orders b <span class="keyword">ON</span> a.Id = b.CustomerId </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">b.Id <span class="keyword">IS</span> <span class="literal">NULL</span></span><br><span class="line"><span class="comment">-- 方法2：</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">NAME</span></span><br><span class="line"><span class="string">'Customers'</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Customers </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line"><span class="keyword">Id</span> <span class="keyword">NOT</span> <span class="keyword">IN</span> ( <span class="keyword">SELECT</span> CustomerId <span class="keyword">FROM</span> Orders )</span><br></pre></td></tr></table></figure><h2 id="184-部门工资最高的员工"><a href="#184-部门工资最高的员工" class="headerlink" title="184. 部门工资最高的员工"></a>184. 部门工资最高的员工</h2><h3 id="描述-6"><a href="#描述-6" class="headerlink" title="描述"></a>描述</h3><p>Employee 表包含所有员工信息，每个员工有其对应的 Id, salary 和 department Id。</p><table><thead><tr><th>Id</th><th>Name</th><th>Salary</th><th>DepartmentId</th></tr></thead><tbody><tr><td>1</td><td>Joe</td><td>70000</td><td>1</td></tr><tr><td>2</td><td>Henry</td><td>80000</td><td>2</td></tr><tr><td>3</td><td>Sam</td><td>60000</td><td>2</td></tr><tr><td>4</td><td>Max</td><td>90000</td><td>1</td></tr></tbody></table><p>Department 表包含公司所有部门的信息。</p><table><thead><tr><th>Id</th><th>Name</th></tr></thead><tbody><tr><td>1</td><td>IT</td></tr><tr><td>2</td><td>Sales</td></tr></tbody></table><p>编写一个 SQL 查询，找出每个部门工资最高的员工。例如，根据上述给定的表格，Max 在 IT 部门有最高工资，Henry 在 Sales 部门有最高工资。</p><table><thead><tr><th>Department</th><th>Employee</th><th>Salary</th></tr></thead><tbody><tr><td>IT</td><td>Max</td><td>90000</td></tr><tr><td>Sales</td><td>Henry</td><td>80000</td></tr></tbody></table><h3 id="数据准备-6"><a href="#数据准备-6" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Employee (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), Salary <span class="built_in">int</span>, DepartmentId <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Department (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Name</span> <span class="built_in">varchar</span>(<span class="number">255</span>));</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Employee;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'Joe'</span>, <span class="string">'70000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Henry'</span>, <span class="string">'80000'</span>, <span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'Sam'</span>, <span class="string">'60000'</span>, <span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'Max'</span>, <span class="string">'90000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Department;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Department (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'IT'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Department (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Sales'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-6"><a href="#SQL语句-6" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    Department.name <span class="keyword">AS</span> <span class="string">'Department'</span>,</span><br><span class="line">    Employee.name <span class="keyword">AS</span> <span class="string">'Employee'</span>,</span><br><span class="line">    Salary</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">    Employee</span><br><span class="line">        <span class="keyword">JOIN</span></span><br><span class="line">    Department <span class="keyword">ON</span> Employee.DepartmentId = Department.Id</span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">    (Employee.DepartmentId , Salary) <span class="keyword">IN</span></span><br><span class="line">    (   <span class="keyword">SELECT</span></span><br><span class="line">            DepartmentId, <span class="keyword">MAX</span>(Salary)</span><br><span class="line">        <span class="keyword">FROM</span></span><br><span class="line">            Employee</span><br><span class="line">        <span class="keyword">GROUP</span> <span class="keyword">BY</span> DepartmentId</span><br><span class="line">)</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="185-部门工资前三高的所有员工"><a href="#185-部门工资前三高的所有员工" class="headerlink" title="185.部门工资前三高的所有员工"></a>185.部门工资前三高的所有员工</h2><h3 id="描述-7"><a href="#描述-7" class="headerlink" title="描述"></a>描述</h3><p>Employee 表包含所有员工信息，每个员工有其对应的工号 Id，姓名 Name，工资 Salary 和部门编号 DepartmentId 。</p><table><thead><tr><th align="center">Id</th><th align="center">Name</th><th align="center">Salary</th><th align="center">DepartmentId</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">Joe</td><td align="center">85000</td><td align="center">1</td></tr><tr><td align="center">2</td><td align="center">Henry</td><td align="center">80000</td><td align="center">2</td></tr><tr><td align="center">3</td><td align="center">Sam</td><td align="center">60000</td><td align="center">2</td></tr><tr><td align="center">4</td><td align="center">Max</td><td align="center">90000</td><td align="center">1</td></tr><tr><td align="center">5</td><td align="center">Janet</td><td align="center">69000</td><td align="center">1</td></tr><tr><td align="center">6</td><td align="center">Randy</td><td align="center">85000</td><td align="center">1</td></tr><tr><td align="center">7</td><td align="center">Will</td><td align="center">70000</td><td align="center">1</td></tr></tbody></table><p>Department 表包含公司所有部门的信息。</p><table><thead><tr><th align="center">Id</th><th align="center">Name</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">IT</td></tr><tr><td align="center">2</td><td align="center">Sales</td></tr></tbody></table><p>编写一个 SQL 查询，找出每个部门获得前三高工资的所有员工。例如，根据上述给定的表，查询结果应返回：</p><table><thead><tr><th align="center">Department</th><th align="center">Employee</th><th align="center">Salary</th></tr></thead><tbody><tr><td align="center">IT</td><td align="center">Max</td><td align="center">90000</td></tr><tr><td align="center">IT</td><td align="center">Randy</td><td align="center">85000</td></tr><tr><td align="center">IT</td><td align="center">Joe</td><td align="center">85000</td></tr><tr><td align="center">IT</td><td align="center">Will</td><td align="center">70000</td></tr><tr><td align="center">Sales</td><td align="center">Henry</td><td align="center">80000</td></tr><tr><td align="center">Sales</td><td align="center">Sam</td><td align="center">60000</td></tr></tbody></table><p>解释：</p><p>IT 部门中，Max 获得了最高的工资，Randy 和 Joe 都拿到了第二高的工资，Will 的工资排第三。销售部门（Sales）只有两名员工，Henry 的工资最高，Sam 的工资排第二。</p><h3 id="数据准备-7"><a href="#数据准备-7" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Employee (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), Salary <span class="built_in">int</span>, DepartmentId <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Department (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Name</span> <span class="built_in">varchar</span>(<span class="number">255</span>));</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Employee;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'Joe'</span>, <span class="string">'85000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Henry'</span>, <span class="string">'80000'</span>, <span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'Sam'</span>, <span class="string">'60000'</span>, <span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'Max'</span>, <span class="string">'90000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'5'</span>, <span class="string">'Janet'</span>, <span class="string">'69000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'6'</span>, <span class="string">'Randy'</span>, <span class="string">'85000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'7'</span>, <span class="string">'Will'</span>, <span class="string">'70000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Department;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Department (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'IT'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Department (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Sales'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-7"><a href="#SQL语句-7" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">d.Name <span class="keyword">AS</span> <span class="string">'Department'</span>, e1.Name <span class="keyword">AS</span> <span class="string">'Employee'</span>, e1.Salary</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Employee e1</span><br><span class="line"><span class="keyword">JOIN</span></span><br><span class="line">Department d <span class="keyword">ON</span> e1.DepartmentId = d.Id</span><br><span class="line"><span class="keyword">WHERE</span> <span class="comment">-- 相关子查询，父查询传递一个元祖到子查询，遍历子查询的的数据，如果满足不超过3个人的工资大于传过来的工资，则保留该元祖的数据，否则就过滤掉</span></span><br><span class="line"><span class="number">3</span> &gt; (<span class="keyword">SELECT</span></span><br><span class="line"><span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> e2.Salary) <span class="comment">-- 对于重复的工资，计数一次，从而保证相同的工资的排名相同</span></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Employee e2</span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">e2.Salary &gt; e1.Salary</span><br><span class="line"><span class="keyword">AND</span> e1.DepartmentId = e2.DepartmentId</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="196-删除重复的邮箱"><a href="#196-删除重复的邮箱" class="headerlink" title="196.删除重复的邮箱"></a>196.删除重复的邮箱</h2><h3 id="描述-8"><a href="#描述-8" class="headerlink" title="描述"></a>描述</h3><p>编写一个 SQL 查询，来删除 Person 表中所有重复的电子邮箱，重复的邮箱里只保留 Id 最小 的那个。</p><table><thead><tr><th>Id</th><th>Email</th></tr></thead><tbody><tr><td>1</td><td><a href="mailto:john@example.com" target="_blank" rel="noopener">john@example.com</a></td></tr><tr><td>2</td><td><a href="mailto:bob@example.com" target="_blank" rel="noopener">bob@example.com</a></td></tr><tr><td>3</td><td><a href="mailto:john@example.com" target="_blank" rel="noopener">john@example.com</a></td></tr></tbody></table><p>Id 是这个表的主键。<br>例如，在运行你的查询语句之后，上面的 Person 表应返回以下几行:</p><table><thead><tr><th align="center">Id</th><th align="center">Email</th></tr></thead><tbody><tr><td align="center">1</td><td align="center"><a href="mailto:john@example.com" target="_blank" rel="noopener">john@example.com</a></td></tr><tr><td align="center">2</td><td align="center"><a href="mailto:bob@example.com" target="_blank" rel="noopener">bob@example.com</a></td></tr></tbody></table><h3 id="数据准备-8"><a href="#数据准备-8" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Person (<span class="keyword">Id</span> <span class="built_in">int</span>,Email <span class="built_in">varchar</span>(<span class="number">20</span>));</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Person;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Person <span class="keyword">values</span> (<span class="string">'1'</span>,  <span class="string">'john@example.com'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Person <span class="keyword">values</span> (<span class="string">'2'</span>,  <span class="string">'bob@example.com'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Person <span class="keyword">values</span> (<span class="string">'3'</span>,  <span class="string">'john@example.com'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-8"><a href="#SQL语句-8" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DELETE</span> p1.* </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Person p1,</span><br><span class="line">Person p2 </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">p1.Email = p2.Email </span><br><span class="line"><span class="keyword">AND</span> p1.Id &gt; p2.Id</span><br></pre></td></tr></table></figure><h2 id="197-上升的温度"><a href="#197-上升的温度" class="headerlink" title="197.上升的温度"></a>197.上升的温度</h2><h3 id="描述-9"><a href="#描述-9" class="headerlink" title="描述"></a>描述</h3><p>给定一个 Weather 表，编写一个 SQL 查询，来查找与之前（昨天的）日期相比温度更高的所有日期的 Id。</p><table><thead><tr><th align="center">Id(INT)</th><th align="center">RecordDate(DATE)</th><th align="center">Temperature(INT)</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">2015-01-01</td><td align="center">10</td></tr><tr><td align="center">2</td><td align="center">2015-01-02</td><td align="center">25</td></tr><tr><td align="center">3</td><td align="center">2015-01-03</td><td align="center">20</td></tr><tr><td align="center">4</td><td align="center">2015-01-04</td><td align="center">30</td></tr></tbody></table><p>例如，根据上述给定的 Weather 表格，返回如下 Id:</p><table><thead><tr><th align="center">id</th></tr></thead><tbody><tr><td align="center">2</td></tr><tr><td align="center">4</td></tr></tbody></table><h3 id="数据准备-9"><a href="#数据准备-9" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Weather (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="built_in">Date</span> <span class="built_in">date</span>, Temperature <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Weather;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Weather (<span class="keyword">Id</span>, <span class="built_in">Date</span>, Temperature) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'2015-01-01'</span>, <span class="string">'10'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Weather (<span class="keyword">Id</span>, <span class="built_in">Date</span>, Temperature) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'2015-01-02'</span>, <span class="string">'25'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Weather (<span class="keyword">Id</span>, <span class="built_in">Date</span>, Temperature) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'2015-01-03'</span>, <span class="string">'20'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Weather (<span class="keyword">Id</span>, <span class="built_in">Date</span>, Temperature) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'2015-01-04'</span>, <span class="string">'30'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-9"><a href="#SQL语句-9" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">a.Id </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Weather a</span><br><span class="line"><span class="keyword">JOIN</span> Weather b <span class="keyword">ON</span> <span class="keyword">DATEDIFF</span>(a.RecordDate,b.RecordDate) = <span class="number">1</span></span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">a.Temperature &gt; b.Temperature</span><br></pre></td></tr></table></figure><h2 id="262-行程与用户"><a href="#262-行程与用户" class="headerlink" title="262.行程与用户"></a>262.行程与用户</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>Trips 表中存所有出租车的行程信息。每段行程有唯一键 Id，Client_Id 和 Driver_Id 是 Users 表中 Users_Id 的外键。Status 是枚举类型，枚举成员为 (‘completed’, ‘cancelled_by_driver’, ‘cancelled_by_client’)。</p><table><thead><tr><th align="center">Id</th><th align="center">Client_Id</th><th align="center">Driver_Id</th><th align="center">City_Id</th><th align="center">Status</th><th align="center">Request_at</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">1</td><td align="center">10</td><td align="center">1</td><td align="center">completed</td><td align="center">2013-10-01</td></tr><tr><td align="center">2</td><td align="center">2</td><td align="center">11</td><td align="center">1</td><td align="center">cancelled_by_driver</td><td align="center">2013-10-01</td></tr><tr><td align="center">3</td><td align="center">3</td><td align="center">12</td><td align="center">6</td><td align="center">cancelled</td><td align="center">2013-10-01</td></tr><tr><td align="center">4</td><td align="center">4</td><td align="center">13</td><td align="center">6</td><td align="center">cancelled_by_client</td><td align="center">2013-10-01</td></tr><tr><td align="center">5</td><td align="center">1</td><td align="center">10</td><td align="center">1</td><td align="center">completed</td><td align="center">2013-10-02</td></tr><tr><td align="center">6</td><td align="center">2</td><td align="center">11</td><td align="center">6</td><td align="center">completed</td><td align="center">2013-10-02</td></tr><tr><td align="center">7</td><td align="center">3</td><td align="center">12</td><td align="center">6</td><td align="center">completed</td><td align="center">2013-10-02</td></tr><tr><td align="center">8</td><td align="center">2</td><td align="center">12</td><td align="center">12</td><td align="center">completed</td><td align="center">2013-10-03</td></tr><tr><td align="center">9</td><td align="center">3</td><td align="center">10</td><td align="center">12</td><td align="center">completed</td><td align="center">2013-10-03</td></tr><tr><td align="center">10</td><td align="center">4</td><td align="center">13</td><td align="center">12</td><td align="center">cancelled_by_driver</td><td align="center">2013-10-03</td></tr></tbody></table><p>Users 表存所有用户。每个用户有唯一键 Users_Id。Banned 表示这个用户是否被禁止，Role 则是一个表示（‘client’, ‘driver’, ‘partner’）的枚举类型。</p><table><thead><tr><th align="center">Users_Id</th><th align="center">Banned</th><th align="center">Role</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">No</td><td align="center">client</td></tr><tr><td align="center">2</td><td align="center">Yes</td><td align="center">client</td></tr><tr><td align="center">3</td><td align="center">No</td><td align="center">client</td></tr><tr><td align="center">4</td><td align="center">No</td><td align="center">client</td></tr><tr><td align="center">10</td><td align="center">No</td><td align="center">driver</td></tr><tr><td align="center">11</td><td align="center">No</td><td align="center">driver</td></tr><tr><td align="center">12</td><td align="center">No</td><td align="center">driver</td></tr><tr><td align="center">13</td><td align="center">No</td><td align="center">driver</td></tr></tbody></table><p>写一段 SQL 语句查出 2013年10月1日 至 2013年10月3日 期间非禁止用户的取消率。基于上表，你的 SQL 语句应返回如下结果，取消率（Cancellation Rate）保留两位小数。</p><p>取消率的计算方式如下：(被司机或乘客取消的非禁止用户生成的订单数量) / (非禁止用户生成的订单总数)</p><table><thead><tr><th align="center">Day</th><th align="center">Cancellation Rate</th></tr></thead><tbody><tr><td align="center">2013-10-01</td><td align="center">0.33</td></tr><tr><td align="center">2013-10-02</td><td align="center">0.00</td></tr><tr><td align="center">2013-10-03</td><td align="center">0.50</td></tr></tbody></table><h3 id="数据准备-10"><a href="#数据准备-10" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Trips (<span class="keyword">Id</span> <span class="built_in">int</span>, Client_Id <span class="built_in">int</span>, Driver_Id <span class="built_in">int</span>,</span><br><span class="line">City_Id <span class="built_in">int</span>, <span class="keyword">Status</span> ENUM(<span class="string">'completed'</span>, <span class="string">'cancelled_by_driver'</span>, <span class="string">'cancelled_by_client'</span>),</span><br><span class="line">Request_at <span class="built_in">varchar</span>(<span class="number">50</span>));</span><br><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> <span class="keyword">Users</span> (Users_Id <span class="built_in">int</span>,</span><br><span class="line">Banned <span class="built_in">varchar</span>(<span class="number">50</span>), <span class="keyword">Role</span> ENUM(<span class="string">'client'</span>, <span class="string">'driver'</span>, <span class="string">'partner'</span>));</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Trips;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'1'</span>, <span class="string">'10'</span>, <span class="string">'1'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-01'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'2'</span>, <span class="string">'11'</span>, <span class="string">'1'</span>, <span class="string">'cancelled_by_driver'</span>, <span class="string">'2013-10-01'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'3'</span>, <span class="string">'12'</span>, <span class="string">'6'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-01'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'4'</span>, <span class="string">'13'</span>, <span class="string">'6'</span>, <span class="string">'cancelled_by_client'</span>, <span class="string">'2013-10-01'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'5'</span>, <span class="string">'1'</span>, <span class="string">'10'</span>, <span class="string">'1'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-02'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'6'</span>, <span class="string">'2'</span>, <span class="string">'11'</span>, <span class="string">'6'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-02'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'7'</span>, <span class="string">'3'</span>, <span class="string">'12'</span>, <span class="string">'6'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-02'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'8'</span>, <span class="string">'2'</span>, <span class="string">'12'</span>, <span class="string">'12'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-03'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'9'</span>, <span class="string">'3'</span>, <span class="string">'10'</span>, <span class="string">'12'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-03'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'10'</span>, <span class="string">'4'</span>, <span class="string">'13'</span>, <span class="string">'12'</span>, <span class="string">'cancelled_by_driver'</span>, <span class="string">'2013-10-03'</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> <span class="keyword">Users</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'No'</span>, <span class="string">'client'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Yes'</span>, <span class="string">'client'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'No'</span>, <span class="string">'client'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'No'</span>, <span class="string">'client'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'10'</span>, <span class="string">'No'</span>, <span class="string">'driver'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'11'</span>, <span class="string">'No'</span>, <span class="string">'driver'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'12'</span>, <span class="string">'No'</span>, <span class="string">'driver'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'13'</span>, <span class="string">'No'</span>, <span class="string">'driver'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-10"><a href="#SQL语句-10" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 方法1：</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">temp1.Request_at <span class="keyword">AS</span> <span class="keyword">DAY</span>,</span><br><span class="line"><span class="keyword">IF</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">cast</span>( ( temp2.cancelled_order / temp1.total_order ) <span class="keyword">AS</span> <span class="built_in">DECIMAL</span> ( <span class="number">3</span>, <span class="number">2</span> ) ) <span class="keyword">IS</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="number">0.00</span>,</span><br><span class="line"><span class="keyword">cast</span>( ( temp2.cancelled_order / temp1.total_order ) <span class="keyword">AS</span> <span class="built_in">DECIMAL</span> ( <span class="number">3</span>, <span class="number">2</span> ) ) </span><br><span class="line">) <span class="keyword">AS</span> <span class="string">'Cancellation Rate'</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">t1.Request_at,</span><br><span class="line"><span class="keyword">count</span>( * ) <span class="keyword">AS</span> total_order </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">( <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Trips <span class="keyword">WHERE</span> Request_at &gt;= <span class="string">'2013-10-01'</span> <span class="keyword">AND</span> Request_at &lt;= <span class="string">'2013-10-03'</span> ) t1</span><br><span class="line"><span class="keyword">JOIN</span> ( <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> <span class="keyword">Users</span> <span class="keyword">WHERE</span> Banned = <span class="string">'NO'</span> ) t2 <span class="keyword">ON</span> t1.Client_Id = t2.Users_Id </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">t1.Request_at </span><br><span class="line">) temp1</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> (</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">t1.Request_at,</span><br><span class="line"><span class="keyword">count</span>( * ) <span class="keyword">AS</span> cancelled_order </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">( <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Trips <span class="keyword">WHERE</span> Request_at &gt;= <span class="string">'2013-10-01'</span> <span class="keyword">AND</span> Request_at &lt;= <span class="string">'2013-10-03'</span> <span class="keyword">AND</span> ( <span class="keyword">STATUS</span> = <span class="string">'cancelled_by_driver'</span> <span class="keyword">OR</span> <span class="keyword">STATUS</span> = <span class="string">'cancelled_by_client'</span> ) ) t1</span><br><span class="line"><span class="keyword">JOIN</span> ( <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> <span class="keyword">Users</span> <span class="keyword">WHERE</span> Banned = <span class="string">'NO'</span> ) t2 <span class="keyword">ON</span> t1.Client_Id = t2.Users_Id </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">t1.Request_at </span><br><span class="line">) temp2 <span class="keyword">ON</span> temp1.Request_at = temp2.Request_at</span><br><span class="line"><span class="comment">-- ---------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 方法2：</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">temp.request_at <span class="keyword">DAY</span>,</span><br><span class="line"><span class="keyword">round</span>( <span class="keyword">sum</span>( <span class="keyword">CASE</span> temp.STATUS <span class="keyword">WHEN</span> <span class="string">'completed'</span> <span class="keyword">THEN</span> <span class="number">0</span> <span class="keyword">ELSE</span> <span class="number">1</span> <span class="keyword">END</span> ) / <span class="keyword">count</span>( temp.STATUS ), <span class="number">2</span> ) <span class="string">'Cancellation Rate'</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">( <span class="keyword">SELECT</span> <span class="keyword">STATUS</span>, request_at <span class="keyword">FROM</span> trips t <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> <span class="keyword">users</span> u <span class="keyword">ON</span> t.client_id = u.users_id <span class="keyword">WHERE</span> u.banned = <span class="string">'no'</span> ) temp </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">request_at <span class="keyword">BETWEEN</span> <span class="string">'2013-10-01'</span> </span><br><span class="line"><span class="keyword">AND</span> <span class="string">'2013-10-03'</span> </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">temp.request_at</span><br></pre></td></tr></table></figure><h2 id="511-游戏玩家分析I"><a href="#511-游戏玩家分析I" class="headerlink" title="511.游戏玩家分析I"></a>511.游戏玩家分析I</h2><h3 id="描述-10"><a href="#描述-10" class="headerlink" title="描述"></a>描述</h3><p>找出每个玩家第一次登录的日期。Activity表如下：</p><table><thead><tr><th align="center">player_id</th><th align="center">device_id</th><th align="center">event_date</th><th align="center">games_played</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">2</td><td align="center">2016<strong>-</strong>03<strong>-</strong>01</td><td align="center">5</td></tr><tr><td align="center">1</td><td align="center">2</td><td align="center">2016-03-02</td><td align="center">6</td></tr><tr><td align="center">2</td><td align="center">3</td><td align="center"><strong>2017</strong>-<strong>06</strong>-25</td><td align="center">1</td></tr><tr><td align="center">3</td><td align="center">1</td><td align="center">2016-03-02</td><td align="center">0</td></tr><tr><td align="center">3</td><td align="center">4</td><td align="center">2018<strong>-</strong>07<strong>-</strong>03</td><td align="center">5</td></tr></tbody></table><p>结果Result表如下：</p><table><thead><tr><th align="center">player_id</th><th align="center">first_login</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">2016-03-01</td></tr><tr><td align="center">2</td><td align="center">2017<strong>-</strong>06<strong>-</strong>25</td></tr><tr><td align="center">3</td><td align="center">2016-03-02</td></tr></tbody></table><h3 id="数据准备-11"><a href="#数据准备-11" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> activity(player_id <span class="built_in">int</span>,device_id <span class="built_in">int</span>,event_date <span class="built_in">date</span>,games_played <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> activity;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> activity <span class="keyword">values</span> (<span class="number">1</span>,<span class="number">2</span>,<span class="string">'2016-03-01'</span>,<span class="number">5</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> activity <span class="keyword">values</span> (<span class="number">1</span>,<span class="number">2</span>,<span class="string">'2016-03-02'</span>,<span class="number">6</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> activity <span class="keyword">values</span> (<span class="number">2</span>,<span class="number">3</span>,<span class="string">'2017-06-25'</span>,<span class="number">1</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> activity <span class="keyword">values</span> (<span class="number">3</span>,<span class="number">1</span>,<span class="string">'2016-03-02'</span>,<span class="number">0</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> activity <span class="keyword">values</span> (<span class="number">3</span>,<span class="number">4</span>,<span class="string">'2018-07-03'</span>,<span class="number">5</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-11"><a href="#SQL语句-11" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> player_id,<span class="keyword">min</span>(event_date) first_login <span class="keyword">from</span> activity <span class="keyword">group</span> <span class="keyword">by</span> player_id ;</span><br></pre></td></tr></table></figure><h2 id="512-游戏玩家分析II"><a href="#512-游戏玩家分析II" class="headerlink" title="512.  游戏玩家分析II"></a>512.  游戏玩家分析II</h2><h3 id="描述-11"><a href="#描述-11" class="headerlink" title="描述"></a>描述</h3><p>显示每个玩家首次登录的设备号(同时显示玩家ID)。</p><h3 id="数据准备-12"><a href="#数据准备-12" class="headerlink" title="数据准备"></a>数据准备</h3><p>见511题</p><h3 id="SQL语句-12"><a href="#SQL语句-12" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">player_id,</span><br><span class="line">device_id </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">activity </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">( player_id, event_date ) <span class="keyword">IN</span> ( <span class="keyword">SELECT</span> player_id, <span class="keyword">min</span>( event_date ) first_login <span class="keyword">FROM</span> activity <span class="keyword">GROUP</span> <span class="keyword">BY</span> player_id )</span><br></pre></td></tr></table></figure><h2 id="534-游戏玩家分析III"><a href="#534-游戏玩家分析III" class="headerlink" title="534 游戏玩家分析III"></a>534 游戏玩家分析III</h2><h3 id="描述-12"><a href="#描述-12" class="headerlink" title="描述"></a>描述</h3><p>编写一个 SQL 查询，同时显示每组玩家、日期以及玩家到目前为止玩了多少游戏。也就是说，在此日期之前玩家所玩的游戏总数。详细情况请查看示例。</p><p>结果为：</p><table><thead><tr><th align="center">player_id</th><th align="center">event_date</th><th align="center">games_played_so_far</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">2016-03-01</td><td align="center">5</td></tr><tr><td align="center">1</td><td align="center">2016-03-02</td><td align="center">11</td></tr><tr><td align="center">1</td><td align="center">2017-06-25</td><td align="center">1</td></tr><tr><td align="center">3</td><td align="center">2016-03-02</td><td align="center">0</td></tr><tr><td align="center">3</td><td align="center">2018-07-03</td><td align="center">5</td></tr></tbody></table><h3 id="数据准备-13"><a href="#数据准备-13" class="headerlink" title="数据准备"></a>数据准备</h3><p>见511题</p><h3 id="SQL语句-13"><a href="#SQL语句-13" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 方法一</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">B.player_id,</span><br><span class="line">B.event_date,</span><br><span class="line"><span class="keyword">SUM</span>( A.games_played ) <span class="keyword">AS</span> <span class="string">`games_played_so_far`</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Activity <span class="keyword">AS</span> A</span><br><span class="line"><span class="keyword">JOIN</span> Activity <span class="keyword">AS</span> B <span class="keyword">ON</span> ( A.player_id = B.player_id <span class="keyword">AND</span> A.event_date &lt;= B.event_date ) </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">B.player_id,</span><br><span class="line">B.event_date</span><br><span class="line"><span class="comment">-- 方法二</span></span><br><span class="line"><span class="keyword">SELECT</span> C.player_id,C.event_date,C.games_played_so_far</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">A.player_id,</span><br><span class="line">A.event_date,</span><br><span class="line">@sum_cnt:=</span><br><span class="line"><span class="keyword">if</span>(A.player_id = @pre_id <span class="keyword">AND</span> A.event_date != @pre_date,</span><br><span class="line">@sum_cnt + A.games_played,</span><br><span class="line">A.games_played </span><br><span class="line">)</span><br><span class="line"><span class="keyword">AS</span> <span class="string">`games_played_so_far`</span>,</span><br><span class="line">@pre_id:=A.player_id <span class="keyword">AS</span> <span class="string">`player_ids`</span>,</span><br><span class="line">@pre_date:=A.event_date <span class="keyword">AS</span> <span class="string">`event_dates`</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">activity <span class="keyword">AS</span> A,(<span class="keyword">SELECT</span> @pre_id:=<span class="literal">NULL</span>,@pre_date:=<span class="literal">NULL</span>,@sum_cnt:=<span class="number">0</span>) <span class="keyword">AS</span> B</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">BY</span> A.player_id,A.event_date</span><br><span class="line">) <span class="keyword">AS</span> C</span><br></pre></td></tr></table></figure><h2 id="550-游戏玩家分析IV"><a href="#550-游戏玩家分析IV" class="headerlink" title="550 游戏玩家分析IV"></a>550 游戏玩家分析IV</h2><h3 id="描述-13"><a href="#描述-13" class="headerlink" title="描述"></a>描述</h3><p>列出首次登录后，紧接着第二天又登录的人数占总人数的比例。比如511题中的数据，只有玩家1连续两天登录了，而总玩家有3个，所以连着两天登录的用户比例为：1/3 ~0.33</p><h3 id="数据准备-14"><a href="#数据准备-14" class="headerlink" title="数据准备"></a>数据准备</h3><p>见511题</p><h3 id="SQL语句-14"><a href="#SQL语句-14" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line"><span class="keyword">ROUND</span>(</span><br><span class="line">(</span><br><span class="line"><span class="comment">-- 求第二天连续登陆的用户数</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line"><span class="keyword">count</span>( <span class="keyword">DISTINCT</span> player_id ) <span class="keyword">AS</span> con_cnt </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">a.player_id,</span><br><span class="line"><span class="keyword">DATEDIFF</span>( b.event_date, a.event_date ) <span class="keyword">AS</span> diff </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">activity a</span><br><span class="line"><span class="keyword">JOIN</span> activity b <span class="keyword">ON</span> ( a.player_id = b.player_id <span class="keyword">AND</span> a.event_date &lt; b.event_date ) </span><br><span class="line">) t1 </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">diff = <span class="number">1</span> </span><br><span class="line">) / ( <span class="keyword">SELECT</span> <span class="keyword">count</span>( <span class="keyword">DISTINCT</span> player_id ) total_cnt <span class="keyword">FROM</span> activity ),<span class="comment">-- 总用户数</span></span><br><span class="line"><span class="number">2</span> </span><br><span class="line">) fraction</span><br></pre></td></tr></table></figure><h2 id="569-员工薪水中位数"><a href="#569-员工薪水中位数" class="headerlink" title="569 员工薪水中位数"></a>569 员工薪水中位数</h2><h3 id="描述-14"><a href="#描述-14" class="headerlink" title="描述"></a>描述</h3><p>有一张员工表Employees，字段为Id，Name，Salary，其中Id为员工Id，Name为公司名称，Salary为员工工资。如下面数据所示：</p><table><thead><tr><th align="center">Id</th><th align="center">Company</th><th align="center">Salary</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">A</td><td align="center">2341</td></tr><tr><td align="center">2</td><td align="center">A</td><td align="center">341</td></tr><tr><td align="center">3</td><td align="center">A</td><td align="center">15</td></tr><tr><td align="center">4</td><td align="center">A</td><td align="center">15314</td></tr><tr><td align="center">5</td><td align="center">A</td><td align="center">451</td></tr><tr><td align="center">6</td><td align="center">A</td><td align="center">513</td></tr><tr><td align="center">7</td><td align="center">B</td><td align="center">15</td></tr><tr><td align="center">8</td><td align="center">B</td><td align="center">13</td></tr><tr><td align="center">9</td><td align="center">B</td><td align="center">1154</td></tr><tr><td align="center">10</td><td align="center">B</td><td align="center">1345</td></tr><tr><td align="center">11</td><td align="center">B</td><td align="center">1221</td></tr><tr><td align="center">12</td><td align="center">B</td><td align="center">234</td></tr><tr><td align="center">13</td><td align="center">C</td><td align="center">2345</td></tr><tr><td align="center">14</td><td align="center">C</td><td align="center">2645</td></tr><tr><td align="center">15</td><td align="center">C</td><td align="center">2645</td></tr><tr><td align="center">16</td><td align="center">C</td><td align="center">2652</td></tr><tr><td align="center">17</td><td align="center">C</td><td align="center">65</td></tr></tbody></table><p>请编写SQL查询来查找每个公司的薪水中位数。结果如下：</p><table><thead><tr><th align="center">Id</th><th align="center">Company</th><th align="center">Salary</th></tr></thead><tbody><tr><td align="center">5</td><td align="center">A</td><td align="center">451</td></tr><tr><td align="center">6</td><td align="center">A</td><td align="center">513</td></tr><tr><td align="center">12</td><td align="center">B</td><td align="center">234</td></tr><tr><td align="center">9</td><td align="center">B</td><td align="center">1154</td></tr><tr><td align="center">14</td><td align="center">C</td><td align="center">2645</td></tr></tbody></table><h3 id="数据准备-15"><a href="#数据准备-15" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span>  <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> employees;</span><br><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> employees(<span class="keyword">Id</span> <span class="built_in">int</span>,Company <span class="built_in">varchar</span>(<span class="number">2</span>),salary <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'A'</span>,<span class="number">2341</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">'A'</span>,<span class="number">341</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">3</span>,<span class="string">'A'</span>,<span class="number">15</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">4</span>,<span class="string">'A'</span>,<span class="number">15314</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">5</span>,<span class="string">'A'</span>,<span class="number">451</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">6</span>,<span class="string">'A'</span>,<span class="number">513</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">7</span>,<span class="string">'B'</span>,<span class="number">15</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">8</span>,<span class="string">'B'</span>,<span class="number">13</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">9</span>,<span class="string">'B'</span>,<span class="number">1154</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">10</span>,<span class="string">'B'</span>,<span class="number">1345</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">11</span>,<span class="string">'B'</span>,<span class="number">1221</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">12</span>,<span class="string">'B'</span>,<span class="number">234</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">13</span>,<span class="string">'C'</span>,<span class="number">2345</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">14</span>,<span class="string">'C'</span>,<span class="number">2645</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">15</span>,<span class="string">'C'</span>,<span class="number">2645</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">16</span>,<span class="string">'C'</span>,<span class="number">2652</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">17</span>,<span class="string">'C'</span>,<span class="number">65</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-15"><a href="#SQL语句-15" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">     t1.id,</span><br><span class="line">     t1.company,</span><br><span class="line">     t1.salary</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line"></span><br><span class="line">(</span><br><span class="line"><span class="comment">-- 查询每个公司员工薪水排名</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">     <span class="keyword">id</span>,</span><br><span class="line">     company,</span><br><span class="line">     salary,</span><br><span class="line">     @<span class="keyword">num</span> := <span class="keyword">if</span>( @company =company  ,@<span class="keyword">num</span> + <span class="number">1</span>,<span class="number">1</span>) <span class="keyword">as</span> <span class="keyword">rank</span>,</span><br><span class="line">     @company := company</span><br><span class="line"><span class="keyword">from</span> employees a ,(<span class="keyword">select</span> @<span class="keyword">num</span> := <span class="number">0</span>,@company:=<span class="string">""</span>) b</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> company,salary) t1 </span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">(</span><br><span class="line"><span class="comment">-- 查询每个公司有多少个员工</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">     company,</span><br><span class="line">     <span class="keyword">count</span>(*) <span class="keyword">as</span> cnt</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    employees</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> company</span><br><span class="line"></span><br><span class="line">) t2 <span class="keyword">on</span> t1.company= t2.company <span class="keyword">and</span> t1.rank = (t2.cnt + <span class="number">1</span>) <span class="keyword">div</span> <span class="number">2</span> <span class="comment">-- （员工总数+1）/2 为中位数</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> LeeCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeeCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>电商业务常用指标分析之SQL实现</title>
      <link href="/2019/12/05/%E7%94%B5%E5%95%86%E4%B8%9A%E5%8A%A1%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E5%88%86%E6%9E%90%E4%B9%8BSQL%E5%AE%9E%E7%8E%B0/"/>
      <url>/2019/12/05/%E7%94%B5%E5%95%86%E4%B8%9A%E5%8A%A1%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E5%88%86%E6%9E%90%E4%B9%8BSQL%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<p>当构建好电商业务数仓之后，需要对业务需要的指标进行计算，从而进一步进行报表的展示，那么，电商的业务知识大概涉及哪些？关于电商业务的常用指标计算都有哪些？这些常用的指标该如何通过Hive数仓进行分析？本文将进行一一梳理.</p><a id="more"></a><h2 id="电商业务领域知识梳理"><a href="#电商业务领域知识梳理" class="headerlink" title="电商业务领域知识梳理"></a>电商业务领域知识梳理</h2><ul><li><strong>用户</strong></li></ul><p>用户以设备为判断标准，在移动统计中，每个独立设备认为是一个独立用户。Android系统根据IMEI号，IOS系统根据OpenUDID来标识一个独立用户，每部手机一个用户。</p><ul><li><strong>新增用户</strong></li></ul><p>首次联网使用应用的用户。如果一个用户首次打开某APP，那这个用户定义为新增用户；卸载再安装的设备，不会被算作一次新增。新增用户包括日新增用户、周新增用户、月新增用户。</p><ul><li><strong>活跃用户</strong></li></ul><p>打开应用的用户即为活跃用户，不考虑用户的使用情况。每天一台设备打开多次会被计为一个活跃用户。</p><ul><li><strong>周（月）活跃用户</strong></li></ul><p>某个自然周（月）内启动过应用的用户，该周（月）内的多次启动只记一个活跃用户。</p><ul><li><strong>月活跃率</strong></li></ul><p>月活跃用户与截止到该月累计的用户总和之间的比例。</p><ul><li><strong>沉默用户</strong></li></ul><p>用户仅在安装当天（次日）启动一次，后续时间无再启动行为。该指标可以反映新增用户质量和用户与APP的匹配程度。</p><ul><li><strong>版本分布</strong></li></ul><p>不同版本的周内各天新增用户数，活跃用户数和启动次数。利于判断APP各个版本之间的优劣和用户行为习惯。</p><ul><li><strong>本周回流用户</strong></li></ul><p>上周未启动过应用，本周启动了应用的用户。</p><ul><li><strong>连续N周活跃用户</strong></li></ul><p>连续n周，每周至少启动一次。</p><ul><li><strong>忠诚用户</strong></li></ul><p>连续活跃5周以上的用户</p><ul><li><strong>连续活跃用户</strong></li></ul><p>连续2周及以上活跃的用户</p><ul><li><strong>近期流失用户</strong></li></ul><p>连续n(2&lt;= n &lt;= 4)周没有启动应用的用户。（第n+1周没有启动过）</p><ul><li><strong>留存用户</strong></li></ul><p>某段时间内的新增用户，经过一段时间后，仍然使用应用的被认作是留存用户；这部分用户占当时新增用户的比例即是留存率。<br>例如，5月份新增用户200，这200人在6月份启动过应用的有100人，7月份启动过应用的有80人，8月份启动过应用的有50人；则5月份新增用户一个月后的留存率是50%，二个月后的留存率是40%，三个月后的留存率是25%。</p><ul><li><strong>用户新鲜度</strong></li></ul><p>每天启动应用的新老用户比例，即新增用户数占活跃用户数的比例。</p><ul><li><strong>单次使用时长</strong></li></ul><p>每次启动使用的时间长度。</p><ul><li><strong>日使用时长</strong></li></ul><p>累计一天内的使用时间长度。</p><ul><li><strong>启动次数计算标准</strong></li></ul><p>IOS平台应用退到后台就算一次独立的启动；Android平台我们规定，两次启动之间的间隔小于30秒，被计算一次启动。用户在使用过程中，若因收发短信或接电话等退出应用30秒又再次返回应用中，那这两次行为应该是延续而非独立的，所以可以被算作一次使用行为，即一次启动。业内大多使用30秒这个标准，但用户还是可以自定义此时间间隔。</p><h2 id="常用的日期函数处理"><a href="#常用的日期函数处理" class="headerlink" title="常用的日期函数处理"></a>常用的日期函数处理</h2><ul><li><strong>date_format函数（根据格式整理日期）</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">select date_format(<span class="string">'2019-12-05'</span>,<span class="string">'yyyy-MM'</span>);</span><br></pre></td></tr></table></figure><p>输出：2019-12</p><ul><li><strong>date_add函数（加减日期）</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">select date_add(<span class="string">'2019-12-05'</span>,-1);</span><br></pre></td></tr></table></figure><p>输出：2019-12-04</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">select date_add(<span class="string">'2019-12-05'</span>,1);</span><br></pre></td></tr></table></figure><p>输出：2019-12-06</p><ul><li><strong>next_day函数(返回当前时间的下一个星期X所对应的日期)</strong></li></ul><p>1）取当前天的下一个周一</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">select next_day(<span class="string">'2019-12-05'</span>,<span class="string">'MO'</span>)</span><br></pre></td></tr></table></figure><p>输出：2019-12-09</p><p>说明：星期一到星期日的英文（Monday，Tuesday、Wednesday、Thursday、Friday、Saturday、Sunday）</p><p>2）取当前周的周一</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">select date_add(next_day(<span class="string">'2019-12-05'</span>,<span class="string">'MO'</span>),-7);</span><br></pre></td></tr></table></figure><p>输出：2019-12-02</p><ul><li><strong>last_day函数（返回这个月的最后一天的日期）</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">select last_day(<span class="string">'2019-12-05'</span>);</span><br></pre></td></tr></table></figure><p>输出：2019-12-31</p><h2 id="业务指标分析"><a href="#业务指标分析" class="headerlink" title="业务指标分析"></a>业务指标分析</h2><h3 id="用户活跃相关指标分析"><a href="#用户活跃相关指标分析" class="headerlink" title="用户活跃相关指标分析"></a>用户活跃相关指标分析</h3><p>数仓的DWS层会建立好每日的活跃用户表明细、每周的活跃用户表明细以及每月的活跃用户明细表。</p><ul><li><strong>每日活跃用户明细表结构</strong></li></ul><p>每天一个分区，存储当天的日活明细，该表根据mid_id进行去重。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table dws_uv_detail_day</span><br><span class="line">(</span><br><span class="line">    `mid_id` string COMMENT <span class="string">'设备唯一标识'</span>,</span><br><span class="line">    `user_id` string COMMENT <span class="string">'用户标识'</span>, </span><br><span class="line">    `version_code` string COMMENT <span class="string">'程序版本号'</span>, </span><br><span class="line">    `version_name` string COMMENT <span class="string">'程序版本名'</span>, </span><br><span class="line">    `lang` string COMMENT <span class="string">'系统语言'</span>, </span><br><span class="line">    `<span class="built_in">source</span>` string COMMENT <span class="string">'渠道号'</span>, </span><br><span class="line">    `os` string COMMENT <span class="string">'安卓系统版本'</span>, </span><br><span class="line">    `area` string COMMENT <span class="string">'区域'</span>, </span><br><span class="line">    `model` string COMMENT <span class="string">'手机型号'</span>, </span><br><span class="line">    `brand` string COMMENT <span class="string">'手机品牌'</span>, </span><br><span class="line">    `sdk_version` string COMMENT <span class="string">'sdkVersion'</span>, </span><br><span class="line">    `gmail` string COMMENT <span class="string">'gmail'</span>, </span><br><span class="line">    `height_width` string COMMENT <span class="string">'屏幕宽高'</span>,</span><br><span class="line">    `app_time` string COMMENT <span class="string">'客户端日志产生时的时间'</span>,</span><br><span class="line">    `network` string COMMENT <span class="string">'网络模式'</span>,</span><br><span class="line">    `lng` string COMMENT <span class="string">'经度'</span>,</span><br><span class="line">    `lat` string COMMENT <span class="string">'纬度'</span></span><br><span class="line">)</span><br><span class="line">partitioned by(dt string)</span><br><span class="line">stored as parquet</span><br><span class="line">location <span class="string">'/warehouse/gmall/dws/dws_uv_detail_day'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><ul><li><strong>每周活跃用户明细表</strong></li></ul><p>根据日用户访问明细，获得周用户访问明细,周明细表按周一日期和周末日期拼接字段进行分区。即每个分区存储的是本周内的活跃用户明细，该表按mid_id进行去重，即一周内获取多次，只记录一条记录。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table dws_uv_detail_wk( </span><br><span class="line">    `mid_id` string COMMENT <span class="string">'设备唯一标识'</span>,</span><br><span class="line">    `user_id` string COMMENT <span class="string">'用户标识'</span>, </span><br><span class="line">    `version_code` string COMMENT <span class="string">'程序版本号'</span>, </span><br><span class="line">    `version_name` string COMMENT <span class="string">'程序版本名'</span>, </span><br><span class="line">    `lang` string COMMENT <span class="string">'系统语言'</span>, </span><br><span class="line">    `<span class="built_in">source</span>` string COMMENT <span class="string">'渠道号'</span>, </span><br><span class="line">    `os` string COMMENT <span class="string">'安卓系统版本'</span>, </span><br><span class="line">    `area` string COMMENT <span class="string">'区域'</span>, </span><br><span class="line">    `model` string COMMENT <span class="string">'手机型号'</span>, </span><br><span class="line">    `brand` string COMMENT <span class="string">'手机品牌'</span>, </span><br><span class="line">    `sdk_version` string COMMENT <span class="string">'sdkVersion'</span>, </span><br><span class="line">    `gmail` string COMMENT <span class="string">'gmail'</span>, </span><br><span class="line">    `height_width` string COMMENT <span class="string">'屏幕宽高'</span>,</span><br><span class="line">    `app_time` string COMMENT <span class="string">'客户端日志产生时的时间'</span>,</span><br><span class="line">    `network` string COMMENT <span class="string">'网络模式'</span>,</span><br><span class="line">    `lng` string COMMENT <span class="string">'经度'</span>,</span><br><span class="line">    `lat` string COMMENT <span class="string">'纬度'</span>,</span><br><span class="line">    `monday_date` string COMMENT <span class="string">'周一日期'</span>,</span><br><span class="line">    `sunday_date` string COMMENT  <span class="string">'周日日期'</span> </span><br><span class="line">) COMMENT <span class="string">'活跃用户按周明细'</span></span><br><span class="line">PARTITIONED BY (`wk_dt` string)</span><br><span class="line">stored as parquet</span><br><span class="line">location <span class="string">'/warehouse/gmall/dws/dws_uv_detail_wk/'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><ul><li><strong>每月活跃用户明细</strong></li></ul><p>该表按月进行分区，并按mid_id去重，数据来源与日活明细表</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table dws_uv_detail_mn( </span><br><span class="line">    `mid_id` string COMMENT <span class="string">'设备唯一标识'</span>,</span><br><span class="line">    `user_id` string COMMENT <span class="string">'用户标识'</span>, </span><br><span class="line">    `version_code` string COMMENT <span class="string">'程序版本号'</span>, </span><br><span class="line">    `version_name` string COMMENT <span class="string">'程序版本名'</span>, </span><br><span class="line">    `lang` string COMMENT <span class="string">'系统语言'</span>, </span><br><span class="line">    `<span class="built_in">source</span>` string COMMENT <span class="string">'渠道号'</span>, </span><br><span class="line">    `os` string COMMENT <span class="string">'安卓系统版本'</span>, </span><br><span class="line">    `area` string COMMENT <span class="string">'区域'</span>, </span><br><span class="line">    `model` string COMMENT <span class="string">'手机型号'</span>, </span><br><span class="line">    `brand` string COMMENT <span class="string">'手机品牌'</span>, </span><br><span class="line">    `sdk_version` string COMMENT <span class="string">'sdkVersion'</span>, </span><br><span class="line">    `gmail` string COMMENT <span class="string">'gmail'</span>, </span><br><span class="line">    `height_width` string COMMENT <span class="string">'屏幕宽高'</span>,</span><br><span class="line">    `app_time` string COMMENT <span class="string">'客户端日志产生时的时间'</span>,</span><br><span class="line">    `network` string COMMENT <span class="string">'网络模式'</span>,</span><br><span class="line">    `lng` string COMMENT <span class="string">'经度'</span>,</span><br><span class="line">    `lat` string COMMENT <span class="string">'纬度'</span></span><br><span class="line">) COMMENT <span class="string">'活跃用户按月明细'</span></span><br><span class="line">PARTITIONED BY (`mn` string)</span><br><span class="line">stored as parquet</span><br><span class="line">location <span class="string">'/warehouse/gmall/dws/dws_uv_detail_mn/'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><ul><li><strong>建立ADS层的活跃用户指标表</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table ads_uv_count( </span><br><span class="line">    `dt` string COMMENT <span class="string">'统计日期'</span>,</span><br><span class="line">    `day_count` bigint COMMENT <span class="string">'当日用户数量'</span>,</span><br><span class="line">    `wk_count`  bigint COMMENT <span class="string">'当周用户数量'</span>,</span><br><span class="line">    `mn_count`  bigint COMMENT <span class="string">'当月用户数量'</span>,</span><br><span class="line">    `is_weekend` string COMMENT <span class="string">'Y,N是否是周末,用于得到本周最终结果'</span>,</span><br><span class="line">    `is_monthend` string COMMENT <span class="string">'Y,N是否是月末,用于得到本月最终结果'</span> </span><br><span class="line">) COMMENT <span class="string">'活跃设备数'</span></span><br><span class="line">row format delimited fields terminated by <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/warehouse/gmall/ads/ads_uv_count/'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><p><strong>SQL具体实现：</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">insert into table ads_uv_count </span><br><span class="line">select  </span><br><span class="line">  <span class="string">'2019-02-10'</span> dt,</span><br><span class="line">   daycount.ct,</span><br><span class="line">   wkcount.ct,</span><br><span class="line">   mncount.ct,</span><br><span class="line">   <span class="keyword">if</span>(date_add(next_day(<span class="string">'2019-02-10'</span>,<span class="string">'MO'</span>),-1)=<span class="string">'2019-02-10'</span>,<span class="string">'Y'</span>,<span class="string">'N'</span>) , -- 判断跑任务的当天是否是周末</span><br><span class="line">   <span class="keyword">if</span>(last_day(<span class="string">'2019-02-10'</span>)=<span class="string">'2019-02-10'</span>,<span class="string">'Y'</span>,<span class="string">'N'</span>)  -- 判断跑任务的当天是否是月末</span><br><span class="line">from </span><br><span class="line">(</span><br><span class="line">-- 计算当天的日活</span><br><span class="line">   select  </span><br><span class="line">      <span class="string">'2019-02-10'</span> dt,</span><br><span class="line">       count(*) ct</span><br><span class="line">   from dws_uv_detail_day</span><br><span class="line">   <span class="built_in">where</span> dt=<span class="string">'2019-02-10'</span>  </span><br><span class="line">)daycount join </span><br><span class="line">( </span><br><span class="line">-- 计算当天所属周的周活</span><br><span class="line">   select  </span><br><span class="line">     <span class="string">'2019-02-10'</span> dt,</span><br><span class="line">     count (*) ct</span><br><span class="line">   from dws_uv_detail_wk</span><br><span class="line">   <span class="built_in">where</span> wk_dt=concat(date_add(next_day(<span class="string">'2019-02-10'</span>,<span class="string">'MO'</span>),-7),<span class="string">'_'</span> ,date_add(next_day(<span class="string">'2019-02-10'</span>,<span class="string">'MO'</span>),-1) )</span><br><span class="line">) wkcount on daycount.dt=wkcount.dt</span><br><span class="line">join </span><br><span class="line">( </span><br><span class="line">-- 计算当天所属月的月活</span><br><span class="line">   select  </span><br><span class="line">     <span class="string">'2019-02-10'</span> dt,</span><br><span class="line">     count (*) ct</span><br><span class="line">   from dws_uv_detail_mn</span><br><span class="line">   <span class="built_in">where</span> mn=date_format(<span class="string">'2019-02-10'</span>,<span class="string">'yyyy-MM'</span>)  </span><br><span class="line">)mncount on daycount.dt=mncount.dt</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h3 id="新增用户指标分析"><a href="#新增用户指标分析" class="headerlink" title="新增用户指标分析"></a>新增用户指标分析</h3><p>首次联网使用应用的用户。如果一个用户首次打开某APP，那这个用户定义为新增用户；卸载再安装的设备，不会被算作一次新增。新增用户包括日新增用户、周新增用户、月新增用户。</p><ul><li><strong>每日新增用户明细表</strong></li></ul><p>每日新增用户明细表来源于每天的日活表，使用每天的日活表去LEFT JOIN 每天新增用户明细表，关联的条件是mid_id,筛选条件为，每日新增设备表中为空</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table dws_new_mid_day</span><br><span class="line">(</span><br><span class="line">    `mid_id` string COMMENT <span class="string">'设备唯一标识'</span>,</span><br><span class="line">    `user_id` string COMMENT <span class="string">'用户标识'</span>, </span><br><span class="line">    `version_code` string COMMENT <span class="string">'程序版本号'</span>, </span><br><span class="line">    `version_name` string COMMENT <span class="string">'程序版本名'</span>, </span><br><span class="line">    `lang` string COMMENT <span class="string">'系统语言'</span>, </span><br><span class="line">    `<span class="built_in">source</span>` string COMMENT <span class="string">'渠道号'</span>, </span><br><span class="line">    `os` string COMMENT <span class="string">'安卓系统版本'</span>, </span><br><span class="line">    `area` string COMMENT <span class="string">'区域'</span>, </span><br><span class="line">    `model` string COMMENT <span class="string">'手机型号'</span>, </span><br><span class="line">    `brand` string COMMENT <span class="string">'手机品牌'</span>, </span><br><span class="line">    `sdk_version` string COMMENT <span class="string">'sdkVersion'</span>, </span><br><span class="line">    `gmail` string COMMENT <span class="string">'gmail'</span>, </span><br><span class="line">    `height_width` string COMMENT <span class="string">'屏幕宽高'</span>,</span><br><span class="line">    `app_time` string COMMENT <span class="string">'客户端日志产生时的时间'</span>,</span><br><span class="line">    `network` string COMMENT <span class="string">'网络模式'</span>,</span><br><span class="line">    `lng` string COMMENT <span class="string">'经度'</span>,</span><br><span class="line">    `lat` string COMMENT <span class="string">'纬度'</span>,</span><br><span class="line">    `create_date`  string  comment <span class="string">'创建时间'</span> </span><br><span class="line">)  COMMENT <span class="string">'每日新增设备信息'</span></span><br><span class="line">stored as parquet</span><br><span class="line">location <span class="string">'/warehouse/gmall/dws/dws_new_mid_day/'</span>;</span><br></pre></td></tr></table></figure><ul><li><strong>每日新增用户表</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table ads_new_mid_count</span><br><span class="line">(</span><br><span class="line">    `create_date`     string comment <span class="string">'创建时间'</span> ,</span><br><span class="line">    `new_mid_count`   BIGINT comment <span class="string">'新增设备数量'</span> </span><br><span class="line">)  COMMENT <span class="string">'每日新增设备信息数量'</span></span><br><span class="line">row format delimited fields terminated by <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/warehouse/gmall/ads/ads_new_mid_count/'</span>;</span><br></pre></td></tr></table></figure><p><strong>每日新增用户表装载SQL实现</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">insert into table ads_new_mid_count </span><br><span class="line">select</span><br><span class="line">create_date,</span><br><span class="line">count(*)</span><br><span class="line">from dws_new_mid_day</span><br><span class="line"><span class="built_in">where</span> create_date=<span class="string">'2019-02-10'</span></span><br><span class="line">group by create_date;</span><br></pre></td></tr></table></figure><h3 id="用户留存指标分析"><a href="#用户留存指标分析" class="headerlink" title="用户留存指标分析"></a>用户留存指标分析</h3><p><img src="//jiamaoxiang.top/2019/12/05/电商业务常用指标分析之SQL实现/%E7%94%A8%E6%88%B7%E7%95%99%E5%AD%98.png" alt></p><ul><li><strong>每日用户留存明细</strong></li></ul><p>该表以天作为分区，每天计算前1天的新用户访问留存明细，</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table dws_user_retention_day </span><br><span class="line">(</span><br><span class="line">    `mid_id` string COMMENT <span class="string">'设备唯一标识'</span>,</span><br><span class="line">    `user_id` string COMMENT <span class="string">'用户标识'</span>, </span><br><span class="line">    `version_code` string COMMENT <span class="string">'程序版本号'</span>, </span><br><span class="line">    `version_name` string COMMENT <span class="string">'程序版本名'</span>, </span><br><span class="line">`lang` string COMMENT <span class="string">'系统语言'</span>, </span><br><span class="line">`<span class="built_in">source</span>` string COMMENT <span class="string">'渠道号'</span>, </span><br><span class="line">`os` string COMMENT <span class="string">'安卓系统版本'</span>, </span><br><span class="line">`area` string COMMENT <span class="string">'区域'</span>, </span><br><span class="line">`model` string COMMENT <span class="string">'手机型号'</span>, </span><br><span class="line">`brand` string COMMENT <span class="string">'手机品牌'</span>, </span><br><span class="line">`sdk_version` string COMMENT <span class="string">'sdkVersion'</span>, </span><br><span class="line">`gmail` string COMMENT <span class="string">'gmail'</span>, </span><br><span class="line">`height_width` string COMMENT <span class="string">'屏幕宽高'</span>,</span><br><span class="line">`app_time` string COMMENT <span class="string">'客户端日志产生时的时间'</span>,</span><br><span class="line">`network` string COMMENT <span class="string">'网络模式'</span>,</span><br><span class="line">`lng` string COMMENT <span class="string">'经度'</span>,</span><br><span class="line">`lat` string COMMENT <span class="string">'纬度'</span>,</span><br><span class="line">   `create_date`    string  comment <span class="string">'设备新增时间'</span>,</span><br><span class="line">   `retention_day`  int comment <span class="string">'截止当前日期留存天数'</span></span><br><span class="line">)  COMMENT <span class="string">'每日用户留存情况'</span></span><br><span class="line">PARTITIONED BY (`dt` string)</span><br><span class="line">stored as parquet</span><br><span class="line">location <span class="string">'/warehouse/gmall/dws/dws_user_retention_day/'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>每日用户留存明细装载语句</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">insert overwrite table dws_user_retention_day</span><br><span class="line">partition(dt=<span class="string">"2019-02-11"</span>)</span><br><span class="line">select  </span><br><span class="line">    nm.mid_id,</span><br><span class="line">    nm.user_id , </span><br><span class="line">    nm.version_code , </span><br><span class="line">    nm.version_name , </span><br><span class="line">    nm.lang , </span><br><span class="line">    nm.source, </span><br><span class="line">    nm.os, </span><br><span class="line">    nm.area, </span><br><span class="line">    nm.model, </span><br><span class="line">    nm.brand, </span><br><span class="line">    nm.sdk_version, </span><br><span class="line">    nm.gmail, </span><br><span class="line">    nm.height_width,</span><br><span class="line">    nm.app_time,</span><br><span class="line">    nm.network,</span><br><span class="line">    nm.lng,</span><br><span class="line">    nm.lat,</span><br><span class="line">nm.create_date,</span><br><span class="line">1 retention_day </span><br><span class="line">from dws_uv_detail_day ud join dws_new_mid_day nm   on ud.mid_id =nm.mid_id </span><br><span class="line"><span class="built_in">where</span> ud.dt=<span class="string">'2019-02-11'</span> and nm.create_date=date_add(<span class="string">'2019-02-11'</span>,-1);</span><br></pre></td></tr></table></figure><ul><li><strong>留存用户数</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table ads_user_retention_day_count </span><br><span class="line">(</span><br><span class="line">   `create_date`       string  comment <span class="string">'设备新增日期'</span>,</span><br><span class="line">   `retention_day`     int comment <span class="string">'截止当前日期留存天数'</span>,</span><br><span class="line">   `retention_count`    bigint comment  <span class="string">'留存数量'</span></span><br><span class="line">)  COMMENT <span class="string">'每日用户留存情况'</span></span><br><span class="line">row format delimited fields terminated by <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/warehouse/gmall/ads/ads_user_retention_day_count/'</span>;</span><br></pre></td></tr></table></figure><p>留存用户数装载SQL</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">insert into table ads_user_retention_day_count </span><br><span class="line">select</span><br><span class="line">    create_date,</span><br><span class="line">    retention_day,</span><br><span class="line">    count(*) retention_count</span><br><span class="line">from dws_user_retention_day</span><br><span class="line"><span class="built_in">where</span> dt=<span class="string">'2019-02-11'</span> </span><br><span class="line">group by create_date,retention_day;</span><br></pre></td></tr></table></figure><h3 id="流失用户数分析"><a href="#流失用户数分析" class="headerlink" title="流失用户数分析"></a>流失用户数分析</h3><p>流失用户：最近7天未登录我们称之为流失用户</p><ul><li><strong>流失用户数表</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table ads_wastage_count( </span><br><span class="line">    `dt` string COMMENT <span class="string">'统计日期'</span>,</span><br><span class="line">    `wastage_count` bigint COMMENT <span class="string">'流失设备数'</span></span><br><span class="line">) </span><br><span class="line">row format delimited fields terminated by <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/warehouse/gmall/ads/ads_wastage_count'</span>;</span><br></pre></td></tr></table></figure><p>装载SQL,如果统计日期为2019-02-20，则7天未登陆的用户数的计算逻辑为：</p><p>查询日活表，并按mid_id进行分组，并且设备的最近访问时间小于等于当前时间的一周前，即活跃的最大日期(最近一次访问日期)小于等于2019-02-20</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">insert into table ads_wastage_count</span><br><span class="line">select</span><br><span class="line">     <span class="string">'2019-02-20'</span>,</span><br><span class="line">     count(*)</span><br><span class="line">from </span><br><span class="line">(</span><br><span class="line">    select mid_id</span><br><span class="line">from dws_uv_detail_day</span><br><span class="line">    group by mid_id</span><br><span class="line">    having max(dt)&lt;=date_add(<span class="string">'2019-02-20'</span>,-7)</span><br><span class="line">)t1;</span><br></pre></td></tr></table></figure><h3 id="最近七天内连续三天活跃用户数指标分析"><a href="#最近七天内连续三天活跃用户数指标分析" class="headerlink" title="最近七天内连续三天活跃用户数指标分析"></a>最近七天内连续三天活跃用户数指标分析</h3><ul><li><strong>最近七天内连续三天活跃用户数表</strong></li></ul><p>需要使用日活表，来获取最近7天内连续3天活跃用户数</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table ads_continuity_uv_count( </span><br><span class="line">    `dt` string COMMENT <span class="string">'统计日期'</span>,</span><br><span class="line">    `wk_dt` string COMMENT <span class="string">'最近7天日期'</span>,</span><br><span class="line">    `continuity_count` bigint</span><br><span class="line">) COMMENT <span class="string">'连续活跃设备数'</span></span><br><span class="line">row format delimited fields terminated by <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/warehouse/gmall/ads/ads_continuity_uv_count'</span>;</span><br></pre></td></tr></table></figure><p>装载语句为：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert into table ads_continuity_uv_count</span><br><span class="line">select</span><br><span class="line">    <span class="string">'2019-02-12'</span>,</span><br><span class="line">    concat(date_add(<span class="string">'2019-02-12'</span>,-6),<span class="string">'_'</span>,<span class="string">'2019-02-12'</span>),</span><br><span class="line">    count(*)</span><br><span class="line">from</span><br><span class="line"></span><br><span class="line">(</span><br><span class="line">select</span><br><span class="line">mid_id</span><br><span class="line">from </span><br><span class="line">( -- 筛选出连续3的活跃用户，可能存在重复</span><br><span class="line">select</span><br><span class="line">mid_id</span><br><span class="line"></span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">-- 计算活跃用户的活跃日期与其排名的差值</span><br><span class="line">select </span><br><span class="line"></span><br><span class="line">mid_id,</span><br><span class="line">date_sub(dt,rank) date_dif</span><br><span class="line"></span><br><span class="line">from </span><br><span class="line">(-- 查询出最近7天的活跃用户，并对活跃日期进行排名</span><br><span class="line">select</span><br><span class="line">mid_id,</span><br><span class="line">dt,</span><br><span class="line">rank() over (partition by mid_id order by dt) rank</span><br><span class="line">from dws_uv_detail_day</span><br><span class="line"><span class="built_in">where</span> dt &gt;= date_add(<span class="string">'2019-02-12'</span>,-6) and dt &lt;= <span class="string">'2109-02-12'</span></span><br><span class="line">) t1</span><br><span class="line">) t2</span><br><span class="line">group by mid_id,date_dif -- 对用户设备id和差值进行分组</span><br><span class="line">having count(*) &gt;=3   -- 统计大于等于3的差值数据筛选出来</span><br><span class="line"></span><br><span class="line">) t3</span><br><span class="line">group by mid_id -- 对mid_id进行去重</span><br><span class="line">) t4</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CDH集群之YARN性能调优</title>
      <link href="/2019/12/03/CDH%E9%9B%86%E7%BE%A4%E4%B9%8BYARN%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
      <url>/2019/12/03/CDH%E9%9B%86%E7%BE%A4%E4%B9%8BYARN%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<p>本文主要讨论CDH集群的YARN调优配置，关于YARN的调优配置，主要关注CPU和内存的调优，其中CPU是指物理CPU个数乘以CPU核数，即Vcores = CPU数量*CPU核数。YARN是以container容器的形式封装资源的，task在container内部执行。</p><a id="more"></a><h2 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h2><p>集群的配置主要包括三步，第一是先规划集群的工作主机以及每台主机的配置，第二是规划每台主机的安装的组件及其资源分配，第三是规划集群的规模大小。</p><h3 id="工作主机的配置"><a href="#工作主机的配置" class="headerlink" title="工作主机的配置"></a>工作主机的配置</h3><p>如下表所示：主机的内存为256G，4个6核CPU，CPU支持超线程，网络带宽为2G</p><table>    <tr>        <td bgcolor="#6495ED">主机组件</td>        <td bgcolor="#6495ED">数量</td>         <td bgcolor="#6495ED">大小</td>        <td bgcolor="#6495ED">总计</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>RAM</td>        <td>256G</td>         <td></td>        <td>256G</td>          <td>内存大小</td>     </tr>    <tr>        <td>CPU</td>        <td>4</td>         <td>6</td>        <td>48</td>         <td>总CPU核数</td>      </tr>    <tr>        <td>HyperThreading CPU</td>        <td>YES</td>         <td></td>        <td></td>         <td>超线程CPU，使操作系统认为处理器的核心数是实际核心数的2倍，因此如果有24个核心的处理器，操作系统会认为处理器有48个核心</td>      </tr>    <tr>        <td>网络</td>        <td>2</td>         <td>1G</td>        <td>2G</td>         <td>网络带宽</td>      </tr>  </table><h3 id="工作主机安装组件配置"><a href="#工作主机安装组件配置" class="headerlink" title="工作主机安装组件配置"></a>工作主机安装组件配置</h3><p>第一步已经明确每台主机的内存和CPU配置，下面为每台节点的服务分配资源，主要分配CPU和内存</p><table>    <tr>        <td bgcolor="#6495ED">服务</td>        <td bgcolor="#6495ED">类别</td>         <td bgcolor="#6495ED">CPU核数</td>        <td bgcolor="#6495ED">内存(MB)</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>操作系统</td>        <td>Overhead</td>         <td>1</td>        <td>8192</td>          <td>为操作系统分配1核8G内存，一般4~8G</td>     </tr>    <tr>        <td>其它服务</td>        <td>Overhead</td>         <td>0</td>        <td>0</td>         <td>非CDH集群、非操作系统占用的资源</td>      </tr>    <tr>        <td>Cloudera Manager agent</td>        <td>Overhead</td>         <td>1</td>        <td>1024</td>         <td>分配1核1G</td>      </tr>    <tr>        <td>HDFS DataNode</td>        <td>CDH</td>         <td>1</td>        <td>1024</td>         <td>默认1核1G</td>      </tr>       <tr>        <td>YARN NodeManager</td>        <td>CDH</td>         <td>1</td>        <td>1024</td>         <td>默认1核1G</td>      </tr>           <tr>        <td>Impala daemon</td>        <td>CDH</td>         <td>0</td>        <td>0</td>         <td>可选的服务，建议至少为impala demon分配16G内存</td>      </tr>           <tr>        <td>Hbase RegionServer</td>        <td>CDH</td>         <td>0</td>        <td>0</td>         <td>可选的服务，建议12~16G内存</td>      </tr>           <tr>        <td>Solr Server</td>        <td>CDH</td>         <td>0</td>        <td>0</td>         <td>可选的服务，最低1G内存</td>      </tr>           <tr>        <td>Kudu Server</td>        <td>CDH</td>         <td>0</td>        <td>0</td>         <td>可选的服务，kudu Tablet server最低1G内存</td>      </tr>           <tr>        <td>Available Container  Resources</td>        <td></td>         <td>44</td>        <td>250880</td>         <td>剩余分配给yarn的container</td>      </tr></table><p>container资源分配<br>Physical Cores to Vcores Multiplier：每个container的cpu core的并发线程数，本文设置为1</p><p>YARN Available Vcores：YARN可用的CPU核数=Available Container  Resources * Physical Cores to Vcores Multiplier，即为44</p><p>YARN Available Memory：250880</p><h3 id="集群大小"><a href="#集群大小" class="headerlink" title="集群大小"></a>集群大小</h3><p>集群的工作节点个数：10</p><h2 id="YARN配置"><a href="#YARN配置" class="headerlink" title="YARN配置"></a>YARN配置</h2><h3 id="YARN-NodeManager配置属性"><a href="#YARN-NodeManager配置属性" class="headerlink" title="YARN NodeManager配置属性"></a>YARN NodeManager配置属性</h3><table>     <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>yarn.nodemanager.resource.cpu-vcores</td>        <td>44</td>          <td>yarn 的nodemanager分配44核，每台节点剩余的CPU</td>     </tr>    <tr>        <td>yarn.nodemanager.resource.memory-mb</td>        <td>250800</td>         <td>分配的内存大小，每台节点剩余的内存</td>      </tr></table><h3 id="验证YARN的配置"><a href="#验证YARN的配置" class="headerlink" title="验证YARN的配置"></a>验证YARN的配置</h3><p>登录YARN的resourcemanager的WEBUI：http://<resourcemanagerip>:8088/，验证’Memory Total’与’Vcores Total’，如果节点都正常，那么Vcores Total应该为440，Memory应该为2450G，即250800/1024*10</resourcemanagerip></p><h3 id="YARN的container配置"><a href="#YARN的container配置" class="headerlink" title="YARN的container配置"></a>YARN的container配置</h3><p>YARN的container的Vcore配置</p><table>    <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>yarn.scheduler.minimum-allocation-vcores</td>        <td>1</td>          <td>分配给container的最小vcore个数</td>     </tr>    <tr>        <td>yarn.scheduler.maximum-allocation-vcores</td>        <td>44</td>         <td>分配给container的最大vcore数</td>      </tr>    <tr>        <td>yarn.scheduler.increment-allocation-vcores</td>        <td>1</td>         <td>容器虚拟CPU内核增量</td>      </tr></table><p>YARN的container内存配置</p><table>    <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>yarn.scheduler.minimum-allocation-mb</td>        <td>1024</td>          <td>分配给container的最小内存大小，为1G</td>     </tr>    <tr>        <td>yarn.scheduler.maximum-allocation-mb</td>        <td>250880</td>         <td>分配给container的最大内存，等于245G，即为每台节点剩余的最大内存</td>      </tr>    <tr>        <td>yarn.scheduler.increment-allocation-mb</td>        <td>512</td>         <td>容器内存增量，默认512M</td>      </tr></table><h3 id="集群资源分配估计"><a href="#集群资源分配估计" class="headerlink" title="集群资源分配估计"></a>集群资源分配估计</h3><table>    <tr>        <td bgcolor="#6495ED">描述</td>        <td bgcolor="#6495ED">最小值</td>         <td bgcolor="#6495ED">最大值</td>      </tr>    <tr>        <td>根据每个container的最小内存分配，集群最大的container数量为</td>        <td></td>          <td>2450</td>     </tr>    <tr>        <td>根据每个container的最小Vcore分配，集群最大的container数量为</td>        <td></td>         <td>440</td>      </tr>    <tr>        <td>根据每个container的最大内存分配，集群的最小container数为</td>        <td>10</td>         <td></td>      </tr>    <tr>        <td>根据每个container的最大Vcores分配，集群的最小container数为</td>        <td>10</td>         <td></td>      </tr></table><h3 id="container合理配置检查"><a href="#container合理配置检查" class="headerlink" title="container合理配置检查"></a>container合理配置检查</h3><table>    <tr>        <td bgcolor="#6495ED">配置约束</td>        <td bgcolor="#6495ED">描述</td>     </tr>    <tr>        <td>最大的Vcore数量必须大于等于分配的最小Vcore数</td>        <td>yarn.scheduler.maximum-allocation-vcores >= yarn.scheduler.minimum-allocation-vcores</td>      </tr>    <tr>        <td>分配的最大内存数必须大于等于分配的最小内存数</td>        <td>yarn.scheduler.maximum-allocation-mb >= yarn.scheduler.minimum-allocation-mb</td>     </tr>    <tr>        <td>分配的最小核数必须大于等于0</td>        <td>yarn.scheduler.minimum-allocation-vcores >= 0</td>      </tr>    <tr>        <td>分配的最大Vcore数必须大于等于1</td>        <td>yarn.scheduler.maximum-allocation-vcores >= 1</td>      </tr>    <tr>        <td>每台主机分配给nodemanaer的vcore总数必须大于分配的最小vcore数</td>        <td> yarn.nodemanager.resource.cpu-vcores >= yarn.scheduler.minimum-allocation-vcores</td>      </tr>    <tr>        <td>每台主机分配给nodemanaer的vcore总数必须大于分配的最大vcore数</td>        <td>yarn.nodemanager.resource.cpu-vcores >= yarn.scheduler.maximum-allocation-vcores </td>      </tr>    <tr>        <td>每台主机分配给nodemanaer的内存必须大于调度分配的最小内存</td>        <td>yarn.nodemanager.resource.memory-mb >= yarn.scheduler.maximum-allocation-mb</td>      </tr>    <tr>        <td>每台主机分配给nodemanaer的内存必须大于调度分配的最大内存</td>        <td>yarn.nodemanager.resource.memory-mb >= yarn.scheduler.minimum-allocation-mb</td>      </tr>    <tr>        <td>container最小配置</td>        <td>如果yarn.scheduler.minimum-allocation-mb小于1GB，container可能会被YARN杀死，因为会出现OutOfMemory内存溢出的现象</td>      </tr></table><h2 id="MapReduce配置"><a href="#MapReduce配置" class="headerlink" title="MapReduce配置"></a>MapReduce配置</h2><h3 id="ApplicationMaster配置"><a href="#ApplicationMaster配置" class="headerlink" title="ApplicationMaster配置"></a>ApplicationMaster配置</h3><table>     <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>yarn.app.mapreduce.am.resource.cpu-vcores</td>        <td>1</td>          <td>ApplicationMaster 的虚拟CPU内核数</td>     </tr>    <tr>        <td>yarn.app.mapreduce.am.resource.mb</td>        <td>1024</td>         <td>ApplicationMaster的物理内存要求(MiB)</td>      </tr>    <tr>        <td>yarn.app.mapreduce.am.command-opts</td>        <td>800</td>         <td>传递到 MapReduce ApplicationMaster 的 Java 命令行参数，AM Java heap 大小，为800M</td>      </tr></table><h3 id="堆与容器大小之比"><a href="#堆与容器大小之比" class="headerlink" title="堆与容器大小之比"></a>堆与容器大小之比</h3><table>    <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>task自动堆大小</td>        <td>yes</td>          <td></td>     </tr>    <tr>        <td>mapreduce.job.heap.memory-mb.ratio</td>        <td>0.8</td>         <td>Map 和 Reduce 任务的堆大小与容器大小之比。堆大小应小于容器大小，以允许 JVM 的某些开销，默认为0.8</td>      </tr></table><h3 id="map-task配置"><a href="#map-task配置" class="headerlink" title="map task配置"></a>map task配置</h3><table>    <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>mapreduce.map.cpu.vcores</td>        <td>1</td>          <td>分配给map task的vcore数</td>     </tr>    <tr>        <td>mapreduce.map.memory.mb</td>        <td>1024</td>         <td>分配给map task的内存数，1G</td>      </tr>    <tr>        <td>mapreduce.task.io.sort.mb</td>        <td>400</td>         <td>I/O 排序内存缓冲 (MiB),默认256M，一般不用修改</td>      </tr></table><h3 id="reduce-task配置"><a href="#reduce-task配置" class="headerlink" title="reduce task配置"></a>reduce task配置</h3><table>    <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>mapreduce.reduce.cpu.vcores</td>        <td>1</td>          <td>分配给reduce task的vcore数</td>     </tr>    <tr>        <td>mapreduce.reduce.memory.mb</td>        <td>1024</td>         <td>分配给reduce task的内存数，1G</td>      </tr></table><h3 id="MapReduce配置合理性检查"><a href="#MapReduce配置合理性检查" class="headerlink" title="MapReduce配置合理性检查"></a>MapReduce配置合理性检查</h3><ul><li>Application Master配置的合理性检查</li></ul><p>yarn.scheduler.minimum-allocation-vcores &lt;=  <strong>yarn.app.mapreduce.am.resource.cpu-vcores</strong>&lt;= yarn-scheduler.maximum-allocation-vcores</p><p>yarn.scheduler.minimum-allocation-mb &lt;= <strong>yarn.app.mapreduce.am.resource.cpu-vcores</strong> &lt;= yarn.scheduler.maximum-allocation-mb  </p><p>Java Heap大小是container大小的75%~90%: 太低会造成资源浪费, 太高会造成OOM<br>Map Task配置的合理性检查</p><ul><li>Reduce Task配置的合理性检查</li></ul><p>yarn.scheduler.minimum-allocation-vcores &lt;= <strong>mapreduce.map.cpu.vcores</strong> &lt;= yarn-scheduler.maximum-allocation-vcores</p><p>yarn.scheduler.minimum-allocation-mb &lt;= <strong>mapreduce.map.memory.mb</strong> &lt;= yarn.scheduler.maximum-allocation-mb</p><p>Spill/Sort内存为每个task堆内存的40%~60%</p><ul><li>Reduce Task配置的合理性检查</li></ul><p>yarn.scheduler.minimum-allocation-vcores &lt;= <strong>mapreduce.reduce.cpu.vcores</strong> &lt;= yarn-scheduler.maximum-allocation-vcores   </p><p>yarn.scheduler.minimum-allocation-mb &lt;= <strong>mapreduce.reduce.memory.mb</strong> &lt;= yarn.scheduler.maximum-allocation-mb</p><h2 id="YARN和MapReduce配置参数总结"><a href="#YARN和MapReduce配置参数总结" class="headerlink" title="YARN和MapReduce配置参数总结"></a>YARN和MapReduce配置参数总结</h2><table>    <tr>        <td bgcolor="#6495ED">YARN/MapReduce参数配置</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>yarn.nodemanager.resource.cpu-vcores</td>        <td>分配给container的虚拟cpu数</td>     </tr>    <tr>        <td>yarn.nodemanager.resource.memory-mb</td>        <td>分配给container的内存大小</td>     </tr>    <tr>        <td>yarn.scheduler.minimum-allocation-vcores</td>        <td>分配给container的最小虚拟cpu数</td>     </tr>    <tr>        <td>    yarn.scheduler.maximum-allocation-vcores</td>        <td>分配给container的最大虚拟cpu数</td>     </tr>    <tr>        <td>yarn.scheduler.increment-allocation-vcores</td>        <td>分配给container的递增虚拟cpu数</td>     </tr>    <tr>        <td>yarn.scheduler.minimum-allocation-mb</td>        <td>分配给container的最小内存大小</td>     </tr>    <tr>        <td>yarn.scheduler.maximum-allocation-mb</td>        <td>分配给container的最大内存</td>     </tr>    <tr>        <td>yarn.scheduler.increment-allocation-mb</td>        <td>分配给container的递增内存大小</td>     </tr>    <tr>        <td>yarn.app.mapreduce.am.resource.cpu-vcores</td>        <td>ApplicationMaste的虚拟cpu数</td>     </tr>    <tr>        <td>yarn.app.mapreduce.am.resource.mb</td>        <td>ApplicationMaste的内存大小</td>     </tr>    <tr>        <td>mapreduce.map.cpu.vcores</td>        <td>map task的虚拟CPU数</td>     </tr>    <tr>        <td>mapreduce.map.memory.mb</td>        <td>map task的内存大小</td>     </tr>    <tr>        <td>mapreduce.reduce.cpu.vcores</td>        <td>reduce task的虚拟cpu数</td>     </tr>    <tr>        <td>mapreduce.reduce.memory.mb</td>        <td>reduce task的内存大小</td>     </tr>    <tr>        <td>mapreduce.task.io.sort.mb</td>        <td>I/O排序内存大小</td>     </tr></table><p>note：在CDH5.5或者更高版本中，参数<strong>mapreduce.map.java.opts</strong>, <strong>mapreduce.reduce.java.opts</strong>, <strong>yarn.app.mapreduce.am.command-opts</strong>会基于container堆内存的比例进行自动配置 </p>]]></content>
      
      
      <categories>
          
          <category> CDH,YARN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH,YARN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>历史拉链表实战</title>
      <link href="/2019/11/08/%E5%8E%86%E5%8F%B2%E6%8B%89%E9%93%BE%E8%A1%A8%E5%AE%9E%E6%88%98/"/>
      <url>/2019/11/08/%E5%8E%86%E5%8F%B2%E6%8B%89%E9%93%BE%E8%A1%A8%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[<p>历史拉链表是一种数据模型，主要是针对数据仓库设计中表存储数据的方式而定义的。所谓历史拉链表，就是指记录一个事物从开始一直到当前状态的所有变化信息。拉所有记录链表可以避免按每一天存储造成的海量存储问题，同时也是处理缓慢变化数据的一种常见方式。</p><a id="more"></a><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>现假设有如下场景：一个企业拥有5000万会员信息，每天有20万会员资料变更，需要在数仓中记录会员表的历史变化以备分析使用，即每天都要保留一个快照供查询，反映历史数据的情况。在此场景中，需要反映5000万会员的历史变化，如果保留快照，存储两年就需要2X365X5000W条数据存储空间，数据量为365亿，如果存储更长时间，则无法估计需要的存储空间。而利用拉链算法存储，每日只向历史表中添加新增和变化的数据，每日不过20万条，存储4年也只需要3亿存储空间。</p><h2 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h2><p>在拉链表中，每一条数据都有一个生效日期(effective_date)和失效日期(expire_date)。假设在一个用户表中，在2019年11月8日新增了两个用户，如下表所示，则这两条记录的生效时间为当天，由于到2019年11月8日为止,这两条就还没有被修改过，所以失效时间为一个给定的比较大的值，比如：3000-12-31  </p><table>    <tr>        <td> member_id</td>         <td>phoneno</td>        <td>create_time</td>        <td>update_time</td>     </tr>    <tr>        <td>10001</td>        <td>13300000001</td>          <td>2019-11-08</td>         <td>3000-12-31</td>     </tr>    <tr>        <td>10002</td>        <td>13500000002</td>         <td>2019-11-08</td>        <td>3000-12-31</td>       </tr></table><p>第二天(2019-11-09)，用户10001被删除了，用户10002的电话号码被修改成13600000002.为了保留历史状态，用户10001的失效时间被修改为2019-11-09，用户10002则变成了两条记录，如下表所示：  </p><table>    <tr>        <td> member_id</td>         <td>phoneno</td>        <td>create_time</td>        <td>update_time</td>     </tr>    <tr>        <td>10001</td>        <td>13300000001</td>          <td>2019-11-08</td>         <td>2019-11-09</td>     </tr>    <tr>        <td>10002</td>        <td>13500000002</td>         <td>2019-11-08</td>        <td>2019-11-09</td>       </tr>    <tr>        <td>10002</td>        <td>13600000002</td>         <td>2019-11-09</td>        <td>3000-12-31</td>       </tr></table><p>第三天(2019-11-10),又新增了用户10003，则用户表数据如小表所示：  </p><table>    <tr>        <td> member_id</td>         <td>phoneno</td>        <td>create_time</td>        <td>update_time</td>     </tr>    <tr>        <td>10001</td>        <td>13300000001</td>          <td>2019-11-08</td>         <td>2019-11-09</td>     </tr>    <tr>        <td>10002</td>        <td>13500000002</td>         <td>2019-11-08</td>        <td>2019-11-09</td>       </tr>    <tr>        <td>10002</td>        <td>13600000002</td>         <td>2019-11-09</td>        <td>3000-12-31</td>       </tr>    <tr>        <td>10003</td>        <td>13300000006</td>         <td>2019-11-10</td>        <td>3000-12-31</td>       </tr></table><p>如果要查询最新的数据，那么只要查询失效时间为3000-12-31的数据即可，如果要查11月8号的历史数据，则筛选生效时间&lt;= 2019-11-08并且失效时间&gt;2019-11-08的数据即可。如果查询11月9号的数据，那么筛选条件则是生效时间&lt;=2019-11-09并且失效时间&gt;2019-11-09</p><h2 id="表结构"><a href="#表结构" class="headerlink" title="表结构"></a>表结构</h2><ul><li>MySQL源member表</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE member(</span><br><span class="line">             member_id VARCHAR ( 64 ),</span><br><span class="line">             phoneno VARCHAR ( 20 ), </span><br><span class="line">             create_time datetime,</span><br><span class="line">             update_time datetime );</span><br></pre></td></tr></table></figure><ul><li>ODS层增量表member_delta,每天一个分区</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">CREATE TABLE member_delta</span><br><span class="line">            (member_id string,</span><br><span class="line">             phoneno string,</span><br><span class="line">             create_time string,</span><br><span class="line">             update_time string) </span><br><span class="line">PARTITIONED BY (DAY string);</span><br></pre></td></tr></table></figure><ul><li>临时表</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">CREATE TABLE member_his_tmp</span><br><span class="line">            (member_id string,</span><br><span class="line">             phoneno string,</span><br><span class="line">             effective_date date,</span><br><span class="line">             expire_date date</span><br><span class="line">             );</span><br></pre></td></tr></table></figure><ul><li>DW层历史拉链表</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">CREATE TABLE member_his</span><br><span class="line">            (member_id string,</span><br><span class="line">             phoneno string,</span><br><span class="line">             effective_date date,</span><br><span class="line">             expire_date date);</span><br></pre></td></tr></table></figure><h2 id="Demo数据准备"><a href="#Demo数据准备" class="headerlink" title="Demo数据准备"></a>Demo数据准备</h2><p>2019-11-08的数据为：  </p><table>    <tr>        <td> member_id</td>         <td>phoneno</td>        <td>create_time</td>        <td>update_time</td>     </tr>    <tr>        <td>10001</td>        <td>13500000001</td>          <td>2019-11-08 14:47:55</td>         <td>2019-11-08 14:47:55</td>     </tr>    <tr>        <td>10002</td>        <td>13500000002</td>         <td>2019-11-08 14:48:33</td>        <td>2019-11-08 14:48:33</td>       </tr>    <tr>        <td>10003</td>        <td>13500000003</td>         <td>2019-11-08 14:48:53</td>        <td>2019-11-08 14:48:53</td>       </tr>    <tr>        <td>10004</td>        <td>13500000004</td>         <td>2019-11-08 14:49:02</td>        <td>2019-11-08 14:49:02</td>       </tr></table><p>2019-11-09的数据为：其中蓝色代表新增数据，红色代表修改的数据</p><table>    <tr>        <td> member_id</td>         <td>phoneno</td>        <td>create_time</td>        <td>update_time</td>     </tr>    <tr>        <td>10001</td>        <td>13500000001</td>          <td>2019-11-08 14:47:55</td>         <td>2019-11-08 14:47:55</td>     </tr>    <tr>        <td bgcolor="#DC143C">10002</td>        <td bgcolor="#DC143C">13600000002</td>         <td bgcolor="#DC143C">2019-11-08 14:48:33</td>        <td bgcolor="#DC143C">2019-11-09 14:48:33</td>       </tr>    <tr>        <td>10003</td>        <td>13500000003</td>         <td>2019-11-08 14:48:53</td>        <td>2019-11-08 14:48:53</td>       </tr>    <tr>        <td>10004</td>        <td>13500000004</td>         <td>2019-11-08 14:49:02</td>        <td>2019-11-08 14:49:02</td>       </tr>    <tr>        <td bgcolor="#6495ED">10005</td>         <td bgcolor="#6495ED">13500000005</td>        <td bgcolor="#6495ED">2019-11-09 08:54:03 </td>        <td bgcolor="#6495ED">2019-11-09 08:54:03</td>    </tr>    <tr>        <td bgcolor="#6495ED">10006</td>         <td bgcolor="#6495ED">13500000006</td>        <td bgcolor="#6495ED">2019-11-09 09:54:25 </td>        <td bgcolor="#6495ED">2019-11-09 09:54:25</td>    </tr></table><p>2019-11-10的数据：其中蓝色代表新增数据，红色代表修改的数据  </p><table>    <tr>        <td> member_id</td>         <td>phoneno</td>        <td>create_time</td>        <td>update_time</td>     </tr>    <tr>        <td>10001</td>        <td>13500000001</td>          <td>2019-11-08 14:47:55</td>         <td>2019-11-08 14:47:55</td>     </tr>    <tr>        <td>10002</td>        <td>13600000002</td>         <td>2019-11-08 14:48:33</td>        <td>2019-11-09 14:48:33</td>       </tr>    <tr>        <td>10003</td>        <td>13500000003</td>         <td>2019-11-08 14:48:53</td>        <td>2019-11-08 14:48:53</td>       </tr>    <tr>        <td bgcolor="#DC143C">10004</td>        <td bgcolor="#DC143C">13600000004</td>         <td bgcolor="#DC143C">2019-11-08 14:49:02</td>        <td bgcolor="#DC143C">2019-11-10 14:49:02</td>       </tr>    <tr>        <td>10005</td>         <td>13500000005</td>        <td>2019-11-09 08:54:03 </td>        <td>2019-11-09 08:54:03</td>    </tr>    <tr>        <td>10006</td>         <td>13500000006</td>        <td>2019-11-09 09:54:25 </td>        <td>2019-11-09 09:54:25</td>    </tr>    <tr>        <td bgcolor="#6495ED">10007</td>         <td bgcolor="#6495ED">13500000007</td>        <td bgcolor="#6495ED">2019-11-10 17:41:49 </td>        <td bgcolor="#6495ED">2019-11-10 17:41:49</td>    </tr></table><h2 id="全量初始装载"><a href="#全量初始装载" class="headerlink" title="全量初始装载"></a>全量初始装载</h2><p>在启用拉链表时，先对其进行初始装载，比如以2019-11-08为开始时间，<br>那么将MySQL源表全量抽取到ODS层member_delta表的2018-11-08的分区中，<br>然后初始装载DW层的拉链表member_his</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">INSERT overwrite TABLE member_his </span><br><span class="line">SELECT</span><br><span class="line">   member_id,</span><br><span class="line">   phoneno,</span><br><span class="line">   to_date ( create_time ) AS effective_date,</span><br><span class="line">  <span class="string">'3000-12-31'</span> </span><br><span class="line">FROM</span><br><span class="line">member_delta </span><br><span class="line">WHERE</span><br><span class="line">DAY = <span class="string">'2019-11-08'</span></span><br></pre></td></tr></table></figure><p>查询初始的历史拉链表数据</p><p><img src="//jiamaoxiang.top/2019/11/08/历史拉链表实战/1.png" alt></p><h2 id="增量抽取数据"><a href="#增量抽取数据" class="headerlink" title="增量抽取数据"></a>增量抽取数据</h2><p>每天，从源系统member表中，将前一天的增量数据抽取到ODS层的增量数据表member_delta对应的分区中。这里的增量需要通过member表中的创建时间和修改时间来确定，或者使用sqoop job监控update时间来进行增联抽取。<br>比如，本案例中2019-11-09和2019-11-10为两个分区，分别存储了2019-11-09和2019-11-10日的增量数据。<br>2019-11-09分区的数据为:</p><p><img src="//jiamaoxiang.top/2019/11/08/历史拉链表实战/2.png" alt></p><p>2019-11-10分区的数据为：</p><p><img src="//jiamaoxiang.top/2019/11/08/历史拉链表实战/3.png" alt></p><h2 id="增量刷新历史拉链数据"><a href="#增量刷新历史拉链数据" class="headerlink" title="增量刷新历史拉链数据"></a>增量刷新历史拉链数据</h2><ul><li>2019-11-09增量刷新历史拉链表<br>将数据放进临时表</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">INSERT overwrite TABLE member_his_tmp</span><br><span class="line">SELECT *</span><br><span class="line">FROM</span><br><span class="line">  (</span><br><span class="line">-- 2019-11-09增量数据，代表最新的状态，该数据的生效时间是2019-11-09，过期时间为3000-12-31</span><br><span class="line">-- 这些增量的数据需要被全部加载到历史拉链表中</span><br><span class="line">SELECT member_id,</span><br><span class="line">       phoneno,</span><br><span class="line">       <span class="string">'2019-11-09'</span> effective_date,</span><br><span class="line">                    <span class="string">'3000-12-31'</span> expire_date</span><br><span class="line">   FROM member_delta</span><br><span class="line">   WHERE DAY=<span class="string">'2019-11-09'</span></span><br><span class="line">   UNION ALL </span><br><span class="line">-- 用当前为生效状态的拉链数据，去left join 增量数据，</span><br><span class="line">-- 如果匹配得上，则表示该数据已发生了更新，</span><br><span class="line">-- 此时，需要将发生更新的数据的过期时间更改为当前时间.</span><br><span class="line">-- 如果匹配不上，则表明该数据没有发生更新，此时过期时间不变</span><br><span class="line">SELECT a.member_id,</span><br><span class="line">       a.phoneno,</span><br><span class="line">       a.effective_date,</span><br><span class="line">       <span class="keyword">if</span>(b.member_id IS NULL, to_date(a.expire_date), to_date(b.day)) expire_date</span><br><span class="line">   FROM</span><br><span class="line">     (SELECT *</span><br><span class="line">      FROM member_his</span><br><span class="line">    ) a</span><br><span class="line">   LEFT JOIN</span><br><span class="line">     (SELECT *</span><br><span class="line">      FROM member_delta</span><br><span class="line">      WHERE DAY=<span class="string">'2019-11-09'</span>) b ON a.member_id=b.member_id)his</span><br></pre></td></tr></table></figure><p>将数据覆盖到历史拉链表</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">INSERT overwrite TABLE member_his</span><br><span class="line">SELECT *</span><br><span class="line">FROM member_his_tmp</span><br></pre></td></tr></table></figure><p>查看历史拉链表</p><p><img src="//jiamaoxiang.top/2019/11/08/历史拉链表实战/4.png" alt></p><ul><li>2019-11-10增量刷新历史拉链表</li></ul><p>将数据放进临时表</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">INSERT overwrite TABLE member_his_tmp</span><br><span class="line">SELECT *</span><br><span class="line">FROM</span><br><span class="line">  (</span><br><span class="line">-- 2019-11-10增量数据，代表最新的状态，该数据的生效时间是2019-11-10，过期时间为3000-12-31</span><br><span class="line">-- 这些增量的数据需要被全部加载到历史拉链表中</span><br><span class="line">SELECT member_id,</span><br><span class="line">       phoneno,</span><br><span class="line">       <span class="string">'2019-11-10'</span> effective_date,</span><br><span class="line">                    <span class="string">'3000-12-31'</span> expire_date</span><br><span class="line">   FROM member_delta</span><br><span class="line">   WHERE DAY=<span class="string">'2019-11-10'</span></span><br><span class="line">   UNION ALL </span><br><span class="line">-- 用当前为生效状态的拉链数据，去left join 增量数据，</span><br><span class="line">-- 如果匹配得上，则表示该数据已发生了更新，</span><br><span class="line">-- 此时，需要将发生更新的数据的过期时间更改为当前时间.</span><br><span class="line">-- 如果匹配不上，则表明该数据没有发生更新，此时过期时间不变</span><br><span class="line">SELECT a.member_id,</span><br><span class="line">       a.phoneno,</span><br><span class="line">       a.effective_date,</span><br><span class="line">       <span class="keyword">if</span>(b.member_id IS NULL, to_date(a.expire_date), to_date(b.day)) expire_date</span><br><span class="line">   FROM</span><br><span class="line">     (SELECT *</span><br><span class="line">      FROM member_his</span><br><span class="line">    ) a</span><br><span class="line">   LEFT JOIN</span><br><span class="line">     (SELECT *</span><br><span class="line">      FROM member_delta</span><br><span class="line">      WHERE DAY=<span class="string">'2019-11-10'</span>) b ON a.member_id=b.member_id)his</span><br></pre></td></tr></table></figure><p>查看历史拉链表</p><p><img src="//jiamaoxiang.top/2019/11/08/历史拉链表实战/5.png" alt></p><p>将以上脚本封装成shell调度的脚本</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天</span></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$1</span>"</span> ] ;<span class="keyword">then</span></span><br><span class="line">do_date=<span class="variable">$1</span></span><br><span class="line"><span class="keyword">else</span> </span><br><span class="line">do_date=`date -d <span class="string">"-1 day"</span> +%F`  </span><br><span class="line"><span class="keyword">fi</span> </span><br><span class="line"></span><br><span class="line">sql=<span class="string">"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">INSERT overwrite TABLE member_his_tmp</span></span><br><span class="line"><span class="string">SELECT *</span></span><br><span class="line"><span class="string">FROM</span></span><br><span class="line"><span class="string">  (</span></span><br><span class="line"><span class="string">-- 2019-11-10增量数据，代表最新的状态，该数据的生效时间是2019-11-10，过期时间为3000-12-31</span></span><br><span class="line"><span class="string">-- 这些增量的数据需要被全部加载到历史拉链表中</span></span><br><span class="line"><span class="string">SELECT member_id,</span></span><br><span class="line"><span class="string">       phoneno,</span></span><br><span class="line"><span class="string">       '<span class="variable">$do_date</span>' effective_date,</span></span><br><span class="line"><span class="string">       '3000-12-31' expire_date</span></span><br><span class="line"><span class="string">   FROM member_delta</span></span><br><span class="line"><span class="string">   WHERE DAY='<span class="variable">$do_date</span>'</span></span><br><span class="line"><span class="string">   UNION ALL </span></span><br><span class="line"><span class="string">-- 用当前为生效状态的拉链数据，去left join 增量数据，</span></span><br><span class="line"><span class="string">-- 如果匹配得上，则表示该数据已发生了更新，</span></span><br><span class="line"><span class="string">-- 此时，需要将发生更新的数据的过期时间更改为当前时间.</span></span><br><span class="line"><span class="string">-- 如果匹配不上，则表明该数据没有发生更新，此时过期时间不变</span></span><br><span class="line"><span class="string">SELECT a.member_id,</span></span><br><span class="line"><span class="string">       a.phoneno,</span></span><br><span class="line"><span class="string">       a.effective_date,</span></span><br><span class="line"><span class="string">       if(b.member_id IS NULL, to_date(a.expire_date), to_date(b.day)) expire_date</span></span><br><span class="line"><span class="string">   FROM</span></span><br><span class="line"><span class="string">     (SELECT *</span></span><br><span class="line"><span class="string">      FROM member_his</span></span><br><span class="line"><span class="string"> ) a</span></span><br><span class="line"><span class="string">   LEFT JOIN</span></span><br><span class="line"><span class="string">     (SELECT *</span></span><br><span class="line"><span class="string">      FROM member_delta</span></span><br><span class="line"><span class="string">      WHERE DAY='<span class="variable">$do_date</span>') b ON a.member_id=b.member_id)his;</span></span><br><span class="line"><span class="string">"</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$hive</span> -e <span class="string">"<span class="variable">$sql</span>"</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数据仓库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink运行架构剖析</title>
      <link href="/2019/10/23/Flink%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84%E5%89%96%E6%9E%90/"/>
      <url>/2019/10/23/Flink%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84%E5%89%96%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>本文主要介绍 Flink Runtime 的作业执行的核心机制。首先介绍 Flink Runtime 的整体架构以及 Job 的基本执行流程，然后介绍Flink 的Standalone运行架构，最后对Flink on YARN的两种模式进行了详细剖析。</p><a id="more"></a><h2 id="Flink-Runtime作业执行流程分析"><a href="#Flink-Runtime作业执行流程分析" class="headerlink" title="Flink Runtime作业执行流程分析"></a>Flink Runtime作业执行流程分析</h2><h3 id="整体架构图"><a href="#整体架构图" class="headerlink" title="整体架构图"></a>整体架构图</h3><p>Flink Runtime 层的主要架构如下图所示，它展示了一个 Flink 集群的基本结构。整体来说，它采用了标准 master-slave 的结构，master负责管理整个集群中的资源和作业；TaskExecutor 则是 Slave，负责提供具体的资源并实际执行作业。  </p><p><img src="//jiamaoxiang.top/2019/10/23/Flink运行架构剖析/Runtime.png" alt></p><h3 id="执行流程分析"><a href="#执行流程分析" class="headerlink" title="执行流程分析"></a>执行流程分析</h3><h4 id="组件介绍"><a href="#组件介绍" class="headerlink" title="组件介绍"></a>组件介绍</h4><p>Application Master 部分包含了三个组件，即 Dispatcher、ResourceManager 和 JobManager。其中，Dispatcher 负责接收用户提供的作业，并且负责为这个新提交的作业拉起一个新的 JobManager 组件。ResourceManager 负责资源的管理，在整个 Flink 集群中只有一个 ResourceManager。JobManager 负责管理作业的执行，在一个 Flink 集群中可能有多个作业同时执行，每个作业都有自己的 JobManager 组件。这三个组件都包含在 AppMaster 进程。  </p><p>TaskManager主要负责执行具体的task任务，<a href="https://jiamaoxiang.top/2019/08/23/Flink%E7%9A%84%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF-State-Backends/">StateBackend</a> 主要应用于状态的checkpoint。  </p><p>Cluster Manager是集群管理器，比如Standalone、YARN、K8s等。  </p><h4 id="流程分析"><a href="#流程分析" class="headerlink" title="流程分析"></a>流程分析</h4><p>1.当用户提交作业的时候，提交脚本会首先启动一个 Client进程负责作业的编译与提交。它首先将用户编写的代码编译为一个 JobGraph，在这个过程，它还会进行一些检查或优化等工作，例如判断哪些 Operator 可以 Chain 到同一个 Task 中。然后，Client 将产生的 JobGraph 提交到集群中执行。此时有两种情况，一种是类似于 Standalone 这种 Session 模式，AM 会预先启动，此时 Client 直接与 Dispatcher 建立连接并提交作业即可。另一种是 Per-Job 模式，AM 不会预先启动，此时 Client 将首先向资源管理系统 （如Yarn、K8S）申请资源来启动 AM，然后再向 AM 中的 Dispatcher 提交作业。  </p><p>2.当作业到 Dispatcher 后，Dispatcher 会首先启动一个 JobManager 组件，然后 JobManager 会向 ResourceManager 申请资源来启动作业中具体的任务。如果是Session模式，则TaskManager已经启动了，就可以直接分配资源。如果是per-Job模式，ResourceManager 也需要首先向外部资源管理系统申请资源来启动 TaskExecutor，然后等待 TaskExecutor 注册相应资源后再继续选择空闲资源进程分配，JobManager 收到 TaskExecutor 注册上来的 Slot 后，就可以实际提交 Task 了。  </p><p>3.TaskExecutor 收到 JobManager 提交的 Task 之后，会启动一个新的线程来执行该 Task。Task 启动后就会开始进行预先指定的计算，并通过数据 Shuffle 模块互相交换数据。</p><h2 id="Flink-Standalone运行架构"><a href="#Flink-Standalone运行架构" class="headerlink" title="Flink Standalone运行架构"></a>Flink Standalone运行架构</h2><p>Flink Standalone运行架构如下图所示：</p><p><img src="//jiamaoxiang.top/2019/10/23/Flink运行架构剖析/standalone.png" alt><br>Standalone模式需要先启动Jobmanager和TaskManager进程，每一个作业都是自己的JobManager。<br>Client：任务提交，生成JobGraph  </p><p>JobManager：调度Job，协调Task，通信，申请资源  </p><p>TaskManager：具体任务执行，请求资源</p><h2 id="Flink-On-YARN运行架构"><a href="#Flink-On-YARN运行架构" class="headerlink" title="Flink On YARN运行架构"></a>Flink On YARN运行架构</h2><p>关于YARN的基本架构原理，详见另一篇我的博客<a href="https://blog.csdn.net/jmx_bigdata/article/details/84320188" target="_blank" rel="noopener">YARN架构原理</a></p><h3 id="Per-Job模式"><a href="#Per-Job模式" class="headerlink" title="Per-Job模式"></a>Per-Job模式</h3><p>Per-job 模式下整个 Flink 集群只执行单个作业，即每个作业会独享 Dispatcher 和 ResourceManager 组件。此外，Per-job 模式下 AppMaster 和 TaskExecutor 都是按需申请的。因此，Per-job 模式更适合运行执行时间较长的大作业，这些作业对稳定性要求较高，并且对申请资源的时间不敏感。<br>1.独享Dispatcher与ResourceManager  </p><p>2.按需申请资源(TaskExecutor)  </p><p>3.适合执行时间较长的大作业  </p><p><img src="//jiamaoxiang.top/2019/10/23/Flink运行架构剖析/perjob.png" alt></p><h3 id="Session模式"><a href="#Session模式" class="headerlink" title="Session模式"></a>Session模式</h3><p>在 Session 模式下，Flink 预先启动 AppMaster 以及一组 TaskExecutor，然后在整个集群的生命周期中会执行多个作业。可以看出，Session 模式更适合规模小，执行时间短的作业。<br>1.共享Dispatcher与ResourceManager  </p><p>2.共享资源  </p><p>3.适合小规模，执行时间较短的作业  </p><p><img src="//jiamaoxiang.top/2019/10/23/Flink运行架构剖析/session.png" alt></p><p>Reference:<br>[1]<a href="https://ververica.cn/developers/advanced-tutorial-1-analysis-of-the-core-mechanism-of-runtime/" target="_blank" rel="noopener">https://ververica.cn/developers/advanced-tutorial-1-analysis-of-the-core-mechanism-of-runtime/</a><br>[2]<a href="https://ververica.cn/developers/flink-training-course2/" target="_blank" rel="noopener">https://ververica.cn/developers/flink-training-course2/</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>经典Hive-SQL面试题</title>
      <link href="/2019/10/15/%E7%BB%8F%E5%85%B8Hive-SQL%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
      <url>/2019/10/15/%E7%BB%8F%E5%85%B8Hive-SQL%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>HQL练习</p><a id="more"></a><h2 id="第一题"><a href="#第一题" class="headerlink" title="第一题"></a>第一题</h2><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">我们有如下的用户访问数据</span><br><span class="line">userId  visitDate   visitCount</span><br><span class="line">u01 2017/1/21   5</span><br><span class="line">u02 2017/1/23   6</span><br><span class="line">u03 2017/1/22   8</span><br><span class="line">u04 2017/1/20   3</span><br><span class="line">u01 2017/1/23   6</span><br><span class="line">u01 2017/2/21   8</span><br><span class="line">U02 2017/1/23   6</span><br><span class="line">U01 2017/2/22   4</span><br><span class="line">要求使用SQL统计出每个用户的累积访问次数，如下表所示：</span><br><span class="line">用户id    月份  小计  累积</span><br><span class="line">u01 2017-01 11  11</span><br><span class="line">u01 2017-02 12  23</span><br><span class="line">u02 2017-01 12  12</span><br><span class="line">u03 2017-01 8   8</span><br><span class="line">u04 2017-01 3   3</span><br></pre></td></tr></table></figure><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test1 ( </span><br><span class="line">userId string, </span><br><span class="line">visitDate string,</span><br><span class="line">visitCount INT )</span><br><span class="line">ROW format delimited FIELDS TERMINATED BY <span class="string">"\t"</span>;</span><br><span class="line">INSERT INTO TABLE test_sql.test1</span><br><span class="line">VALUES</span><br><span class="line">( <span class="string">'u01'</span>, <span class="string">'2017/1/21'</span>, 5 ),</span><br><span class="line">( <span class="string">'u02'</span>, <span class="string">'2017/1/23'</span>, 6 ),</span><br><span class="line">( <span class="string">'u03'</span>, <span class="string">'2017/1/22'</span>, 8 ),</span><br><span class="line">( <span class="string">'u04'</span>, <span class="string">'2017/1/20'</span>, 3 ),</span><br><span class="line">( <span class="string">'u01'</span>, <span class="string">'2017/1/23'</span>, 6 ),</span><br><span class="line">( <span class="string">'u01'</span>, <span class="string">'2017/2/21'</span>, 8 ),</span><br><span class="line">( <span class="string">'u02'</span>, <span class="string">'2017/1/23'</span>, 6 ),</span><br><span class="line">( <span class="string">'u01'</span>, <span class="string">'2017/2/22'</span>, 4 );</span><br></pre></td></tr></table></figure><h4 id="查询SQL"><a href="#查询SQL" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT t2.userid,</span><br><span class="line">   t2.visitmonth,</span><br><span class="line">   subtotal_visit_cnt,</span><br><span class="line">   sum(subtotal_visit_cnt) over (partition BY userid</span><br><span class="line"> ORDER BY visitmonth) AS total_visit_cnt</span><br><span class="line">FROM</span><br><span class="line">  (SELECT userid,</span><br><span class="line">  visitmonth,</span><br><span class="line">  sum(visitcount) AS subtotal_visit_cnt</span><br><span class="line">   FROM</span><br><span class="line"> (SELECT userid,</span><br><span class="line"> date_format(regexp_replace(visitdate,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM'</span>) AS visitmonth,</span><br><span class="line"> visitcount</span><br><span class="line">  FROM test_sql.test1) t1</span><br><span class="line">   GROUP BY userid,</span><br><span class="line">visitmonth)t2</span><br><span class="line">ORDER BY t2.userid,</span><br><span class="line"> t2.visitmonth</span><br></pre></td></tr></table></figure><h2 id="第二题"><a href="#第二题" class="headerlink" title="第二题"></a>第二题</h2><h3 id="需求-1"><a href="#需求-1" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">有50W个京东店铺，每个顾客访客访问任何一个店铺的任何一个商品时都会产生一条访问日志，</span><br><span class="line">访问日志存储的表名为Visit，访客的用户id为user_id，被访问的店铺名称为shop，数据如下：</span><br><span class="line"></span><br><span class="line">u1a</span><br><span class="line">u2b</span><br><span class="line">u1b</span><br><span class="line">u1a</span><br><span class="line">u3c</span><br><span class="line">u4b</span><br><span class="line">u1a</span><br><span class="line">u2c</span><br><span class="line">u5b</span><br><span class="line">u4b</span><br><span class="line">u6c</span><br><span class="line">u2c</span><br><span class="line">u1b</span><br><span class="line">u2a</span><br><span class="line">u2a</span><br><span class="line">u3a</span><br><span class="line">u5a</span><br><span class="line">u5a</span><br><span class="line">u5a</span><br><span class="line">请统计：</span><br><span class="line">(1)每个店铺的UV（访客数）</span><br><span class="line">(2)每个店铺访问次数top3的访客信息。输出店铺名称、访客id、访问次数</span><br></pre></td></tr></table></figure><h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-1"><a href="#数据准备-1" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test2 ( </span><br><span class="line"> user_id string, </span><br><span class="line"> shop string )</span><br><span class="line">ROW format delimited FIELDS TERMINATED BY <span class="string">'\t'</span>;</span><br><span class="line">INSERT INTO TABLE test_sql.test2 VALUES</span><br><span class="line">( <span class="string">'u1'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u2'</span>, <span class="string">'b'</span> ),</span><br><span class="line">( <span class="string">'u1'</span>, <span class="string">'b'</span> ),</span><br><span class="line">( <span class="string">'u1'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u3'</span>, <span class="string">'c'</span> ),</span><br><span class="line">( <span class="string">'u4'</span>, <span class="string">'b'</span> ),</span><br><span class="line">( <span class="string">'u1'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u2'</span>, <span class="string">'c'</span> ),</span><br><span class="line">( <span class="string">'u5'</span>, <span class="string">'b'</span> ),</span><br><span class="line">( <span class="string">'u4'</span>, <span class="string">'b'</span> ),</span><br><span class="line">( <span class="string">'u6'</span>, <span class="string">'c'</span> ),</span><br><span class="line">( <span class="string">'u2'</span>, <span class="string">'c'</span> ),</span><br><span class="line">( <span class="string">'u1'</span>, <span class="string">'b'</span> ),</span><br><span class="line">( <span class="string">'u2'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u2'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u3'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u5'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u5'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u5'</span>, <span class="string">'a'</span> );</span><br></pre></td></tr></table></figure><h4 id="查询SQL实现"><a href="#查询SQL实现" class="headerlink" title="查询SQL实现"></a>查询SQL实现</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(1)方式1：</span><br><span class="line">SELECT shop,</span><br><span class="line">   count(DISTINCT user_id)</span><br><span class="line">FROM test_sql.test2</span><br><span class="line">GROUP BY shop</span><br><span class="line">方式2：</span><br><span class="line">SELECT t.shop,</span><br><span class="line">   count(*)</span><br><span class="line">FROM</span><br><span class="line">  (SELECT user_id,</span><br><span class="line">  shop</span><br><span class="line">   FROM test_sql.test2</span><br><span class="line">   GROUP BY user_id,</span><br><span class="line">shop) t</span><br><span class="line">GROUP BY t.shop</span><br><span class="line">(2)</span><br><span class="line">SELECT t2.shop,</span><br><span class="line">   t2.user_id,</span><br><span class="line">   t2.cnt</span><br><span class="line">FROM</span><br><span class="line">  (SELECT t1.*,</span><br><span class="line">  row_number() over(partition BY t1.shop</span><br><span class="line">ORDER BY t1.cnt DESC) rank</span><br><span class="line">   FROM</span><br><span class="line"> (SELECT user_id,</span><br><span class="line"> shop,</span><br><span class="line"> count(*) AS cnt</span><br><span class="line">  FROM test_sql.test2</span><br><span class="line">  GROUP BY user_id,</span><br><span class="line">   shop) t1)t2</span><br><span class="line">WHERE rank &lt;= 3</span><br></pre></td></tr></table></figure><h2 id="第三题"><a href="#第三题" class="headerlink" title="第三题"></a>第三题</h2><h3 id="需求-2"><a href="#需求-2" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">已知一个表STG.ORDER，有如下字段:Date，Order_id，User_id，amount。</span><br><span class="line">数据样例:2017-01-01,10029028,1000003251,33.57。</span><br><span class="line">请给出sql进行统计:</span><br><span class="line">(1)给出 2017年每个月的订单数、用户数、总成交金额。</span><br><span class="line">(2)给出2017年11月的新客数(指在11月才有第一笔订单)</span><br></pre></td></tr></table></figure><h3 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-2"><a href="#数据准备-2" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test3 ( </span><br><span class="line">dt string,</span><br><span class="line">order_id string, </span><br><span class="line">user_id string, </span><br><span class="line">amount DECIMAL ( 10, 2 ) )</span><br><span class="line">ROW format delimited FIELDS TERMINATED BY <span class="string">'\t'</span>;</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-01-01'</span>,<span class="string">'10029028'</span>,<span class="string">'1000003251'</span>,33.57);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-01-01'</span>,<span class="string">'10029029'</span>,<span class="string">'1000003251'</span>,33.57);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-01-01'</span>,<span class="string">'100290288'</span>,<span class="string">'1000003252'</span>,33.57);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-02-02'</span>,<span class="string">'10029088'</span>,<span class="string">'1000003251'</span>,33.57);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-02-02'</span>,<span class="string">'100290281'</span>,<span class="string">'1000003251'</span>,33.57);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-02-02'</span>,<span class="string">'100290282'</span>,<span class="string">'1000003253'</span>,33.57);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-11-02'</span>,<span class="string">'10290282'</span>,<span class="string">'100003253'</span>,234);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2018-11-02'</span>,<span class="string">'10290284'</span>,<span class="string">'100003243'</span>,234);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-1"><a href="#查询SQL-1" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(1)</span><br><span class="line">SELECT t1.mon,</span><br><span class="line">   count(t1.order_id) AS order_cnt,</span><br><span class="line">   count(DISTINCT t1.user_id) AS user_cnt,</span><br><span class="line">   sum(amount) AS total_amount</span><br><span class="line">FROM</span><br><span class="line">  (SELECT order_id,</span><br><span class="line">  user_id,</span><br><span class="line">  amount,</span><br><span class="line">  date_format(dt,<span class="string">'yyyy-MM'</span>) mon</span><br><span class="line">   FROM test_sql.test3</span><br><span class="line">   WHERE date_format(dt,<span class="string">'yyyy'</span>) = <span class="string">'2017'</span>) t1</span><br><span class="line">GROUP BY t1.mon</span><br><span class="line">(2)</span><br><span class="line">SELECT count(user_id)</span><br><span class="line">FROM test_sql.test3</span><br><span class="line">GROUP BY user_id</span><br><span class="line">HAVING date_format(min(dt),<span class="string">'yyyy-MM'</span>)=<span class="string">'2017-11'</span>;</span><br></pre></td></tr></table></figure><h2 id="第四题"><a href="#第四题" class="headerlink" title="第四题"></a>第四题</h2><h3 id="需求-3"><a href="#需求-3" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">有一个5000万的用户文件(user_id，name，age)，一个2亿记录的用户看电影的记录文件(user_id，url)，根据年龄段观看电影的次数进行排序？</span><br></pre></td></tr></table></figure><h3 id="实现-3"><a href="#实现-3" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-3"><a href="#数据准备-3" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test4user</span><br><span class="line">   (user_id string,</span><br><span class="line">name string,</span><br><span class="line">age int);</span><br><span class="line"></span><br><span class="line">CREATE TABLE test_sql.test4log</span><br><span class="line">(user_id string,</span><br><span class="line">url string);</span><br><span class="line"></span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'001'</span>,<span class="string">'u1'</span>,10);</span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'002'</span>,<span class="string">'u2'</span>,15);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'003'</span>,<span class="string">'u3'</span>,15);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'004'</span>,<span class="string">'u4'</span>,20);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'005'</span>,<span class="string">'u5'</span>,25);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'006'</span>,<span class="string">'u6'</span>,35);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'007'</span>,<span class="string">'u7'</span>,40);</span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'008'</span>,<span class="string">'u8'</span>,45);  </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'009'</span>,<span class="string">'u9'</span>,50);  </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'0010'</span>,<span class="string">'u10'</span>,65);  </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'001'</span>,<span class="string">'url1'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'002'</span>,<span class="string">'url1'</span>);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'003'</span>,<span class="string">'url2'</span>);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'004'</span>,<span class="string">'url3'</span>);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'005'</span>,<span class="string">'url3'</span>);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'006'</span>,<span class="string">'url1'</span>);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'007'</span>,<span class="string">'url5'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'008'</span>,<span class="string">'url7'</span>);  </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'009'</span>,<span class="string">'url5'</span>);  </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'0010'</span>,<span class="string">'url1'</span>);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-2"><a href="#查询SQL-2" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT </span><br><span class="line">t2.age_phase,</span><br><span class="line">sum(t1.cnt) as view_cnt</span><br><span class="line">FROM</span><br><span class="line"></span><br><span class="line">(SELECT user_id,</span><br><span class="line">  count(*) cnt</span><br><span class="line">FROM test_sql.test4log</span><br><span class="line">GROUP BY user_id) t1</span><br><span class="line">JOIN</span><br><span class="line">(SELECT user_id,</span><br><span class="line">  CASE WHEN age &lt;= 10 AND age &gt; 0 THEN <span class="string">'0-10'</span> </span><br><span class="line">  WHEN age &lt;= 20 AND age &gt; 10 THEN <span class="string">'10-20'</span></span><br><span class="line">  WHEN age &gt;20 AND age &lt;=30 THEN <span class="string">'20-30'</span></span><br><span class="line">  WHEN age &gt;30 AND age &lt;=40 THEN <span class="string">'30-40'</span></span><br><span class="line">  WHEN age &gt;40 AND age &lt;=50 THEN <span class="string">'40-50'</span></span><br><span class="line">  WHEN age &gt;50 AND age &lt;=60 THEN <span class="string">'50-60'</span></span><br><span class="line">  WHEN age &gt;60 AND age &lt;=70 THEN <span class="string">'60-70'</span></span><br><span class="line">  ELSE <span class="string">'70以上'</span> END as age_phase</span><br><span class="line">FROM test_sql.test4user) t2 ON t1.user_id = t2.user_id </span><br><span class="line">GROUP BY t2.age_phase</span><br></pre></td></tr></table></figure><h2 id="第五题"><a href="#第五题" class="headerlink" title="第五题"></a>第五题</h2><h3 id="需求-4"><a href="#需求-4" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">有日志如下，请写出代码求得所有用户和活跃用户的总数及平均年龄。（活跃用户指连续两天都有访问记录的用户）</span><br><span class="line">日期 用户 年龄</span><br><span class="line">2019-02-11,test_1,23</span><br><span class="line">2019-02-11,test_2,19</span><br><span class="line">2019-02-11,test_3,39</span><br><span class="line">2019-02-11,test_1,23</span><br><span class="line">2019-02-11,test_3,39</span><br><span class="line">2019-02-11,test_1,23</span><br><span class="line">2019-02-12,test_2,19</span><br><span class="line">2019-02-13,test_1,23</span><br><span class="line">2019-02-15,test_2,19</span><br><span class="line">2019-02-16,test_2,19</span><br></pre></td></tr></table></figure><h3 id="实现-4"><a href="#实现-4" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-4"><a href="#数据准备-4" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test5(</span><br><span class="line">dt string,</span><br><span class="line">user_id string,</span><br><span class="line">age int)</span><br><span class="line">ROW format delimited fields terminated BY <span class="string">','</span>;</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-11'</span>,<span class="string">'test_1'</span>,23);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-11'</span>,<span class="string">'test_2'</span>,19);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-11'</span>,<span class="string">'test_3'</span>,39);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-11'</span>,<span class="string">'test_1'</span>,23);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-11'</span>,<span class="string">'test_3'</span>,39);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-11'</span>,<span class="string">'test_1'</span>,23);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-12'</span>,<span class="string">'test_2'</span>,19);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-13'</span>,<span class="string">'test_1'</span>,23);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-15'</span>,<span class="string">'test_2'</span>,19);                                        </span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-16'</span>,<span class="string">'test_2'</span>,19);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-3"><a href="#查询SQL-3" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT sum(total_user_cnt) total_user_cnt,</span><br><span class="line">   sum(total_user_avg_age) total_user_avg_age,</span><br><span class="line">   sum(two_days_cnt) two_days_cnt,</span><br><span class="line">   sum(avg_age) avg_age</span><br><span class="line">FROM</span><br><span class="line">  (SELECT 0 total_user_cnt,</span><br><span class="line">  0 total_user_avg_age,</span><br><span class="line">  count(*) AS two_days_cnt,</span><br><span class="line">  cast(sum(age) / count(*) AS decimal(5,2)) AS avg_age</span><br><span class="line">   FROM</span><br><span class="line"> (SELECT user_id,</span><br><span class="line"> max(age) age</span><br><span class="line">  FROM</span><br><span class="line">(SELECT user_id,</span><br><span class="line">max(age) age</span><br><span class="line"> FROM</span><br><span class="line">   (SELECT user_id,</span><br><span class="line">   age,</span><br><span class="line">   date_sub(dt,rank) flag</span><br><span class="line">FROM</span><br><span class="line">  (SELECT dt,</span><br><span class="line">  user_id,</span><br><span class="line">  max(age) age,</span><br><span class="line">  row_number() over(PARTITION BY user_id</span><br><span class="line">ORDER BY dt) rank</span><br><span class="line">   FROM test_sql.test5</span><br><span class="line">   GROUP BY dt,</span><br><span class="line">user_id) t1) t2</span><br><span class="line"> GROUP BY user_id,</span><br><span class="line">  flag</span><br><span class="line"> HAVING count(*) &gt;=2) t3</span><br><span class="line">  GROUP BY user_id) t4</span><br><span class="line">   UNION ALL SELECT count(*) total_user_cnt,</span><br><span class="line">cast(sum(age) /count(*) AS decimal(5,2)) total_user_avg_age,</span><br><span class="line">0 two_days_cnt,</span><br><span class="line">0 avg_age</span><br><span class="line">   FROM</span><br><span class="line"> (SELECT user_id,</span><br><span class="line"> max(age) age</span><br><span class="line">  FROM test_sql.test5</span><br><span class="line">  GROUP BY user_id) t5) t6</span><br></pre></td></tr></table></figure><h2 id="第六题"><a href="#第六题" class="headerlink" title="第六题"></a>第六题</h2><h3 id="需求-5"><a href="#需求-5" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">请用sql写出所有用户中在今年10月份第一次购买商品的金额，</span><br><span class="line">表ordertable字段:</span><br><span class="line">(购买用户：userid，金额：money，购买时间：paymenttime(格式：2017-10-01)，订单id：orderid</span><br></pre></td></tr></table></figure><h3 id="实现-5"><a href="#实现-5" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-5"><a href="#数据准备-5" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test6 (</span><br><span class="line">userid string,</span><br><span class="line">money decimal(10,2),</span><br><span class="line">paymenttime string,</span><br><span class="line">orderid string);</span><br><span class="line"></span><br><span class="line">INSERT INTO TABLE test_sql.test6 VALUES(<span class="string">'001'</span>,100,<span class="string">'2017-10-01'</span>,<span class="string">'123'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test6 VALUES(<span class="string">'001'</span>,200,<span class="string">'2017-10-02'</span>,<span class="string">'124'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test6 VALUES(<span class="string">'002'</span>,500,<span class="string">'2017-10-01'</span>,<span class="string">'125'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test6 VALUES(<span class="string">'001'</span>,100,<span class="string">'2017-11-01'</span>,<span class="string">'126'</span>);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-4"><a href="#查询SQL-4" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT</span><br><span class="line">userid,</span><br><span class="line">paymenttime,</span><br><span class="line">money,</span><br><span class="line">orderid</span><br><span class="line">from</span><br><span class="line">(SELECT userid,</span><br><span class="line">   money,</span><br><span class="line">   paymenttime,</span><br><span class="line">   orderid,</span><br><span class="line">   row_number() over (PARTITION BY userid</span><br><span class="line">  ORDER BY paymenttime) rank</span><br><span class="line">FROM test_sql.test6</span><br><span class="line">WHERE date_format(paymenttime,<span class="string">'yyyy-MM'</span>) = <span class="string">'2017-10'</span>) t</span><br><span class="line">WHERE rank = 1</span><br></pre></td></tr></table></figure><h2 id="第七题"><a href="#第七题" class="headerlink" title="第七题"></a>第七题</h2><h3 id="需求-6"><a href="#需求-6" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">现有图书管理数据库的三个数据模型如下：</span><br><span class="line">图书（数据表名：BOOK）</span><br><span class="line">序号  字段名称    字段描述    字段类型</span><br><span class="line">1   BOOK_ID 总编号 文本</span><br><span class="line">2   SORT    分类号 文本</span><br><span class="line">3   BOOK_NAME   书名  文本</span><br><span class="line">4   WRITER  作者  文本</span><br><span class="line">5   OUTPUT  出版单位    文本</span><br><span class="line">6   PRICE   单价  数值（保留小数点后2位）</span><br><span class="line">读者（数据表名：READER）</span><br><span class="line">序号  字段名称    字段描述    字段类型</span><br><span class="line">1   READER_ID   借书证号    文本</span><br><span class="line">2   COMPANY 单位  文本</span><br><span class="line">3   NAME    姓名  文本</span><br><span class="line">4   SEX 性别  文本</span><br><span class="line">5   GRADE   职称  文本</span><br><span class="line">6   ADDR    地址  文本</span><br><span class="line">借阅记录（数据表名：BORROW LOG）</span><br><span class="line">序号  字段名称    字段描述    字段类型</span><br><span class="line">1   READER_ID   借书证号    文本</span><br><span class="line">2   BOOK_ID  总编号 文本</span><br><span class="line">3   BORROW_DATE  借书日期    日期</span><br><span class="line">（1）创建图书管理库的图书、读者和借阅三个基本表的表结构。请写出建表语句。</span><br><span class="line">（2）找出姓李的读者姓名（NAME）和所在单位（COMPANY）。</span><br><span class="line">（3）查找“高等教育出版社”的所有图书名称（BOOK_NAME）及单价（PRICE），结果按单价降序排序。</span><br><span class="line">（4）查找价格介于10元和20元之间的图书种类(SORT）出版单位（OUTPUT）和单价（PRICE），结果按出版单位（OUTPUT）和单价（PRICE）升序排序。</span><br><span class="line">（5）查找所有借了书的读者的姓名（NAME）及所在单位（COMPANY）。</span><br><span class="line">（6）求”科学出版社”图书的最高单价、最低单价、平均单价。</span><br><span class="line">（7）找出当前至少借阅了2本图书（大于等于2本）的读者姓名及其所在单位。</span><br><span class="line">（8）考虑到数据安全的需要，需定时将“借阅记录”中数据进行备份，请使用一条SQL语句，在备份用户bak下创建与“借阅记录”表结构完全一致的数据表BORROW_LOG_BAK.井且将“借阅记录”中现有数据全部复制到BORROW_L0G_ BAK中。</span><br><span class="line">（9）现在需要将原Oracle数据库中数据迁移至Hive仓库，请写出“图书”在Hive中的建表语句（Hive实现，提示：列分隔符|；数据表数据需要外部导入：分区分别以month＿part、day＿part 命名）</span><br><span class="line">（10）Hive中有表A，现在需要将表A的月分区　201505　中　user＿id为20000的user＿dinner字段更新为bonc8920，其他用户user＿dinner字段数据不变，请列出更新的方法步骤。（Hive实现，提示：Hlive中无update语法，请通过其他办法进行数据更新）</span><br></pre></td></tr></table></figure><h3 id="实现-6"><a href="#实现-6" class="headerlink" title="实现"></a>实现</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(1)</span><br><span class="line"></span><br><span class="line">-- 创建图书表book</span><br><span class="line"></span><br><span class="line">CREATE TABLE test_sql.book(book_id string,</span><br><span class="line">   `SORT` string,</span><br><span class="line">   book_name string,</span><br><span class="line">   writer string,</span><br><span class="line">   OUTPUT string,</span><br><span class="line">   price decimal(10,2));</span><br><span class="line">INSERT INTO TABLE test_sql.book VALUES (<span class="string">'001'</span>,<span class="string">'TP391'</span>,<span class="string">'信息处理'</span>,<span class="string">'author1'</span>,<span class="string">'机械工业出版社'</span>,<span class="string">'20'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.book VALUES (<span class="string">'002'</span>,<span class="string">'TP392'</span>,<span class="string">'数据库'</span>,<span class="string">'author12'</span>,<span class="string">'科学出版社'</span>,<span class="string">'15'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.book VALUES (<span class="string">'003'</span>,<span class="string">'TP393'</span>,<span class="string">'计算机网络'</span>,<span class="string">'author3'</span>,<span class="string">'机械工业出版社'</span>,<span class="string">'29'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.book VALUES (<span class="string">'004'</span>,<span class="string">'TP399'</span>,<span class="string">'微机原理'</span>,<span class="string">'author4'</span>,<span class="string">'科学出版社'</span>,<span class="string">'39'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.book VALUES (<span class="string">'005'</span>,<span class="string">'C931'</span>,<span class="string">'管理信息系统'</span>,<span class="string">'author5'</span>,<span class="string">'机械工业出版社'</span>,<span class="string">'40'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.book VALUES (<span class="string">'006'</span>,<span class="string">'C932'</span>,<span class="string">'运筹学'</span>,<span class="string">'author6'</span>,<span class="string">'科学出版社'</span>,<span class="string">'55'</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-- 创建读者表reader</span><br><span class="line"></span><br><span class="line">CREATE TABLE test_sql.reader (reader_id string,</span><br><span class="line">  company string,</span><br><span class="line">  name string,</span><br><span class="line">  sex string,</span><br><span class="line">  grade string,</span><br><span class="line">  addr string);</span><br><span class="line">INSERT INTO TABLE test_sql.reader VALUES (<span class="string">'0001'</span>,<span class="string">'阿里巴巴'</span>,<span class="string">'jack'</span>,<span class="string">'男'</span>,<span class="string">'vp'</span>,<span class="string">'addr1'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.reader VALUES (<span class="string">'0002'</span>,<span class="string">'百度'</span>,<span class="string">'robin'</span>,<span class="string">'男'</span>,<span class="string">'vp'</span>,<span class="string">'addr2'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.reader VALUES (<span class="string">'0003'</span>,<span class="string">'腾讯'</span>,<span class="string">'tony'</span>,<span class="string">'男'</span>,<span class="string">'vp'</span>,<span class="string">'addr3'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.reader VALUES (<span class="string">'0004'</span>,<span class="string">'京东'</span>,<span class="string">'jasper'</span>,<span class="string">'男'</span>,<span class="string">'cfo'</span>,<span class="string">'addr4'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.reader VALUES (<span class="string">'0005'</span>,<span class="string">'网易'</span>,<span class="string">'zhangsan'</span>,<span class="string">'女'</span>,<span class="string">'ceo'</span>,<span class="string">'addr5'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.reader VALUES (<span class="string">'0006'</span>,<span class="string">'搜狐'</span>,<span class="string">'lisi'</span>,<span class="string">'女'</span>,<span class="string">'ceo'</span>,<span class="string">'addr6'</span>);</span><br><span class="line"></span><br><span class="line">-- 创建借阅记录表borrow_log</span><br><span class="line"></span><br><span class="line">CREATE TABLE test_sql.borrow_log(reader_id string,</span><br><span class="line"> book_id string,</span><br><span class="line"> borrow_date string);</span><br><span class="line"> </span><br><span class="line">INSERT INTO TABLE test_sql.borrow_log VALUES (<span class="string">'0001'</span>,<span class="string">'002'</span>,<span class="string">'2019-10-14'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.borrow_log VALUES (<span class="string">'0002'</span>,<span class="string">'001'</span>,<span class="string">'2019-10-13'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.borrow_log VALUES (<span class="string">'0003'</span>,<span class="string">'005'</span>,<span class="string">'2019-09-14'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.borrow_log VALUES (<span class="string">'0004'</span>,<span class="string">'006'</span>,<span class="string">'2019-08-15'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.borrow_log VALUES (<span class="string">'0005'</span>,<span class="string">'003'</span>,<span class="string">'2019-10-10'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.borrow_log VALUES (<span class="string">'0006'</span>,<span class="string">'004'</span>,<span class="string">'2019-17-13'</span>);</span><br><span class="line"></span><br><span class="line">(2)</span><br><span class="line">SELECT name,</span><br><span class="line">   company</span><br><span class="line">FROM test_sql.reader</span><br><span class="line">WHERE name LIKE <span class="string">'李%'</span>;</span><br><span class="line">(3)</span><br><span class="line">SELECT book_name,</span><br><span class="line">   price</span><br><span class="line">FROM test_sql.book</span><br><span class="line">WHERE OUTPUT = <span class="string">"高等教育出版社"</span></span><br><span class="line">ORDER BY price DESC;</span><br><span class="line">(4)</span><br><span class="line">SELECT sort,</span><br><span class="line">   output,</span><br><span class="line">   price</span><br><span class="line">FROM test_sql.book</span><br><span class="line">WHERE price &gt;= 10 and price &lt;= 20</span><br><span class="line">ORDER BY output,price ;</span><br><span class="line">(5)</span><br><span class="line">SELECT b.name,</span><br><span class="line">   b.company</span><br><span class="line">FROM test_sql.borrow_log a</span><br><span class="line">JOIN test_sql.reader b ON a.reader_id = b.reader_id;</span><br><span class="line">(6)</span><br><span class="line">SELECT max(price),</span><br><span class="line">   min(price),</span><br><span class="line">   avg(price)</span><br><span class="line">FROM test_sql.book</span><br><span class="line">WHERE OUTPUT = <span class="string">'科学出版社'</span>;</span><br><span class="line">(7)</span><br><span class="line">SELECT b.name,</span><br><span class="line">   b.company</span><br><span class="line">FROM</span><br><span class="line">  (SELECT reader_id</span><br><span class="line">   FROM test_sql.borrow_log</span><br><span class="line">   GROUP BY reader_id</span><br><span class="line">   HAVING count(*) &gt;= 2) a</span><br><span class="line">JOIN test_sql.reader b ON a.reader_id = b.reader_id;</span><br><span class="line"></span><br><span class="line">(8)</span><br><span class="line">CREATE TABLE test_sql.borrow_log_bak AS</span><br><span class="line">SELECT *</span><br><span class="line">FROM test_sql.borrow_log;</span><br><span class="line">(9)</span><br><span class="line">CREATE TABLE book_hive ( </span><br><span class="line">book_id string,</span><br><span class="line">SORT string, </span><br><span class="line">book_name string,</span><br><span class="line">writer string, </span><br><span class="line">OUTPUT string, </span><br><span class="line">price DECIMAL ( 10, 2 ) )</span><br><span class="line">partitioned BY ( month_part string, day_part string )</span><br><span class="line">ROW format delimited FIELDS TERMINATED BY <span class="string">'\\|'</span> stored AS textfile;</span><br><span class="line">(10)</span><br><span class="line">方式1：配置hive支持事务操作，分桶表，orc存储格式</span><br><span class="line">方式2：第一步找到要更新的数据，将要更改的字段替换为新的值，第二步找到不需要更新的数据，第三步将上两步的数据插入一张新表中。</span><br></pre></td></tr></table></figure><h2 id="第八题"><a href="#第八题" class="headerlink" title="第八题"></a>第八题</h2><h3 id="需求-7"><a href="#需求-7" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">有一个线上服务器访问日志格式如下（用sql答题）</span><br><span class="line">时间                    接口                         ip地址</span><br><span class="line">2016-11-09 14:22:05/api/user/login110.23.5.33</span><br><span class="line">2016-11-09 14:23:10/api/user/detail57.3.2.16</span><br><span class="line">2016-11-09 15:59:40/api/user/login200.6.5.166</span><br><span class="line">… …</span><br><span class="line">求11月9号下午14点（14-15点），访问/api/user/login接口的top10的ip地址</span><br></pre></td></tr></table></figure><h3 id="实现-7"><a href="#实现-7" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-6"><a href="#数据准备-6" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test8(`date` string,</span><br><span class="line">interface string,</span><br><span class="line">ip string);</span><br><span class="line"></span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES (<span class="string">'2016-11-09 11:22:05'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'110.23.5.23'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES (<span class="string">'2016-11-09 11:23:10'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'57.3.2.16'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES (<span class="string">'2016-11-09 23:59:40'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'200.6.5.166'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 11:14:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'136.79.47.70'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 11:15:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'94.144.143.141'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 11:16:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'197.161.8.206'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 12:14:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'240.227.107.145'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 13:14:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'79.130.122.205'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:14:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'65.228.251.189'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:15:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'245.23.122.44'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:17:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'22.74.142.137'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:19:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'54.93.212.87'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:20:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'218.15.167.248'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:24:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'20.117.19.75'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 15:14:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'183.162.66.97'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 16:14:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'108.181.245.147'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:17:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'22.74.142.137'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:19:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'22.74.142.137'</span>);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-5"><a href="#查询SQL-5" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT ip,</span><br><span class="line">   count(*) AS cnt</span><br><span class="line">FROM test_sql.test8</span><br><span class="line">WHERE date_format(date,<span class="string">'yyyy-MM-dd HH'</span>) &gt;= <span class="string">'2016-11-09 14'</span></span><br><span class="line">  AND date_format(date,<span class="string">'yyyy-MM-dd HH'</span>) &lt; <span class="string">'2016-11-09 15'</span></span><br><span class="line">  AND interface=<span class="string">'/api/user/login'</span></span><br><span class="line">GROUP BY ip</span><br><span class="line">ORDER BY cnt desc</span><br><span class="line">LIMIT 10;</span><br></pre></td></tr></table></figure><h2 id="第九题"><a href="#第九题" class="headerlink" title="第九题"></a>第九题</h2><h3 id="需求-8"><a href="#需求-8" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">有一个充值日志表credit_log，字段如下：</span><br><span class="line"></span><br><span class="line">`dist_id` int  <span class="string">'区组id'</span>,</span><br><span class="line">`account` string  <span class="string">'账号'</span>,</span><br><span class="line">`money` int   <span class="string">'充值金额'</span>,</span><br><span class="line">`create_time` string  <span class="string">'订单时间'</span></span><br><span class="line"></span><br><span class="line">请写出SQL语句，查询充值日志表2019年01月02号每个区组下充值额最大的账号，要求结果：</span><br><span class="line">区组id，账号，金额，充值时间</span><br></pre></td></tr></table></figure><h3 id="实现-8"><a href="#实现-8" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-7"><a href="#数据准备-7" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test9(</span><br><span class="line">dist_id string COMMENT <span class="string">'区组id'</span>,</span><br><span class="line">account string COMMENT <span class="string">'账号'</span>,</span><br><span class="line">   `money` decimal(10,2) COMMENT <span class="string">'充值金额'</span>,</span><br><span class="line">create_time string COMMENT <span class="string">'订单时间'</span>);</span><br><span class="line"></span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'11'</span>,100006,<span class="string">'2019-01-02 13:00:01'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'22'</span>,110000,<span class="string">'2019-01-02 13:00:02'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'33'</span>,102000,<span class="string">'2019-01-02 13:00:03'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'44'</span>,100300,<span class="string">'2019-01-02 13:00:04'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'55'</span>,100040,<span class="string">'2019-01-02 13:00:05'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'66'</span>,100005,<span class="string">'2019-01-02 13:00:06'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'77'</span>,180000,<span class="string">'2019-01-03 13:00:07'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'88'</span>,106000,<span class="string">'2019-01-02 13:00:08'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'99'</span>,100400,<span class="string">'2019-01-02 13:00:09'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'12'</span>,100030,<span class="string">'2019-01-02 13:00:10'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'13'</span>,100003,<span class="string">'2019-01-02 13:00:20'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'14'</span>,100020,<span class="string">'2019-01-02 13:00:30'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'15'</span>,100500,<span class="string">'2019-01-02 13:00:40'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'16'</span>,106000,<span class="string">'2019-01-02 13:00:50'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'17'</span>,100800,<span class="string">'2019-01-02 13:00:59'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'2'</span>,<span class="string">'18'</span>,100800,<span class="string">'2019-01-02 13:00:11'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'2'</span>,<span class="string">'19'</span>,100030,<span class="string">'2019-01-02 13:00:12'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'2'</span>,<span class="string">'10'</span>,100000,<span class="string">'2019-01-02 13:00:13'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'2'</span>,<span class="string">'45'</span>,100010,<span class="string">'2019-01-02 13:00:14'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'2'</span>,<span class="string">'78'</span>,100070,<span class="string">'2019-01-02 13:00:15'</span>);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-6"><a href="#查询SQL-6" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">WITH TEMP AS</span><br><span class="line">  (SELECT dist_id,</span><br><span class="line">  account,</span><br><span class="line">  sum(`money`) sum_money</span><br><span class="line">   FROM test_sql.test9</span><br><span class="line">   WHERE date_format(create_time,<span class="string">'yyyy-MM-dd'</span>) = <span class="string">'2019-01-02'</span></span><br><span class="line">   GROUP BY dist_id,</span><br><span class="line">account)</span><br><span class="line">SELECT t1.dist_id,</span><br><span class="line">   t1.account,</span><br><span class="line">   t1.sum_money</span><br><span class="line">FROM</span><br><span class="line">  (SELECT temp.dist_id,</span><br><span class="line">  temp.account,</span><br><span class="line">  temp.sum_money,</span><br><span class="line">  rank() over(partition BY temp.dist_id</span><br><span class="line">  ORDER BY temp.sum_money DESC) ranks</span><br><span class="line">   FROM TEMP) t1</span><br><span class="line">WHERE ranks = 1</span><br></pre></td></tr></table></figure><h2 id="第十题"><a href="#第十题" class="headerlink" title="第十题"></a>第十题</h2><h3 id="需求-9"><a href="#需求-9" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">有一个账号表如下，请写出SQL语句，查询各自区组的money排名前十的账号（分组取前10）</span><br><span class="line">dist_id string  <span class="string">'区组id'</span>,</span><br><span class="line">account string  <span class="string">'账号'</span>,</span><br><span class="line">gold     int    <span class="string">'金币'</span></span><br></pre></td></tr></table></figure><h3 id="实现-9"><a href="#实现-9" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-8"><a href="#数据准备-8" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test10(</span><br><span class="line">`dist_id` string COMMENT <span class="string">'区组id'</span>,</span><br><span class="line">`account` string COMMENT <span class="string">'账号'</span>,</span><br><span class="line">`gold` int COMMENT <span class="string">'金币'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'77'</span>,18);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'88'</span>,106);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'99'</span>,10);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'12'</span>,13);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'13'</span>,14);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'14'</span>,25);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'15'</span>,36);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'16'</span>,12);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'17'</span>,158);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'2'</span>,<span class="string">'18'</span>,12);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'2'</span>,<span class="string">'19'</span>,44);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'2'</span>,<span class="string">'10'</span>,66);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'2'</span>,<span class="string">'45'</span>,80);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'2'</span>,<span class="string">'78'</span>,98);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-7"><a href="#查询SQL-7" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT dist_id,</span><br><span class="line">   account,</span><br><span class="line">   gold</span><br><span class="line">FROM</span><br><span class="line">(SELECT dist_id,</span><br><span class="line">  account,</span><br><span class="line">  gold,</span><br><span class="line">  row_number () over (PARTITION BY dist_id</span><br><span class="line">  ORDER BY gold DESC) rank</span><br><span class="line">FROM test_sql.test10) t</span><br><span class="line">WHERE rank &lt;= 10</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的Window Function介绍</title>
      <link href="/2019/09/26/Flink%E7%9A%84Window-Function%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/09/26/Flink%E7%9A%84Window-Function%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Impala使用的端口</title>
      <link href="/2019/08/29/Impala%E4%BD%BF%E7%94%A8%E7%9A%84%E7%AB%AF%E5%8F%A3/"/>
      <url>/2019/08/29/Impala%E4%BD%BF%E7%94%A8%E7%9A%84%E7%AB%AF%E5%8F%A3/</url>
      
        <content type="html"><![CDATA[<p>本文主要介绍了Impala所使用的端口号，在部署Impala的时候，确保下面列出的端口是开启的。</p><a id="more"></a><table><thead><tr><th align="center">组件</th><th>服务</th><th>端口</th><th align="center"><span style="white-space:nowrap;">访问需求&emsp;&emsp;</span></th><th>备注</th></tr></thead><tbody><tr><td align="center">Impala Daemon</td><td>Impala Daemon Frontend Port</td><td>21000</td><td align="center">外部</td><td>被 impala-shell, Beeswax, Cloudera ODBC 1.2 驱动 用于传递命令和接收结果</td></tr><tr><td align="center">Impala Daemon</td><td>Impala Daemon Frontend Port</td><td>21050</td><td align="center">外部</td><td>被使用 JDBC 或 Cloudera ODBC 2.0 及以上驱动的诸如 BI 工具之类的应用用来传递命令和接收结果</td></tr><tr><td align="center">Impala Daemon</td><td>Impala Daemon Backend Port</td><td>22000</td><td align="center">内部</td><td>仅内部使用。Impala</td></tr><tr><td align="center">Impala Daemon</td><td>StateStoreSubscriber Service Port</td><td>23000</td><td align="center">内部</td><td>仅内部使用。Impala 守护进程监听该端口接收来源于 state store 的更新</td></tr><tr><td align="center">Catalog Daemon</td><td>StateStoreSubscriber Service Port</td><td>23020</td><td align="center">内部</td><td>仅内部使用，catalog daemon监听该端口接收来源于 state store 的更新</td></tr><tr><td align="center">Impala Daemon</td><td>Impala Daemon HTTP Server Port</td><td>25000</td><td align="center">外部</td><td>Impala Daemon的Web端口，用于管理员监控和线上故障排查</td></tr><tr><td align="center">Impala StateStore Daemon</td><td>StateStore HTTP Server Port</td><td>25010</td><td align="center">外部</td><td>StateStore的Web端口，用于管理员监控和线上故障排查</td></tr><tr><td align="center">Impala Catalog Daemon</td><td>Catalog HTTP Server Port</td><td>25020</td><td align="center">外部</td><td>Catalog的Web端口，用于管理员监控和线上故障排查，从Impala1.2开始加入</td></tr><tr><td align="center">Impala StateStore Daemon</td><td>StateStore Service Port</td><td>24000</td><td align="center">内部</td><td>仅内部使用，statestore daemon监听的端口，用于registration/unregistration请求</td></tr><tr><td align="center">Impala Catalog Daemon</td><td>Catalog Service Port</td><td>26000</td><td align="center">内部</td><td>仅内部使用，catalog服务使用此端口与Impala Daemon进行通信，从Impala1.2开始加入</td></tr><tr><td align="center">Impala Daemon</td><td>KRPC Port</td><td>27000</td><td align="center">内部</td><td>仅内部使用，Impala daemon使用此端口进行基于krpc的相互通信。</td></tr></tbody></table><hr><p>Refrence:<a href="https://www.cloudera.com/documentation/enterprise/6/latest/topics/impala_ports.html#ports" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/6/latest/topics/impala_ports.html#ports</a></p>]]></content>
      
      
      <categories>
          
          <category> Impala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Impala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban安装部署</title>
      <link href="/2019/08/28/Azkaban%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
      <url>/2019/08/28/Azkaban%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<p>Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。</p><a id="more"></a><h1 id="安装前准备"><a href="#安装前准备" class="headerlink" title="安装前准备"></a>安装前准备</h1><p>(1)将Azkaban Web服务器、Azkaban执行服务器要安装机器的/opt/software目录下：  </p><ul><li>azkaban-web-server-2.5.0.tar.gz  </li><li>azkaban-executor-server-2.5.0.tar.gz  </li><li>azkaban-sql-script-2.5.0.tar.gz  </li><li>mysql-libs.zip  </li></ul><p>(2)目前azkaban只支持 mysql作为元数据库，需安装mysql，本文档中默认已安装好mysql服务器</p><h1 id="安装Azkaban"><a href="#安装Azkaban" class="headerlink" title="安装Azkaban"></a>安装Azkaban</h1><p>(1)在/opt/module/目录下创建azkaban目录<br>(2)解压azkaban-web-server-2.5.0.tar.gz、azkaban-executor-server-2.5.0.tar.gz、azkaban-sql-script-2.5.0.tar.gz到/opt/module/azkaban目录下<br>解压完成后的文件夹如下图所示：<br><img src="//jiamaoxiang.top/2019/08/28/Azkaban安装部署/1.png" alt><br>(3)初始化Azkaban的元数据库<br>登录mysql，创建azkaban的数据库，并执行脚本create-all-sql-2.5.0.sql，如下所示：  </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; create database azkaban;  </span><br><span class="line">Query OK, 1 row affected (0.00 sec)  </span><br><span class="line">mysql&gt; use azkaban;  </span><br><span class="line">Database changed  </span><br><span class="line">mysql&gt; <span class="built_in">source</span> /opt/module/azkaban/azkaban-2.5.0/create-all-sql-2.5.0.sql</span><br></pre></td></tr></table></figure><h2 id="创建SSL配置"><a href="#创建SSL配置" class="headerlink" title="创建SSL配置"></a>创建SSL配置</h2><p>(1)生成 keystore的密码及相应信息  </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban]$ keytool -keystore keystore -<span class="built_in">alias</span> jetty -genkey -keyalg RSA  </span><br><span class="line">输入keystore密码：   </span><br><span class="line">再次输入新密码:    </span><br><span class="line">您的名字与姓氏是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您的组织单位名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您的组织名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您所在的城市或区域名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您所在的州或省份名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">该单位的两字母国家代码是什么    </span><br><span class="line">[Unknown]：  CN    </span><br><span class="line">CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN 正确吗？  </span><br><span class="line">[否]：  y    </span><br><span class="line"> </span><br><span class="line">输入&lt;jetty&gt;的主密码    </span><br><span class="line">（如果和 keystore 密码相同，按回车）  ：</span><br></pre></td></tr></table></figure><p>(2)将keystore 考贝到 azkaban web服务器根目录中</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban]<span class="comment"># mv keystore  azkaban-web-2.5.0/</span></span><br></pre></td></tr></table></figure><h2 id="Web服务器配置"><a href="#Web服务器配置" class="headerlink" title="Web服务器配置"></a>Web服务器配置</h2><p>(1)进入azkaban web服务器安装目录 conf目录，修改azkaban.properties文件<br>(2)按照如下配置修改azkaban.properties文件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Azkaban Personalization Settings      </span></span><br><span class="line"><span class="comment">#服务器UI名称,用于服务器上方显示的名字      </span></span><br><span class="line">azkaban.name=Test      </span><br><span class="line"><span class="comment">#描述                            </span></span><br><span class="line">azkaban.label=My Local Azkaban     </span><br><span class="line"><span class="comment">#UI颜色                         </span></span><br><span class="line">azkaban.color=<span class="comment">#FF3601                                         </span></span><br><span class="line">azkaban.default.servlet.path=/index  </span><br><span class="line"><span class="comment">#根web目录,配置绝对路径，即web的目录  </span></span><br><span class="line">web.resource.dir=/opt/module/azkaban/azkaban-web-2.5.0/web   </span><br><span class="line"><span class="comment">#默认时区,已改为亚洲/上海 默认为美国                                            </span></span><br><span class="line">default.timezone.id=Asia/Shanghai                           </span><br><span class="line"><span class="comment">#用户权限管理默认类  </span></span><br><span class="line">user.manager.class=azkaban.user.XmlUserManager    </span><br><span class="line"><span class="comment">#用户配置,配置绝对路径，即azkaban-users.xml的路径     </span></span><br><span class="line">user.manager.xml.file=/opt/module/azkaban/azkaban-web-2.5.0/conf/azkaban-users.xml             </span><br><span class="line"><span class="comment">#Loader for projects .global配置文件所在位置,即global.properties绝对路径    </span></span><br><span class="line">executor.global.properties=/opt/module/azkaban/azkaban-executor-2.5.0/conf/global.properties    </span><br><span class="line">azkaban.project.dir=projects                                                 </span><br><span class="line"><span class="comment">#数据库类型  </span></span><br><span class="line">database.type=mysql                                                            </span><br><span class="line">mysql.port=3306                                                                  </span><br><span class="line">mysql.host=cdh01                                                    </span><br><span class="line">mysql.database=azkaban                                                      </span><br><span class="line">mysql.user=root  </span><br><span class="line"><span class="comment">#数据库密码                                                               </span></span><br><span class="line">mysql.password=123qwe                                                     </span><br><span class="line">mysql.numconnections=100                                                </span><br><span class="line"><span class="comment"># Velocity dev mode   </span></span><br><span class="line">velocity.dev.mode=<span class="literal">false</span>  </span><br><span class="line"><span class="comment"># Jetty服务器属性.  </span></span><br><span class="line"><span class="comment">#最大线程数     </span></span><br><span class="line">jetty.maxThreads=25   </span><br><span class="line"><span class="comment">#Jetty SSL端口                                                                 </span></span><br><span class="line">jetty.ssl.port=8443  </span><br><span class="line"><span class="comment">#Jetty端口                                                                      </span></span><br><span class="line">jetty.port=8081    </span><br><span class="line"><span class="comment">#SSL文件名,即keystore绝对路径                                                                              </span></span><br><span class="line">jetty.keystore=/opt/module/azkaban/azkaban-web-2.5.0/keystore    </span><br><span class="line"><span class="comment">#SSL文件密码,本配置与keystore密码相同                                                           </span></span><br><span class="line">jetty.password=123qwe  </span><br><span class="line"><span class="comment">#Jetty主密码 与 keystore文件相同                                                          </span></span><br><span class="line">jetty.keypassword=123qwe  </span><br><span class="line"><span class="comment">#SSL文件名,即keystore绝对路径                                                            </span></span><br><span class="line">jetty.truststore=/opt/module/azkaban/azkaban-web-2.5.0/keystore    </span><br><span class="line"><span class="comment"># SSL文件密码                                                             </span></span><br><span class="line">jetty.trustpassword=123qwe                                                    </span><br><span class="line"><span class="comment"># 执行服务器属性, 执行服务器端口  </span></span><br><span class="line">executor.port=12321                                                                </span><br><span class="line"><span class="comment"># 邮件设置,发送邮箱    </span></span><br><span class="line">mail.sender=xxxxxxxx@163.com    </span><br><span class="line"><span class="comment">#发送邮箱smtp地址                                           </span></span><br><span class="line">mail.host=smtp.163.com     </span><br><span class="line"><span class="comment">#发送邮件时显示的名称                                                            </span></span><br><span class="line">mail.user=xxxxxxxx  </span><br><span class="line"><span class="comment">#邮箱密码                                            </span></span><br><span class="line">mail.password=**********   </span><br><span class="line"><span class="comment">#任务失败时发送邮件的地址                                                        </span></span><br><span class="line">job.failure.email=xxxxxxxx@163.com   </span><br><span class="line"><span class="comment">#任务成功时发送邮件的地址                                 </span></span><br><span class="line">job.success.email=xxxxxxxx@163.com                            </span><br><span class="line">lockdown.create.projects=<span class="literal">false</span>    </span><br><span class="line"><span class="comment">#缓存目录                                            </span></span><br><span class="line">cache.directory=cache</span><br></pre></td></tr></table></figure><p>(3)web服务器用户配置<br>在azkaban web服务器安装目录 conf目录，按照如下配置修改azkaban-users.xml 文件，增加管理员用户。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;azkaban-users&gt;  </span><br><span class="line">       &lt;user username=<span class="string">"azkaban"</span> password=<span class="string">"azkaban"</span> roles=<span class="string">"admin"</span> groups=<span class="string">"azkaban"</span> /&gt;    </span><br><span class="line">       &lt;user username=<span class="string">"metrics"</span> password=<span class="string">"metrics"</span> roles=<span class="string">"metrics"</span>/&gt;  </span><br><span class="line">       &lt;user username=<span class="string">"admin"</span> password=<span class="string">"admin"</span> roles=<span class="string">"admin,metrics"</span> /&gt;  </span><br><span class="line">       &lt;role name=<span class="string">"admin"</span> permissions=<span class="string">"ADMIN"</span> /&gt;  </span><br><span class="line">       &lt;role name=<span class="string">"metrics"</span> permissions=<span class="string">"METRICS"</span>/&gt;    </span><br><span class="line">&lt;/azkaban-users&gt;</span><br></pre></td></tr></table></figure><h2 id="executor服务器配置"><a href="#executor服务器配置" class="headerlink" title="executor服务器配置"></a>executor服务器配置</h2><p>(1)进入executor安装目录，修改azkaban.properties</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Azkaban  </span></span><br><span class="line">default.timezone.id=Asia/Shanghai  </span><br><span class="line"><span class="comment"># Azkaban JobTypes Plugins  </span></span><br><span class="line">azkaban.jobtype.plugin.dir=./../plugins/jobtypes  </span><br><span class="line"><span class="comment">#Loader for projects  </span></span><br><span class="line">executor.global.properties=/opt/module/azkaban/azkaban-executor-2.5.0/conf/global.properties  </span><br><span class="line">azkaban.project.dir=projects  </span><br><span class="line">database.type=mysql  </span><br><span class="line">mysql.port=3306  </span><br><span class="line">mysql.host=cdh01  </span><br><span class="line">mysql.database=azkaban  </span><br><span class="line">mysql.user=root  </span><br><span class="line">mysql.password=123qwe    </span><br><span class="line">mysql.numconnections=100    </span><br><span class="line"><span class="comment"># Azkaban Executor settings  </span></span><br><span class="line">executor.maxThreads=50  </span><br><span class="line">executor.port=12321  </span><br><span class="line">executor.flow.threads=30</span><br></pre></td></tr></table></figure><h1 id="启动web服务器"><a href="#启动web服务器" class="headerlink" title="启动web服务器"></a>启动web服务器</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban-web-2.5.0]<span class="comment"># bin/azkaban-web-start.sh  &amp;</span></span><br></pre></td></tr></table></figure><h1 id="启动executor服务器"><a href="#启动executor服务器" class="headerlink" title="启动executor服务器"></a>启动executor服务器</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban-executor-2.5.0]<span class="comment"># bin/azkaban-executor-start.sh  &amp;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的数据类型</title>
      <link href="/2019/08/27/Flink%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
      <url>/2019/08/27/Flink%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>Flink使用type information来代表数据类型，Flink还具有一个类型提取系统，该系统分析函数的输入和返回类型，以自动获取类型信息(type information)，从而获得序列化程序和反序列化程序。但是，在某些情况下，例如lambda函数或泛型类型，需要显式地提供类型信息((type information)),从而提高其性能。本文主要讨论包括：(1)Flink支持的数据类型,(2)如何为数据类型创建type information，（3）如果无法自动推断函数的返回类型，如何使用提示(hints)来帮助Flink的类型系统识别类型信息。</p><a id="more"></a><h2 id="支持的数据类型"><a href="#支持的数据类型" class="headerlink" title="支持的数据类型"></a>支持的数据类型</h2><p>Flink支持Java和Scala中所有常见的数据类型，使用比较广泛的类型主要包括以下五种：</p><ul><li>原始类型  </li><li>Java和Scala的tuple类型  </li><li>Scala样例类  </li><li>POJO类型  </li><li>一些特殊的类型  </li></ul><p><strong>NOTE：</strong>不能被处理的类型将会被视为普通的数据类型，通过Kyro序列化框架进行序列化。</p><h3 id="原始类型"><a href="#原始类型" class="headerlink" title="原始类型"></a>原始类型</h3><p>Flink支持所有Java和Scala的原始类型，比如Int(Java中的Integer)，String、Double等。下面的例子是处理一个Long类型的数据流，处理每个元素+1  </p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> numbers: <span class="type">DataStream</span>[<span class="type">Long</span>] = env.fromElements(<span class="number">1</span>L, <span class="number">2</span>L,<span class="number">3</span>L, <span class="number">4</span>L)  </span><br><span class="line">numbers.map( n =&gt; n + <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="Java和Scala的tuple类型"><a href="#Java和Scala的tuple类型" class="headerlink" title="Java和Scala的tuple类型"></a>Java和Scala的tuple类型</h3><p>基于Scala的DataStream API使用的Scala的tuple。下面的例子是过滤一个具有两个字段的tuple类型的数据流.  </p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// DataStream of Tuple2[String, Integer] for Person(name,age)  </span></span><br><span class="line"><span class="keyword">val</span> persons: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Integer</span>)] = env.fromElements((<span class="string">"Adam"</span>, <span class="number">17</span>),(<span class="string">"Sarah"</span>, <span class="number">23</span>))  </span><br><span class="line"><span class="comment">// filter for persons of age &gt; 18  </span></span><br><span class="line">persons.filter(p =&gt; p._2 &gt; <span class="number">18</span>)</span><br></pre></td></tr></table></figure><p>Flink提供了有效的Java tuple实现，Flink的Java tuple最多包括25个字段，分别为tuple1，tuple2，直到tuple25，tuple类型是强类型的。使用Java DataStream API重写上面的例子:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// DataStream of Tuple2&lt;String, Integer&gt; for Person(name,age)  </span></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; persons =env.fromElements(Tuple2.of(<span class="string">"Adam"</span>, <span class="number">17</span>),Tuple2.of(<span class="string">"Sarah"</span>,<span class="number">23</span>));  </span><br><span class="line"><span class="comment">// filter for persons of age &gt; 18  </span></span><br><span class="line">persons.filter(p -&gt; p.f1 &gt; <span class="number">18</span>);</span><br></pre></td></tr></table></figure><p>Tuple字段可以通过使用f0，f1，f2的形式访问，也可以通过getField(int pos)方法访问，参数的索引起始值为0，比如:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Tuple2&lt;String, Integer&gt; personTuple = Tuple2.of(<span class="string">"Alex"</span>,<span class="string">"42"</span>);  </span><br><span class="line">Integer age = personTuple.getField(<span class="number">1</span>); <span class="comment">// age = 42</span></span><br></pre></td></tr></table></figure><p>与Scala相比，Flink的Java tuple是可变的，所以tuple的元素值是可以被重新复制的。Function可以重用Java tuple,从而减小垃圾回收的压力。下面的例子展示了如何更新一个tuple字段值</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">personTuple.f1 = <span class="number">42</span>; <span class="comment">// set the 2nd field to 42     </span></span><br><span class="line">personTuple.setField(<span class="number">43</span>, <span class="number">1</span>); <span class="comment">// set the 2nd field to 43</span></span><br></pre></td></tr></table></figure><h3 id="Scala的样例类"><a href="#Scala的样例类" class="headerlink" title="Scala的样例类"></a>Scala的样例类</h3><p>Flink支持Scala的样例类，可以通过字段名称来访问样例类的字段，下面的例子定义了一个<code>Person</code>样例类，该样例类有两个字段：<code>name</code>和<code>age</code>,按<code>age</code>过滤DataStream，如下所示</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)  </span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">persons</span></span>: <span class="type">DataStream</span>[<span class="type">Person</span>] = env.fromElements(<span class="type">Person</span>(<span class="string">"Adam"</span>, <span class="number">17</span>),<span class="type">Person</span>(<span class="string">"Sarah"</span>, <span class="number">23</span>))  </span><br><span class="line"><span class="comment">// filter for persons with age &gt; 18  </span></span><br><span class="line">persons.filter(p =&gt; p.age &gt; <span class="number">18</span>)</span><br></pre></td></tr></table></figure><h3 id="POJO"><a href="#POJO" class="headerlink" title="POJO"></a>POJO</h3><p>Flink接受的POJO类型需满足以下条件：</p><ul><li>public 类  </li><li>无参的共有构造方法  </li><li>所有字段都是public的，可以通过getter和setter方法访问  </li><li>所有字段类型必须是Flink能够支持的<br>下面的例子定义一个<code>Person</code>POJO</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;  </span><br><span class="line"><span class="comment">// both fields are public  </span></span><br><span class="line"><span class="keyword">public</span> String name;  </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">int</span> age;  </span><br><span class="line"><span class="comment">// default constructor is present  </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">()</span> </span>&#123;&#125;  </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(String name, <span class="keyword">int</span> age)</span> </span>&#123;  </span><br><span class="line"><span class="keyword">this</span>.name = name;  </span><br><span class="line"><span class="keyword">this</span>.age = age;  </span><br><span class="line">&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">DataStream&lt;Person&gt; persons = env.fromElements(   </span><br><span class="line"><span class="keyword">new</span> Person(<span class="string">"Alex"</span>, <span class="number">42</span>),  </span><br><span class="line"><span class="keyword">new</span> Person(<span class="string">"Wendy"</span>, <span class="number">23</span>));</span><br></pre></td></tr></table></figure><h3 id="一些特殊的类型"><a href="#一些特殊的类型" class="headerlink" title="一些特殊的类型"></a>一些特殊的类型</h3><p>Flink支持一些有特殊作用的数据类型，比如Array，Java中的ArrayList、HashMap和Enum等，也支持Hadoop的Writable类型。  </p><h2 id="为数据类型创建类型信息-type-information"><a href="#为数据类型创建类型信息-type-information" class="headerlink" title="为数据类型创建类型信息(type information)"></a>为数据类型创建类型信息(type information)</h2><h2 id="显示地指定类型信息-type-information"><a href="#显示地指定类型信息-type-information" class="headerlink" title="显示地指定类型信息(type information)"></a>显示地指定类型信息(type information)</h2>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于SparkStreaming的日志分析项目</title>
      <link href="/2019/08/26/%E5%9F%BA%E4%BA%8ESparkStreaming%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E9%A1%B9%E7%9B%AE/"/>
      <url>/2019/08/26/%E5%9F%BA%E4%BA%8ESparkStreaming%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E9%A1%B9%E7%9B%AE/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp;基于SparkStreaming实现实时的日志分析，首先基于discuz搭建一个论坛平台，然后将该论坛的日志写入到指定文件，最后通过SparkStreaming实时对日志进行分析。</p><a id="more"></a><h1 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h1><ul><li>统计指定时间段的热门文章</li></ul><ul><li>统计指定时间段内的最受欢迎的用户（以 ip 为单位）</li></ul><ul><li>统计指定时间段内的不同模块的访问量  </li></ul><h1 id="项目架构"><a href="#项目架构" class="headerlink" title="项目架构"></a>项目架构</h1><p><img src="//jiamaoxiang.top/2019/08/26/基于SparkStreaming的日志分析项目/%E9%A1%B9%E7%9B%AE%E6%9E%B6%E6%9E%84.png" alt></p><h1 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h1><p>resources<br>&emsp;&emsp;&emsp;&emsp;access_log.txt:日志样例<br>&emsp;&emsp;&emsp;&emsp;log_sta.conf ：配置文件<br>scala.com.jmx.analysis<br>&emsp;&emsp;&emsp;&emsp;AccessLogParser.scala :日志解析<br>&emsp;&emsp;&emsp;&emsp;logAnalysis：日志分析<br>scala.com.jmx.util<br>&emsp;&emsp;&emsp;&emsp;Utility.scala:工具类<br>scala<br>&emsp;&emsp;&emsp;&emsp;Run：驱动程序(main)<br>具体代码详见<a href="https://github.com/jiamx/log_analysis" target="_blank" rel="noopener">github</a></p><h1 id="搭建discuz论坛"><a href="#搭建discuz论坛" class="headerlink" title="搭建discuz论坛"></a>搭建discuz论坛</h1><h2 id="安装XAMPP"><a href="#安装XAMPP" class="headerlink" title="安装XAMPP"></a>安装XAMPP</h2><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><p><code>wget https://www.apachefriends.org/xampp-files/5.6.33/xampp-linux-x64-5.6.33-0-installer.run</code></p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><code># 赋予文件执行权限</code><br><code>chmod u+x xampp-linux-x64-5.6.33-0-installer.run</code><br><code># 运行安装文件</code><br>`./xampp-linux-x64-5.6.33-0-installer.run``</p><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><p>将以下内容加入到 ~/.bash_profile<br><code>export XAMPP=/opt/lampp/</code><br><code>export PATH=$PATH:$XAMPP:$XAMPP/bin</code>   </p><h3 id="刷新环境变量"><a href="#刷新环境变量" class="headerlink" title="刷新环境变量"></a>刷新环境变量</h3><p><code>source ~/.bash_profile</code></p><h3 id="启动XAMPP"><a href="#启动XAMPP" class="headerlink" title="启动XAMPP"></a>启动XAMPP</h3><p><code>xampp restart</code></p><h2 id="root用户密码和权限修改"><a href="#root用户密码和权限修改" class="headerlink" title="root用户密码和权限修改"></a>root用户密码和权限修改</h2><p><code>#修改root用户密码为123</code><br><code>update mysql.user set password=PASSWORD(&#39;123&#39;) where user=&#39;root&#39;;</code><br><code>flush privileges;</code><br><code>#赋予root用户远程登录权限</code><br><code>grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;123&#39; with grant option;</code><br><code>flush privileges;</code>  </p><h2 id="安装Discuz"><a href="#安装Discuz" class="headerlink" title="安装Discuz"></a>安装Discuz</h2><h3 id="下载discuz"><a href="#下载discuz" class="headerlink" title="下载discuz"></a>下载discuz</h3><p><code>wget http://download.comsenz.com/DiscuzX/3.2/Discuz_X3.2_SC_UTF8.zip</code>  </p><h3 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h3><p><code>#删除原有的web应用</code><br><code>rm -rf /opt/lampp/htdocs/*</code><br><code>unzip Discuz_X3.2_SC_UTF8.zip –d /opt/lampp/htdocs/</code><br><code>cd /opt/lampp/htdocs/</code><br><code>mv upload/*</code><br><code>#修改目录权限</code><br><code>chmod 777 -R /opt/lampp/htdocs/config/</code><br><code>chmod 777 -R /opt/lampp/htdocs/data/</code><br><code>chmod 777 -R /opt/lampp/htdocs/uc_client/</code><br><code>chmod 777 -R /opt/lampp/htdocs/uc_server/</code>  </p><h2 id="Discuz基本操作"><a href="#Discuz基本操作" class="headerlink" title="Discuz基本操作"></a>Discuz基本操作</h2><h3 id="自定义版块"><a href="#自定义版块" class="headerlink" title="自定义版块"></a>自定义版块</h3><ul><li>进入discuz后台：<a href="http://slave1/admin.php" target="_blank" rel="noopener">http://slave1/admin.php</a>  </li><li>点击顶部的“论坛”菜单  </li><li>按照页面提示创建所需版本，可以创建父子版块  </li></ul><h3 id="查看访问日志"><a href="#查看访问日志" class="headerlink" title="查看访问日志"></a>查看访问日志</h3><p>日志默认地址<br><code>/opt/lampp/logs/access_log</code><br>实时查看日志命令<br><code>tail –f /opt/lampp/logs/access_log</code>  </p><h2 id="Discuz帖子-版块存储简介"><a href="#Discuz帖子-版块存储简介" class="headerlink" title="Discuz帖子/版块存储简介"></a>Discuz帖子/版块存储简介</h2><p><code>mysql -uroot -p123 ultrax # 登录ultrax数据库</code><br><code>查看包含帖子id及标题对应关系的表</code><br><code>#tid, subject（文章id、标题）</code><br><code>select tid, subject from pre_forum_post limit 10;</code><br><code>#fid, name（版块id、标题）</code><br><code>select fid, name from pre_forum_forum limit 40;</code>  </p><h2 id="修改日志格式"><a href="#修改日志格式" class="headerlink" title="修改日志格式"></a>修改日志格式</h2><h3 id="找到Apache配置文件"><a href="#找到Apache配置文件" class="headerlink" title="找到Apache配置文件"></a>找到Apache配置文件</h3><p>Apache配置文件名称为httpd.conf，所在目录为 /opt/lampp/etc/ ，完整路径为 /opt/lampp/etc/httpd.conf</p><h3 id="修改日志格式-1"><a href="#修改日志格式-1" class="headerlink" title="修改日志格式"></a>修改日志格式</h3><p>关闭通用日志文件的使用<br><code>CustomLog &quot;logs/access_log&quot; common</code><br>启用组合日志文件<br><code>CustomLog &quot;logs/access_log&quot; combined</code><br>重新加载配置文件<br><code>xampp reload</code><br>检查访问日志<br><code>tail -f /opt/lampp/logs/access_log</code>  </p><h3 id="Flume与Kafka配置"><a href="#Flume与Kafka配置" class="headerlink" title="Flume与Kafka配置"></a>Flume与Kafka配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#agent的名称为a1  </span><br><span class="line">a1.sources = source1  </span><br><span class="line">a1.channels = channel1  </span><br><span class="line">a1.sinks = sink1</span><br><span class="line">#set source</span><br><span class="line">a1.sources.source1.type = TAILDIR  </span><br><span class="line">a1.sources.source1.filegroups = f1  </span><br><span class="line">a1.sources.source1.filegroups.f1 = /opt/lampp/logs/access_log  </span><br><span class="line">a1sources.source1.fileHeader = flase  </span><br><span class="line">#set sink</span><br><span class="line">a1.sinks.sink1.type = org.apache.flume.sink.kafka.KafkaSink  </span><br><span class="line">a1.sinks.sink1.brokerList=kms-2.apache.com:9092,kms-3.apache.com:9092,kms-4.apache.com:9092    </span><br><span class="line">​a1.sinks.sink1.topic= discuzlog  </span><br><span class="line">​a1.sinks.sink1.kafka.flumeBatchSize = 20  </span><br><span class="line">​a1.sinks.sink1.kafka.producer.acks = 1  </span><br><span class="line">​a1.sinks.sink1.kafka.producer.linger.ms = 1  </span><br><span class="line">​a1.sinks.sink1.kafka.producer.compression.type = snappy  </span><br><span class="line">#set channel</span><br><span class="line">​a1.channels.channel1.type = file  </span><br><span class="line">​a1.channels.channel1.checkpointDir = /home/kms/data/flume_data/checkpoint  </span><br><span class="line">​a1.channels.channel1.dataDirs= /home/kms/data/flume_data/data  </span><br><span class="line">#bind</span><br><span class="line">​a1.sources.source1.channels = channel1  </span><br><span class="line">​a1.sinks.sink1.channel = channel1</span><br></pre></td></tr></table></figure><h2 id="创建MySQL数据库和所需要的表"><a href="#创建MySQL数据库和所需要的表" class="headerlink" title="创建MySQL数据库和所需要的表"></a>创建MySQL数据库和所需要的表</h2><p><strong>创建数据库</strong>  </p><p><code>CREATE DATABASE</code>statistics<code>CHARACTER SET &#39;utf8&#39; COLLATE &#39;utf8_general_ci&#39;;</code>  </p><p><strong>创建表:</strong><br>&emsp;&emsp;特定时间段内不同ip的访问次数：client_ip_access<br>CREATE TABLE client_ip_access (<br>&emsp;&emsp;&emsp;&emsp;client_ip text COMMENT ‘客户端ip’,<br>&emsp;&emsp;&emsp;&emsp;sum BIGINT ( 20 ) NOT NULL COMMENT ‘访问次数’,<br>&emsp;&emsp;&emsp;&emsp;time text NOT NULL COMMENT ‘统计时间’<br>) ENGINE = INNODB DEFAULT CHARSET = utf8;  </p><p>&emsp;&emsp;特定时间段内不同文章的访问次数：hot_article<br>CREATE TABLE hot_article (<br>&emsp;&emsp;&emsp;&emsp;article_id text COMMENT ‘文章id’,<br>&emsp;&emsp;&emsp;&emsp;subject text NOT NULL COMMENT ‘文章标题’,<br>&emsp;&emsp;&emsp;&emsp;sum BIGINT ( 20 ) NOT NULL COMMENT ‘访问次数’,<br>&emsp;&emsp;&emsp;&emsp;time text NOT NULL COMMENT ‘统计时间’<br>) ENGINE = INNODB DEFAULT CHARSET = utf8;  </p><p>&emsp;&emsp;特定时间段内不同版块的访问次数：hot_section<br>CREATE TABLE hot_section (<br>&emsp;&emsp;&emsp;&emsp;section_id text COMMENT ‘版块id’,<br>&emsp;&emsp;&emsp;&emsp;name text NOT NULL COMMENT ‘版块标题’,<br>&emsp;&emsp;&emsp;&emsp;sum BIGINT ( 20 ) NOT NULL COMMENT ‘访问次数’,<br>&emsp;&emsp;&emsp;&emsp;time text NOT NULL COMMENT ‘统计时间’<br>) ENGINE = INNODB DEFAULT CHARSET = utf8; </p><h2 id="打包部署存在的问题"><a href="#打包部署存在的问题" class="headerlink" title="打包部署存在的问题"></a>打包部署存在的问题</h2><p><strong>问题1</strong> </p><pre><code>Exception in thread &quot;main&quot; java.lang.SecurityException: Invalid signature file digest for Manifest main attributes</code></pre><p><strong>解决方式</strong>  </p><p>原因:使用sbt打包的时候导致某些包的重复引用，所以打包之后的META-INF的目录下多出了一些<em>.SF,</em>.DSA,*.RSA文件  </p><p>解决办法：删除掉多于的<em>.SF,</em>.DSA,*.RSA文件  </p><p><code>zip -d log_analysis-1.0-SNAPSHOT.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF</code>  </p><p><strong>问题2</strong>    </p><pre><code>Exception in thread &quot;main&quot; java.io.FileNotFoundException: File file:/data/spark_data/history/event-log does not exist</code></pre><p><strong>解决方式</strong>  </p><p>原因:由于spark的spark-defaults.conf配置文件中配置 eventLog 时指定的路径在本机不存在。  </p><p>解决办法：创建对应的文件夹，并赋予对应权限<br><code>sudo mkdir -p /data/spark_data/history/spark-events</code><br><code>sudo mkdir -p /data/spark_data/history/event-log</code><br><code>sudo chmod 777 -R /data</code>  </p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>首先，基于discuz搭建了论坛，针对论坛产生的日志，对其进行分析。主要的处理流程为log—&gt;flume—&gt;kafka—&gt;sparkstreaming—&gt;MySQL,最后将处理的结果写入MySQL共报表查询。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的状态后端(State Backends)</title>
      <link href="/2019/08/23/Flink%E7%9A%84%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF-State-Backends/"/>
      <url>/2019/08/23/Flink%E7%9A%84%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF-State-Backends/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp;当使用checkpoint时，状态(state)会被持久化到checkpoint上，以防止数据的丢失并确保发生故障时能够完全恢复。状态是通过什么方式在哪里持久化，取决于使用的状态后端。</p><a id="more"></a><h2 id="可用的状态后端"><a href="#可用的状态后端" class="headerlink" title="可用的状态后端"></a>可用的状态后端</h2><p><strong>MemoryStateBackend</strong><br><strong>FsStateBackend</strong><br><strong>FsStateBackend</strong>  </p><p>注意：如果什么都不配置，系统默认的是MemoryStateBackend</p><h2 id="MemoryStateBackend"><a href="#MemoryStateBackend" class="headerlink" title="MemoryStateBackend"></a>MemoryStateBackend</h2><p><img src="//jiamaoxiang.top/2019/08/23/Flink的状态后端-State-Backends/memorystatebackend.png" alt><br>&emsp;&emsp;<code>MemoryStateBackend</code> 是将状态维护在 Java 堆上的一个内部状态后端。键值状态和窗口算子使用哈希表来存储数据（values）和定时器（timers）。当应用程序 checkpoint 时，此后端会在将状态发给 JobManager 之前快照下状态，JobManager 也将状态存储在 Java 堆上。默认情况下，<code>MemoryStateBackend</code> 配置成支持异步快照。异步快照可以避免阻塞数据流的处理，从而避免反压的发生。当然，使用 <code>new MemoryStateBackend(MAX_MEM_STATE_SIZE, false)</code>也可以禁用该特点。</p><p><strong>缺点</strong>：</p><ul><li>默认情况下，每一个状态的大小限制为 5 MB。可以通过 <code>MemoryStateBackend</code> 的构造函数增加这个大小。状态大小受到 akka 帧大小的限制(maxStateSize &lt;= akka.framesize 默认 10 M)，所以无论怎么调整状态大小配置，都不能大于 akka 的帧大小。也可以通过 akka.framesize 调整 akka 帧大小。</li><li>状态的总大小不能超过 JobManager 的内存。</li></ul><p><strong>推荐使用的场景</strong>：</p><ul><li>本地测试、几乎无状态的作业，比如 ETL、JobManager 不容易挂，或挂掉影响不大的情况。</li><li>不推荐在生产场景使用。</li></ul><h2 id="FsStateBackend"><a href="#FsStateBackend" class="headerlink" title="FsStateBackend"></a>FsStateBackend</h2><p><img src="//jiamaoxiang.top/2019/08/23/Flink的状态后端-State-Backends/fsstatebackend.png" alt><br>&emsp;&emsp;<code>FsStateBackend</code>需要配置的主要是文件系统，如 URL（类型，地址，路径）。比如可以是：<br><code>“hdfs://namenode:40010/flink/checkpoints”</code> 或<code>“s3://flink/checkpoints”</code></p><p>&emsp;&emsp;当选择使用 <code>FsStateBackend</code>时，正在进行的数据会被存在TaskManager的内存中。在checkpoint时，此后端会将状态快照写入配置的文件系统和目录的文件中，同时会在JobManager的内存中（在高可用场景下会存在 Zookeeper 中）存储极少的元数据。容量限制上，单 TaskManager 上 State 总量不超过它的内存，总大小不超过配置的文件系统容量。</p><p>&emsp;&emsp;默认情况下，<code>FsStateBackend</code> 配置成提供异步快照，以避免在状态 checkpoint 时阻塞数据流的处理。该特性可以实例化 <code>FsStateBackend</code> 时传入false的布尔标志来禁用掉，例如：<code>new FsStateBackend(path, false)</code></p><p><strong>推荐使用的场景</strong>：</p><ul><li>处理大状态，长窗口，或大键值状态的有状态处理任务， 例如分钟级窗口聚合或 join。</li><li>适合用于高可用方案（需要开启HA的作业）。</li><li>可以在生产环境中使用</li></ul><h2 id="RocksDBStateBackend"><a href="#RocksDBStateBackend" class="headerlink" title="RocksDBStateBackend"></a>RocksDBStateBackend</h2><p><img src="//jiamaoxiang.top/2019/08/23/Flink的状态后端-State-Backends/rocksdbstatebackend.png" alt><br>&emsp;&emsp;<code>RocksDBStateBackend</code> 的配置也需要一个文件系统（类型，地址，路径），如下所示：<br>“hdfs://namenode:40010/flink/checkpoints” 或“s3://flink/checkpoints”<br>RocksDB 是一种嵌入式的本地数据库。RocksDBStateBackend 将处理中的数据使用 <a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a> 存储在本地磁盘上。在 checkpoint 时，整个 RocksDB 数据库会被存储到配置的文件系统中，或者在超大状态作业时可以将增量的数据存储到配置的文件系统中。同时 Flink 会将极少的元数据存储在 JobManager 的内存中，或者在 Zookeeper 中（对于高可用的情况）。<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a> 默认也是配置成异步快照的模式。</p><p>&emsp;&emsp;<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>是一个 key/value 的内存存储系统，和其他的 key/value 一样，先将状态放到内存中，如果内存快满时，则写入到磁盘中，但需要注意<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>不支持同步的 Checkpoint，构造方法中没有同步快照这个选项。不过<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>支持增量的 Checkpoint，也是目前唯一增量 Checkpoint 的 Backend，意味着并不需要把所有 sst 文件上传到 Checkpoint 目录，仅需要上传新生成的 sst 文件即可。它的 Checkpoint 存储在外部文件系统（本地或HDFS），其容量限制只要单个 TaskManager 上 State 总量不超过它的内存+磁盘，单Key最大2G，总大小不超过配置的文件系统容量即可。</p><p><strong>缺点</strong>：</p><ul><li><a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>支持的单key和单value的大小最大为每个 2^31 字节。这是因为 RocksDB 的 JNI API 是基于byte[]的。<br></li><li>对于使用具有合并操作的状态的应用程序，例如 ListState，随着时间可能会累积到超过 2^31 字节大小，这将会导致在接下来的查询中失败。</li></ul><p><strong>推荐使用的场景</strong>：</p><ul><li>最适合用于处理大状态，长窗口，或大键值状态的有状态处理任务。</li><li>非常适合用于高可用方案。</li><li>最好是对状态读写性能要求不高的作业</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;那如何选择状态的类型和存储方式？结合前面的内容，可以看到，首先是要分析清楚业务场景；比如想要做什么，状态到底大不大。比较各个方案的利弊，选择根据需求合适的状态类型和存储方式即可。</p><hr><p><strong>Reference</strong></p><p>[1]<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/state/state_backends.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/state/state_backends.html</a><br>[2]<a href="https://ververica.cn/developers/state-management/" target="_blank" rel="noopener">https://ververica.cn/developers/state-management/</a><br>[3]<a href="https://www.ververica.com/blog/stateful-stream-processing-apache-flink-state-backends" target="_blank" rel="noopener">https://www.ververica.com/blog/stateful-stream-processing-apache-flink-state-backends</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--运维与监控(七)</title>
      <link href="/2019/08/19/%E8%BF%90%E7%BB%B4%E4%B8%8E%E7%9B%91%E6%8E%A7/"/>
      <url>/2019/08/19/%E8%BF%90%E7%BB%B4%E4%B8%8E%E7%9B%91%E6%8E%A7/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--集群与部署(六)</title>
      <link href="/2019/08/19/%E9%9B%86%E7%BE%A4%E4%B8%8E%E9%83%A8%E7%BD%B2/"/>
      <url>/2019/08/19/%E9%9B%86%E7%BE%A4%E4%B8%8E%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--source/sink Connectors(五)</title>
      <link href="/2019/08/19/source-sink-Connectors/"/>
      <url>/2019/08/19/source-sink-Connectors/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--状态与容错（四）</title>
      <link href="/2019/08/19/%E7%8A%B6%E6%80%81%E4%B8%8E%E5%AE%B9%E9%94%99/"/>
      <url>/2019/08/19/%E7%8A%B6%E6%80%81%E4%B8%8E%E5%AE%B9%E9%94%99/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--基于时间的算子(三)</title>
      <link href="/2019/08/19/%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E7%9A%84%E7%AE%97%E5%AD%90/"/>
      <url>/2019/08/19/%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E7%9A%84%E7%AE%97%E5%AD%90/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--DataStream-API简介(二)</title>
      <link href="/2019/08/19/DataStream-API%E7%AE%80%E4%BB%8B/"/>
      <url>/2019/08/19/DataStream-API%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程之--Flink的几个重要概念(一)</title>
      <link href="/2019/08/19/Flink%E7%9A%84%E5%87%A0%E4%B8%AA%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5/"/>
      <url>/2019/08/19/Flink%E7%9A%84%E5%87%A0%E4%B8%AA%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p><h3 id="Event-time"><a href="#Event-time" class="headerlink" title="Event-time"></a>Event-time</h3><p>处理时间(process time)很好理解，指的是机器的本地时间，会产生不一致的、不可重复的结果。相反，事件时间(Event-time)能够产生一致的、可重复的结果。然而，相比基于处理时间的应用，基于事件时间的应用需要额外的配置。支持事件时间的流处理引擎的内部比仅仅支持处理时间的流处理引擎的内部更为复杂。</p><p>Flink不仅为常见的事件时间提供直观且易于使用的处理操作，而且也提供了API去自定义实现更高级的事件时间。 对于这样的高级应用，很好的理解Flink的内部时间处理通常是很有帮助的。Flink主要利用两个概念提供事件时间语义：记录时间戳(record timestamps)和watermarks。 接下来，我们将描述Flink内部如何实现和处理时间戳及watermark以支持流应用程序具有事件时间语义的。</p><h4 id="时间戳-timestamps"><a href="#时间戳-timestamps" class="headerlink" title="时间戳(timestamps)"></a>时间戳(timestamps)</h4><p>对于使用事件时间的应用，所处理的记录(record)必须携带时间戳。时间戳将记录与特定时间点相关联，代表事件发生的时间。当Flink以事件时间模式处理数据流时，比如窗口操作，内部会自动的按时间戳将事件发送到相对应的窗口。 Flink会将时间戳编码为16个字节的Long类型的值，并将它们作为元数据附加到记录中。Flink内置的算子会将Long型的值解析为精确到毫秒的Unix时间戳， 但是，自定义算子可以有自己的解析策略，例如，将精度调整为微秒。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浅析数据库缓冲池与SQL查询成本</title>
      <link href="/2019/08/14/%E6%B5%85%E6%9E%90%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%86%B2%E6%B1%A0%E4%B8%8ESQL%E6%9F%A5%E8%AF%A2%E6%88%90%E6%9C%AC/"/>
      <url>/2019/08/14/%E6%B5%85%E6%9E%90%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%86%B2%E6%B1%A0%E4%B8%8ESQL%E6%9F%A5%E8%AF%A2%E6%88%90%E6%9C%AC/</url>
      
        <content type="html"><![CDATA[<p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/background.jpg" alt><br>&emsp;&emsp;如果我们想要查找多行记录，查询时间是否会成倍地提升呢？其实数据库会采用缓冲池的方式提升页(page)的查找效率。数据库的缓冲池在数据库中起到了怎样的作用？如何查看一条 SQL 语句需要在缓冲池中进行加载的页的数量呢？</p><hr><h2 id="数据库缓冲池"><a href="#数据库缓冲池" class="headerlink" title="数据库缓冲池"></a>数据库缓冲池</h2><p>​        &emsp;&emsp;磁盘 I/O 需要消耗的时间很多，而在内存中进行操作，效率则会高很多，为了能让数据表或者索引中的数据随时被我们所用，DBMS 会申请占用内存来作为数据缓冲池，这样做的好处是可以让磁盘活动最小化，从而减少与磁盘直接进行 I/O 的时间。要知道，这种策略对提升 SQL 语句的查询性能来说至关重要。如果索引的数据在缓冲池里，那么访问的成本就会降低很多。<br>​       &emsp;&emsp;那么缓冲池如何读取数据呢？<br>​        &emsp;&emsp;缓冲池管理器会尽量将经常使用的数据保存起来，在数据库进行页面读操作的时候，首先会判断该页面是否在缓冲池中，如果存在就直接读取，如果不存在，就会通过内存或磁盘将页面存放到缓冲池中再进行读取。</p><h2 id="查看缓冲池大小"><a href="#查看缓冲池大小" class="headerlink" title="查看缓冲池大小"></a>查看缓冲池大小</h2><p>​         &emsp;&emsp;如果使用的是 MyISAM 存储引擎(只缓存索引，不缓存数据)，对应的键缓存参数为 key_buffer_size，可以用它进行查看。<br>​        &emsp;&emsp;如果使用的是 InnoDB 存储引擎，可以通过查看 innodb_buffer_pool_size 变量来查看缓冲池的大小，命令如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show variables like <span class="string">'innodb_buffer_pool_size'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/query_innodb_buffer_size.png" alt><br>​        &emsp;&emsp;此时 InnoDB 的缓冲池大小只有 8388608/1024/1024=8MB，我们可以修改缓冲池大小为 128MB，方法如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; <span class="built_in">set</span> global innodb_buffer_pool_size = 1073741824;</span><br></pre></td></tr></table></figure><p>​      &emsp;&emsp; 在 InnoDB 存储引擎中，可以同时开启多个缓冲池，查看缓冲池的个数，使用命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show variables like <span class="string">'innodb_buffer_pool_instances'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/innodb_buffer_pool_instance.png" alt><br>​        &emsp;&emsp;只有一个缓冲池。实际上innodb_buffer_pool_instances默认情况下为 8，为什么只显示只有一个呢？这里需要说明的是，如果想要开启多个缓冲池，你首先需要将innodb_buffer_pool_size参数设置为大于等于 1GB，这时innodb_buffer_pool_instances才会大于 1。你可以在 MySQL 的配置文件中对innodb_buffer_pool_size进行设置，大于等于 1GB，然后再针对innodb_buffer_pool_instances参数进行修改。</p><h2 id="查看SQL语句的查询成本"><a href="#查看SQL语句的查询成本" class="headerlink" title="查看SQL语句的查询成本"></a>查看SQL语句的查询成本</h2><p>​        &emsp;&emsp; 一条 SQL 查询语句在执行前需要确定查询计划，如果存在多种查询计划的话，MySQL 会计算每个查询计划所需要的成本，从中选择成本最小的一个作为最终执行的查询计划。</p><p>​          &emsp;&emsp;如果查看某条 SQL 语句的查询成本，可以在执行完这条 SQL 语句之后，通过查看当前会话中的 last_query_cost 变量值来得到当前查询的成本。这个查询成本对应的是 SQL 语句所需要读取的页(page)的数量。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show status like <span class="string">'last_query_cost'</span></span><br></pre></td></tr></table></figure><p><strong>example</strong>  </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; select userid,rating from movierating <span class="built_in">where</span> userid = 4169;</span><br></pre></td></tr></table></figure><p>结果：2313 rows in set (0.05 sec) </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show status like <span class="string">'last_query_cost'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/test1.png" alt></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; select userid,rating from movierating <span class="built_in">where</span> userid between 4168 and 4175;</span><br></pre></td></tr></table></figure><p>结果：2643 rows in set (0.01 sec) </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show status like <span class="string">'last_query_cost'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/test2.png" alt></p><p>&emsp;&emsp;你能看到页的数量是刚才的 1.4 倍，但是查询的效率并没有明显的变化，实际上这两个 SQL 查询的时间基本上一样，就是因为采用了顺序读取的方式将页面一次性加载到缓冲池中，然后再进行查找。虽然页数量（last_query_cost）增加了不少，但是通过缓冲池的机制，并没有增加多少查询时间。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程</title>
      <link href="/2019/08/13/Flink%E8%87%AA%E5%AD%A6%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B/"/>
      <url>/2019/08/13/Flink%E8%87%AA%E5%AD%A6%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p><img src="//jiamaoxiang.top/2019/08/13/Flink自学系列教程/logo.png" alt><br>&emsp;&emsp;Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p><hr><h4 id="1-Flink的几个重要概念"><a href="#1-Flink的几个重要概念" class="headerlink" title="1.Flink的几个重要概念"></a><a href="https://jiamaoxiang.top/2019/08/19/Flink%E7%9A%84%E5%87%A0%E4%B8%AA%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5/">1.Flink的几个重要概念</a></h4><h4 id="2-DataFrame-API"><a href="#2-DataFrame-API" class="headerlink" title="2. DataFrame API"></a><a href="https://jiamaoxiang.top/2019/08/19/DataFrame-API/">2. DataFrame API</a></h4><h4 id="3-基于时间的算子"><a href="#3-基于时间的算子" class="headerlink" title="3. 基于时间的算子"></a><a href="https://jiamaoxiang.top/2019/08/19/%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E7%9A%84%E7%AE%97%E5%AD%90/">3. 基于时间的算子</a></h4><h4 id="4-状态与容错"><a href="#4-状态与容错" class="headerlink" title="4. 状态与容错"></a><a href="https://jiamaoxiang.top/2019/08/19/%E7%8A%B6%E6%80%81%E4%B8%8E%E5%AE%B9%E9%94%99/">4. 状态与容错</a></h4><h4 id="5-source-sink-Connectors"><a href="#5-source-sink-Connectors" class="headerlink" title="5. source/sink Connectors"></a><a href="https://jiamaoxiang.top/2019/08/19/source-sink-Connectors/">5. source/sink Connectors</a></h4><h4 id="6-集群与部署"><a href="#6-集群与部署" class="headerlink" title="6. 集群与部署"></a><a href="https://jiamaoxiang.top/2019/08/19/%E9%9B%86%E7%BE%A4%E4%B8%8E%E9%83%A8%E7%BD%B2/">6. 集群与部署</a></h4><h4 id="7-运维与监控"><a href="#7-运维与监控" class="headerlink" title="7.运维与监控"></a><a href="https://jiamaoxiang.top/2019/08/19/%E8%BF%90%E7%BB%B4%E4%B8%8E%E7%9B%91%E6%8E%A7/">7.运维与监控</a></h4>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/08/12/hello-world/"/>
      <url>/2019/08/12/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p><p><img src="//jiamaoxiang.top/2019/08/12/hello-world/logo.png" alt></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
