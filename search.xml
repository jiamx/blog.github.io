<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>篇一|ClickHouse快速入门</title>
      <link href="/2020/09/13/%E7%AF%87%E4%B8%80-ClickHouse%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"/>
      <url>/2020/09/13/%E7%AF%87%E4%B8%80-ClickHouse%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="ClickHouse简介"><a href="#ClickHouse简介" class="headerlink" title="ClickHouse简介"></a>ClickHouse简介</h2><p>ClickHouse是一个用于联机分析(OLAP)的列式数据库管理系统(DBMS)。ClickHouse最初是一款名为Yandex.Metrica的产品，主要用于WEB流量分析。ClickHouse的全称是<strong>Click Stream,Data WareHouse</strong>，简称ClickHouse。</p><p>ClickHouse非常适用于商业智能领域，除此之外，它也能够被广泛应用于广告流量、Web、App流量、电信、金融、电子商务、信息安全、网络游戏、物联网等众多其他领域。ClickHouse具有以下特点：</p><ul><li><p>支持完备的SQL操作</p></li><li><p>列式存储与数据压缩</p></li><li><p>向量化执行引擎</p></li><li><p>关系型模型(与传统数据库类似)</p></li><li><p>丰富的表引擎</p></li><li><p>并行处理 </p></li><li><p>在线查询</p></li><li><p>数据分片</p><p>ClickHouse作为一款高性能OLAP数据库，存在以下不足。</p></li><li><p>不支持事务。</p></li><li><p>不擅长根据主键按行粒度进行查询（虽然支持），故不应该把ClickHouse当作Key-Value数据库使用。</p></li><li><p>不擅长按行删除数据（虽然支持）</p></li></ul><h2 id="单机安装"><a href="#单机安装" class="headerlink" title="单机安装"></a>单机安装</h2><h3 id="下载RPM包"><a href="#下载RPM包" class="headerlink" title="下载RPM包"></a>下载RPM包</h3><p>本文安装方式选择的是离线安装，可以在下面的链接中下载对应的rpm包，也可以直接百度云下载</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-- rpm包地址</span><br><span class="line">https://packagecloud.io/Altinity/clickhouse</span><br><span class="line">-- 百度云地址</span><br><span class="line">链接：https://pan.baidu.com/s/1pFR66SzLvPYMfcpuPJww5A </span><br><span class="line">提取码：gh5a</span><br></pre></td></tr></table></figure><p>在我们安装的软件中包含这些包:</p><ul><li><code>clickhouse-client</code> 包，包含 clickhouse-client 应用程序，它是交互式ClickHouse控制台客户端。</li><li><code>clickhouse-common</code> 包，包含一个ClickHouse可执行文件。</li><li><code>clickhouse-server</code> 包，包含要作为服务端运行的ClickHouse配置文件。</li></ul><p>总共包含四个RPM包，</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">clickhouse-client-19.17.4.11-1.el7.x86_64.rpm</span><br><span class="line">clickhouse-common-static-19.17.4.11-1.el7.x86_64.rpm</span><br><span class="line">clickhouse-server-19.17.4.11-1.el7.x86_64.rpm</span><br><span class="line">clickhouse-server-common-19.17.4.11-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><blockquote><p>尖叫提示：如果安装过程中，报错：依赖检测失败，表示缺少依赖包</p><p>可以先手动安装<strong>libicu-50.2-4.el7_7.x86_64.rpm</strong>依赖包</p></blockquote><h3 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 查看防火墙状态。</span></span><br><span class="line">systemctl status firewalld</span><br><span class="line"><span class="comment">## 临时关闭防火墙命令。重启电脑后，防火墙自动起来。</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line"><span class="comment">## 永久关闭防火墙命令。重启后，防火墙不会自动启动。</span></span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br></pre></td></tr></table></figure><h3 id="系统要求"><a href="#系统要求" class="headerlink" title="系统要求"></a>系统要求</h3><p>ClickHouse可以在任何具有x86_64，AArch64或PowerPC64LE CPU架构的Linux，FreeBSD或Mac OS X上运行。虽然预构建的二进制文件通常是为x86  _64编译并利用SSE 4.2指令集，但除非另有说明，否则使用支持它的CPU将成为额外的系统要求。这是检查当前CPU是否支持SSE 4.2的命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grep -q sse4_2 /proc/cpuinfo &amp;&amp; <span class="built_in">echo</span> <span class="string">"SSE 4.2 supported"</span> || <span class="built_in">echo</span> <span class="string">"SSE 4.2 not supported"</span></span><br><span class="line">SSE 4.2 supported</span><br></pre></td></tr></table></figure><p>要在不支持SSE 4.2或具有AArch64或PowerPC64LE体系结构的处理器上运行ClickHouse，应该<a href="https://clickhouse.tech/docs/zh/getting-started/install/#from-sources" target="_blank" rel="noopener">通过源构建ClickHouse</a>进行适当的配置调整。</p><h3 id="安装RPM包"><a href="#安装RPM包" class="headerlink" title="安装RPM包"></a>安装RPM包</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 将rpm包上传至/opt/software目录下</span></span><br><span class="line"><span class="comment">## 执行如下命令进行安装</span></span><br><span class="line">[root@cdh06 software]<span class="comment"># rpm -ivh *.rpm</span></span><br><span class="line">错误：依赖检测失败：</span><br><span class="line">        libicudata.so.50()(64bit) 被 clickhouse-common-static-19.17.4.11-1.el7.x86_64 需要</span><br><span class="line">        libicui18n.so.50()(64bit) 被 clickhouse-common-static-19.17.4.11-1.el7.x86_64 需要</span><br><span class="line">        libicuuc.so.50()(64bit) 被 clickhouse-common-static-19.17.4.11-1.el7.x86_64 需要</span><br><span class="line">        libicudata.so.50()(64bit) 被 clickhouse-server-19.17.4.11-1.el7.x86_64 需要</span><br><span class="line">        libicui18n.so.50()(64bit) 被 clickhouse-server-19.17.4.11-1.el7.x86_64 需要</span><br><span class="line">        libicuuc.so.50()(64bit) 被 clickhouse-server-19.17.4.11-1.el7.x86_64 需要</span><br><span class="line"><span class="comment">## 上面安装报错，缺少相应的依赖包，</span></span><br><span class="line"><span class="comment">## 需要下载相对应的依赖包</span></span><br><span class="line"><span class="comment">## 下载libicu-50.2-4.el7_7.x86_64.rpm进行安装即可</span></span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2020/09/13/篇一-ClickHouse快速入门/ck%E5%AE%89%E8%A3%85.png" alt></p><h3 id="查看安装信息"><a href="#查看安装信息" class="headerlink" title="查看安装信息"></a>查看安装信息</h3><h4 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h4><ul><li><p><strong>/etc/clickhouse-server</strong>：服务端的配置文件目录，包括全局配置config.xml和用户配置users.xml等。</p></li><li><p><strong>/etc/clickhouse-client</strong>：客户端配置，包括conf.d文件夹和config.xml文件。</p></li><li><p><strong>/var/lib/clickhouse</strong>：默认的数据存储目录（通常会修改默认路径配置，将数据保存到大容量磁盘挂载的路径）。</p></li><li><p><strong>/var/log/clickhouse-server</strong>：默认保存日志的目录（通常会修改路径配置，将日志保存到大容量磁盘挂载的路径）。</p></li></ul><h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><ul><li><strong>/etc/security/limits.d/clickhouse.conf</strong>：文件句柄数量的配置</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh06 clickhouse-server]<span class="comment"># cat /etc/security/limits.d/clickhouse.conf </span></span><br><span class="line">clickhouse      soft    nofile  262144</span><br><span class="line">clickhouse      hard    nofile  262144</span><br></pre></td></tr></table></figure><p>该配置也可以通过config.xml的max_open_files修改</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Set limit on number of open files (default: maximum). This setting makes sense on Mac OS X because getrlimit() fails to retrieve correct maximum value. --&gt;</span></span><br><span class="line">   <span class="comment">&lt;!-- &lt;max_open_files&gt;262144&lt;/max_open_files&gt; --&gt;</span></span><br></pre></td></tr></table></figure><ul><li><strong>/etc/cron.d/clickhouse-server:cron</strong>：定时任务配置，用于恢复因异常原因中断的ClickHouse服务进程，其默认的配置如下。</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh06 cron.d]<span class="comment"># cat /etc/cron.d/clickhouse-server</span></span><br><span class="line"><span class="comment">#*/10 * * * * root (which service &gt; /dev/null 2&gt;&amp;1 &amp;&amp; (service clickhouse-server condstart ||:)) || /etc/init.d/clickhouse-server condstart &gt; /dev/null 2&gt;&amp;1</span></span><br></pre></td></tr></table></figure><h4 id="可执行文件"><a href="#可执行文件" class="headerlink" title="可执行文件"></a>可执行文件</h4><p>最后是一组在/usr/bin路径下的可执行文件：</p><ul><li><p><strong>clickhouse</strong>：主程序的可执行文件。</p></li><li><p><strong>clickhouse-client</strong>：一个指向ClickHouse可执行文件的软链接，供客户端连接使用。</p></li><li><p><strong>clickhouse-server</strong>：一个指向ClickHouse可执行文件的软链接，供服务端启动使用。</p></li><li><p><strong>clickhouse-compressor</strong>：内置提供的压缩工具，可用于数据的正压反解。</p></li></ul><p><img src="//jiamaoxiang.top/2020/09/13/篇一-ClickHouse快速入门/%E5%91%BD%E4%BB%A4%E7%A8%8B%E5%BA%8F.png" alt></p><h3 id="启动-关闭服务"><a href="#启动-关闭服务" class="headerlink" title="启动/关闭服务"></a>启动/关闭服务</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 启动服务</span></span><br><span class="line">[root@cdh06 ~]<span class="comment"># service clickhouse-server start</span></span><br><span class="line">Start clickhouse-server service: Path to data directory <span class="keyword">in</span> /etc/clickhouse-server/config.xml: /var/lib/clickhouse/</span><br><span class="line">DONE</span><br><span class="line"><span class="comment">## 关闭服务</span></span><br><span class="line">[root@cdh06 ~]<span class="comment"># service clickhouse-server stop</span></span><br></pre></td></tr></table></figure><h3 id="客户端连接"><a href="#客户端连接" class="headerlink" title="客户端连接"></a>客户端连接</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh06 ~]<span class="comment"># clickhouse-client </span></span><br><span class="line">ClickHouse client version 19.17.4.11.</span><br><span class="line">Connecting to localhost:9000 as user default.</span><br><span class="line">Connected to ClickHouse server version 19.17.4 revision 54428.</span><br><span class="line"></span><br><span class="line">cdh06 :) show databases;</span><br><span class="line"></span><br><span class="line">SHOW DATABASES</span><br><span class="line"></span><br><span class="line">┌─name────┐</span><br><span class="line">│ default │</span><br><span class="line">│ system  │</span><br><span class="line">└─────────┘</span><br><span class="line"></span><br><span class="line">2 rows <span class="keyword">in</span> <span class="built_in">set</span>. Elapsed: 0.004 sec.</span><br></pre></td></tr></table></figure><h3 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h3><h4 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h4><ul><li>语法</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] db_name [<span class="keyword">ON</span> CLUSTER cluster] [<span class="keyword">ENGINE</span> = <span class="keyword">engine</span>(...)]</span><br></pre></td></tr></table></figure><ul><li>例子</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> tutorial</span><br></pre></td></tr></table></figure><p>默认情况下，ClickHouse使用的是原生的数据库引擎<strong>Ordinary</strong>(在此数据库下可以使用任意类型的<strong>表引擎</strong>，<strong>在绝大多数情况下都只需使用默认的数据库引擎</strong>)。当然也可以使用<strong>Lazy</strong>引擎和<strong>MySQL</strong>引擎，比如使用MySQL引擎，可以直接在ClickHouse中操作MySQL对应数据库中的表。假设MySQL中存在一个名为clickhouse的数据库，可以使用下面的方式连接MySQL数据库。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- --------------------------语法-----------------------------------</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] db_name [<span class="keyword">ON</span> CLUSTER cluster]</span><br><span class="line"><span class="keyword">ENGINE</span> = MySQL(<span class="string">'host:port'</span>, [<span class="string">'database'</span> | <span class="keyword">database</span>], <span class="string">'user'</span>, <span class="string">'password'</span>)</span><br><span class="line"><span class="comment">-- --------------------------示例------------------------------------</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> mysql_db <span class="keyword">ENGINE</span> = MySQL(<span class="string">'192.168.200.241:3306'</span>, <span class="string">'clickhouse'</span>, <span class="string">'root'</span>, <span class="string">'123qwe'</span>);</span><br><span class="line"><span class="comment">-- ---------------------------操作-----------------------------------</span></span><br><span class="line">cdh06 :) <span class="keyword">use</span> mysql_db;</span><br><span class="line">cdh06 :) <span class="keyword">show</span> <span class="keyword">tables</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">TABLES</span></span><br><span class="line"></span><br><span class="line">┌─<span class="keyword">name</span>─┐</span><br><span class="line">│ <span class="keyword">test</span> │</span><br><span class="line">└──────┘</span><br><span class="line"></span><br><span class="line"><span class="number">1</span> <span class="keyword">rows</span> <span class="keyword">in</span> set. Elapsed: <span class="number">0.005</span> sec. </span><br><span class="line"></span><br><span class="line">cdh06 :) <span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">test</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">test</span></span><br><span class="line"></span><br><span class="line">┌─<span class="keyword">id</span>─┬─<span class="keyword">name</span>──┐</span><br><span class="line">│  <span class="number">1</span> │ tom   │</span><br><span class="line">│  <span class="number">2</span> │ jack  │</span><br><span class="line">│  <span class="number">3</span> │ lihua │</span><br><span class="line">└────┴───────┘</span><br><span class="line"></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> <span class="keyword">in</span> set. Elapsed: <span class="number">0.047</span> sec.</span><br></pre></td></tr></table></figure><h4 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h4><ul><li>语法</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db.]table_name [<span class="keyword">ON</span> CLUSTER cluster]</span><br><span class="line">(</span><br><span class="line">    name1 [type1] [<span class="keyword">DEFAULT</span>|<span class="keyword">MATERIALIZED</span>|<span class="keyword">ALIAS</span> expr1] [compression_codec] [TTL expr1],</span><br><span class="line">    name2 [type2] [<span class="keyword">DEFAULT</span>|<span class="keyword">MATERIALIZED</span>|<span class="keyword">ALIAS</span> expr2] [compression_codec] [TTL expr2],</span><br><span class="line">    ...</span><br><span class="line">) <span class="keyword">ENGINE</span> = <span class="keyword">engine</span></span><br></pre></td></tr></table></figure><ul><li>示例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 注意首字母大写</span></span><br><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">test</span>(</span><br><span class="line">    <span class="keyword">id</span> Int32,</span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">String</span></span><br><span class="line">) <span class="keyword">engine</span>=<span class="keyword">Memory</span>;</span><br></pre></td></tr></table></figure><p>上面命令创建了一张内存表，即使用Memory引擎。表引擎决定了数据表的特性，也决定了数据将会被如何存储及加载。Memory引擎是ClickHouse最简单的表引擎，数据只会被保存在内存中，在服务重启时数据会丢失。</p><h2 id="集群安装"><a href="#集群安装" class="headerlink" title="集群安装"></a>集群安装</h2><h3 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h3><p>上面介绍了单机安装的基本步骤和ClickHouse客户端的基本使用。接下来将介绍集群的安装方式。ClickHouse集群安装非常简单，首先重复上面步骤，分别在其他机器上安装ClickHouse，然后再分别配置一下<strong>/etc/clickhouse-server/config.xml</strong>和<strong>/etc/metrika.xml</strong>两个文件即可。值得注意的是，ClickHouse集群的依赖于<strong>Zookeeper</strong>，所以要保证先安装好Zookeeper集群，zk集群的安装步骤非常简单，本文不会涉及。本文演示三个节点的ClickHouse集群安装，具体步骤如下：</p><ul><li><p>首先，重复单机安装的步骤，分别在另外两台机器安装ClickHouse</p></li><li><p>然后，在每台机器上修改<strong>/etc/clickhouse-server/config.xml</strong>文件</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 如果禁用了ipv6，使用下面配置--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">listen_host</span>&gt;</span>0.0.0.0<span class="tag">&lt;/<span class="name">listen_host</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 如果没有禁用ipv6，使用下面配置</span></span><br><span class="line"><span class="comment">&lt;listen_host&gt;::&lt;/listen_host&gt;</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p>尖叫提示（1）：</p><p>在禁用了ipv6时，如果使用<listen_host>::</listen_host>配置，会报如下错误</p><p><error> Application: DB::Exception: Listen [::]:8123 failed: Poco::Exception. Code: 1000, e.code() =0, e.displayText() = DNS error: EAI: -9</error></p><p>尖叫提示（2）：</p><p>ClickHouse默认的tcp端口号是9000，如果存在端口冲突，可以在<strong>/etc/clickhouse-server/config.xml</strong>文件中修改 端口号<tcp_port>9001</tcp_port></p></blockquote></li><li><p>最后在/etc下创建<strong>metrika.xml</strong>文件，内容如下，下面配置是<strong>不包含副本的分片配置</strong>，我们还可以为分片配置多个副本</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">yandex</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 该标签与config.xml的&lt;remote_servers incl="clickhouse_remote_servers" &gt;保持一致</span></span><br><span class="line"><span class="comment"> --&gt;</span>    </span><br><span class="line"><span class="tag">&lt;<span class="name">clickhouse_remote_servers</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 集群名称，可以修改 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">cluster_3shards_1replicas</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 配置三个分片，每个分片对应一台机器--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">host</span>&gt;</span>cdh04<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">port</span>&gt;</span>9001<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">host</span>&gt;</span>cdh05<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">port</span>&gt;</span>9001<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">host</span>&gt;</span>cdh06<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">port</span>&gt;</span>9001<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">cluster_3shards_1replicas</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">clickhouse_remote_servers</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 该标签与config.xml的&lt;zookeeper incl="zookeeper-servers" optional="true" /&gt;</span></span><br><span class="line"><span class="comment">保持一致</span></span><br><span class="line"><span class="comment"> --&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">zookeeper-servers</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">node</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">host</span>&gt;</span>cdh02<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">node</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">host</span>&gt;</span>cdh03<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">node</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">host</span>&gt;</span>cdh06<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">zookeeper-servers</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 分片和副本标识，shard标签配置分片编号，&lt;replica&gt;配置分片副本主机名</span></span><br><span class="line"><span class="comment">  需要修改对应主机上的配置--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">macros</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">shard</span>&gt;</span>01<span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">replica</span>&gt;</span>cdh04<span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">macros</span>&gt;</span>    </span><br><span class="line"><span class="tag">&lt;/<span class="name">yandex</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>分别在各自的机器上启动clickhouse-server</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># service clickhouse-server start</span></span><br></pre></td></tr></table></figure></li><li><p>（可选配置）修改<strong>/etc/clickhouse-client/config.xml</strong>文件</p><p>由于clickhouse-client默认连接的主机是localhost，默认连接的端口号是9000，由于我们修改了默认的端口号，所以需要修改客户端默认连接的端口号，在该文件里添加如下内容：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">port</span>&gt;</span>9001<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br></pre></td></tr></table></figure><p>当然也可以不用修改，但是记得在使用客户端连接时，加上<strong>–port 9001</strong>参数指明要连接的端口号，否则会报错：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Connecting to localhost:9000 as user default.</span><br><span class="line">Code: 210. DB::NetException: Connection refused (localhost:9000)</span><br></pre></td></tr></table></figure></li></ul><h3 id="基本操作-1"><a href="#基本操作-1" class="headerlink" title="基本操作"></a>基本操作</h3><h4 id="验证集群"><a href="#验证集群" class="headerlink" title="验证集群"></a>验证集群</h4><p>在完成上述配置之后，在各自机器上启动clickhouse-server，并开启clickhouse-clent</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">// 启动server</span><br><span class="line"><span class="comment"># service clickhouse-server start</span></span><br><span class="line">// 启动客户端，-m参数支持多行输入</span><br><span class="line"><span class="comment"># clickhouse-client -m</span></span><br></pre></td></tr></table></figure><p>可以查询系统表验证集群配置是否已被加载：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cdh04 :) <span class="keyword">select</span> cluster,shard_num,replica_num,host_name,port,<span class="keyword">user</span> <span class="keyword">from</span> system.clusters;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2020/09/13/篇一-ClickHouse快速入门/%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE.png" alt></p><p>接下来再来看一下集群的分片信息(宏变量)：分别在各自机器上执行下面命令：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">cdh04 :) <span class="keyword">select</span> * <span class="keyword">from</span> system.macros;</span><br><span class="line">┌─macro───┬─substitution─┐</span><br><span class="line">│ replica │ cdh04        │</span><br><span class="line">│ shard   │ 01           │</span><br><span class="line">└─────────┴──────────────┘</span><br><span class="line"></span><br><span class="line">cdh05 :) <span class="keyword">select</span> * <span class="keyword">from</span> system.macros;</span><br><span class="line">┌─macro───┬─substitution─┐</span><br><span class="line">│ replica │ cdh05        │</span><br><span class="line">│ shard   │ 02           │</span><br><span class="line">└─────────┴──────────────┘</span><br><span class="line"></span><br><span class="line">cdh06 :) <span class="keyword">select</span> * <span class="keyword">from</span> system.macros;</span><br><span class="line">┌─macro───┬─substitution─┐</span><br><span class="line">│ replica │ cdh06        │</span><br><span class="line">│ shard   │ 03           │</span><br><span class="line">└─────────┴──────────────┘</span><br></pre></td></tr></table></figure><h4 id="分布式DDL操作"><a href="#分布式DDL操作" class="headerlink" title="分布式DDL操作"></a>分布式DDL操作</h4><p>默认情况下，CREATE、DROP、ALTER、RENAME操作仅仅在当前执行该命令的server上生效。在集群环境下，可以使用<strong>ON CLUSTER</strong>语句，这样就可以在整个集群发挥作用。</p><p>比如创建一张分布式表：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> user_cluster <span class="keyword">ON</span> CLUSTER cluster_3shards_1replicas</span><br><span class="line">(</span><br><span class="line">    <span class="keyword">id</span> Int32,</span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">String</span></span><br><span class="line">)<span class="keyword">ENGINE</span> = <span class="keyword">Distributed</span>(cluster_3shards_1replicas, <span class="keyword">default</span>, user_local,<span class="keyword">id</span>);</span><br></pre></td></tr></table></figure><p><strong>Distributed表引擎的定义形式如下所示</strong>：关于ClickHouse的表引擎，后续文章会做详细解释。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">Distributed(cluster_name, database_name, table_name[, sharding_key])</span><br></pre></td></tr></table></figure><p>各个参数的含义分别如下：</p><ul><li><strong>cluster_name</strong>：集群名称，与集群配置中的自定义名称相对应。</li><li><strong>database_name</strong>：数据库名称</li><li><strong>table_name</strong>：表名称</li><li><strong>sharding_key</strong>：可选的，用于分片的key值，在数据写入的过程中，分布式表会依据分片key的规则，将数据分布到各个节点的本地表。</li></ul><blockquote><p>尖叫提示：</p><p>创建分布式表是<strong>读时检查的机制</strong>，也就是说对<strong>创建分布式表和本地表的顺序并没有强制要求</strong>。</p><p>同样值得注意的是，在上面的语句中使用了ON CLUSTER分布式DDL，这意味着在集群的每个分片节点上，都会创建一张Distributed表，这样便可以从其中任意一端发起对所有分片的读、写请求。</p></blockquote><p>创建完成上面的分布式表时，在每台机器上查看表，发现每台机器上都存在一张刚刚创建好的表。</p><p>接下来就需要创建本地表了，在每台机器上分别创建一张本地表：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> user_local </span><br><span class="line">(</span><br><span class="line">    <span class="keyword">id</span> Int32,</span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">String</span></span><br><span class="line">)<span class="keyword">ENGINE</span> = MergeTree()</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">id</span></span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">id</span></span><br><span class="line">PRIMARY <span class="keyword">KEY</span> <span class="keyword">id</span>;</span><br></pre></td></tr></table></figure><p>我们先在一台机器上，对user_local表进行插入数据，然后再查询user_cluster表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 插入数据</span></span><br><span class="line">cdh04 :) <span class="keyword">INSERT</span> <span class="keyword">INTO</span> user_local <span class="keyword">VALUES</span>(<span class="number">1</span>,<span class="string">'tom'</span>),(<span class="number">2</span>,<span class="string">'jack'</span>);</span><br><span class="line"><span class="comment">-- 查询user_cluster表,可见通过user_cluster表可以操作所有的user_local表</span></span><br><span class="line">cdh04 :) <span class="keyword">select</span> * <span class="keyword">from</span> user_cluster;</span><br><span class="line">┌─id─┬─name─┐</span><br><span class="line">│  2 │ jack │</span><br><span class="line">└────┴──────┘</span><br><span class="line">┌─id─┬─name─┐</span><br><span class="line">│  1 │ tom  │</span><br><span class="line">└────┴──────┘</span><br></pre></td></tr></table></figure><p>接下来，我们再向user_cluster中插入一些数据，观察user_local表数据变化，可以发现数据被分散存储到了其他节点上了。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 向user_cluster插入数据</span></span><br><span class="line">cdh04 :)  <span class="keyword">INSERT</span> <span class="keyword">INTO</span> user_cluster <span class="keyword">VALUES</span>(<span class="number">3</span>,<span class="string">'lilei'</span>),(<span class="number">4</span>,<span class="string">'lihua'</span>); </span><br><span class="line"><span class="comment">-- 查看user_cluster数据</span></span><br><span class="line">cdh04 :) <span class="keyword">select</span> * <span class="keyword">from</span> user_cluster;</span><br><span class="line">┌─id─┬─name─┐</span><br><span class="line">│  2 │ jack │</span><br><span class="line">└────┴──────┘</span><br><span class="line">┌─id─┬─name──┐</span><br><span class="line">│  3 │ lilei │</span><br><span class="line">└────┴───────┘</span><br><span class="line">┌─id─┬─name─┐</span><br><span class="line">│  1 │ tom  │</span><br><span class="line">└────┴──────┘</span><br><span class="line">┌─id─┬─name──┐</span><br><span class="line">│  4 │ lihua │</span><br><span class="line">└────┴───────┘</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 在cdh04上查看user_local</span></span><br><span class="line">cdh04 :) <span class="keyword">select</span> * <span class="keyword">from</span> user_local;</span><br><span class="line">┌─id─┬─name─┐</span><br><span class="line">│  2 │ jack │</span><br><span class="line">└────┴──────┘</span><br><span class="line">┌─id─┬─name──┐</span><br><span class="line">│  3 │ lilei │</span><br><span class="line">└────┴───────┘</span><br><span class="line">┌─id─┬─name─┐</span><br><span class="line">│  1 │ tom  │</span><br><span class="line">└────┴──────┘</span><br><span class="line"><span class="comment">-- 在cdh05上查看user_local</span></span><br><span class="line">cdh05 :) <span class="keyword">select</span> * <span class="keyword">from</span> user_local;</span><br><span class="line">┌─id─┬─name──┐</span><br><span class="line">│  4 │ lihua │</span><br><span class="line">└────┴───────┘</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先介绍了ClickHouse的基本特点和使用场景，接着阐述了ClickHouse单机版与集群版离线安装步骤，并给出了ClickHouse的简单使用案例。本文是ClickHouse的一个简单入门，在接下来的分享中，会逐步深入探索ClickHouse的世界。</p>]]></content>
      
      
      <categories>
          
          <category> ClickHouse </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ClickHouse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>面试|Kafka常见面试问题总结</title>
      <link href="/2020/09/08/%E9%9D%A2%E8%AF%95-Kafka%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/"/>
      <url>/2020/09/08/%E9%9D%A2%E8%AF%95-Kafka%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>现如今，Kafka已不再是一个单纯的消息队列系统。Kafka是一个分布式的流处理平台，被越来越多的公司使用，Kafka可以被用于高性能的数据管道，流处理分析，数据集成等场景。本文分享总结了几个Kafka常见的面试问题，希望对你有所帮助。主要包括以下内容：</p><ul><li><p>Kafka是如何保障数据不丢失的？</p></li><li><p>如何解决Kafka数据丢失问题？</p></li><li><p>Kafka可以保障永久不丢失数据吗？</p></li><li><p>如何保障Kafka中的消息是有序的？</p></li><li><p>如何确定Kafka主题的分区数量？</p></li><li><p>如何调整生产环境中Kafka主题的分区数量？</p></li><li><p>如何重平衡Kafka集群？</p></li><li><p>如何查看消费者组是否存在滞后消费？</p></li></ul><h2 id="Q1：Kafka是如何保障数据不丢失的？"><a href="#Q1：Kafka是如何保障数据不丢失的？" class="headerlink" title="Q1：Kafka是如何保障数据不丢失的？"></a>Q1：Kafka是如何保障数据不丢失的？</h2><p>该问题已经成为了Kafka面试的惯例，如同Java的<strong>HashMap</strong>，属于高频出现的面试问题。那么，我们该怎么理解这个问题呢？问题是<strong>Kafka如何保障数据不丢失</strong>，即<strong>Kafka的Broker提供了什么机制保证数据不丢失的。</strong></p><p>其实对于Kafka的Broker而言，Kafka 的<strong>复制机制</strong>和<strong>分区的多副本</strong>架构是Kafka 可靠性保证的核心。把消息写入多个副本可以使Kafka 在发生崩溃时仍能保证消息的持久性。</p><p>搞清楚了问题的核心，再来看一下该怎么回答这个问题：主要包括三个方面</p><blockquote><p>1.Topic 副本因子个数：replication.factor &gt;= 3</p><p>2.同步副本列表(ISR)：min.insync.replicas = 2</p><p>3.禁用unclean选举： unclean.leader.election.enable=false</p></blockquote><p>下面将会逐步分析上面的三个配置：</p><ul><li><strong>副本因子</strong></li></ul><p>Kafka的topic是可以分区的，并且可以为分区配置多个副本，该配置可以通过<code>replication.factor</code>参数实现。Kafka中的分区副本包括两种类型：领导者副本（Leader Replica）和追随者副本（Follower Replica)，每个分区在创建时都要选举一个副本作为领导者副本，其余的副本自动变为追随者副本。在 Kafka 中，追随者副本是不对外提供服务的，也就是说，任何一个追随者副本都不能响应消费者和生产者的读写请求。所有的请求都必须由领导者副本来处理。换句话说，所有的读写请求都必须发往领导者副本所在的 Broker，由该 Broker 负责处理。追随者副本不处理客户端请求，它唯一的任务就是从领导者副本<strong>异步拉取</strong>消息，并写入到自己的提交日志中，从而实现与领导者副本的同步。</p><p>一般来说，副本设为3可以满足大部分的使用场景，也有可能是5个副本(比如银行)。如果副本因子为N，那么在N-1个broker 失效的情况下，仍然能够从主题读取数据或向主题写入数据。所以，更高的副本因子会带来更高的可用性、可靠性和更少的故障。另一方面，副本因子N需要至少N个broker ，而且会有N个数据副本，也就是说它们会占用N倍的磁盘空间。实际生产环境中一般会在可用性和存储硬件之间作出权衡。</p><p>除此之外，副本的分布同样也会影响可用性。默认情况下，Kafka会确保分区的每个副本分布在不同的Broker上，但是如果这些Broker在同一个机架上，一旦机架的交换机发生故障，分区就会不可用。所以建议把Broker分布在不同的机架上，可以使用<strong>broker.rack</strong>参数配置Broker所在机架的名称。</p><ul><li><strong>同步副本列表</strong></li></ul><p>In-sync replica(ISR)称之为同步副本，ISR中的副本都是与Leader进行同步的副本，所以不在该列表的follower会被认为与Leader是不同步的。那么，ISR中存在是什么副本呢？首先可以明确的是：Leader副本总是存在于ISR中。而follower副本是否在ISR中，取决于该follower副本是否与Leader副本保持了“同步”。</p><p>Kafka的broker端有一个参数<strong><code>replica.lag.time.max.ms</code></strong>, 该参数表示follower副本滞后与Leader副本的最长时间间隔，默认是10秒。这就意味着，只要follower副本落后于leader副本的时间间隔不超过10秒，就可以认为该follower副本与leader副本是同步的，所以哪怕当前follower副本落后于Leader副本几条消息，只要在10秒之内赶上Leader副本，就不会被踢出出局。</p><p>可以看出ISR是一个动态的，所以即便是为分区配置了3个副本，还是会出现同步副本列表中只有一个副本的情况(其他副本由于不能够与leader及时保持同步，被移出ISR列表)。如果这个同步副本变为不可用，我们必须在<strong>可用性</strong>和<strong>一致性</strong>之间作出选择(CAP理论)。</p><p>根据Kafka 对可靠性保证的定义，消息只有在被写入到所有同步副本之后才被认为是已提交的。但如果这里的“所有副本”只包含一个同步副本，那么在这个副本变为不可用时，数据就会丢失。如果要确保已提交的数据被写入不止一个副本，就需要把最小同步副本数量设置为大一点的值。对于一个包含3 个副本的主题分区，如果<strong>min.insync.replicas=2</strong> ，那么至少要存在两个同步副本才能向分区写入数据。</p><p>如果进行了上面的配置，此时必须要保证ISR中至少存在两个副本，如果ISR中的副本个数小于2，那么Broker就会停止接受生产者的请求。尝试发送数据的生产者会收到<strong>NotEnoughReplicasException</strong>异常，消费者仍然可以继续读取已有的数据。</p><ul><li><strong>禁用unclean选举</strong></li></ul><p>选择一个同步副本列表中的分区作为leader 分区的过程称为<strong>clean leader election</strong>。注意，这里要与在非同步副本中选一个分区作为leader分区的过程区分开，在非同步副本中选一个分区作为leader的过程称之为<strong>unclean leader election</strong>。由于ISR是动态调整的，所以会存在ISR列表为空的情况，通常来说，非同步副本落后 Leader 太多，因此，如果选择这些副本作为新 Leader，就可能出现数据的丢失。毕竟，这些副本中保存的消息远远落后于老 Leader 中的消息。在 Kafka 中，选举这种副本的过程可以通过Broker 端参数 *<em>unclean.leader.election.enable *</em>控制是否允许 Unclean 领导者选举。开启 Unclean 领导者选举可能会造成数据丢失，但好处是，它使得分区 Leader 副本一直存在，不至于停止对外提供服务，因此提升了高可用性。反之，禁止 Unclean Leader 选举的好处在于维护了数据的一致性，避免了消息丢失，但牺牲了高可用性。分布式系统的CAP理论说的就是这种情况。</p><p>不幸的是，<strong>unclean leader election</strong>的选举过程仍可能会造成数据的不一致，因为同步副本并不是<strong>完全</strong>同步的。由于复制是<strong>异步</strong>完成的，因此无法保证follower可以获取最新消息。比如Leader分区的最后一条消息的offset是100，此时副本的offset可能不是100，这受到两个参数的影响：</p><blockquote><ul><li><p><strong>replica.lag.time.max.ms</strong>：同步副本滞后与leader副本的时间</p></li><li><p><strong>zookeeper.session.timeout.ms</strong>：与zookeeper会话超时时间</p></li></ul></blockquote><p>简而言之，如果我们允许不同步的副本成为leader，那么就要承担丢失数据和出现数据不一致的风险。如果不允许它们成为leader，那么就要接受较低的可用性，因为我们必须等待原先的首领恢复到可用状态。</p><p>关于unclean选举，不同的场景有不同的配置方式。对<strong>数据质量和数据一致性</strong>要求较高的系统会禁用这种unclean的leader选举(比如银行)。如果在<strong>可用性</strong>要求较高的系统里，比如实时点击流分析系统， 一般不会禁用unclean的leader选举。</p><h2 id="Q2：如何解决Kafka数据丢失问题？"><a href="#Q2：如何解决Kafka数据丢失问题？" class="headerlink" title="Q2：如何解决Kafka数据丢失问题？"></a>Q2：如何解决Kafka数据丢失问题？</h2><p>你可能会问：这个问题跟Q1有什么区别呢？其实一般在面试问题中可以理解成一个问题。之所以在这里做出区分，是因为两者的解决方式不一样。<strong>Q1问题是从Kafka的Broker侧来看待数据丢失的问题，而Q2是从Kafka的生产者与消费者的角度来看待数据丢失的问题</strong>。</p><p>先来看一下如何回答这个问题：主要包括两个方面：</p><blockquote><ul><li><p><strong>Producer</strong></p><ul><li><p>retries=Long.MAX_VALUE</p><p>设置 retries 为一个较大的值。这里的 retries 同样是 Producer 的参数，对应前面提到的 Producer 自动重试。当出现网络的瞬时抖动时，消息发送可能会失败，此时配置了 retries &gt; 0 的 Producer 能够自动重试消息发送，避免消息丢失。</p></li><li><p>acks=all</p><p>设置 acks = all。acks 是 Producer 的一个参数，代表了你对“已提交”消息的定义。如果设置成 all，则表明所有副本 Broker 都要接收到消息，该消息才算是“已提交”。这是最高等级的“已提交”定义。</p></li><li><p>max.in.flight.requests.per.connections=1</p><p>该参数指定了生产者在收到服务器晌应之前可以发送多少个消息。它的值越高，就会占用越多的内存，不过也会提升吞吐量。把它设为1 可以保证消息是按照发送的顺序写入服务器的，即使发生了重试。</p></li><li><p>Producer要使用带有回调通知的API，也就是说不要使用 producer.send(msg)，而要使用 producer.send(msg, callback)。</p></li><li><p>其他错误处理</p><p>使用生产者内置的重试机制，可以在不造成消息丢失的情况下轻松地处理大部分错误，不过<br>仍然需要处理其他类型的错误，例如消息大小错误、序列化错误等等。</p></li></ul></li><li><p><strong>Consumer</strong></p><ul><li><p>禁用自动提交：enable.auto.commit=false</p></li><li><p>消费者处理完消息之后再提交offset</p></li><li><p>配置auto.offset.reset</p><p>这个参数指定了在没有偏移量可提交时(比如消费者第l次启动时)或者请求的偏移量在broker上不存在时(比如数据被删了)，消费者会做些什么。</p><p>这个参数有两种配置。一种是<strong>earliest</strong>：消费者会从分区的开始位置读取数据，不管偏移量是否有效，这样会导致消费者读取大量的重复数据，但可以保证最少的数据丢失。一种是<strong>latest(默认)</strong>，如果选择了这种配置， 消费者会从分区的末尾开始读取数据，这样可以减少重复处理消息，但很有可能会错过一些消息。</p></li></ul></li></ul></blockquote><h2 id="Q3：Kafka可以保障永久不丢失数据吗？"><a href="#Q3：Kafka可以保障永久不丢失数据吗？" class="headerlink" title="Q3：Kafka可以保障永久不丢失数据吗？"></a>Q3：Kafka可以保障永久不丢失数据吗？</h2><p>上面分析了一些保障数据不丢失的措施，在一定程度上可以避免数据的丢失。但是请注意：<strong>Kafka 只对“已提交”的消息（committed message）做有限度的持久化保证</strong>。所以说，Kafka不能够完全保证数据不丢失，需要做出一些权衡。</p><p>首先，要理解什么是<strong>已提交的消息</strong>，当 Kafka 的若干个 Broker 成功地接收到一条消息并写入到日志文件后，它们会告诉生产者程序这条消息已成功提交。此时，这条消息在 Kafka 看来就正式变为<strong>已提交</strong>消息了。所以说无论是ack=all，还是ack=1,不论哪种情况，Kafka 只对已提交的消息做持久化保证这件事情是不变的。</p><p>其次，要理解<strong>有限度的持久化保证</strong>，也就是说 Kafka 不可能保证在任何情况下都做到不丢失消息。必须保证Kafka的Broker是可用的，换句话说，假如消息保存在 N 个 Kafka Broker 上，那么这个前提条件就是这 N 个 Broker 中至少有 1 个存活。只要这个条件成立，Kafka 就能保证你的这条消息永远不会丢失。</p><p>总结一下，Kafka 是能做到不丢失消息的，<strong>只不过这些消息必须是已提交的消息</strong>，而且还要满足一定的条件。</p><h2 id="Q4：如何保障Kafka中的消息是有序的？"><a href="#Q4：如何保障Kafka中的消息是有序的？" class="headerlink" title="Q4：如何保障Kafka中的消息是有序的？"></a>Q4：如何保障Kafka中的消息是有序的？</h2><p>首先需要明确的是：<strong>Kafka的主题是分区有序的</strong>，如果一个主题有多个分区，那么Kafka会按照key将其发送到对应的分区中，所以，对于给定的key，与其对应的record在分区内是有序的。</p><p>Kafka可以保证同一个分区里的消息是有序的，即生产者按照一定的顺序发送消息，Broker就会按照这个顺序将他们写入对应的分区中，同理，消费者也会按照这个顺序来消费他们。</p><p>在一些场景下，消息的顺序是非常重要的。比如，<strong>先存钱再取钱</strong>与<strong>先取钱再存钱</strong>是截然不同的两种结果。</p><p>上面的问题中提到一个参数<strong>max.in.flight.requests.per.connections=1</strong>,该参数的作用是在重试次数大于等于1时，保证数据写入的顺序。如果该参数不为1，那么当第一个批次写入失败时，第二个批次写入成功，Broker会重试写入第一个批次，如果此时第一个批次重试写入成功，那么这两个批次消息的顺序就反过来了。</p><p>一般来说，如果对消息的顺序有要求，那么在为了保障数据不丢失，需要先设置发送重试次数retries&gt;0,同时需要把<strong>max.in.flight.requests.per.connections</strong>参数设为1，这样在生产者尝试发送第一批消息时，就不会有其他的消息发送给broker，虽然会影响吞吐量，但是可以保证消息的顺序。</p><p>除此之外，还可以使用单分区的Topic，但是会严重影响吞吐量。</p><h2 id="Q5：如何确定合适的Kafka主题的分区数量？"><a href="#Q5：如何确定合适的Kafka主题的分区数量？" class="headerlink" title="Q5：如何确定合适的Kafka主题的分区数量？"></a>Q5：如何确定合适的Kafka主题的分区数量？</h2><p>选择合适的分区数量可以达到高度并行读写和负载均衡的目的，在分区上达到均衡负载是实现吞吐量的关键。需要根据每个分区的生产者和消费者的期望吞吐量进行估计。</p><p>举个栗子：假设期望读取数据的速率(吞吐量)为<strong>1GB/Sec</strong>，而一个消费者的读取速率为<strong>50MB/Sec</strong>，此时至少需要20个分区以及20个消费者(一个消费者组)。同理，如果期望生产数据的速率为<strong>1GB/Sec</strong>，而每个生产者的生产速率为<strong>100MB/Sec</strong>，此时就需要有10个分区。在这种情况下，如果设置20个分区，既可以保障<strong>1GB/Sec</strong>的生产速率，也可以保障消费者的吞吐量。通常需要将分区的数量调整为消费者或者生产者的数量，只有这样才可以同时实现生产者和消费者的吞吐量。</p><p>一个简单的计算公式为：<strong>分区数 = max(生产者数量，消费者数量)</strong></p><ul><li>生产者数量=整体生产吞吐量/每个生产者对单个分区的最大生产吞吐量</li><li>消费者数量=整体消费吞吐量/每个消费者从单个分区消费的最大吞吐量</li></ul><h2 id="Q6：如何调整生产环境中Kafka主题的分区数量？"><a href="#Q6：如何调整生产环境中Kafka主题的分区数量？" class="headerlink" title="Q6：如何调整生产环境中Kafka主题的分区数量？"></a>Q6：如何调整生产环境中Kafka主题的分区数量？</h2><p>需要注意的是：当我们增加主题的分区数量时，会违背<strong>同一个key进行同一个分区</strong>的事实。我们可以创建一个新的主题，使得该主题有更多的分区数，然后暂停生产者，将旧的主题中的数据复制到新的主题中，然后将消费者和生产者切换到新的主题，操作起来会非常棘手。</p><h2 id="Q7-如何重平衡Kafka集群？"><a href="#Q7-如何重平衡Kafka集群？" class="headerlink" title="Q7:如何重平衡Kafka集群？"></a>Q7:如何重平衡Kafka集群？</h2><p>在下面情况发生时，需要重平衡集群：</p><ul><li>主题分区在整个集群里的不均衡分布造成了集群负载的不均衡。</li><li>broker离线造成分区不同步。</li><li>新加入的broker 需要从集群里获得负载。</li></ul><p>使用<strong>kafka-reassign-partitions.sh</strong>命令进行重平衡</p><h2 id="Q8-如何查看消费者组是否存在滞后消费？"><a href="#Q8-如何查看消费者组是否存在滞后消费？" class="headerlink" title="Q8:如何查看消费者组是否存在滞后消费？"></a>Q8:如何查看消费者组是否存在滞后消费？</h2><p>我们可以使用<strong>kafka-consumer-groups.sh</strong>命令进行查看，比如：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ bin/kafka-consumer-groups.sh --bootstrap-server cdh02:9092 --describe --group my-group</span><br><span class="line"><span class="comment">## 会显示下面的一些指标信息</span></span><br><span class="line">TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET   LAG          CONSUMER-ID HOST CLIENT-ID</span><br><span class="line">主题   分区       当前offset      LEO           滞后消息数       消费者id     主机   客户端id</span><br></pre></td></tr></table></figure><p>一般情况下，如果运行良好，<strong>CURRENT-OFFSET</strong>的值会与*<em>LOG-END-OFFSET *</em>的值非常接近。通过这个命令可以查看哪个分区的消费出现了滞后。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要分享了8个常见的Kafka面试题，对于每个题目都给出了相应的答案。对照这些问题，相信会对Kafka会有更深刻的认识。</p><blockquote><p>如果觉得本文对你有所帮助，请分享、点赞、转发。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive SQL使用过程中的奇怪现象</title>
      <link href="/2020/09/07/Hive-SQL%E4%BD%BF%E7%94%A8%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E5%A5%87%E6%80%AA%E7%8E%B0%E8%B1%A1/"/>
      <url>/2020/09/07/Hive-SQL%E4%BD%BF%E7%94%A8%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E5%A5%87%E6%80%AA%E7%8E%B0%E8%B1%A1/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>hive是基于Hadoop的一个数据仓库工具，用来进行数据的ETL，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。hive能将结构化的数据文件映射为一张数据库表，并提供SQL查询功能。Hive SQL是一种类SQL语言，与关系型数据库所支持的SQL语法存在微小的差异。本文对比MySQL和Hive所支持的SQL语法，发现相同的SQL语句在Hive和MySQL中输出结果的会有所不同。</p><h2 id="两个整数除"><a href="#两个整数除" class="headerlink" title="两个整数除"></a>两个整数除</h2><p>除法是SQL引擎难以解释的算术运算。如果将两个整数相加，相减或相乘，则始终会得到一个整数。值得注意的是，如果将两个整数相除，不同的SQL查询引擎输出的结果不尽相同。在Hive和MySQL中，运算两个整数相除，输出的结果都是<strong>decimal</strong>类型。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- Hive中查询</span></span><br><span class="line"><span class="keyword">select</span> <span class="number">10</span>/<span class="number">3</span>       <span class="comment">-- 输出：3.3333333333333335</span></span><br><span class="line"><span class="comment">-- 在MySQL中查询</span></span><br><span class="line"><span class="keyword">select</span> <span class="number">10</span>/<span class="number">3</span>       <span class="comment">-- 输出：3.3333</span></span><br></pre></td></tr></table></figure><p>如果使用下面的方式，则会返回整形类型</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- Hive中查询</span></span><br><span class="line"><span class="keyword">select</span> <span class="number">10</span> <span class="keyword">div</span> <span class="number">3</span>       <span class="comment">-- 输出：3</span></span><br><span class="line"><span class="comment">-- 在MySQL中查询</span></span><br><span class="line"><span class="keyword">select</span> <span class="number">10</span> <span class="keyword">div</span> <span class="number">3</span>       <span class="comment">-- 输出：3</span></span><br></pre></td></tr></table></figure><h2 id="区分大小写"><a href="#区分大小写" class="headerlink" title="区分大小写"></a>区分大小写</h2><p>当我们比较两个字符串时，在不同的SQL引擎会产生不同的结果。需要注意的是，在字符串比较中，Apache Hive是区分大小写，看下面的例子。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- Hive中查询</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">'Bigdata'</span> = <span class="string">'bigdata'</span>   <span class="comment">-- 输出false</span></span><br><span class="line"><span class="comment">-- 在MySQL中查询</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">'Bigdata'</span> = <span class="string">'bigdata'</span>  <span class="comment">-- 输出1</span></span><br></pre></td></tr></table></figure><p>可以看出：相同的SQL语句，如果使用MySQL，则同一查询将返回<strong>1</strong>，因为在进行字符串比较时MySQL不区分大小写。这意味着只要它们具有相同的字母，MySQL便会将两个字符串解释为相同的字符串。</p><p>我们再来看一下另外一个现象，当我们把表名写成大写的，会出现什么现象呢？</p><p>这取决于所使用的SQL引擎，在引用数据库中的表时需要注意区分大小写。如果使用Hive，则在引用表时无需担心大小写，因为它们始终将字母转换为小写字母。但是在MySQL中会报<em>1146 - Table ‘XX’ doesn’t exist</em>的错误。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 假设Hive、MySQL中有一张test表</span></span><br><span class="line"><span class="comment">-- 在Hive中查询</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">Test</span>   <span class="comment">-- 正常输出结果</span></span><br><span class="line"><span class="comment">-- 在MySQL中查询</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">Test</span>   <span class="comment">-- 报错：1146 - Table 'Test' doesn't exist</span></span><br></pre></td></tr></table></figure><h2 id="在GROUP-BY中使用别名"><a href="#在GROUP-BY中使用别名" class="headerlink" title="在GROUP BY中使用别名"></a>在GROUP BY中使用别名</h2><p>假设有如下查询：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 使用别名，在Hive中查询</span></span><br><span class="line"><span class="comment">-- 报错Error while compiling statement: FAILED: SemanticException [Error 10004]: line 7:9 Invalid table alias or column reference 'inventory_status': (possible column names are: userid, visitdate, visitcount)</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">CASE</span></span><br><span class="line">           <span class="keyword">WHEN</span> visicount &gt; <span class="number">5</span> <span class="keyword">THEN</span> <span class="string">"more than 5"</span></span><br><span class="line">           <span class="keyword">ELSE</span> <span class="string">"less than 5"</span></span><br><span class="line">       <span class="keyword">END</span> <span class="keyword">AS</span> inventory_status,</span><br><span class="line">       <span class="keyword">count</span>(*) <span class="keyword">AS</span> cnt</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">test</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> inventory_status</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 不使用别名，如果使用下面的语句，则会正常输出结果</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">CASE</span></span><br><span class="line">           <span class="keyword">WHEN</span> visitcount &gt; <span class="number">5</span> <span class="keyword">THEN</span> <span class="string">"more than 5"</span></span><br><span class="line">           <span class="keyword">ELSE</span> <span class="string">"less than 5"</span></span><br><span class="line">       <span class="keyword">END</span> <span class="keyword">AS</span> inventory_status,</span><br><span class="line">       <span class="keyword">count</span>(*) <span class="keyword">AS</span> cnt</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">test</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">CASE</span></span><br><span class="line">           <span class="keyword">WHEN</span> visitcount &gt; <span class="number">5</span> <span class="keyword">THEN</span> <span class="string">"more than 5"</span></span><br><span class="line">           <span class="keyword">ELSE</span> <span class="string">"less than 5"</span></span><br><span class="line">       <span class="keyword">END</span></span><br></pre></td></tr></table></figure><p>相同的查询语句在MySQL中进行查询，会正常输出结果。</p><h2 id="非数值类型的字符串转为数值类型"><a href="#非数值类型的字符串转为数值类型" class="headerlink" title="非数值类型的字符串转为数值类型"></a>非数值类型的字符串转为数值类型</h2><p>使用SQL，我们可以使用<strong>CAST</strong>命令转换表中列的数据类型。如果要将字符串列转换为整数，可以执行以下操作。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">CAST</span>(column_name <span class="keyword">AS</span> <span class="built_in">INT</span>) <span class="keyword">FROM</span> table_name</span><br></pre></td></tr></table></figure><p>那么，如果我们将一个非数值类型的字符串转为数值类型，会出现什么样的结果呢？</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 在Hive中查询</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">cast</span>(<span class="string">"bigdata"</span> <span class="keyword">as</span> <span class="built_in">int</span>) <span class="comment">-- 返回null</span></span><br><span class="line"><span class="comment">-- 在MySQL中查询</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">cast</span>(<span class="string">"bigdata"</span> <span class="keyword">as</span> signed <span class="built_in">int</span>)  <span class="comment">-- 返回0</span></span><br></pre></td></tr></table></figure><h2 id="Hive中的视图与SQL查询语句"><a href="#Hive中的视图与SQL查询语句" class="headerlink" title="Hive中的视图与SQL查询语句"></a>Hive中的视图与SQL查询语句</h2><p>当我们在Hive中创建视图时，其底层是将视图对应的SQL语句存储到了一张表中的某个字段中，以Hive为例，其元数据中存在下面的一张表：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`TBLS`</span> (</span><br><span class="line">  <span class="string">`TBL_ID`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`CREATE_TIME`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`DB_ID`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`LAST_ACCESS_TIME`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`OWNER`</span> <span class="built_in">varchar</span>(<span class="number">767</span>) <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> latin1 <span class="keyword">COLLATE</span> latin1_bin <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`OWNER_TYPE`</span> <span class="built_in">varchar</span>(<span class="number">10</span>) <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> latin1 <span class="keyword">COLLATE</span> latin1_bin <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`RETENTION`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`SD_ID`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`TBL_NAME`</span> <span class="built_in">varchar</span>(<span class="number">128</span>) <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> latin1 <span class="keyword">COLLATE</span> latin1_bin <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`TBL_TYPE`</span> <span class="built_in">varchar</span>(<span class="number">128</span>) <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> latin1 <span class="keyword">COLLATE</span> latin1_bin <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`VIEW_EXPANDED_TEXT`</span> mediumtext <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8,</span><br><span class="line">  <span class="string">`VIEW_ORIGINAL_TEXT`</span> mediumtext <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8,</span><br><span class="line">  <span class="string">`LINK_TARGET_ID`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`TBL_ID`</span>),</span><br><span class="line">  <span class="keyword">UNIQUE</span> <span class="keyword">KEY</span> <span class="string">`UNIQUETABLE`</span> (<span class="string">`TBL_NAME`</span>,<span class="string">`DB_ID`</span>),</span><br><span class="line">  <span class="keyword">KEY</span> <span class="string">`TBLS_N50`</span> (<span class="string">`SD_ID`</span>),</span><br><span class="line">  <span class="keyword">KEY</span> <span class="string">`TBLS_N49`</span> (<span class="string">`DB_ID`</span>),</span><br><span class="line">  <span class="keyword">KEY</span> <span class="string">`TBLS_N51`</span> (<span class="string">`LINK_TARGET_ID`</span>),</span><br><span class="line">  <span class="keyword">CONSTRAINT</span> <span class="string">`TBLS_FK1`</span> <span class="keyword">FOREIGN</span> <span class="keyword">KEY</span> (<span class="string">`SD_ID`</span>) <span class="keyword">REFERENCES</span> <span class="string">`SDS`</span> (<span class="string">`SD_ID`</span>),</span><br><span class="line">  <span class="keyword">CONSTRAINT</span> <span class="string">`TBLS_FK2`</span> <span class="keyword">FOREIGN</span> <span class="keyword">KEY</span> (<span class="string">`DB_ID`</span>) <span class="keyword">REFERENCES</span> <span class="string">`DBS`</span> (<span class="string">`DB_ID`</span>),</span><br><span class="line">  <span class="keyword">CONSTRAINT</span> <span class="string">`TBLS_FK3`</span> <span class="keyword">FOREIGN</span> <span class="keyword">KEY</span> (<span class="string">`LINK_TARGET_ID`</span>) <span class="keyword">REFERENCES</span> <span class="string">`TBLS`</span> (<span class="string">`TBL_ID`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=latin1;</span><br></pre></td></tr></table></figure><p>上面的这张表存储了Hive中表和视图的元数据信息，如果创建一张视图，则<strong>VIEW_EXPANDED_TEXT字段与<br>  VIEW_ORIGINAL_TEXT字段</strong>存储了视图对应的SQL语句。</p><p>当我们使用下面的SQL语句创建视图或者直接执行时，可能会出现不一样的结果：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> <span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">test</span> <span class="keyword">where</span> <span class="keyword">name</span> <span class="keyword">like</span> <span class="string">"%大数据"</span></span><br></pre></td></tr></table></figure><p>如果是直接执行SQL语句，则会按照条件筛选出想要的结果。但是，如果是创建视图，则可能不会出现想要的结果。上面提到，视图对应的SQL语句是作为一个字段存储到Hive的元数据中的，对应其中的一张表。如上面的SQL语句，<strong>like “%大数据”</strong>中包含中文，该中文字符会出现乱码现象，即存储到表中时会变成下面的形式：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> <span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">test</span> <span class="keyword">where</span> <span class="keyword">name</span> <span class="keyword">like</span> <span class="string">"???"</span></span><br></pre></td></tr></table></figure><p>解决上面的问题很简单，只需要修改元数据中该字段的编码即可：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> <span class="string">`TBLS`</span> <span class="keyword">MODIFY</span> <span class="keyword">COLUMN</span> VIEW_EXPANDED_TEXT mediumtext <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8；</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> <span class="string">`TBLS`</span> <span class="keyword">MODIFY</span> <span class="keyword">COLUMN</span> VIEW_ORIGINAL_TEXT mediumtext <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8；</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文分享了Hive使用过程中存在的一些问题，并给出了相对应的示例，我们在使用的过程中可以留意一下这些问题，对比相同的SQL语句在MySQL和Apache Hive上的结果上的不同。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Hive SQL的窗口函数进行商务数据分析</title>
      <link href="/2020/09/07/%E4%BD%BF%E7%94%A8Hive-SQL%E7%9A%84%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E5%95%86%E5%8A%A1%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
      <url>/2020/09/07/%E4%BD%BF%E7%94%A8Hive-SQL%E7%9A%84%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E5%95%86%E5%8A%A1%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>本文会从一个商务分析案例入手，说明SQL窗口函数的使用方式。通过本文的5个需求分析，可以看出SQL窗口函数的功能十分强大，不仅能够使我们编写的SQL逻辑更加清晰，而且在某种程度上可以简化需求开发。</p><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>本文主要分析只涉及一张订单表orders，操作过程在Hive中完成，具体数据如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> orders(</span><br><span class="line">    order_id <span class="built_in">int</span>,</span><br><span class="line">    customer_id <span class="keyword">string</span>,</span><br><span class="line">    city <span class="keyword">string</span>,</span><br><span class="line">    add_time <span class="keyword">string</span>,</span><br><span class="line">    amount <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 准备数据                              </span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> orders <span class="keyword">VALUES</span></span><br><span class="line">(<span class="number">1</span>,<span class="string">"A"</span>,<span class="string">"上海"</span>,<span class="string">"2020-01-01 00:00:00.000000"</span>,<span class="number">200</span>),</span><br><span class="line">(<span class="number">2</span>,<span class="string">"B"</span>,<span class="string">"上海"</span>,<span class="string">"2020-01-05 00:00:00.000000"</span>,<span class="number">250</span>),</span><br><span class="line">(<span class="number">3</span>,<span class="string">"C"</span>,<span class="string">"北京"</span>,<span class="string">"2020-01-12 00:00:00.000000"</span>,<span class="number">200</span>),</span><br><span class="line">(<span class="number">4</span>,<span class="string">"A"</span>,<span class="string">"上海"</span>,<span class="string">"2020-02-04 00:00:00.000000"</span>,<span class="number">400</span>),</span><br><span class="line">(<span class="number">5</span>,<span class="string">"D"</span>,<span class="string">"上海"</span>,<span class="string">"2020-02-05 00:00:00.000000"</span>,<span class="number">250</span>),</span><br><span class="line">(<span class="number">5</span>,<span class="string">"D"</span>,<span class="string">"上海"</span>,<span class="string">"2020-02-05 12:00:00.000000"</span>,<span class="number">300</span>),</span><br><span class="line">(<span class="number">6</span>,<span class="string">"C"</span>,<span class="string">"北京"</span>,<span class="string">"2020-02-19 00:00:00.000000"</span>,<span class="number">300</span>),</span><br><span class="line">(<span class="number">7</span>,<span class="string">"A"</span>,<span class="string">"上海"</span>,<span class="string">"2020-03-01 00:00:00.000000"</span>,<span class="number">150</span>),</span><br><span class="line">(<span class="number">8</span>,<span class="string">"E"</span>,<span class="string">"北京"</span>,<span class="string">"2020-03-05 00:00:00.000000"</span>,<span class="number">500</span>),</span><br><span class="line">(<span class="number">9</span>,<span class="string">"F"</span>,<span class="string">"上海"</span>,<span class="string">"2020-03-09 00:00:00.000000"</span>,<span class="number">250</span>),</span><br><span class="line">(<span class="number">10</span>,<span class="string">"B"</span>,<span class="string">"上海"</span>,<span class="string">"2020-03-21 00:00:00.000000"</span>,<span class="number">600</span>);</span><br></pre></td></tr></table></figure><h2 id="需求1：收入增长"><a href="#需求1：收入增长" class="headerlink" title="需求1：收入增长"></a>需求1：收入增长</h2><p>在业务方面，第m1个月的收入增长计算如下：<strong>100 *（m1-m0）/ m0</strong></p><p>其中，<em>m1</em>是给定月份的收入，<em>m0</em>是上个月的收入。因此，从技术上讲，我们需要找到每个月的收入，然后以某种方式将每个月的收入与上一个收入相关联，以便进行上述计算。计算当时如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">WITH</span></span><br><span class="line">monthly_revenue <span class="keyword">as</span> (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">    trunc(add_time,<span class="string">'MM'</span>) <span class="keyword">as</span> <span class="keyword">month</span>,</span><br><span class="line">    <span class="keyword">sum</span>(amount) <span class="keyword">as</span> revenue</span><br><span class="line">    <span class="keyword">FROM</span> orders</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="number">1</span></span><br><span class="line">)</span><br><span class="line">,prev_month_revenue <span class="keyword">as</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> </span><br><span class="line">    <span class="keyword">month</span>,</span><br><span class="line">    revenue,</span><br><span class="line">    lag(revenue) <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">month</span>) <span class="keyword">as</span> prev_month_revenue <span class="comment">-- 上一月收入</span></span><br><span class="line">    <span class="keyword">FROM</span> monthly_revenue</span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">  <span class="keyword">month</span>,</span><br><span class="line">  revenue,</span><br><span class="line">  prev_month_revenue,</span><br><span class="line">  <span class="keyword">round</span>(<span class="number">100.0</span>*(revenue-prev_month_revenue)/prev_month_revenue,<span class="number">1</span>) <span class="keyword">as</span> revenue_growth</span><br><span class="line"><span class="keyword">FROM</span> prev_month_revenue</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p><strong>结果输出</strong></p><table><thead><tr><th align="left">month</th><th align="left">revenue</th><th align="left">prev_month_revenue</th><th align="left">revenue_growth</th></tr></thead><tbody><tr><td align="left">2020-01-01</td><td align="left">650</td><td align="left">NULL</td><td align="left">NULL</td></tr><tr><td align="left">2020-02-01</td><td align="left">1250</td><td align="left">650</td><td align="left">92.3</td></tr><tr><td align="left">2020-03-01</td><td align="left">1500</td><td align="left">1250</td><td align="left">20</td></tr></tbody></table><p>我们还可以按照按城市分组进行统计，查看某个城市某个月份的收入增长情况</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">WITH</span></span><br><span class="line">monthly_revenue <span class="keyword">as</span> (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">     trunc(add_time,<span class="string">'MM'</span>) <span class="keyword">as</span> <span class="keyword">month</span>,</span><br><span class="line">    city,</span><br><span class="line">    <span class="keyword">sum</span>(amount) <span class="keyword">as</span> revenue</span><br><span class="line">    <span class="keyword">FROM</span> orders</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="number">1</span>,<span class="number">2</span></span><br><span class="line">)</span><br><span class="line">,prev_month_revenue <span class="keyword">as</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> </span><br><span class="line">    <span class="keyword">month</span>,</span><br><span class="line">    city,</span><br><span class="line">    revenue,</span><br><span class="line">    lag(revenue) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> city <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">month</span>) <span class="keyword">as</span> prev_month_revenue</span><br><span class="line">    <span class="keyword">FROM</span> monthly_revenue</span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line"><span class="keyword">month</span>,</span><br><span class="line">city,</span><br><span class="line">revenue,</span><br><span class="line"><span class="keyword">round</span>(<span class="number">100.0</span>*(revenue-prev_month_revenue)/prev_month_revenue,<span class="number">1</span>) <span class="keyword">as</span> revenue_growth</span><br><span class="line"><span class="keyword">FROM</span> prev_month_revenue</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="number">2</span>,<span class="number">1</span></span><br></pre></td></tr></table></figure><p><strong>结果输出</strong></p><table><thead><tr><th align="left">month</th><th align="left">city</th><th align="left">revenue</th><th align="left">revenue_growth</th></tr></thead><tbody><tr><td align="left">2020-01-01</td><td align="left">上海</td><td align="left">450</td><td align="left">NULL</td></tr><tr><td align="left">2020-02-01</td><td align="left">上海</td><td align="left">950</td><td align="left">111.1</td></tr><tr><td align="left">2020-03-01</td><td align="left">上海</td><td align="left">1000</td><td align="left">5.3</td></tr><tr><td align="left">2020-01-01</td><td align="left">北京</td><td align="left">200</td><td align="left">NULL</td></tr><tr><td align="left">2020-02-01</td><td align="left">北京</td><td align="left">300</td><td align="left">50</td></tr><tr><td align="left">2020-03-01</td><td align="left">北京</td><td align="left">500</td><td align="left">66.7</td></tr></tbody></table><h2 id="需求2：累计求和"><a href="#需求2：累计求和" class="headerlink" title="需求2：累计求和"></a>需求2：累计求和</h2><p>累计汇总，即当前元素和所有先前元素的总和，如下面的SQL：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">WITH</span></span><br><span class="line">monthly_revenue <span class="keyword">as</span> (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">    trunc(add_time,<span class="string">'MM'</span>) <span class="keyword">as</span> <span class="keyword">month</span>,</span><br><span class="line">    <span class="keyword">sum</span>(amount) <span class="keyword">as</span> revenue</span><br><span class="line">    <span class="keyword">FROM</span> orders</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="number">1</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line"><span class="keyword">month</span>,</span><br><span class="line">revenue,</span><br><span class="line"><span class="keyword">sum</span>(revenue) <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">month</span> <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">unbounded</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> running_total</span><br><span class="line"><span class="keyword">FROM</span> monthly_revenue</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p><strong>结果输出</strong></p><table><thead><tr><th align="left">month</th><th align="left">revenue</th><th align="left">running_total</th></tr></thead><tbody><tr><td align="left">2020-01-01</td><td align="left">650</td><td align="left">650</td></tr><tr><td align="left">2020-02-01</td><td align="left">1250</td><td align="left">1900</td></tr><tr><td align="left">2020-03-01</td><td align="left">1500</td><td align="left">3400</td></tr></tbody></table><p>我们还可以使用下面的组合方式进行分析，SQL如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">   order_id,</span><br><span class="line">   customer_id,</span><br><span class="line">   city,</span><br><span class="line">   add_time,</span><br><span class="line">   amount,</span><br><span class="line">   <span class="keyword">sum</span>(amount) <span class="keyword">over</span> () <span class="keyword">as</span> amount_total, <span class="comment">-- 所有数据求和</span></span><br><span class="line">   <span class="keyword">sum</span>(amount) <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> order_id <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">unbounded</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> running_sum, <span class="comment">-- 累计求和</span></span><br><span class="line">   <span class="keyword">sum</span>(amount) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> customer_id <span class="keyword">order</span> <span class="keyword">by</span> add_time <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">unbounded</span>    <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> running_sum_by_customer, </span><br><span class="line">   <span class="keyword">avg</span>(amount) <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> add_time <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">5</span> <span class="keyword">preceding</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span>  trailing_avg <span class="comment">-- 滚动求平均</span></span><br><span class="line"><span class="keyword">FROM</span> orders</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p><strong>结果输出</strong>：</p><table><thead><tr><th align="left">order_id</th><th align="left">customer_id</th><th align="left">city</th><th align="left">add_time</th><th align="left">amount</th><th align="left">amount_total</th><th align="left">running_sum</th><th align="left">running_sum_by_customer</th><th align="left">trailing_avg</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">A</td><td align="left">上海</td><td align="left">2020-01-01 00:00:00.000000</td><td align="left">200</td><td align="left">3400</td><td align="left">200</td><td align="left">200</td><td align="left">200</td></tr><tr><td align="left">2</td><td align="left">B</td><td align="left">上海</td><td align="left">2020-01-05 00:00:00.000000</td><td align="left">250</td><td align="left">3400</td><td align="left">450</td><td align="left">250</td><td align="left">225</td></tr><tr><td align="left">3</td><td align="left">C</td><td align="left">北京</td><td align="left">2020-01-12 00:00:00.000000</td><td align="left">200</td><td align="left">3400</td><td align="left">650</td><td align="left">200</td><td align="left">216.666667</td></tr><tr><td align="left">4</td><td align="left">A</td><td align="left">上海</td><td align="left">2020-02-04 00:00:00.000000</td><td align="left">400</td><td align="left">3400</td><td align="left">1050</td><td align="left">600</td><td align="left">262.5</td></tr><tr><td align="left">5</td><td align="left">D</td><td align="left">上海</td><td align="left">2020-02-05 00:00:00.000000</td><td align="left">250</td><td align="left">3400</td><td align="left">1300</td><td align="left">250</td><td align="left">260</td></tr><tr><td align="left">5</td><td align="left">D</td><td align="left">上海</td><td align="left">2020-02-05 12:00:00.000000</td><td align="left">300</td><td align="left">3400</td><td align="left">1600</td><td align="left">550</td><td align="left">266.666667</td></tr><tr><td align="left">6</td><td align="left">C</td><td align="left">北京</td><td align="left">2020-02-19 00:00:00.000000</td><td align="left">300</td><td align="left">3400</td><td align="left">1900</td><td align="left">500</td><td align="left">283.333333</td></tr><tr><td align="left">7</td><td align="left">A</td><td align="left">上海</td><td align="left">2020-03-01 00:00:00.000000</td><td align="left">150</td><td align="left">3400</td><td align="left">2050</td><td align="left">750</td><td align="left">266.666667</td></tr><tr><td align="left">8</td><td align="left">E</td><td align="left">北京</td><td align="left">2020-03-05 00:00:00.000000</td><td align="left">500</td><td align="left">3400</td><td align="left">2550</td><td align="left">500</td><td align="left">316.666667</td></tr><tr><td align="left">9</td><td align="left">F</td><td align="left">上海</td><td align="left">2020-03-09 00:00:00.000000</td><td align="left">250</td><td align="left">3400</td><td align="left">2800</td><td align="left">250</td><td align="left">291.666667</td></tr><tr><td align="left">10</td><td align="left">B</td><td align="left">上海</td><td align="left">2020-03-21 00:00:00.000000</td><td align="left">600</td><td align="left">3400</td><td align="left">3400</td><td align="left">850</td><td align="left"></td></tr></tbody></table><h2 id="需求3：处理重复数据"><a href="#需求3：处理重复数据" class="headerlink" title="需求3：处理重复数据"></a>需求3：处理重复数据</h2><p>从上面的数据可以看出，存在两条重复的数据<strong>(5,”D”,”上海”,”2020-02-05 00:00:00.000000”,250),<br>(5,”D”,”上海”,”2020-02-05 12:00:00.000000”,300),</strong>显然需要对其进行清洗去重，保留最新的一条数据，SQL如下：</p><p>我们先进行分组排名，然后保留最新的那条数据即可：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> *,</span><br><span class="line">    row_number() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> order_id <span class="keyword">order</span> <span class="keyword">by</span> add_time <span class="keyword">desc</span>) <span class="keyword">as</span> <span class="keyword">rank</span></span><br><span class="line">    <span class="keyword">FROM</span> orders</span><br><span class="line">) t</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">rank</span>=<span class="number">1</span></span><br></pre></td></tr></table></figure><p><strong>结果输出</strong>：</p><table><thead><tr><th align="left">t.order_id</th><th align="left">t.customer_id</th><th align="left">t.city</th><th align="left">t.add_time</th><th align="left">t.amount</th><th align="left">t.rank</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">A</td><td align="left">上海</td><td align="left">2020-01-01 00:00:00.000000</td><td align="left">200</td><td align="left">1</td></tr><tr><td align="left">2</td><td align="left">B</td><td align="left">上海</td><td align="left">2020-01-05 00:00:00.000000</td><td align="left">250</td><td align="left">1</td></tr><tr><td align="left">3</td><td align="left">C</td><td align="left">北京</td><td align="left">2020-01-12 00:00:00.000000</td><td align="left">200</td><td align="left">1</td></tr><tr><td align="left">4</td><td align="left">A</td><td align="left">上海</td><td align="left">2020-02-04 00:00:00.000000</td><td align="left">400</td><td align="left">1</td></tr><tr><td align="left">5</td><td align="left">D</td><td align="left">上海</td><td align="left">2020-02-05 12:00:00.000000</td><td align="left">300</td><td align="left">1</td></tr><tr><td align="left">6</td><td align="left">C</td><td align="left">北京</td><td align="left">2020-02-19 00:00:00.000000</td><td align="left">300</td><td align="left">1</td></tr><tr><td align="left">7</td><td align="left">A</td><td align="left">上海</td><td align="left">2020-03-01 00:00:00.000000</td><td align="left">150</td><td align="left">1</td></tr><tr><td align="left">8</td><td align="left">E</td><td align="left">北京</td><td align="left">2020-03-05 00:00:00.000000</td><td align="left">500</td><td align="left">1</td></tr><tr><td align="left">9</td><td align="left">F</td><td align="left">上海</td><td align="left">2020-03-09 00:00:00.000000</td><td align="left">250</td><td align="left">1</td></tr><tr><td align="left">10</td><td align="left">B</td><td align="left">上海</td><td align="left">2020-03-21 00:00:00.000000</td><td align="left">600</td><td align="left">1</td></tr></tbody></table><p>经过上面的清洗过程，对数据进行了去重。重新计算上面的需求1，正确SQL脚本为：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">WITH</span></span><br><span class="line">orders_cleaned <span class="keyword">as</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> *</span><br><span class="line">    <span class="keyword">FROM</span> (</span><br><span class="line">        <span class="keyword">SELECT</span> *,</span><br><span class="line">        row_number() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> order_id <span class="keyword">order</span> <span class="keyword">by</span> add_time <span class="keyword">desc</span>) <span class="keyword">as</span> <span class="keyword">rank</span></span><br><span class="line">        <span class="keyword">FROM</span> orders</span><br><span class="line">    )t</span><br><span class="line">    <span class="keyword">WHERE</span> <span class="keyword">rank</span>=<span class="number">1</span></span><br><span class="line">)</span><br><span class="line">,monthly_revenue <span class="keyword">as</span> (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">    trunc(add_time,<span class="string">'MM'</span>) <span class="keyword">as</span> <span class="keyword">month</span>,</span><br><span class="line">    <span class="keyword">sum</span>(amount) <span class="keyword">as</span> revenue</span><br><span class="line">    <span class="keyword">FROM</span> orders_cleaned</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="number">1</span></span><br><span class="line">)</span><br><span class="line">,prev_month_revenue <span class="keyword">as</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> </span><br><span class="line">    <span class="keyword">month</span>,</span><br><span class="line">    revenue,</span><br><span class="line">    lag(revenue) <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">month</span>) <span class="keyword">as</span> prev_month_revenue</span><br><span class="line">    <span class="keyword">FROM</span> monthly_revenue</span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line"><span class="keyword">month</span>,</span><br><span class="line">revenue,</span><br><span class="line"><span class="keyword">round</span>(<span class="number">100.0</span>*(revenue-prev_month_revenue)/prev_month_revenue,<span class="number">1</span>) <span class="keyword">as</span> revenue_growth</span><br><span class="line"><span class="keyword">FROM</span> prev_month_revenue</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p><strong>结果输出</strong>：</p><table><thead><tr><th align="left">month</th><th align="left">revenue</th><th align="left">revenue_growth</th></tr></thead><tbody><tr><td align="left">2020-01-01</td><td align="left">650</td><td align="left">NULL</td></tr><tr><td align="left">2020-02-01</td><td align="left">1000</td><td align="left">53.8</td></tr><tr><td align="left">2020-03-01</td><td align="left">1500</td><td align="left">50</td></tr></tbody></table><p>将清洗后的数据创建成视图，方便以后使用</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> orders_cleaned <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    order_id, </span><br><span class="line">    customer_id, </span><br><span class="line">    city, </span><br><span class="line">    add_time, </span><br><span class="line">    amount</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> *,</span><br><span class="line">    row_number() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> order_id <span class="keyword">order</span> <span class="keyword">by</span> add_time <span class="keyword">desc</span>) <span class="keyword">as</span> <span class="keyword">rank</span></span><br><span class="line">    <span class="keyword">FROM</span> orders</span><br><span class="line">)t</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">rank</span>=<span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="需求4：分组取TopN"><a href="#需求4：分组取TopN" class="headerlink" title="需求4：分组取TopN"></a>需求4：分组取TopN</h2><p>分组取topN是最长见的SQL窗口函数使用场景，下面的SQL是计算每个月份的top2订单金额，如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">WITH</span> orders_ranked <span class="keyword">as</span> (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">    trunc(add_time,<span class="string">'MM'</span>) <span class="keyword">as</span> <span class="keyword">month</span>,</span><br><span class="line">    *,</span><br><span class="line">    row_number() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> trunc(add_time,<span class="string">'MM'</span>) <span class="keyword">order</span> <span class="keyword">by</span> amount <span class="keyword">desc</span>, add_time) <span class="keyword">as</span> <span class="keyword">rank</span></span><br><span class="line">    <span class="keyword">FROM</span> orders_cleaned</span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    <span class="keyword">month</span>,</span><br><span class="line">    order_id,</span><br><span class="line">    customer_id,</span><br><span class="line">    city,</span><br><span class="line">    add_time,</span><br><span class="line">    amount</span><br><span class="line"><span class="keyword">FROM</span> orders_ranked</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">rank</span> &lt;=<span class="number">2</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="需求5：重复购买行为"><a href="#需求5：重复购买行为" class="headerlink" title="需求5：重复购买行为"></a>需求5：重复购买行为</h2><p>下面的SQL计算重复购买率：重复购买的人数/总人数*100%以及第一笔订单金额与第二笔订单金额之间的典型差额:avg(第二笔订单金额/第一笔订单金额)</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">WITH</span> customer_orders <span class="keyword">as</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> *,</span><br><span class="line">    row_number() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> customer_id <span class="keyword">order</span> <span class="keyword">by</span> add_time) <span class="keyword">as</span> customer_order_n,</span><br><span class="line">    lag(amount) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> customer_id <span class="keyword">order</span> <span class="keyword">by</span> add_time) <span class="keyword">as</span> prev_order_amount</span><br><span class="line">    <span class="keyword">FROM</span> orders_cleaned</span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line"><span class="keyword">round</span>(<span class="number">100.0</span>*<span class="keyword">sum</span>(<span class="keyword">case</span> <span class="keyword">when</span> customer_order_n=<span class="number">2</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)/<span class="keyword">count</span>(<span class="keyword">distinct</span> customer_id),<span class="number">1</span>) <span class="keyword">as</span> repeat_purchases,<span class="comment">-- 重复购买率</span></span><br><span class="line"><span class="keyword">avg</span>(<span class="keyword">case</span> <span class="keyword">when</span> customer_order_n=<span class="number">2</span> <span class="keyword">then</span> <span class="number">1.0</span>*amount/prev_order_amount <span class="keyword">end</span>) <span class="keyword">as</span> revenue_expansion <span class="comment">-- 重复购买较上次购买差异，第一笔订单金额与第二笔订单金额之间的典型差额</span></span><br><span class="line"><span class="keyword">FROM</span> customer_orders</span><br></pre></td></tr></table></figure><p><strong>结果输出</strong>：</p><p>WITH结果输出：</p><table><thead><tr><th align="left">orders_cleaned.order_id</th><th align="left">orders_cleaned.customer_id</th><th align="left">orders_cleaned.city</th><th align="left">orders_cleaned.add_time</th><th align="left">orders_cleaned.amount</th><th align="left">customer_order_n</th><th align="left">prev_order_amount</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">A</td><td align="left">上海</td><td align="left">2020-01-01 00:00:00.000000</td><td align="left">200</td><td align="left">1</td><td align="left">NULL</td></tr><tr><td align="left">4</td><td align="left">A</td><td align="left">上海</td><td align="left">2020-02-04 00:00:00.000000</td><td align="left">400</td><td align="left">2</td><td align="left">200</td></tr><tr><td align="left">7</td><td align="left">A</td><td align="left">上海</td><td align="left">2020-03-01 00:00:00.000000</td><td align="left">150</td><td align="left">3</td><td align="left">400</td></tr><tr><td align="left">2</td><td align="left">B</td><td align="left">上海</td><td align="left">2020-01-05 00:00:00.000000</td><td align="left">250</td><td align="left">1</td><td align="left">NULL</td></tr><tr><td align="left">10</td><td align="left">B</td><td align="left">上海</td><td align="left">2020-03-21 00:00:00.000000</td><td align="left">600</td><td align="left">2</td><td align="left">250</td></tr><tr><td align="left">3</td><td align="left">C</td><td align="left">北京</td><td align="left">2020-01-12 00:00:00.000000</td><td align="left">200</td><td align="left">1</td><td align="left">NULL</td></tr><tr><td align="left">6</td><td align="left">C</td><td align="left">北京</td><td align="left">2020-02-19 00:00:00.000000</td><td align="left">300</td><td align="left">2</td><td align="left">200</td></tr><tr><td align="left">5</td><td align="left">D</td><td align="left">上海</td><td align="left">2020-02-05 12:00:00.000000</td><td align="left">300</td><td align="left">1</td><td align="left">NULL</td></tr><tr><td align="left">8</td><td align="left">E</td><td align="left">北京</td><td align="left">2020-03-05 00:00:00.000000</td><td align="left">500</td><td align="left">1</td><td align="left">NULL</td></tr><tr><td align="left">9</td><td align="left">F</td><td align="left">上海</td><td align="left">2020-03-09 00:00:00.000000</td><td align="left">250</td><td align="left"></td><td align="left"></td></tr></tbody></table><p>最终结果输出：</p><table><thead><tr><th align="left">repeat_purchases</th><th align="left">revenue_expansion</th></tr></thead><tbody><tr><td align="left">50</td><td align="left">1.9666666666666668</td></tr></tbody></table><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要分享了SQL窗口函数的基本使用方式以及使用场景，并结合了具体的分析案例。通过本文的分析案例，可以加深对SQL窗口函数的理解。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>项目实践|基于Flink的用户行为日志分析系统</title>
      <link href="/2020/08/29/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5-%E5%9F%BA%E4%BA%8EFlink%E7%9A%84%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F/"/>
      <url>/2020/08/29/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5-%E5%9F%BA%E4%BA%8EFlink%E7%9A%84%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>用户行为日志分析是实时数据处理很常见的一个应用场景，比如常见的PV、UV统计。本文将基于Flink从0到1构建一个用户行为日志分析系统，包括架构设计与代码实现。本文分享将完整呈现日志分析系统的数据处理链路，通过本文，你可以了解到：</p><ul><li>基于discuz搭建一个论坛平台</li><li>Flume日志收集系统使用方式</li><li>Apache日志格式分析</li><li>Flume与Kafka集成</li><li>日志分析处理流程</li><li>架构设计与完整的代码实现</li></ul><h2 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h2><p>本文分享会从0到1基于Flink实现一个实时的用户行为日志分析系统，基本架构图如下：</p><p><img src="//jiamaoxiang.top/2020/08/29/项目实践-基于Flink的用户行为日志分析系统/%E6%97%A5%E5%BF%97%E6%9E%B6%E6%9E%84.png" alt></p><p>首先会先搭建一个论坛平台，对论坛平台产生的用户点击日志进行分析。然后使用Flume日志收集系统对产生的Apache日志进行收集，并将其推送到Kafka。接着我们使用Flink对日志进行实时分析处理，将处理之后的结果写入MySQL供前端应用可视化展示。本文主要实现以下三个指标计算：</p><ul><li>统计热门板块，即访问量最高的板块</li><li>统计热门文章，即访问量最高的帖子文章</li><li>统计不同客户端对版块和文章的总访问量</li></ul><h2 id="基于discuz搭建一个论坛平台"><a href="#基于discuz搭建一个论坛平台" class="headerlink" title="基于discuz搭建一个论坛平台"></a>基于discuz搭建一个论坛平台</h2><h3 id="安装XAMPP"><a href="#安装XAMPP" class="headerlink" title="安装XAMPP"></a>安装XAMPP</h3><ul><li>下载</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://www.apachefriends.org/xampp-files/5.6.33/xampp-linux-x64-5.6.33-0-installer.run</span><br></pre></td></tr></table></figure><ul><li>安装</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 赋予文件执行权限</span></span><br><span class="line">chmod u+x xampp-linux-x64-5.6.33-0-installer.run</span><br><span class="line"><span class="comment"># 运行安装文件</span></span><br><span class="line">./xampp-linux-x64-5.6.33-0-installer.run</span><br></pre></td></tr></table></figure><ul><li><p>配置环境变量</p><p>将以下内容加入到 ~/.bash_profile </p></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> XAMPP=/opt/lampp/</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$XAMPP</span>:<span class="variable">$XAMPP</span>/bin</span><br></pre></td></tr></table></figure><ul><li>刷新环境变量</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bash_profile</span><br></pre></td></tr></table></figure><ul><li>启动XAMPP</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xampp restart</span><br></pre></td></tr></table></figure><ul><li>MySQL的root用户密码和权限修改</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#修改root用户密码为123qwe </span></span><br><span class="line">update mysql.user <span class="built_in">set</span> password=PASSWORD(<span class="string">'123qwe'</span>) <span class="built_in">where</span> user=<span class="string">'root'</span>; </span><br><span class="line">flush privileges;  </span><br><span class="line"><span class="comment">#赋予root用户远程登录权限 </span></span><br><span class="line">grant all privileges on *.* to <span class="string">'root'</span>@<span class="string">'%'</span> identified by <span class="string">'123qwe'</span> with grant option;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><h3 id="安装Discuz"><a href="#安装Discuz" class="headerlink" title="安装Discuz"></a>安装Discuz</h3><ul><li>下载discuz</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget http://download.comsenz.com/DiscuzX/3.2/Discuz_X3.2_SC_UTF8.zip</span><br></pre></td></tr></table></figure><ul><li>安装</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#删除原有的web应用  </span></span><br><span class="line">rm -rf /opt/lampp/htdocs/*</span><br><span class="line">unzip Discuz_X3.2_SC_UTF8.zip –d /opt/lampp/htdocs/</span><br><span class="line"><span class="built_in">cd</span> /opt/lampp/htdocs/  </span><br><span class="line">mv upload/*   </span><br><span class="line"><span class="comment">#修改目录权限 </span></span><br><span class="line">chmod 777 -R /opt/lampp/htdocs/config/</span><br><span class="line">chmod 777 -R /opt/lampp/htdocs/data/</span><br><span class="line">chmod 777 -R /opt/lampp/htdocs/uc_client/  </span><br><span class="line">chmod 777 -R /opt/lampp/htdocs/uc_server/</span><br></pre></td></tr></table></figure><h3 id="Discuz基本操作"><a href="#Discuz基本操作" class="headerlink" title="Discuz基本操作"></a>Discuz基本操作</h3><ul><li>自定义版块  </li><li>进入discuz后台：<a href="http://kms-4/admin.php" target="_blank" rel="noopener">http://kms-4/admin.php</a>  </li><li>点击顶部的<strong>论坛</strong>菜单  </li><li>按照页面提示创建所需版本，可以创建父子版块  </li></ul><img src="//jiamaoxiang.top/2020/08/29/项目实践-基于Flink的用户行为日志分析系统/板块.png" style="zoom: 50%;"><h3 id="Discuz帖子-版块存储数据库表介"><a href="#Discuz帖子-版块存储数据库表介" class="headerlink" title="Discuz帖子/版块存储数据库表介"></a>Discuz帖子/版块存储数据库表介</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 登录ultrax数据库</span></span><br><span class="line">mysql -uroot -p123 ultrax </span><br><span class="line"><span class="comment">-- 查看包含帖子id及标题对应关系的表</span></span><br><span class="line"><span class="comment">-- tid, subject（文章id、标题）</span></span><br><span class="line"><span class="keyword">select</span> tid, subject <span class="keyword">from</span> pre_forum_post <span class="keyword">limit</span> <span class="number">10</span>;</span><br><span class="line"><span class="comment">-- fid, name（版块id、标题）</span></span><br><span class="line"><span class="keyword">select</span> fid, <span class="keyword">name</span> <span class="keyword">from</span> pre_forum_forum <span class="keyword">limit</span> <span class="number">40</span>;</span><br></pre></td></tr></table></figure><p>当我们在各个板块添加帖子之后，如下所示：</p><img src="//jiamaoxiang.top/2020/08/29/项目实践-基于Flink的用户行为日志分析系统/帖子.png" style="zoom:50%;"><h3 id="修改日志格式"><a href="#修改日志格式" class="headerlink" title="修改日志格式"></a>修改日志格式</h3><ul><li>查看访问日志</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 日志默认地址  </span></span><br><span class="line">/opt/lampp/logs/access_log </span><br><span class="line"><span class="comment"># 实时查看日志命令  </span></span><br><span class="line">tail –f /opt/lampp/logs/access_log</span><br></pre></td></tr></table></figure><ul><li>修改日志格式</li></ul><p>Apache配置文件名称为httpd.conf，完整路径为<code>/opt/lampp/etc/httpd.conf</code>。由于默认的日志类型为<strong>common</strong>类型，总共有7个字段。为了获取更多的日志信息，我们需要将其格式修改为<strong>combined</strong>格式，该日志格式共有9个字段。修改方式如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启用组合日志文件</span></span><br><span class="line">CustomLog <span class="string">"logs/access_log"</span> combined</span><br></pre></td></tr></table></figure><img src="//jiamaoxiang.top/2020/08/29/项目实践-基于Flink的用户行为日志分析系统/日志格式.png" style="zoom:50%;"><ul><li>重新加载配置文件 </li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xampp reload</span><br></pre></td></tr></table></figure><h3 id="Apache日志格式介绍"><a href="#Apache日志格式介绍" class="headerlink" title="Apache日志格式介绍"></a>Apache日志格式介绍</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">192.168.10.1 - - [30/Aug/2020:15:53:15 +0800] <span class="string">"GET /forum.php?mod=forumdisplay&amp;fid=43 HTTP/1.1"</span> 200 30647 <span class="string">"http://kms-4/forum.php"</span> <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36"</span></span><br></pre></td></tr></table></figure><p>上面的日志格式共有9个字段，分别用空格隔开。每个字段的具体含义如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">192.168.10.1 <span class="comment">##(1)客户端的IP地址</span></span><br><span class="line">- <span class="comment">## (2)客户端identity标识,该字段为"-"</span></span><br><span class="line">- <span class="comment">## (3)客户端userid标识,该字段为"-"</span></span><br><span class="line">[30/Aug/2020:15:53:15 +0800] <span class="comment">## (4)服务器完成请求处理时的时间</span></span><br><span class="line"><span class="string">"GET /forum.php?mod=forumdisplay&amp;fid=43 HTTP/1.1"</span> <span class="comment">## (5)请求类型 请求的资源 使用的协议</span></span><br><span class="line">200 <span class="comment">## (6)服务器返回给客户端的状态码，200表示成功</span></span><br><span class="line">30647 <span class="comment">## (7)返回给客户端不包括响应头的字节数，如果没有信息返回，则此项应该是"-"</span></span><br><span class="line"><span class="string">"http://kms-4/forum.php"</span> <span class="comment">## (8)Referer请求头</span></span><br><span class="line"><span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36"</span> <span class="comment">## (9)客户端的浏览器信息</span></span><br></pre></td></tr></table></figure><p>关于上面的日志格式，可以使用正则表达式进行匹配：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(\d&#123;1,3&#125;\.\d&#123;1,3&#125;\.\d&#123;1,3&#125;\.\d&#123;1,3&#125;) (\S+) (\S+) (\[.+?\]) (\"(.*?)\") (\d&#123;3&#125;) (\S+) (\"(.*?)\") (\"(.*?)\")</span><br></pre></td></tr></table></figure><h2 id="Flume与Kafka集成"><a href="#Flume与Kafka集成" class="headerlink" title="Flume与Kafka集成"></a>Flume与Kafka集成</h2><p>本文使用Flume对产生的Apache日志进行收集，然后推送至Kafka。需要启动Flume agent对日志进行收集，对应的配置文件如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># agent的名称为a1</span></span><br><span class="line">a1.sources = source1</span><br><span class="line">a1.channels = channel1</span><br><span class="line">a1.sinks = sink1</span><br><span class="line"></span><br><span class="line"><span class="comment"># set source</span></span><br><span class="line">a1.sources.source1.type = TAILDIR</span><br><span class="line">a1.sources.source1.filegroups = f1</span><br><span class="line">a1.sources.source1.filegroups.f1 = /opt/lampp/logs/access_log</span><br><span class="line">a1sources.source1.fileHeader = flase</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置sink</span></span><br><span class="line">a1.sinks.sink1.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.sink1.brokerList=kms-2:9092,kms-3:9092,kms-4:9092</span><br><span class="line">a1.sinks.sink1.topic= user_access_logs</span><br><span class="line">a1.sinks.sink1.kafka.flumeBatchSize = 20</span><br><span class="line">a1.sinks.sink1.kafka.producer.acks = 1</span><br><span class="line">a1.sinks.sink1.kafka.producer.linger.ms = 1</span><br><span class="line">a1.sinks.sink1.kafka.producer.compression.type = snappy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置channel</span></span><br><span class="line">a1.channels.channel1.type = file</span><br><span class="line">a1.channels.channel1.checkpointDir = /home/kms/data/flume_data/checkpoint</span><br><span class="line">a1.channels.channel1.dataDirs= /home/kms/data/flume_data/data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置bind</span></span><br><span class="line">a1.sources.source1.channels = channel1</span><br><span class="line">a1.sinks.sink1.channel = channel1</span><br></pre></td></tr></table></figure><blockquote><p>知识点：</p><p><strong>Taildir Source</strong>相比<strong>Exec Source</strong>、<strong>Spooling Directory Source</strong>的优势是什么？</p><p><strong>TailDir Source</strong>：断点续传、多目录。Flume1.6以前需要自己自定义Source记录每次读取文件位置,实现断点续传</p><p><strong>Exec Source</strong>：可以实时收集数据,但是在Flume不运行或者Shell命令出错的情况下,数据将会丢失</p><p><strong>Spooling Directory Source</strong>：监控目录,不支持断点续传</p></blockquote><p>值得注意的是，上面的配置是直接将原始日志push到Kafka。除此之外，我们还可以自定义Flume的拦截器对原始日志先进行过滤处理，同时也可以实现将不同的日志push到Kafka的不同Topic中。</p><h3 id="启动Flume-Agent"><a href="#启动Flume-Agent" class="headerlink" title="启动Flume Agent"></a>启动Flume Agent</h3><p>将启动Agent的命令封装成shell脚本:*<em>start-log-collection.sh *</em>,脚本内容如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">echo "start log agent !!!"</span><br><span class="line">/opt/modules/apache-flume-1.9.0-bin/bin/flume-ng agent --conf-file /opt/modules/apache-flume-1.9.0-bin/conf/log_collection.conf --name a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><h3 id="查看push到Kafka的日志数据"><a href="#查看push到Kafka的日志数据" class="headerlink" title="查看push到Kafka的日志数据"></a>查看push到Kafka的日志数据</h3><p>将控制台消费者命令封装成shell脚本：<strong>kafka-consumer.sh</strong>，脚本内容如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"kafka consumer "</span></span><br><span class="line">bin/kafka-console-consumer.sh  --bootstrap-server kms-2.apache.com:9092,kms-3.apache.com:9092,kms-4.apache.com:9092  --topic <span class="variable">$1</span> --from-beginning</span><br></pre></td></tr></table></figure><p>使用下面命令消费Kafka中的数据：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[kms@kms-2 kafka_2.11-2.1.0]$ ./kafka-consumer.sh  user_access_logs</span><br></pre></td></tr></table></figure><h2 id="日志分析处理流程"><a href="#日志分析处理流程" class="headerlink" title="日志分析处理流程"></a>日志分析处理流程</h2><p>为了方便解释，下面会对重要代码进行讲解，完整代码移步<strong>github</strong>：<a href="https://github.com/jiamx/flink-log-analysis" target="_blank" rel="noopener">https://github.com/jiamx/flink-log-analysis</a></p><p><img src="//jiamaoxiang.top/2020/08/29/项目实践-基于Flink的用户行为日志分析系统/F:%5Cnpm%5Cmywebsite%5Csource_posts%5C%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5-%E5%9F%BA%E4%BA%8EFlink%E7%9A%84%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F%5C%E9%A1%B9%E7%9B%AE%E4%BB%A3%E7%A0%81.png" alt></p><h3 id="创建MySQL数据库和目标表"><a href="#创建MySQL数据库和目标表" class="headerlink" title="创建MySQL数据库和目标表"></a>创建MySQL数据库和目标表</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 客户端访问量统计</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`client_ip_access`</span> (</span><br><span class="line">  <span class="string">`client_ip`</span> <span class="built_in">char</span>(<span class="number">50</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'客户端ip'</span>,</span><br><span class="line">  <span class="string">`client_access_cnt`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'访问次数'</span>,</span><br><span class="line">  <span class="string">`statistic_time`</span> <span class="built_in">text</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'统计时间'</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`client_ip`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8;</span><br><span class="line"><span class="comment">-- 热门文章统计</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`hot_article`</span> (</span><br><span class="line">  <span class="string">`article_id`</span> <span class="built_in">int</span>(<span class="number">10</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'文章id'</span>,</span><br><span class="line">  <span class="string">`subject`</span> <span class="built_in">varchar</span>(<span class="number">80</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'文章标题'</span>,</span><br><span class="line">  <span class="string">`article_pv`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'访问次数'</span>,</span><br><span class="line">  <span class="string">`statistic_time`</span> <span class="built_in">text</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'统计时间'</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`article_id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8;</span><br><span class="line"><span class="comment">-- 热门板块统计</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`hot_section`</span> (</span><br><span class="line">  <span class="string">`section_id`</span> <span class="built_in">int</span>(<span class="number">10</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'版块id'</span>,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">char</span>(<span class="number">50</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'版块标题'</span>,</span><br><span class="line">  <span class="string">`section_pv`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'访问次数'</span>,</span><br><span class="line">  <span class="string">`statistic_time`</span> <span class="built_in">text</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'统计时间'</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`section_id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8;</span><br></pre></td></tr></table></figure><h3 id="AccessLogRecord类"><a href="#AccessLogRecord类" class="headerlink" title="AccessLogRecord类"></a>AccessLogRecord类</h3><p>该类封装了日志所包含的字段数据，共有9个字段。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用lombok</span></span><br><span class="line"><span class="comment"> * 原始日志封装类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AccessLogRecord</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> String clientIpAddress; <span class="comment">// 客户端ip地址</span></span><br><span class="line">    <span class="keyword">public</span> String clientIdentity; <span class="comment">// 客户端身份标识,该字段为 `-`</span></span><br><span class="line">    <span class="keyword">public</span> String remoteUser; <span class="comment">// 用户标识,该字段为 `-`</span></span><br><span class="line">    <span class="keyword">public</span> String dateTime; <span class="comment">//日期,格式为[day/month/yearhourminutesecond zone]</span></span><br><span class="line">    <span class="keyword">public</span> String request; <span class="comment">// url请求,如：`GET /foo ...`</span></span><br><span class="line">    <span class="keyword">public</span> String httpStatusCode; <span class="comment">// 状态码，如：200; 404.</span></span><br><span class="line">    <span class="keyword">public</span> String bytesSent; <span class="comment">// 传输的字节数，有可能是 `-`</span></span><br><span class="line">    <span class="keyword">public</span> String referer; <span class="comment">// 参考链接,即来源页</span></span><br><span class="line">    <span class="keyword">public</span> String userAgent;  <span class="comment">// 浏览器和操作系统类型</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="LogParse类"><a href="#LogParse类" class="headerlink" title="LogParse类"></a>LogParse类</h3><p>该类是日志解析类，通过正则表达式对日志进行匹配，对匹配上的日志进行按照字段解析。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogParse</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构建正则表达式</span></span><br><span class="line">    <span class="keyword">private</span> String regex = <span class="string">"(\\d&#123;1,3&#125;\\.\\d&#123;1,3&#125;\\.\\d&#123;1,3&#125;\\.\\d&#123;1,3&#125;) (\\S+) (\\S+) (\\[.+?\\]) (\\\"(.*?)\\\") (\\d&#123;3&#125;) (\\S+) (\\\"(.*?)\\\") (\\\"(.*?)\\\")"</span>;</span><br><span class="line">    <span class="keyword">private</span> Pattern p = Pattern.compile(regex);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     *构造访问日志的封装类对象</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> AccessLogRecord <span class="title">buildAccessLogRecord</span><span class="params">(Matcher matcher)</span> </span>&#123;</span><br><span class="line">        AccessLogRecord record = <span class="keyword">new</span> AccessLogRecord();</span><br><span class="line">        record.setClientIpAddress(matcher.group(<span class="number">1</span>));</span><br><span class="line">        record.setClientIdentity(matcher.group(<span class="number">2</span>));</span><br><span class="line">        record.setRemoteUser(matcher.group(<span class="number">3</span>));</span><br><span class="line">        record.setDateTime(matcher.group(<span class="number">4</span>));</span><br><span class="line">        record.setRequest(matcher.group(<span class="number">5</span>));</span><br><span class="line">        record.setHttpStatusCode(matcher.group(<span class="number">6</span>));</span><br><span class="line">        record.setBytesSent(matcher.group(<span class="number">7</span>));</span><br><span class="line">        record.setReferer(matcher.group(<span class="number">8</span>));</span><br><span class="line">        record.setUserAgent(matcher.group(<span class="number">9</span>));</span><br><span class="line">        <span class="keyword">return</span> record;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> record:record表示一条apache combined 日志</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 解析日志记录，将解析的日志封装成一个AccessLogRecord类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> AccessLogRecord <span class="title">parseRecord</span><span class="params">(String record)</span> </span>&#123;</span><br><span class="line">        Matcher matcher = p.matcher(record);</span><br><span class="line">        <span class="keyword">if</span> (matcher.find()) &#123;</span><br><span class="line">            <span class="keyword">return</span> buildAccessLogRecord(matcher);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> request url请求，类型为字符串，类似于 "GET /the-uri-here HTTP/1.1"</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 一个三元组(requestType, uri, httpVersion). requestType表示请求类型，如GET, POST等</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Tuple3&lt;String, String, String&gt; <span class="title">parseRequestField</span><span class="params">(String request)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//请求的字符串格式为：“GET /test.php HTTP/1.1”，用空格切割</span></span><br><span class="line">        String[] arr = request.split(<span class="string">" "</span>);</span><br><span class="line">        <span class="keyword">if</span> (arr.length == <span class="number">3</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> Tuple3.of(arr[<span class="number">0</span>], arr[<span class="number">1</span>], arr[<span class="number">2</span>]);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 将apache日志中的英文日期转化为指定格式的中文日期</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> dateTime 传入的apache日志中的日期字符串，"[21/Jul/2009:02:48:13 -0700]"</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">parseDateField</span><span class="params">(String dateTime)</span> <span class="keyword">throws</span> ParseException </span>&#123;</span><br><span class="line">        <span class="comment">// 输入的英文日期格式</span></span><br><span class="line">        String inputFormat = <span class="string">"dd/MMM/yyyy:HH:mm:ss"</span>;</span><br><span class="line">        <span class="comment">// 输出的日期格式</span></span><br><span class="line">        String outPutFormat = <span class="string">"yyyy-MM-dd HH:mm:ss"</span>;</span><br><span class="line"></span><br><span class="line">        String dateRegex = <span class="string">"\\[(.*?) .+]"</span>;</span><br><span class="line">        Pattern datePattern = Pattern.compile(dateRegex);</span><br><span class="line"></span><br><span class="line">        Matcher dateMatcher = datePattern.matcher(dateTime);</span><br><span class="line">        <span class="keyword">if</span> (dateMatcher.find()) &#123;</span><br><span class="line">            String dateString = dateMatcher.group(<span class="number">1</span>);</span><br><span class="line">            SimpleDateFormat dateInputFormat = <span class="keyword">new</span> SimpleDateFormat(inputFormat, Locale.ENGLISH);</span><br><span class="line">            Date date = dateInputFormat.parse(dateString);</span><br><span class="line"></span><br><span class="line">            SimpleDateFormat dateOutFormat = <span class="keyword">new</span> SimpleDateFormat(outPutFormat);</span><br><span class="line"></span><br><span class="line">            String formatDate = dateOutFormat.format(date);</span><br><span class="line">            <span class="keyword">return</span> formatDate;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 解析request,即访问页面的url信息解析</span></span><br><span class="line"><span class="comment">     * "GET /about/forum.php?mod=viewthread&amp;tid=5&amp;extra=page%3D1 HTTP/1.1"</span></span><br><span class="line"><span class="comment">     * 匹配出访问的fid:版本id</span></span><br><span class="line"><span class="comment">     * 以及tid：文章id</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> request</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title">parseSectionIdAndArticleId</span><span class="params">(String request)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 匹配出前面是"forumdisplay&amp;fid="的数字记为版块id</span></span><br><span class="line">        String sectionIdRegex = <span class="string">"(\\?mod=forumdisplay&amp;fid=)(\\d+)"</span>;</span><br><span class="line">        Pattern sectionPattern = Pattern.compile(sectionIdRegex);</span><br><span class="line">        <span class="comment">// 匹配出前面是"tid="的数字记为文章id</span></span><br><span class="line">        String articleIdRegex = <span class="string">"(\\?mod=viewthread&amp;tid=)(\\d+)"</span>;</span><br><span class="line">        Pattern articlePattern = Pattern.compile(articleIdRegex);</span><br><span class="line"></span><br><span class="line">        String[] arr = request.split(<span class="string">" "</span>);</span><br><span class="line">        String sectionId = <span class="string">""</span>;</span><br><span class="line">        String articleId = <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">if</span> (arr.length == <span class="number">3</span>) &#123;</span><br><span class="line">            Matcher sectionMatcher = sectionPattern.matcher(arr[<span class="number">1</span>]);</span><br><span class="line">            Matcher articleMatcher = articlePattern.matcher(arr[<span class="number">1</span>]);</span><br><span class="line">                sectionId = (sectionMatcher.find()) ? sectionMatcher.group(<span class="number">2</span>) : <span class="string">""</span>;</span><br><span class="line">               articleId = (articleMatcher.find()) ? articleMatcher.group(<span class="number">2</span>) : <span class="string">""</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span>  Tuple2.of(sectionId, articleId);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="LogAnalysis类"><a href="#LogAnalysis类" class="headerlink" title="LogAnalysis类"></a>LogAnalysis类</h3><p>该类是日志处理的基本逻辑</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogAnalysis</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment senv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">// 开启checkpoint，时间间隔为毫秒</span></span><br><span class="line">        senv.enableCheckpointing(<span class="number">5000L</span>);</span><br><span class="line">        <span class="comment">// 选择状态后端</span></span><br><span class="line">        <span class="comment">// 本地测试</span></span><br><span class="line">        <span class="comment">// senv.setStateBackend(new FsStateBackend("file:///E://checkpoint"));</span></span><br><span class="line">        <span class="comment">// 集群运行</span></span><br><span class="line">        senv.setStateBackend(<span class="keyword">new</span> FsStateBackend(<span class="string">"hdfs://kms-1:8020/flink-checkpoints"</span>));</span><br><span class="line">        <span class="comment">// 重启策略</span></span><br><span class="line">        senv.setRestartStrategy(</span><br><span class="line">                RestartStrategies.fixedDelayRestart(<span class="number">3</span>, Time.of(<span class="number">2</span>, TimeUnit.SECONDS) ));</span><br><span class="line"></span><br><span class="line">        EnvironmentSettings settings = EnvironmentSettings.newInstance()</span><br><span class="line">                .useBlinkPlanner()</span><br><span class="line">                .inStreamingMode()</span><br><span class="line">                .build();</span><br><span class="line">        StreamTableEnvironment tEnv = StreamTableEnvironment.create(senv, settings);</span><br><span class="line">        <span class="comment">// kafka参数配置</span></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// kafka broker地址</span></span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"kms-2:9092,kms-3:9092,kms-4:9092"</span>);</span><br><span class="line">        <span class="comment">// 消费者组</span></span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"log_consumer"</span>);</span><br><span class="line">        <span class="comment">// kafka 消息的key序列化器</span></span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        <span class="comment">// kafka 消息的value序列化器</span></span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"earliest"</span>);</span><br><span class="line"></span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; kafkaConsumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;String&gt;(</span><br><span class="line">                <span class="string">"user_access_logs"</span>,</span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; logSource = senv.addSource(kafkaConsumer);</span><br><span class="line">        <span class="comment">// 获取有效的日志数据</span></span><br><span class="line">        DataStream&lt;AccessLogRecord&gt; availableAccessLog = LogAnalysis.getAvailableAccessLog(logSource);</span><br><span class="line">        <span class="comment">// 获取[clienIP,accessDate,sectionId,articleId]</span></span><br><span class="line">        DataStream&lt;Tuple4&lt;String, String, Integer, Integer&gt;&gt; fieldFromLog = LogAnalysis.getFieldFromLog(availableAccessLog);</span><br><span class="line">        <span class="comment">//从DataStream中创建临时视图,名称为logs</span></span><br><span class="line">        <span class="comment">// 添加一个计算字段:proctime,用于维表JOIN</span></span><br><span class="line">        tEnv.createTemporaryView(<span class="string">"logs"</span>,</span><br><span class="line">                fieldFromLog,</span><br><span class="line">                $(<span class="string">"clientIP"</span>),</span><br><span class="line">                $(<span class="string">"accessDate"</span>),</span><br><span class="line">                $(<span class="string">"sectionId"</span>),</span><br><span class="line">                $(<span class="string">"articleId"</span>),</span><br><span class="line">                $(<span class="string">"proctime"</span>).proctime());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 需求1：统计热门板块</span></span><br><span class="line">        LogAnalysis.getHotSection(tEnv);</span><br><span class="line">        <span class="comment">// 需求2：统计热门文章</span></span><br><span class="line">       LogAnalysis.getHotArticle(tEnv);</span><br><span class="line">        <span class="comment">// 需求3：统计不同客户端ip对版块和文章的总访问量</span></span><br><span class="line">       LogAnalysis.getClientAccess(tEnv);</span><br><span class="line">        senv.execute(<span class="string">"log-analysisi"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 统计不同客户端ip对版块和文章的总访问量</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tEnv</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getClientAccess</span><span class="params">(StreamTableEnvironment tEnv)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// sink表</span></span><br><span class="line">        <span class="comment">// [client_ip,client_access_cnt,statistic_time]</span></span><br><span class="line">        <span class="comment">// [客户端ip,访问次数,统计时间]</span></span><br><span class="line">        String client_ip_access_ddl = <span class="string">""</span> +</span><br><span class="line">                <span class="string">"CREATE TABLE client_ip_access (\n"</span> +</span><br><span class="line">                <span class="string">"    client_ip STRING ,\n"</span> +</span><br><span class="line">                <span class="string">"    client_access_cnt BIGINT,\n"</span> +</span><br><span class="line">                <span class="string">"    statistic_time STRING,\n"</span> +</span><br><span class="line">                <span class="string">"    PRIMARY KEY (client_ip) NOT ENFORCED\n"</span> +</span><br><span class="line">                <span class="string">")WITH (\n"</span> +</span><br><span class="line">                <span class="string">"    'connector' = 'jdbc',\n"</span> +</span><br><span class="line">                <span class="string">"    'url' = 'jdbc:mysql://kms-4:3306/statistics?useUnicode=true&amp;characterEncoding=utf-8',\n"</span> +</span><br><span class="line">                <span class="string">"    'table-name' = 'client_ip_access', \n"</span> +</span><br><span class="line">                <span class="string">"    'driver' = 'com.mysql.jdbc.Driver',\n"</span> +</span><br><span class="line">                <span class="string">"    'username' = 'root',\n"</span> +</span><br><span class="line">                <span class="string">"    'password' = '123qwe'\n"</span> +</span><br><span class="line">                <span class="string">") "</span>;</span><br><span class="line"></span><br><span class="line">        tEnv.executeSql(client_ip_access_ddl);</span><br><span class="line"></span><br><span class="line">        String client_ip_access_sql = <span class="string">""</span> +</span><br><span class="line">                <span class="string">"INSERT INTO client_ip_access\n"</span> +</span><br><span class="line">                <span class="string">"SELECT\n"</span> +</span><br><span class="line">                <span class="string">"    clientIP,\n"</span> +</span><br><span class="line">                <span class="string">"    count(1) AS access_cnt,\n"</span> +</span><br><span class="line">                <span class="string">"    FROM_UNIXTIME(UNIX_TIMESTAMP()) AS statistic_time\n"</span> +</span><br><span class="line">                <span class="string">"FROM\n"</span> +</span><br><span class="line">                <span class="string">"    logs \n"</span> +</span><br><span class="line">                <span class="string">"WHERE\n"</span> +</span><br><span class="line">                <span class="string">"    articleId &lt;&gt; 0 \n"</span> +</span><br><span class="line">                <span class="string">"    OR sectionId &lt;&gt; 0 \n"</span> +</span><br><span class="line">                <span class="string">"GROUP BY\n"</span> +</span><br><span class="line">                <span class="string">"    clientIP "</span></span><br><span class="line">               ;</span><br><span class="line">        tEnv.executeSql(client_ip_access_sql);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 统计热门文章</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tEnv</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getHotArticle</span><span class="params">(StreamTableEnvironment tEnv)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// JDBC数据源</span></span><br><span class="line">        <span class="comment">// 文章id及标题对应关系的表,[tid, subject]分别为：文章id和标题</span></span><br><span class="line">        String pre_forum_post_ddl = <span class="string">""</span> +</span><br><span class="line">                <span class="string">"CREATE TABLE pre_forum_post (\n"</span> +</span><br><span class="line">                <span class="string">"    tid INT,\n"</span> +</span><br><span class="line">                <span class="string">"    subject STRING,\n"</span> +</span><br><span class="line">                <span class="string">"    PRIMARY KEY (tid) NOT ENFORCED\n"</span> +</span><br><span class="line">                <span class="string">") WITH (\n"</span> +</span><br><span class="line">                <span class="string">"    'connector' = 'jdbc',\n"</span> +</span><br><span class="line">                <span class="string">"    'url' = 'jdbc:mysql://kms-4:3306/ultrax',\n"</span> +</span><br><span class="line">                <span class="string">"    'table-name' = 'pre_forum_post', \n"</span> +</span><br><span class="line">                <span class="string">"    'driver' = 'com.mysql.jdbc.Driver',\n"</span> +</span><br><span class="line">                <span class="string">"    'username' = 'root',\n"</span> +</span><br><span class="line">                <span class="string">"    'password' = '123qwe'\n"</span> +</span><br><span class="line">                <span class="string">")"</span>;</span><br><span class="line">        <span class="comment">// 创建pre_forum_post数据源</span></span><br><span class="line">        tEnv.executeSql(pre_forum_post_ddl);</span><br><span class="line">        <span class="comment">// 创建MySQL的sink表</span></span><br><span class="line">        <span class="comment">// [article_id,subject,article_pv,statistic_time]</span></span><br><span class="line">        <span class="comment">// [文章id,标题名称,访问次数,统计时间]</span></span><br><span class="line">        String hot_article_ddl = <span class="string">""</span> +</span><br><span class="line">                <span class="string">"CREATE TABLE hot_article (\n"</span> +</span><br><span class="line">                <span class="string">"    article_id INT,\n"</span> +</span><br><span class="line">                <span class="string">"    subject STRING,\n"</span> +</span><br><span class="line">                <span class="string">"    article_pv BIGINT ,\n"</span> +</span><br><span class="line">                <span class="string">"    statistic_time STRING,\n"</span> +</span><br><span class="line">                <span class="string">"    PRIMARY KEY (article_id) NOT ENFORCED\n"</span> +</span><br><span class="line">                <span class="string">")WITH (\n"</span> +</span><br><span class="line">                <span class="string">"    'connector' = 'jdbc',\n"</span> +</span><br><span class="line">                <span class="string">"    'url' = 'jdbc:mysql://kms-4:3306/statistics?useUnicode=true&amp;characterEncoding=utf-8',\n"</span> +</span><br><span class="line">                <span class="string">"    'table-name' = 'hot_article', \n"</span> +</span><br><span class="line">                <span class="string">"    'driver' = 'com.mysql.jdbc.Driver',\n"</span> +</span><br><span class="line">                <span class="string">"    'username' = 'root',\n"</span> +</span><br><span class="line">                <span class="string">"    'password' = '123qwe'\n"</span> +</span><br><span class="line">                <span class="string">")"</span>;</span><br><span class="line">        tEnv.executeSql(hot_article_ddl);</span><br><span class="line">        <span class="comment">// 向MySQL目标表insert数据</span></span><br><span class="line">        String hot_article_sql = <span class="string">""</span> +</span><br><span class="line">                <span class="string">"INSERT INTO hot_article\n"</span> +</span><br><span class="line">                <span class="string">"SELECT \n"</span> +</span><br><span class="line">                <span class="string">"    a.articleId,\n"</span> +</span><br><span class="line">                <span class="string">"    b.subject,\n"</span> +</span><br><span class="line">                <span class="string">"    count(1) as article_pv,\n"</span> +</span><br><span class="line">                <span class="string">"    FROM_UNIXTIME(UNIX_TIMESTAMP()) AS statistic_time\n"</span> +</span><br><span class="line">                <span class="string">"FROM logs a \n"</span> +</span><br><span class="line">                <span class="string">"  JOIN pre_forum_post FOR SYSTEM_TIME AS OF a.proctime as b ON a.articleId = b.tid\n"</span> +</span><br><span class="line">                <span class="string">"WHERE a.articleId &lt;&gt; 0\n"</span> +</span><br><span class="line">                <span class="string">"GROUP BY a.articleId,b.subject\n"</span> +</span><br><span class="line">                <span class="string">"ORDER BY count(1) desc\n"</span> +</span><br><span class="line">                <span class="string">"LIMIT 10"</span>;</span><br><span class="line"></span><br><span class="line">        tEnv.executeSql(hot_article_sql);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 统计热门板块</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tEnv</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getHotSection</span><span class="params">(StreamTableEnvironment tEnv)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 板块id及其名称对应关系表,[fid, name]分别为：版块id和板块名称</span></span><br><span class="line">        String pre_forum_forum_ddl = <span class="string">""</span> +</span><br><span class="line">                <span class="string">"CREATE TABLE pre_forum_forum (\n"</span> +</span><br><span class="line">                <span class="string">"    fid INT,\n"</span> +</span><br><span class="line">                <span class="string">"    name STRING,\n"</span> +</span><br><span class="line">                <span class="string">"    PRIMARY KEY (fid) NOT ENFORCED\n"</span> +</span><br><span class="line">                <span class="string">") WITH (\n"</span> +</span><br><span class="line">                <span class="string">"    'connector' = 'jdbc',\n"</span> +</span><br><span class="line">                <span class="string">"    'url' = 'jdbc:mysql://kms-4:3306/ultrax',\n"</span> +</span><br><span class="line">                <span class="string">"    'table-name' = 'pre_forum_forum', \n"</span> +</span><br><span class="line">                <span class="string">"    'driver' = 'com.mysql.jdbc.Driver',\n"</span> +</span><br><span class="line">                <span class="string">"    'username' = 'root',\n"</span> +</span><br><span class="line">                <span class="string">"    'password' = '123qwe',\n"</span> +</span><br><span class="line">                <span class="string">"    'lookup.cache.ttl' = '10',\n"</span> +</span><br><span class="line">                <span class="string">"    'lookup.cache.max-rows' = '1000'"</span> +</span><br><span class="line">                <span class="string">")"</span>;</span><br><span class="line">        <span class="comment">// 创建pre_forum_forum数据源</span></span><br><span class="line">        tEnv.executeSql(pre_forum_forum_ddl);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建MySQL的sink表</span></span><br><span class="line">        <span class="comment">// [section_id,name,section_pv,statistic_time]</span></span><br><span class="line">        <span class="comment">// [板块id,板块名称,访问次数,统计时间]</span></span><br><span class="line">        String hot_section_ddl = <span class="string">""</span> +</span><br><span class="line">                <span class="string">"CREATE TABLE hot_section (\n"</span> +</span><br><span class="line">                <span class="string">"    section_id INT,\n"</span> +</span><br><span class="line">                <span class="string">"    name STRING ,\n"</span> +</span><br><span class="line">                <span class="string">"    section_pv BIGINT,\n"</span> +</span><br><span class="line">                <span class="string">"    statistic_time STRING,\n"</span> +</span><br><span class="line">                <span class="string">"    PRIMARY KEY (section_id) NOT ENFORCED  \n"</span> +</span><br><span class="line">                <span class="string">") WITH (\n"</span> +</span><br><span class="line">                <span class="string">"    'connector' = 'jdbc',\n"</span> +</span><br><span class="line">                <span class="string">"    'url' = 'jdbc:mysql://kms-4:3306/statistics?useUnicode=true&amp;characterEncoding=utf-8',\n"</span> +</span><br><span class="line">                <span class="string">"    'table-name' = 'hot_section', \n"</span> +</span><br><span class="line">                <span class="string">"    'driver' = 'com.mysql.jdbc.Driver',\n"</span> +</span><br><span class="line">                <span class="string">"    'username' = 'root',\n"</span> +</span><br><span class="line">                <span class="string">"    'password' = '123qwe'\n"</span> +</span><br><span class="line">                <span class="string">")"</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建sink表:hot_section</span></span><br><span class="line">        tEnv.executeSql(hot_section_ddl);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//统计热门板块</span></span><br><span class="line">        <span class="comment">// 使用日志流与MySQL的维表数据进行JOIN</span></span><br><span class="line">        <span class="comment">// 从而获取板块名称</span></span><br><span class="line">        String hot_section_sql = <span class="string">""</span> +</span><br><span class="line">                <span class="string">"INSERT INTO hot_section\n"</span> +</span><br><span class="line">                <span class="string">"SELECT\n"</span> +</span><br><span class="line">                <span class="string">"    a.sectionId,\n"</span> +</span><br><span class="line">                <span class="string">"    b.name,\n"</span> +</span><br><span class="line">                <span class="string">"    count(1) as section_pv,\n"</span> +</span><br><span class="line">                <span class="string">"    FROM_UNIXTIME(UNIX_TIMESTAMP()) AS statistic_time \n"</span> +</span><br><span class="line">                <span class="string">"FROM\n"</span> +</span><br><span class="line">                <span class="string">"    logs a\n"</span> +</span><br><span class="line">                <span class="string">"    JOIN pre_forum_forum FOR SYSTEM_TIME AS OF a.proctime as b ON a.sectionId = b.fid \n"</span> +</span><br><span class="line">                <span class="string">"WHERE\n"</span> +</span><br><span class="line">                <span class="string">"    a.sectionId &lt;&gt; 0 \n"</span> +</span><br><span class="line">                <span class="string">"GROUP BY a.sectionId, b.name\n"</span> +</span><br><span class="line">                <span class="string">"ORDER BY count(1) desc\n"</span> +</span><br><span class="line">                <span class="string">"LIMIT 10"</span>;</span><br><span class="line">        <span class="comment">// 执行数据insert</span></span><br><span class="line">        tEnv.executeSql(hot_section_sql);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取[clienIP,accessDate,sectionId,articleId]</span></span><br><span class="line"><span class="comment">     * 分别为客户端ip,访问日期,板块id,文章id</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> logRecord</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> DataStream&lt;Tuple4&lt;String, String, Integer, Integer&gt;&gt; getFieldFromLog(DataStream&lt;AccessLogRecord&gt; logRecord) &#123;</span><br><span class="line">        DataStream&lt;Tuple4&lt;String, String, Integer, Integer&gt;&gt; fieldFromLog = logRecord.map(<span class="keyword">new</span> MapFunction&lt;AccessLogRecord, Tuple4&lt;String, String, Integer, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple4&lt;String, String, Integer, Integer&gt; <span class="title">map</span><span class="params">(AccessLogRecord accessLogRecord)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                LogParse parse = <span class="keyword">new</span> LogParse();</span><br><span class="line"></span><br><span class="line">                String clientIpAddress = accessLogRecord.getClientIpAddress();</span><br><span class="line">                String dateTime = accessLogRecord.getDateTime();</span><br><span class="line">                String request = accessLogRecord.getRequest();</span><br><span class="line">                String formatDate = parse.parseDateField(dateTime);</span><br><span class="line">                Tuple2&lt;String, String&gt; sectionIdAndArticleId = parse.parseSectionIdAndArticleId(request);</span><br><span class="line">                <span class="keyword">if</span> (formatDate == <span class="string">""</span> || sectionIdAndArticleId == Tuple2.of(<span class="string">""</span>, <span class="string">""</span>)) &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> Tuple4&lt;String, String, Integer, Integer&gt;(<span class="string">"0.0.0.0"</span>, <span class="string">"0000-00-00 00:00:00"</span>, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">                &#125;</span><br><span class="line">                Integer sectionId = (sectionIdAndArticleId.f0 == <span class="string">""</span>) ? <span class="number">0</span> : Integer.parseInt(sectionIdAndArticleId.f0);</span><br><span class="line">                Integer articleId = (sectionIdAndArticleId.f1 == <span class="string">""</span>) ? <span class="number">0</span> : Integer.parseInt(sectionIdAndArticleId.f1);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple4&lt;&gt;(clientIpAddress, formatDate, sectionId, articleId);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="keyword">return</span> fieldFromLog;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 筛选可用的日志记录</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> accessLog</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> DataStream&lt;AccessLogRecord&gt; <span class="title">getAvailableAccessLog</span><span class="params">(DataStream&lt;String&gt; accessLog)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> LogParse logParse = <span class="keyword">new</span> LogParse();</span><br><span class="line">        <span class="comment">//解析原始日志，将其解析为AccessLogRecord格式</span></span><br><span class="line">        DataStream&lt;AccessLogRecord&gt; filterDS = accessLog.map(<span class="keyword">new</span> MapFunction&lt;String, AccessLogRecord&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> AccessLogRecord <span class="title">map</span><span class="params">(String log)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> logParse.parseRecord(log);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).filter(<span class="keyword">new</span> FilterFunction&lt;AccessLogRecord&gt;() &#123;</span><br><span class="line">            <span class="comment">//过滤掉无效日志</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(AccessLogRecord accessLogRecord)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> !(accessLogRecord == <span class="keyword">null</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).filter(<span class="keyword">new</span> FilterFunction&lt;AccessLogRecord&gt;() &#123;</span><br><span class="line">            <span class="comment">//过滤掉状态码非200的记录，即保留请求成功的日志记录</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(AccessLogRecord accessLogRecord)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> !accessLogRecord.getHttpStatusCode().equals(<span class="string">"200"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="keyword">return</span> filterDS;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将上述代码打包上传到集群运行，在执行提交命令之前，需要先将Hadoop的依赖jar包放置在Flink安装目录下的lib文件下：<strong>flink-shaded-hadoop-2-uber-2.7.5-10.0.jar</strong>，因为我们配置了HDFS上的状态后端，而Flink的release包不含有Hadoop的依赖Jar包。</p><p><img src="//jiamaoxiang.top/2020/08/29/项目实践-基于Flink的用户行为日志分析系统/jar%E5%8C%85.png" alt></p><p>否则会报如下错误：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Caused by: org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Hadoop is not <span class="keyword">in</span> the classpath/dependencies.</span><br></pre></td></tr></table></figure><h3 id="提交到集群"><a href="#提交到集群" class="headerlink" title="提交到集群"></a>提交到集群</h3><p>编写提交命令脚本</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">/opt/modules/flink-1.11.1/bin/flink run -m kms-1:8081 \</span><br><span class="line">-c com.jmx.analysis.LogAnalysis \</span><br><span class="line">/opt/softwares/com.jmx-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><p>提交之后，访问Flink的Web界面，查看任务：</p><p><img src="//jiamaoxiang.top/2020/08/29/项目实践-基于Flink的用户行为日志分析系统/FlinkWEB.png" alt></p><p>此时访问论坛，点击板块和帖子文章，观察数据库变化：</p><p><img src="//jiamaoxiang.top/2020/08/29/项目实践-基于Flink的用户行为日志分析系统/%E7%BB%93%E6%9E%9C.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要分享了从0到1构建一个用户行为日志分析系统。首先，基于discuz搭建了论坛平台，针对论坛产生的日志，使用Flume进行收集并push到Kafka中；接着使用Flink对其进行分析处理；最后将处理结果写入MySQL供可视化展示使用。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>元数据管理|Hive Hooks和Metastore监听器介绍</title>
      <link href="/2020/08/20/%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86-Hive-Hooks%E5%92%8CMetastore%E7%9B%91%E5%90%AC%E5%99%A8%E4%BB%8B%E7%BB%8D/"/>
      <url>/2020/08/20/%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86-Hive-Hooks%E5%92%8CMetastore%E7%9B%91%E5%90%AC%E5%99%A8%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<p>元数据管理是数据仓库的核心，它不仅定义了数据仓库有什么，还指明了数据仓库中数据的内容和位置，刻画了数据的提取和转换规则，存储了与数据仓库主题有关的各种商业信息。本文主要介绍Hive Hook和MetaStore Listener，使用这些功能可以进行自动的元数据管理。通过本文你可以了解到：</p><ul><li>元数据管理</li><li><strong>Hive Hooks 和 Metastore Listeners</strong></li><li><strong>Hive Hooks</strong>基本使用</li><li><strong>Metastore Listeners</strong>基本使用</li></ul><h2 id="元数据管理"><a href="#元数据管理" class="headerlink" title="元数据管理"></a>元数据管理</h2><h3 id="元数据定义"><a href="#元数据定义" class="headerlink" title="元数据定义"></a>元数据定义</h3><p>按照传统的定义，元数据（ Metadata ）是关于数据的数据。元数据打通了源数据、数据仓库、数据应用，记录了数据从产生到消费的全过程。元数据主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及ETL 的任务运行状态。在数据仓库系统中，元数据可以帮助数据仓库管理员和开发人员非常方便地找到他们所关心的数据，用于指导其进行数据管理和开发工作，提高工作效率。将元数据按用途的不同分为两类：技术元数据（ Technical Metadata)和业务元数据（ Business Metadata ）。技术元数据是存储关于数据仓库系统技术细节的数据，是用于开发和管理数据仓库使用的数据。</p><h3 id="元数据分类"><a href="#元数据分类" class="headerlink" title="元数据分类"></a>元数据分类</h3><h4 id="技术元数据"><a href="#技术元数据" class="headerlink" title="技术元数据"></a>技术元数据</h4><ul><li>分布式计算系统存储元数据</li></ul><p>如Hive表、列、分区等信息。记录了表的表名。分区信息、责任人信息、文件大小、表类型，以及列的字段名、字段类型、字段备注、是否是分区字段等信息。</p><ul><li><p>分布式计算系统运行元数据</p><p>类似于Hive 的Job 日志，包括作业类型、实例名称、输入输出、SQL 、运行参数、执行时间等。</p></li><li><p>任务调度元数据</p><p>任务的依赖类型、依赖关系等，以及不同类型调度任务的运行日志等。</p></li></ul><h4 id="业务元数据"><a href="#业务元数据" class="headerlink" title="业务元数据"></a>业务元数据</h4><p>业务元数据从业务角度描述了数据仓库中的数据，它提供了介于使用者和实际系统之间的语义层，使得不懂计算机技术的业务人员也能够“ 读懂”数据仓库中的数据。常见的业务元数据有：如维度及属性、业务过程、指标等的规范化定义，用于更好地管理和使用数据；数据应用元数据，如数据报表、数据产品等的配置和运行元数据。</p><h3 id="元数据应用"><a href="#元数据应用" class="headerlink" title="元数据应用"></a>元数据应用</h3><p>数据的真正价值在于数据驱动决策，通过数据指导运营。通过数据驱动的方法，我们能够判断趋势，从而展开有效行动，帮助自己发现问题，推动创新或解决方案的产生。这就是数据化运营。同样，对于元数据，可以用于指导数据相关人员进行日常工作，实现数据化“运营”。比如对于数据使用者，可以通过元数据让其快速找到所需要的数据；对于ETL 工程师，可以通过元数据指导其进行模型设计、任务优化和任务下线等各种日常ETL 工作；对于运维工程师，可以通过元数据指导其进行整个集群的存储、计算和系统优化等运维工作。</p><h2 id="Hive-Hooks-和-Metastore-Listeners"><a href="#Hive-Hooks-和-Metastore-Listeners" class="headerlink" title="Hive Hooks 和 Metastore Listeners"></a><strong>Hive Hooks 和 Metastore Listeners</strong></h2><h3 id="Hive-Hooks"><a href="#Hive-Hooks" class="headerlink" title="Hive Hooks"></a><strong>Hive Hooks</strong></h3><p>关于数据治理和元数据管理框架，业界有许多开源的系统，比如<strong>Apache Atlas</strong>，这些开源的软件可以在复杂的场景下满足元数据管理的需求。其实<strong>Apache Atlas</strong>对于Hive的元数据管理，使用的是Hive的Hooks。需要进行如下配置：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.post.hooks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.atlas.hive.hook.HiveHook<span class="tag">&lt;<span class="name">value</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>通过Hook监听Hive的各种事件，比如创建表，修改表等，然后按照特定的格式把收集的数据推送到Kafka，最后消费元数据并存储。</p><h4 id="Hive-Hooks分类"><a href="#Hive-Hooks分类" class="headerlink" title="Hive Hooks分类"></a>Hive Hooks分类</h4><p><strong>那么，究竟什么是Hooks呢？</strong></p><p>Hooks 是一种事件和消息机制， 可以将事件绑定在内部 Hive 的执行流程中，而无需重新编译 Hive。Hook 提供了扩展和继承外部组件的方式。根据不同的 Hook 类型，可以在不同的阶段运行。关于Hooks的类型，主要分为以下几种：</p><ul><li><strong>hive.exec.pre.hooks</strong></li></ul><p>从名称可以看出，在执行引擎执行查询之前被调用。这个需要在 Hive 对查询计划进行过优化之后才可以使用。使用该Hooks需要实现接口：<strong>org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext</strong>，具体在<strong>hive-site.xml</strong>中的配置如下：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.pre.hooks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>实现类的全限定名<span class="tag">&lt;<span class="name">value</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><strong>hive.exec.post.hooks</strong></li></ul><p>在执行计划执行结束结果返回给用户之前被调用。使用时需要实现接口：<strong>org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext</strong>，具体在<strong>hive-site.xml</strong>中的配置如下：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.post.hooks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>实现类的全限定名<span class="tag">&lt;<span class="name">value</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><strong>hive.exec.failure.hooks</strong></li></ul><p>在执行计划失败之后被调用。使用时需要实现接口：<strong>org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext</strong>,具体在<strong>hive-site.xml</strong>中的配置如下：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.failure.hooks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>实现类的全限定名<span class="tag">&lt;<span class="name">value</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><strong>hive.metastore.init.hooks</strong></li></ul><p>HMSHandler初始化是被调用。使用时需要实现接口：<strong>org.apache.hadoop.hive.metastore.MetaStoreInitListener</strong>，具体在<strong>hive-site.xml</strong>中的配置如下：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.init.hooks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>实现类的全限定名<span class="tag">&lt;<span class="name">value</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><strong>hive.exec.driver.run.hooks</strong></li></ul><p>在Driver.run开始或结束时运行，使用时需要实现接口：<strong>org.apache.hadoop.hive.ql.HiveDriverRunHook</strong>，具体在<strong>hive-site.xml</strong>中的配置如下：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.driver.run.hooks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>实现类的全限定名<span class="tag">&lt;<span class="name">value</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><strong>hive.semantic.analyzer.hook</strong></li></ul><p>Hive 对查询语句进行语义分析的时候调用。使用时需要集成抽象类：<strong>org.apache.hadoop.hive.ql.parse.AbstractSemanticAnalyzerHook</strong>，具体在<strong>hive-site.xml</strong>中的配置如下：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.semantic.analyzer.hook<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>实现类的全限定名<span class="tag">&lt;<span class="name">value</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="Hive-Hooks的优缺点"><a href="#Hive-Hooks的优缺点" class="headerlink" title="Hive Hooks的优缺点"></a>Hive Hooks的优缺点</h4><ul><li>优点<ul><li>可以很方便地在各种查询阶段嵌入或者运行自定义的代码</li><li>可以被用作更新元数据</li></ul></li><li>缺点<ul><li>当使用Hooks时，获取到的元数据通常需要进一步解析，否则很难理解</li><li>会影响查询的过程</li></ul></li></ul><blockquote><p>对于Hive Hooks，本文将给出<strong>hive.exec.post.hook</strong>的使用案例，该Hooks会在查询执行之后，返回结果之前运行。</p></blockquote><h3 id="Metastore-Listeners"><a href="#Metastore-Listeners" class="headerlink" title="Metastore Listeners"></a>Metastore Listeners</h3><p>所谓Metastore Listeners，指的是对Hive metastore的监听。用户可以自定义一些代码，用来使用对元数据的监听。</p><p>当我们看<strong>HiveMetaStore</strong>这个类的源码时，会发现：<strong>在创建HiveMetaStore的init()方法中，同时创建了三种Listener,分别为MetaStorePreEventListener，MetaStoreEventListener和MetaStoreEndFunctionListener，这些Listener用于对每一步事件的监听。</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HiveMetaStore</span> <span class="keyword">extends</span> <span class="title">ThriftHiveMetastore</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...省略代码</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">HMSHandler</span> <span class="keyword">extends</span> <span class="title">FacebookBase</span> <span class="keyword">implements</span></span></span><br><span class="line"><span class="class">            <span class="title">IHMSHandler</span> </span>&#123;</span><br><span class="line">        <span class="comment">// ...省略代码</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> MetaException </span>&#123;</span><br><span class="line">            <span class="comment">// ...省略代码</span></span><br><span class="line">            <span class="comment">// 获取MetaStorePreEventListener</span></span><br><span class="line">            preListeners = MetaStoreUtils.getMetaStoreListeners(MetaStorePreEventListener.class,</span><br><span class="line">                    hiveConf,</span><br><span class="line">                    hiveConf.getVar(HiveConf.ConfVars.METASTORE_PRE_EVENT_LISTENERS));</span><br><span class="line">            <span class="comment">// 获取MetaStoreEventListener</span></span><br><span class="line">            listeners = MetaStoreUtils.getMetaStoreListeners(MetaStoreEventListener.class,</span><br><span class="line">                    hiveConf,</span><br><span class="line">                    hiveConf.getVar(HiveConf.ConfVars.METASTORE_EVENT_LISTENERS));</span><br><span class="line">            listeners.add(<span class="keyword">new</span> SessionPropertiesListener(hiveConf));</span><br><span class="line">            <span class="comment">// 获取MetaStoreEndFunctionListener</span></span><br><span class="line">            endFunctionListeners = MetaStoreUtils.getMetaStoreListeners(</span><br><span class="line">                    MetaStoreEndFunctionListener.class, </span><br><span class="line">                    hiveConf,</span><br><span class="line">                    hiveConf.getVar(HiveConf.ConfVars.METASTORE_END_FUNCTION_LISTENERS));</span><br><span class="line">            <span class="comment">// ...省略代码</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Metastore-Listeners分类"><a href="#Metastore-Listeners分类" class="headerlink" title="Metastore Listeners分类"></a>Metastore Listeners分类</h4><ul><li><strong>hive.metastore.pre.event.listeners</strong></li></ul><p>需要扩展此抽象类，以提供在metastore上发生特定事件之前需要执行的操作实现。在metastore上发生事件之前，将调用这些方法。</p><p>使用时需要继承抽象类：<strong>org.apache.hadoop.hive.metastore.MetaStorePreEventListener</strong>，在<strong>Hive-site.xml</strong>中的配置为：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.pre.event.listeners<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>实现类的全限定名<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><strong>hive.metastore.event.listeners</strong></li></ul><p>需要扩展此抽象类，以提供在metastore上发生特定事件时需要执行的操作实现。每当Metastore上发生事件时，就会调用这些方法。</p><p>使用时需要继承抽象类：<strong>org.apache.hadoop.hive.metastore.MetaStoreEventListener</strong>，在<strong>Hive-site.xml</strong>中的配置为：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.event.listeners<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>实现类的全限定名<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><strong>hive.metastore.end.function.listeners</strong></li></ul><p>每当函数结束时，将调用这些方法。</p><p>使用时需要继承抽象类：<strong>org.apache.hadoop.hive.metastore.MetaStoreEndFunctionListener **，在</strong>Hive-site.xml**中的配置为：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.end.function.listeners<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>实现类的全限定名<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="Metastore-Listeners优缺点"><a href="#Metastore-Listeners优缺点" class="headerlink" title="Metastore Listeners优缺点"></a>Metastore Listeners优缺点</h4><ul><li>优点<ul><li>元数据已经被解析好了，很容易理解</li><li>不影响查询的过程，是只读的</li></ul></li><li>缺点<ul><li>不灵活，仅仅能够访问属于当前事件的对象</li></ul></li></ul><blockquote><p>对于metastore listener，本文会给出<strong>MetaStoreEventListener</strong>的使用案例，具体会实现两个方法：onCreateTable和onAlterTable</p></blockquote><h2 id="Hive-Hooks基本使用"><a href="#Hive-Hooks基本使用" class="headerlink" title="Hive Hooks基本使用"></a><strong>Hive Hooks</strong>基本使用</h2><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>具体实现代码如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomPostHook</span> <span class="keyword">implements</span> <span class="title">ExecuteWithHookContext</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(CustomPostHook.class);</span><br><span class="line">    <span class="comment">// 存储Hive的SQL操作类型</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> HashSet&lt;String&gt; OPERATION_NAMES = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// HiveOperation是一个枚举类，封装了Hive的SQL操作类型</span></span><br><span class="line">    <span class="comment">// 监控SQL操作类型</span></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="comment">// 建表</span></span><br><span class="line">        OPERATION_NAMES.add(HiveOperation.CREATETABLE.getOperationName());</span><br><span class="line">        <span class="comment">// 修改数据库属性</span></span><br><span class="line">        OPERATION_NAMES.add(HiveOperation.ALTERDATABASE.getOperationName());</span><br><span class="line">        <span class="comment">// 修改数据库属主</span></span><br><span class="line">        OPERATION_NAMES.add(HiveOperation.ALTERDATABASE_OWNER.getOperationName());</span><br><span class="line">        <span class="comment">// 修改表属性,添加列</span></span><br><span class="line">        OPERATION_NAMES.add(HiveOperation.ALTERTABLE_ADDCOLS.getOperationName());</span><br><span class="line">        <span class="comment">// 修改表属性,表存储路径</span></span><br><span class="line">        OPERATION_NAMES.add(HiveOperation.ALTERTABLE_LOCATION.getOperationName());</span><br><span class="line">        <span class="comment">// 修改表属性</span></span><br><span class="line">        OPERATION_NAMES.add(HiveOperation.ALTERTABLE_PROPERTIES.getOperationName());</span><br><span class="line">        <span class="comment">// 表重命名</span></span><br><span class="line">        OPERATION_NAMES.add(HiveOperation.ALTERTABLE_RENAME.getOperationName());</span><br><span class="line">        <span class="comment">// 列重命名</span></span><br><span class="line">        OPERATION_NAMES.add(HiveOperation.ALTERTABLE_RENAMECOL.getOperationName());</span><br><span class="line">        <span class="comment">// 更新列,先删除当前的列,然后加入新的列</span></span><br><span class="line">        OPERATION_NAMES.add(HiveOperation.ALTERTABLE_REPLACECOLS.getOperationName());</span><br><span class="line">        <span class="comment">// 创建数据库</span></span><br><span class="line">        OPERATION_NAMES.add(HiveOperation.CREATEDATABASE.getOperationName());</span><br><span class="line">        <span class="comment">// 删除数据库</span></span><br><span class="line">        OPERATION_NAMES.add(HiveOperation.DROPDATABASE.getOperationName());</span><br><span class="line">        <span class="comment">// 删除表</span></span><br><span class="line">        OPERATION_NAMES.add(HiveOperation.DROPTABLE.getOperationName());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(HookContext hookContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">assert</span> (hookContext.getHookType() == HookType.POST_EXEC_HOOK);</span><br><span class="line">        <span class="comment">// 执行计划</span></span><br><span class="line">        QueryPlan plan = hookContext.getQueryPlan();</span><br><span class="line">        <span class="comment">// 操作名称</span></span><br><span class="line">        String operationName = plan.getOperationName();</span><br><span class="line">        logWithHeader(<span class="string">"执行的SQL语句: "</span> + plan.getQueryString());</span><br><span class="line">        logWithHeader(<span class="string">"操作名称: "</span> + operationName);</span><br><span class="line">        <span class="keyword">if</span> (OPERATION_NAMES.contains(operationName) &amp;&amp; !plan.isExplain()) &#123;</span><br><span class="line">            logWithHeader(<span class="string">"监控SQL操作"</span>);</span><br><span class="line"></span><br><span class="line">            Set&lt;ReadEntity&gt; inputs = hookContext.getInputs();</span><br><span class="line">            Set&lt;WriteEntity&gt; outputs = hookContext.getOutputs();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (Entity entity : inputs) &#123;</span><br><span class="line">                logWithHeader(<span class="string">"Hook metadata输入值: "</span> + toJson(entity));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (Entity entity : outputs) &#123;</span><br><span class="line">                logWithHeader(<span class="string">"Hook metadata输出值: "</span> + toJson(entity));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            logWithHeader(<span class="string">"不在监控范围，忽略该hook!"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> String <span class="title">toJson</span><span class="params">(Entity entity)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ObjectMapper mapper = <span class="keyword">new</span> ObjectMapper();</span><br><span class="line">        <span class="comment">//  entity的类型</span></span><br><span class="line">        <span class="comment">// 主要包括：</span></span><br><span class="line">        <span class="comment">// DATABASE, TABLE, PARTITION, DUMMYPARTITION, DFS_DIR, LOCAL_DIR, FUNCTION</span></span><br><span class="line">        <span class="keyword">switch</span> (entity.getType()) &#123;</span><br><span class="line">            <span class="keyword">case</span> DATABASE:</span><br><span class="line">                Database db = entity.getDatabase();</span><br><span class="line">                <span class="keyword">return</span> mapper.writeValueAsString(db);</span><br><span class="line">            <span class="keyword">case</span> TABLE:</span><br><span class="line">                <span class="keyword">return</span> mapper.writeValueAsString(entity.getTable().getTTable());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 日志格式</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> obj</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">logWithHeader</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">        LOGGER.info(<span class="string">"[CustomPostHook][Thread: "</span> + Thread.currentThread().getName() + <span class="string">"] | "</span> + obj);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="使用过程解释"><a href="#使用过程解释" class="headerlink" title="使用过程解释"></a>使用过程解释</h3><p>首先将上述代码编译成jar包，放在$HIVE_HOME/lib目录下，或者使用在Hive的客户端中执行添加jar包的命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://localhost:10000&gt; add jar /opt/softwares/com.jmx.hive-1.0-SNAPSHOT.jar;</span><br></pre></td></tr></table></figure><p>接着配置Hive-site.xml文件，为了方便，我们直接使用客户端命令进行配置：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://localhost:10000&gt; set hive.exec.post.hooks=com.jmx.hooks.CustomPostHook;</span><br></pre></td></tr></table></figure><h4 id="查看表操作"><a href="#查看表操作" class="headerlink" title="查看表操作"></a>查看表操作</h4><p>上面的代码中我们对一些操作进行了监控，当监控到这些操作时会触发一些自定义的代码(比如输出日志)。当我们在Hive的beeline客户端中输入下面命令时：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://localhost:10000&gt; show tables;</span><br></pre></td></tr></table></figure><p>在$HIVE_HOME/logs/hive.log文件可以看到：</p><p><img src="//jiamaoxiang.top/2020/08/20/元数据管理-Hive-Hooks和Metastore监听器介绍/%E5%9B%BE1.png" alt></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[CustomPostHook][Thread: cab9a763-c63e-4f25-9f9a-affacb3cecdb main] | 执行的SQL语句: show tables</span><br><span class="line">[CustomPostHook][Thread: cab9a763-c63e-4f25-9f9a-affacb3cecdb main] | 操作名称: SHOWTABLES</span><br><span class="line">[CustomPostHook][Thread: cab9a763-c63e-4f25-9f9a-affacb3cecdb main] |不在监控范围，忽略该hook!</span><br></pre></td></tr></table></figure><p>上面的查看表操作，不在监控范围，所以没有相对应的元数据日志。</p><h4 id="建表操作"><a href="#建表操作" class="headerlink" title="建表操作"></a>建表操作</h4><p>当我们在Hive的beeline客户端中创建一张表时，如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> testposthook(</span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">int</span> <span class="keyword">COMMENT</span> <span class="string">"id"</span>,</span><br><span class="line">  <span class="keyword">name</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">"姓名"</span></span><br><span class="line">)<span class="keyword">COMMENT</span> <span class="string">"建表_测试Hive Hooks"</span></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line">LOCATION <span class="string">'/user/hive/warehouse/'</span>;</span><br></pre></td></tr></table></figure><p>观察hive.log日志：</p><p><img src="//jiamaoxiang.top/2020/08/20/元数据管理-Hive-Hooks和Metastore监听器介绍/%E5%9B%BE2.png" alt></p><p>上面的Hook metastore输出值有两个：<strong>第一个是数据库的元数据信息</strong>，<strong>第二个是表的元数据信息</strong></p><ul><li>数据库元数据</li></ul><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"name"</span>:<span class="string">"default"</span>,</span><br><span class="line">    <span class="attr">"description"</span>:<span class="string">"Default Hive database"</span>,</span><br><span class="line">    <span class="attr">"locationUri"</span>:<span class="string">"hdfs://kms-1.apache.com:8020/user/hive/warehouse"</span>,</span><br><span class="line">    <span class="attr">"parameters"</span>:&#123;</span><br><span class="line"></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"privileges"</span>:<span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"ownerName"</span>:<span class="string">"public"</span>,</span><br><span class="line">    <span class="attr">"ownerType"</span>:<span class="string">"ROLE"</span>,</span><br><span class="line">    <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"parametersSize"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"setOwnerName"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setOwnerType"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setPrivileges"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setName"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setDescription"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setLocationUri"</span>:<span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>表元数据</li></ul><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"tableName"</span>:<span class="string">"testposthook"</span>,</span><br><span class="line">    <span class="attr">"dbName"</span>:<span class="string">"default"</span>,</span><br><span class="line">    <span class="attr">"owner"</span>:<span class="string">"anonymous"</span>,</span><br><span class="line">    <span class="attr">"createTime"</span>:<span class="number">1597985444</span>,</span><br><span class="line">    <span class="attr">"lastAccessTime"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"retention"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"sd"</span>:&#123;</span><br><span class="line">        <span class="attr">"cols"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"location"</span>:<span class="literal">null</span>,</span><br><span class="line">        <span class="attr">"inputFormat"</span>:<span class="string">"org.apache.hadoop.mapred.SequenceFileInputFormat"</span>,</span><br><span class="line">        <span class="attr">"outputFormat"</span>:<span class="string">"org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat"</span>,</span><br><span class="line">        <span class="attr">"compressed"</span>:<span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"numBuckets"</span>:<span class="number">-1</span>,</span><br><span class="line">        <span class="attr">"serdeInfo"</span>:&#123;</span><br><span class="line">            <span class="attr">"name"</span>:<span class="literal">null</span>,</span><br><span class="line">            <span class="attr">"serializationLib"</span>:<span class="string">"org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe"</span>,</span><br><span class="line">            <span class="attr">"parameters"</span>:&#123;</span><br><span class="line">                <span class="attr">"serialization.format"</span>:<span class="string">"1"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"setSerializationLib"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"parametersSize"</span>:<span class="number">1</span>,</span><br><span class="line">            <span class="attr">"setName"</span>:<span class="literal">false</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"bucketCols"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"sortCols"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"parameters"</span>:&#123;</span><br><span class="line"></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"skewedInfo"</span>:&#123;</span><br><span class="line">            <span class="attr">"skewedColNames"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValues"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValueLocationMaps"</span>:&#123;</span><br><span class="line"></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"skewedColNamesIterator"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValuesSize"</span>:<span class="number">0</span>,</span><br><span class="line">            <span class="attr">"skewedColValuesIterator"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValueLocationMapsSize"</span>:<span class="number">0</span>,</span><br><span class="line">            <span class="attr">"setSkewedColNames"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"setSkewedColValues"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"setSkewedColValueLocationMaps"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"skewedColNamesSize"</span>:<span class="number">0</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"storedAsSubDirectories"</span>:<span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"colsSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"parametersSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"setOutputFormat"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setSerdeInfo"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setBucketCols"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setSortCols"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setSkewedInfo"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"colsIterator"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"setCompressed"</span>:<span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"setNumBuckets"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"bucketColsSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"bucketColsIterator"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"sortColsSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"sortColsIterator"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"setStoredAsSubDirectories"</span>:<span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"setCols"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setLocation"</span>:<span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"setInputFormat"</span>:<span class="literal">true</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"partitionKeys"</span>:[</span><br><span class="line"></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"parameters"</span>:&#123;</span><br><span class="line"></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"viewOriginalText"</span>:<span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"viewExpandedText"</span>:<span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"tableType"</span>:<span class="string">"MANAGED_TABLE"</span>,</span><br><span class="line">    <span class="attr">"privileges"</span>:<span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"temporary"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"rewriteEnabled"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"partitionKeysSize"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"setDbName"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setSd"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setCreateTime"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setLastAccessTime"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"parametersSize"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"setTableName"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setPrivileges"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setOwner"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setPartitionKeys"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setViewOriginalText"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setViewExpandedText"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setTableType"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setRetention"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"partitionKeysIterator"</span>:[</span><br><span class="line"></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"setTemporary"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setRewriteEnabled"</span>:<span class="literal">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们发现上面的表元数据信息中，<strong>cols[]</strong>列没有数据，即没有建表时的字段<code>id</code>和字段<code>name</code>的信息。如果要获取这些信息，可以执行下面的命令：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> testposthook</span><br><span class="line"> <span class="keyword">ADD</span> <span class="keyword">COLUMNS</span> (age <span class="built_in">int</span> <span class="keyword">COMMENT</span> <span class="string">'年龄'</span>);</span><br></pre></td></tr></table></figure><p>再次观察日志信息：</p><p><img src="//jiamaoxiang.top/2020/08/20/元数据管理-Hive-Hooks和Metastore监听器介绍/%E5%9B%BE3.png" alt></p><p>上面的日志中，Hook metastore只有一个输入和一个输出：都表示table的元数据信息。</p><ul><li>输入</li></ul><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"tableName"</span>:<span class="string">"testposthook"</span>,</span><br><span class="line">    <span class="attr">"dbName"</span>:<span class="string">"default"</span>,</span><br><span class="line">    <span class="attr">"owner"</span>:<span class="string">"anonymous"</span>,</span><br><span class="line">    <span class="attr">"createTime"</span>:<span class="number">1597985445</span>,</span><br><span class="line">    <span class="attr">"lastAccessTime"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"retention"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"sd"</span>:&#123;</span><br><span class="line">        <span class="attr">"cols"</span>:[</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"int"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"name"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"string"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"姓名"</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"location"</span>:<span class="string">"hdfs://kms-1.apache.com:8020/user/hive/warehouse"</span>,</span><br><span class="line">        <span class="attr">"inputFormat"</span>:<span class="string">"org.apache.hadoop.mapred.TextInputFormat"</span>,</span><br><span class="line">        <span class="attr">"outputFormat"</span>:<span class="string">"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat"</span>,</span><br><span class="line">        <span class="attr">"compressed"</span>:<span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"numBuckets"</span>:<span class="number">-1</span>,</span><br><span class="line">        <span class="attr">"serdeInfo"</span>:&#123;</span><br><span class="line">            <span class="attr">"name"</span>:<span class="literal">null</span>,</span><br><span class="line">            <span class="attr">"serializationLib"</span>:<span class="string">"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"</span>,</span><br><span class="line">            <span class="attr">"parameters"</span>:&#123;</span><br><span class="line">                <span class="attr">"serialization.format"</span>:<span class="string">""</span>,</span><br><span class="line">                <span class="attr">"field.delim"</span>:<span class="string">""</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"setSerializationLib"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"parametersSize"</span>:<span class="number">2</span>,</span><br><span class="line">            <span class="attr">"setName"</span>:<span class="literal">false</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"bucketCols"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"sortCols"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"parameters"</span>:&#123;</span><br><span class="line"></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"skewedInfo"</span>:&#123;</span><br><span class="line">            <span class="attr">"skewedColNames"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValues"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValueLocationMaps"</span>:&#123;</span><br><span class="line"></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"skewedColNamesIterator"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValuesSize"</span>:<span class="number">0</span>,</span><br><span class="line">            <span class="attr">"skewedColValuesIterator"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValueLocationMapsSize"</span>:<span class="number">0</span>,</span><br><span class="line">            <span class="attr">"setSkewedColNames"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"setSkewedColValues"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"setSkewedColValueLocationMaps"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"skewedColNamesSize"</span>:<span class="number">0</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"storedAsSubDirectories"</span>:<span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"colsSize"</span>:<span class="number">2</span>,</span><br><span class="line">        <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"parametersSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"setOutputFormat"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setSerdeInfo"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setBucketCols"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setSortCols"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setSkewedInfo"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"colsIterator"</span>:[</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"int"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"name"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"string"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"姓名"</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"setCompressed"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setNumBuckets"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"bucketColsSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"bucketColsIterator"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"sortColsSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"sortColsIterator"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"setStoredAsSubDirectories"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setCols"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setLocation"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setInputFormat"</span>:<span class="literal">true</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"partitionKeys"</span>:[</span><br><span class="line"></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"parameters"</span>:&#123;</span><br><span class="line">        <span class="attr">"transient_lastDdlTime"</span>:<span class="string">"1597985445"</span>,</span><br><span class="line">        <span class="attr">"comment"</span>:<span class="string">"建表_测试Hive Hooks"</span>,</span><br><span class="line">        <span class="attr">"totalSize"</span>:<span class="string">"0"</span>,</span><br><span class="line">        <span class="attr">"numFiles"</span>:<span class="string">"0"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"viewOriginalText"</span>:<span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"viewExpandedText"</span>:<span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"tableType"</span>:<span class="string">"MANAGED_TABLE"</span>,</span><br><span class="line">    <span class="attr">"privileges"</span>:<span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"temporary"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"rewriteEnabled"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"partitionKeysSize"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"setDbName"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setSd"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setCreateTime"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setLastAccessTime"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"parametersSize"</span>:<span class="number">4</span>,</span><br><span class="line">    <span class="attr">"setTableName"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setPrivileges"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setOwner"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setPartitionKeys"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setViewOriginalText"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setViewExpandedText"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setTableType"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setRetention"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"partitionKeysIterator"</span>:[</span><br><span class="line"></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"setTemporary"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setRewriteEnabled"</span>:<span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上面的json中可以看出<strong>“cols”</strong>列的字段元数据信息，我们再来看一下输出json：</p><ul><li>输出</li></ul><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"tableName"</span>:<span class="string">"testposthook"</span>,</span><br><span class="line">    <span class="attr">"dbName"</span>:<span class="string">"default"</span>,</span><br><span class="line">    <span class="attr">"owner"</span>:<span class="string">"anonymous"</span>,</span><br><span class="line">    <span class="attr">"createTime"</span>:<span class="number">1597985445</span>,</span><br><span class="line">    <span class="attr">"lastAccessTime"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"retention"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"sd"</span>:&#123;</span><br><span class="line">        <span class="attr">"cols"</span>:[</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"int"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"name"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"string"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"姓名"</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"location"</span>:<span class="string">"hdfs://kms-1.apache.com:8020/user/hive/warehouse"</span>,</span><br><span class="line">        <span class="attr">"inputFormat"</span>:<span class="string">"org.apache.hadoop.mapred.TextInputFormat"</span>,</span><br><span class="line">        <span class="attr">"outputFormat"</span>:<span class="string">"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat"</span>,</span><br><span class="line">        <span class="attr">"compressed"</span>:<span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"numBuckets"</span>:<span class="number">-1</span>,</span><br><span class="line">        <span class="attr">"serdeInfo"</span>:&#123;</span><br><span class="line">            <span class="attr">"name"</span>:<span class="literal">null</span>,</span><br><span class="line">            <span class="attr">"serializationLib"</span>:<span class="string">"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"</span>,</span><br><span class="line">            <span class="attr">"parameters"</span>:&#123;</span><br><span class="line">                <span class="attr">"serialization.format"</span>:<span class="string">""</span>,</span><br><span class="line">                <span class="attr">"field.delim"</span>:<span class="string">""</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"setSerializationLib"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"parametersSize"</span>:<span class="number">2</span>,</span><br><span class="line">            <span class="attr">"setName"</span>:<span class="literal">false</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"bucketCols"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"sortCols"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"parameters"</span>:&#123;</span><br><span class="line"></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"skewedInfo"</span>:&#123;</span><br><span class="line">            <span class="attr">"skewedColNames"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValues"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValueLocationMaps"</span>:&#123;</span><br><span class="line"></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"skewedColNamesIterator"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValuesSize"</span>:<span class="number">0</span>,</span><br><span class="line">            <span class="attr">"skewedColValuesIterator"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValueLocationMapsSize"</span>:<span class="number">0</span>,</span><br><span class="line">            <span class="attr">"setSkewedColNames"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"setSkewedColValues"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"setSkewedColValueLocationMaps"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"skewedColNamesSize"</span>:<span class="number">0</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"storedAsSubDirectories"</span>:<span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"colsSize"</span>:<span class="number">2</span>,</span><br><span class="line">        <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"parametersSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"setOutputFormat"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setSerdeInfo"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setBucketCols"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setSortCols"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setSkewedInfo"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"colsIterator"</span>:[</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"int"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"name"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"string"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"姓名"</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"setCompressed"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setNumBuckets"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"bucketColsSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"bucketColsIterator"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"sortColsSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"sortColsIterator"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"setStoredAsSubDirectories"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setCols"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setLocation"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setInputFormat"</span>:<span class="literal">true</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"partitionKeys"</span>:[</span><br><span class="line"></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"parameters"</span>:&#123;</span><br><span class="line">        <span class="attr">"transient_lastDdlTime"</span>:<span class="string">"1597985445"</span>,</span><br><span class="line">        <span class="attr">"comment"</span>:<span class="string">"建表_测试Hive Hooks"</span>,</span><br><span class="line">        <span class="attr">"totalSize"</span>:<span class="string">"0"</span>,</span><br><span class="line">        <span class="attr">"numFiles"</span>:<span class="string">"0"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"viewOriginalText"</span>:<span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"viewExpandedText"</span>:<span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"tableType"</span>:<span class="string">"MANAGED_TABLE"</span>,</span><br><span class="line">    <span class="attr">"privileges"</span>:<span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"temporary"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"rewriteEnabled"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"partitionKeysSize"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"setDbName"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setSd"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setCreateTime"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setLastAccessTime"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"parametersSize"</span>:<span class="number">4</span>,</span><br><span class="line">    <span class="attr">"setTableName"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setPrivileges"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setOwner"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setPartitionKeys"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setViewOriginalText"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setViewExpandedText"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setTableType"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setRetention"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"partitionKeysIterator"</span>:[</span><br><span class="line"></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"setTemporary"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setRewriteEnabled"</span>:<span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>该<code>output</code>对象不包含新列<code>age</code>，它表示修改表之前的元数据信息</p></blockquote><h2 id="Metastore-Listeners基本使用"><a href="#Metastore-Listeners基本使用" class="headerlink" title="Metastore Listeners基本使用"></a><strong>Metastore Listeners</strong>基本使用</h2><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><p>具体实现代码如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomListener</span> <span class="keyword">extends</span> <span class="title">MetaStoreEventListener</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(CustomListener.class);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> ObjectMapper objMapper = <span class="keyword">new</span> ObjectMapper();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">CustomListener</span><span class="params">(Configuration config)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(config);</span><br><span class="line">        logWithHeader(<span class="string">" created "</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 监听建表操作</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCreateTable</span><span class="params">(CreateTableEvent event)</span> </span>&#123;</span><br><span class="line">        logWithHeader(event.getTable());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 监听修改表操作</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAlterTable</span><span class="params">(AlterTableEvent event)</span> </span>&#123;</span><br><span class="line">        logWithHeader(event.getOldTable());</span><br><span class="line">        logWithHeader(event.getNewTable());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">logWithHeader</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">        LOGGER.info(<span class="string">"[CustomListener][Thread: "</span> + Thread.currentThread().getName() + <span class="string">"] | "</span> + objToStr(obj));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> String <span class="title">objToStr</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> objMapper.writeValueAsString(obj);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            LOGGER.error(<span class="string">"Error on conversion"</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="使用过程解释-1"><a href="#使用过程解释-1" class="headerlink" title="使用过程解释"></a>使用过程解释</h3><p>使用方式与Hooks有一点不同，Hive Hook是与Hiveserver进行交互的，而Listener是与Metastore交互的，即Listener运行在Metastore进程中的。具体使用方式如下：</p><p>首先将jar包放在$HIVE_HOME/lib目录下，然后配置hive-site.xml文件，配置内容为：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.event.listeners<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.jmx.hooks.CustomListener<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>/&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置完成之后，需要重新启动元数据服务：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/hive --service metastore &amp;</span><br></pre></td></tr></table></figure><h4 id="建表操作-1"><a href="#建表操作-1" class="headerlink" title="建表操作"></a>建表操作</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> testlistener(</span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">int</span> <span class="keyword">COMMENT</span> <span class="string">"id"</span>,</span><br><span class="line">  <span class="keyword">name</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">"姓名"</span></span><br><span class="line">)<span class="keyword">COMMENT</span> <span class="string">"建表_测试Hive Listener"</span></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line">LOCATION <span class="string">'/user/hive/warehouse/'</span>;</span><br></pre></td></tr></table></figure><p>观察hive.log日志：</p><p><img src="//jiamaoxiang.top/2020/08/20/元数据管理-Hive-Hooks和Metastore监听器介绍/%E5%9B%BE4.png" alt></p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"tableName"</span>:<span class="string">"testlistener"</span>,</span><br><span class="line">    <span class="attr">"dbName"</span>:<span class="string">"default"</span>,</span><br><span class="line">    <span class="attr">"owner"</span>:<span class="string">"anonymous"</span>,</span><br><span class="line">    <span class="attr">"createTime"</span>:<span class="number">1597989316</span>,</span><br><span class="line">    <span class="attr">"lastAccessTime"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"retention"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"sd"</span>:&#123;</span><br><span class="line">        <span class="attr">"cols"</span>:[</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"int"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"name"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"string"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"姓名"</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"location"</span>:<span class="string">"hdfs://kms-1.apache.com:8020/user/hive/warehouse"</span>,</span><br><span class="line">        <span class="attr">"inputFormat"</span>:<span class="string">"org.apache.hadoop.mapred.TextInputFormat"</span>,</span><br><span class="line">        <span class="attr">"outputFormat"</span>:<span class="string">"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat"</span>,</span><br><span class="line">        <span class="attr">"compressed"</span>:<span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"numBuckets"</span>:<span class="number">-1</span>,</span><br><span class="line">        <span class="attr">"serdeInfo"</span>:&#123;</span><br><span class="line">            <span class="attr">"name"</span>:<span class="literal">null</span>,</span><br><span class="line">            <span class="attr">"serializationLib"</span>:<span class="string">"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"</span>,</span><br><span class="line">            <span class="attr">"parameters"</span>:&#123;</span><br><span class="line">                <span class="attr">"serialization.format"</span>:<span class="string">""</span>,</span><br><span class="line">                <span class="attr">"field.delim"</span>:<span class="string">""</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"setSerializationLib"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"parametersSize"</span>:<span class="number">2</span>,</span><br><span class="line">            <span class="attr">"setName"</span>:<span class="literal">false</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"bucketCols"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"sortCols"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"parameters"</span>:&#123;</span><br><span class="line"></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"skewedInfo"</span>:&#123;</span><br><span class="line">            <span class="attr">"skewedColNames"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValues"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValueLocationMaps"</span>:&#123;</span><br><span class="line"></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"setSkewedColNames"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"setSkewedColValues"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"setSkewedColValueLocationMaps"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"skewedColNamesSize"</span>:<span class="number">0</span>,</span><br><span class="line">            <span class="attr">"skewedColNamesIterator"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValuesSize"</span>:<span class="number">0</span>,</span><br><span class="line">            <span class="attr">"skewedColValuesIterator"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValueLocationMapsSize"</span>:<span class="number">0</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"storedAsSubDirectories"</span>:<span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"setCols"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setOutputFormat"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setSerdeInfo"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setBucketCols"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setSortCols"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"colsSize"</span>:<span class="number">2</span>,</span><br><span class="line">        <span class="attr">"colsIterator"</span>:[</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"int"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"name"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"string"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"姓名"</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"setCompressed"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setNumBuckets"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"bucketColsSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"bucketColsIterator"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"sortColsSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"sortColsIterator"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"setStoredAsSubDirectories"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setLocation"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setInputFormat"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"parametersSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"setSkewedInfo"</span>:<span class="literal">true</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"partitionKeys"</span>:[</span><br><span class="line"></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"parameters"</span>:&#123;</span><br><span class="line">        <span class="attr">"transient_lastDdlTime"</span>:<span class="string">"1597989316"</span>,</span><br><span class="line">        <span class="attr">"comment"</span>:<span class="string">"建表_测试Hive Listener"</span>,</span><br><span class="line">        <span class="attr">"totalSize"</span>:<span class="string">"0"</span>,</span><br><span class="line">        <span class="attr">"numFiles"</span>:<span class="string">"0"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"viewOriginalText"</span>:<span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"viewExpandedText"</span>:<span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"tableType"</span>:<span class="string">"MANAGED_TABLE"</span>,</span><br><span class="line">    <span class="attr">"privileges"</span>:&#123;</span><br><span class="line">        <span class="attr">"userPrivileges"</span>:&#123;</span><br><span class="line">            <span class="attr">"anonymous"</span>:[</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="attr">"privilege"</span>:<span class="string">"INSERT"</span>,</span><br><span class="line">                    <span class="attr">"createTime"</span>:<span class="number">-1</span>,</span><br><span class="line">                    <span class="attr">"grantor"</span>:<span class="string">"anonymous"</span>,</span><br><span class="line">                    <span class="attr">"grantorType"</span>:<span class="string">"USER"</span>,</span><br><span class="line">                    <span class="attr">"grantOption"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setGrantOption"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setCreateTime"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setGrantor"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setGrantorType"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setPrivilege"</span>:<span class="literal">true</span></span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="attr">"privilege"</span>:<span class="string">"SELECT"</span>,</span><br><span class="line">                    <span class="attr">"createTime"</span>:<span class="number">-1</span>,</span><br><span class="line">                    <span class="attr">"grantor"</span>:<span class="string">"anonymous"</span>,</span><br><span class="line">                    <span class="attr">"grantorType"</span>:<span class="string">"USER"</span>,</span><br><span class="line">                    <span class="attr">"grantOption"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setGrantOption"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setCreateTime"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setGrantor"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setGrantorType"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setPrivilege"</span>:<span class="literal">true</span></span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="attr">"privilege"</span>:<span class="string">"UPDATE"</span>,</span><br><span class="line">                    <span class="attr">"createTime"</span>:<span class="number">-1</span>,</span><br><span class="line">                    <span class="attr">"grantor"</span>:<span class="string">"anonymous"</span>,</span><br><span class="line">                    <span class="attr">"grantorType"</span>:<span class="string">"USER"</span>,</span><br><span class="line">                    <span class="attr">"grantOption"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setGrantOption"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setCreateTime"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setGrantor"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setGrantorType"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setPrivilege"</span>:<span class="literal">true</span></span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="attr">"privilege"</span>:<span class="string">"DELETE"</span>,</span><br><span class="line">                    <span class="attr">"createTime"</span>:<span class="number">-1</span>,</span><br><span class="line">                    <span class="attr">"grantor"</span>:<span class="string">"anonymous"</span>,</span><br><span class="line">                    <span class="attr">"grantorType"</span>:<span class="string">"USER"</span>,</span><br><span class="line">                    <span class="attr">"grantOption"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setGrantOption"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setCreateTime"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setGrantor"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setGrantorType"</span>:<span class="literal">true</span>,</span><br><span class="line">                    <span class="attr">"setPrivilege"</span>:<span class="literal">true</span></span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"groupPrivileges"</span>:<span class="literal">null</span>,</span><br><span class="line">        <span class="attr">"rolePrivileges"</span>:<span class="literal">null</span>,</span><br><span class="line">        <span class="attr">"setUserPrivileges"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setGroupPrivileges"</span>:<span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"setRolePrivileges"</span>:<span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"userPrivilegesSize"</span>:<span class="number">1</span>,</span><br><span class="line">        <span class="attr">"groupPrivilegesSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"rolePrivilegesSize"</span>:<span class="number">0</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"temporary"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"rewriteEnabled"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setPartitionKeys"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"partitionKeysSize"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"setSd"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setLastAccessTime"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setRetention"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"partitionKeysIterator"</span>:[</span><br><span class="line"></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"parametersSize"</span>:<span class="number">4</span>,</span><br><span class="line">    <span class="attr">"setTemporary"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setRewriteEnabled"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setTableName"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setDbName"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setOwner"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setViewOriginalText"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setViewExpandedText"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setTableType"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setPrivileges"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setCreateTime"</span>:<span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当我们再执行修改表操作时</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> testlistener</span><br><span class="line"> <span class="keyword">ADD</span> <span class="keyword">COLUMNS</span> (age <span class="built_in">int</span> <span class="keyword">COMMENT</span> <span class="string">'年龄'</span>);</span><br></pre></td></tr></table></figure><p>再次观察日志：</p><p><img src="//jiamaoxiang.top/2020/08/20/元数据管理-Hive-Hooks和Metastore监听器介绍/%E5%9B%BE5.png" alt></p><p>可以看出上面有两条记录，第一条记录是old table的信息，第二条是修改之后的表的信息。</p><ul><li>old table</li></ul><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"tableName"</span>:<span class="string">"testlistener"</span>,</span><br><span class="line">    <span class="attr">"dbName"</span>:<span class="string">"default"</span>,</span><br><span class="line">    <span class="attr">"owner"</span>:<span class="string">"anonymous"</span>,</span><br><span class="line">    <span class="attr">"createTime"</span>:<span class="number">1597989316</span>,</span><br><span class="line">    <span class="attr">"lastAccessTime"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"retention"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"sd"</span>:&#123;</span><br><span class="line">        <span class="attr">"cols"</span>:[</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"int"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"name"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"string"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"姓名"</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"location"</span>:<span class="string">"hdfs://kms-1.apache.com:8020/user/hive/warehouse"</span>,</span><br><span class="line">        <span class="attr">"inputFormat"</span>:<span class="string">"org.apache.hadoop.mapred.TextInputFormat"</span>,</span><br><span class="line">        <span class="attr">"outputFormat"</span>:<span class="string">"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat"</span>,</span><br><span class="line">        <span class="attr">"compressed"</span>:<span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"numBuckets"</span>:<span class="number">-1</span>,</span><br><span class="line">        <span class="attr">"serdeInfo"</span>:&#123;</span><br><span class="line">            <span class="attr">"name"</span>:<span class="literal">null</span>,</span><br><span class="line">            <span class="attr">"serializationLib"</span>:<span class="string">"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"</span>,</span><br><span class="line">            <span class="attr">"parameters"</span>:&#123;</span><br><span class="line">                <span class="attr">"serialization.format"</span>:<span class="string">""</span>,</span><br><span class="line">                <span class="attr">"field.delim"</span>:<span class="string">""</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"setSerializationLib"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"parametersSize"</span>:<span class="number">2</span>,</span><br><span class="line">            <span class="attr">"setName"</span>:<span class="literal">false</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"bucketCols"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"sortCols"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"parameters"</span>:&#123;</span><br><span class="line"></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"skewedInfo"</span>:&#123;</span><br><span class="line">            <span class="attr">"skewedColNames"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValues"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValueLocationMaps"</span>:&#123;</span><br><span class="line"></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"setSkewedColNames"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"setSkewedColValues"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"setSkewedColValueLocationMaps"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"skewedColNamesSize"</span>:<span class="number">0</span>,</span><br><span class="line">            <span class="attr">"skewedColNamesIterator"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValuesSize"</span>:<span class="number">0</span>,</span><br><span class="line">            <span class="attr">"skewedColValuesIterator"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValueLocationMapsSize"</span>:<span class="number">0</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"storedAsSubDirectories"</span>:<span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"setCols"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setOutputFormat"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setSerdeInfo"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setBucketCols"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setSortCols"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"colsSize"</span>:<span class="number">2</span>,</span><br><span class="line">        <span class="attr">"colsIterator"</span>:[</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"int"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"name"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"string"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"姓名"</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"setCompressed"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setNumBuckets"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"bucketColsSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"bucketColsIterator"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"sortColsSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"sortColsIterator"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"setStoredAsSubDirectories"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setLocation"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setInputFormat"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"parametersSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"setSkewedInfo"</span>:<span class="literal">true</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"partitionKeys"</span>:[</span><br><span class="line"></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"parameters"</span>:&#123;</span><br><span class="line">        <span class="attr">"totalSize"</span>:<span class="string">"0"</span>,</span><br><span class="line">        <span class="attr">"numFiles"</span>:<span class="string">"0"</span>,</span><br><span class="line">        <span class="attr">"transient_lastDdlTime"</span>:<span class="string">"1597989316"</span>,</span><br><span class="line">        <span class="attr">"comment"</span>:<span class="string">"建表_测试Hive Listener"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"viewOriginalText"</span>:<span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"viewExpandedText"</span>:<span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"tableType"</span>:<span class="string">"MANAGED_TABLE"</span>,</span><br><span class="line">    <span class="attr">"privileges"</span>:<span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"temporary"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"rewriteEnabled"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setPartitionKeys"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"partitionKeysSize"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"setSd"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setLastAccessTime"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setRetention"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"partitionKeysIterator"</span>:[</span><br><span class="line"></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"parametersSize"</span>:<span class="number">4</span>,</span><br><span class="line">    <span class="attr">"setTemporary"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setRewriteEnabled"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setTableName"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setDbName"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setOwner"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setViewOriginalText"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setViewExpandedText"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setTableType"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setPrivileges"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setCreateTime"</span>:<span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>new table</li></ul><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"tableName"</span>:<span class="string">"testlistener"</span>,</span><br><span class="line">    <span class="attr">"dbName"</span>:<span class="string">"default"</span>,</span><br><span class="line">    <span class="attr">"owner"</span>:<span class="string">"anonymous"</span>,</span><br><span class="line">    <span class="attr">"createTime"</span>:<span class="number">1597989316</span>,</span><br><span class="line">    <span class="attr">"lastAccessTime"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"retention"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"sd"</span>:&#123;</span><br><span class="line">        <span class="attr">"cols"</span>:[</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"int"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"name"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"string"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"姓名"</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"age"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"int"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"年龄"</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"location"</span>:<span class="string">"hdfs://kms-1.apache.com:8020/user/hive/warehouse"</span>,</span><br><span class="line">        <span class="attr">"inputFormat"</span>:<span class="string">"org.apache.hadoop.mapred.TextInputFormat"</span>,</span><br><span class="line">        <span class="attr">"outputFormat"</span>:<span class="string">"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat"</span>,</span><br><span class="line">        <span class="attr">"compressed"</span>:<span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"numBuckets"</span>:<span class="number">-1</span>,</span><br><span class="line">        <span class="attr">"serdeInfo"</span>:&#123;</span><br><span class="line">            <span class="attr">"name"</span>:<span class="literal">null</span>,</span><br><span class="line">            <span class="attr">"serializationLib"</span>:<span class="string">"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"</span>,</span><br><span class="line">            <span class="attr">"parameters"</span>:&#123;</span><br><span class="line">                <span class="attr">"serialization.format"</span>:<span class="string">""</span>,</span><br><span class="line">                <span class="attr">"field.delim"</span>:<span class="string">""</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"setSerializationLib"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"parametersSize"</span>:<span class="number">2</span>,</span><br><span class="line">            <span class="attr">"setName"</span>:<span class="literal">false</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"bucketCols"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"sortCols"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"parameters"</span>:&#123;</span><br><span class="line"></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"skewedInfo"</span>:&#123;</span><br><span class="line">            <span class="attr">"skewedColNames"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValues"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValueLocationMaps"</span>:&#123;</span><br><span class="line"></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"setSkewedColNames"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"setSkewedColValues"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"setSkewedColValueLocationMaps"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"skewedColNamesSize"</span>:<span class="number">0</span>,</span><br><span class="line">            <span class="attr">"skewedColNamesIterator"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValuesSize"</span>:<span class="number">0</span>,</span><br><span class="line">            <span class="attr">"skewedColValuesIterator"</span>:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"skewedColValueLocationMapsSize"</span>:<span class="number">0</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"storedAsSubDirectories"</span>:<span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"setCols"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setOutputFormat"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setSerdeInfo"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setBucketCols"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setSortCols"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"colsSize"</span>:<span class="number">3</span>,</span><br><span class="line">        <span class="attr">"colsIterator"</span>:[</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"int"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"id"</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"name"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"string"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"姓名"</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="attr">"name"</span>:<span class="string">"age"</span>,</span><br><span class="line">                <span class="attr">"type"</span>:<span class="string">"int"</span>,</span><br><span class="line">                <span class="attr">"comment"</span>:<span class="string">"年龄"</span>,</span><br><span class="line">                <span class="attr">"setComment"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setType"</span>:<span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"setName"</span>:<span class="literal">true</span></span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"setCompressed"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setNumBuckets"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"bucketColsSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"bucketColsIterator"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"sortColsSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"sortColsIterator"</span>:[</span><br><span class="line"></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"setStoredAsSubDirectories"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setLocation"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"setInputFormat"</span>:<span class="literal">true</span>,</span><br><span class="line">        <span class="attr">"parametersSize"</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">"setSkewedInfo"</span>:<span class="literal">true</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"partitionKeys"</span>:[</span><br><span class="line"></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"parameters"</span>:&#123;</span><br><span class="line">        <span class="attr">"totalSize"</span>:<span class="string">"0"</span>,</span><br><span class="line">        <span class="attr">"last_modified_time"</span>:<span class="string">"1597989660"</span>,</span><br><span class="line">        <span class="attr">"numFiles"</span>:<span class="string">"0"</span>,</span><br><span class="line">        <span class="attr">"transient_lastDdlTime"</span>:<span class="string">"1597989660"</span>,</span><br><span class="line">        <span class="attr">"comment"</span>:<span class="string">"建表_测试Hive Listener"</span>,</span><br><span class="line">        <span class="attr">"last_modified_by"</span>:<span class="string">"anonymous"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"viewOriginalText"</span>:<span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"viewExpandedText"</span>:<span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"tableType"</span>:<span class="string">"MANAGED_TABLE"</span>,</span><br><span class="line">    <span class="attr">"privileges"</span>:<span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"temporary"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"rewriteEnabled"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setParameters"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setPartitionKeys"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"partitionKeysSize"</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="attr">"setSd"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setLastAccessTime"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setRetention"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"partitionKeysIterator"</span>:[</span><br><span class="line"></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"parametersSize"</span>:<span class="number">6</span>,</span><br><span class="line">    <span class="attr">"setTemporary"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setRewriteEnabled"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setTableName"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setDbName"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setOwner"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setViewOriginalText"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setViewExpandedText"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setTableType"</span>:<span class="literal">true</span>,</span><br><span class="line">    <span class="attr">"setPrivileges"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="attr">"setCreateTime"</span>:<span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看出：修改之后的表的元数据信息中，包含新添加的列<code>age</code>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在本文中，我们介绍了如何在Hive中操作元数据，从而能够自动进行元数据管理。我们给出了Hive Hooks和Metastore Listener的基本使用方式，这些方式可以帮助我们实现操作元数据。当然也可以将这些元数据信息推送到Kafka中，以此构建自己的元数据管理系统。</p><blockquote><p>公众号『大数据技术与数仓』，回复『资料』领取大数据资料包</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQL查询的执行顺序分析</title>
      <link href="/2020/08/19/SQL%E6%9F%A5%E8%AF%A2%E7%9A%84%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F%E5%88%86%E6%9E%90/"/>
      <url>/2020/08/19/SQL%E6%9F%A5%E8%AF%A2%E7%9A%84%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p><strong>SQL 语言无处不在</strong>。SQL 已经不仅仅是技术人员的专属技能了，似乎人人都会写SQL，就如同人人都是产品经理一样。如果你是做后台开发的，那么CRUD就是家常便饭。如果你是做数仓开发的，那么写SQL可能占据了你的大部分工作时间。我们在理解 SELECT 语法的时候，还需要了解 SELECT 执行时的底层原理。只有这样，才能让我们对 SQL 有更深刻的认识。本文分享将逐步分解SQL的执行过程，希望对你有所帮助。</p><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>本文旨在说明SQL查询的执行过程，不会涉及太复杂的SQL操作，主要涉及两张表：<strong>citizen</strong>和<strong>city</strong>,具体数据如下所示：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> citizen ( </span><br><span class="line">    <span class="keyword">name</span> <span class="built_in">CHAR</span> ( <span class="number">20</span> ), </span><br><span class="line">    city_id <span class="built_in">INT</span> ( <span class="number">10</span> ) </span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> city (</span><br><span class="line">    city_id <span class="built_in">INT</span> ( <span class="number">10</span> ), </span><br><span class="line">    city_name <span class="built_in">CHAR</span> ( <span class="number">20</span> ) </span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> city</span><br><span class="line"><span class="keyword">VALUES</span></span><br><span class="line">( <span class="number">1</span>, <span class="string">"上海"</span> ),</span><br><span class="line">( <span class="number">2</span>, <span class="string">"北京"</span> ),</span><br><span class="line">( <span class="number">3</span>, <span class="string">"杭州"</span> );</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> citizen</span><br><span class="line"><span class="keyword">VALUES</span></span><br><span class="line">(<span class="string">"tom"</span>,<span class="number">3</span>),</span><br><span class="line">(<span class="string">"jack"</span>,<span class="number">2</span>),</span><br><span class="line">(<span class="string">"robin"</span>,<span class="number">1</span>),</span><br><span class="line">(<span class="string">"jasper"</span>,<span class="number">3</span>),</span><br><span class="line">(<span class="string">"kevin"</span>,<span class="number">1</span>),</span><br><span class="line">(<span class="string">"rachel"</span>,<span class="number">2</span>),</span><br><span class="line">(<span class="string">"trump"</span>,<span class="number">3</span>),</span><br><span class="line">(<span class="string">"lilei"</span>,<span class="number">1</span>),</span><br><span class="line">(<span class="string">"hanmeiei"</span>,<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h2 id="查询执行顺序"><a href="#查询执行顺序" class="headerlink" title="查询执行顺序"></a>查询执行顺序</h2><p>本文所涉及的查询语句如下，主要是citizen表与city表进行join，然后筛掉city_name != “上海”的数据，接着按照city_name进行分组，统计每个城市总人数大于2的城市，具体如下：</p><h3 id="查询语句"><a href="#查询语句" class="headerlink" title="查询语句"></a>查询语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    city.city_name <span class="keyword">AS</span> <span class="string">"City"</span>,</span><br><span class="line">    <span class="keyword">COUNT</span>(*) <span class="keyword">AS</span> <span class="string">"citizen_cnt"</span></span><br><span class="line"><span class="keyword">FROM</span> citizen</span><br><span class="line">  <span class="keyword">JOIN</span> city <span class="keyword">ON</span> citizen.city_id = city.city_id </span><br><span class="line"><span class="keyword">WHERE</span> city.city_name != <span class="string">'上海'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> city.city_name</span><br><span class="line"><span class="keyword">HAVING</span> <span class="keyword">COUNT</span>(*) &gt;= <span class="number">2</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> city.city_name <span class="keyword">ASC</span></span><br><span class="line"><span class="keyword">LIMIT</span> <span class="number">2</span></span><br></pre></td></tr></table></figure><h3 id="执行步骤"><a href="#执行步骤" class="headerlink" title="执行步骤"></a>执行步骤</h3><p>上面SQL查询语句的书写书序是：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> ... <span class="keyword">FROM</span> ... <span class="keyword">WHERE</span> ... <span class="keyword">GROUP</span> <span class="keyword">BY</span> ... <span class="keyword">HAVING</span> ... <span class="keyword">ORDER</span> <span class="keyword">BY</span> ...</span><br></pre></td></tr></table></figure><p>但是执行顺序并不是这样，具体的执行顺序如下步骤所示：</p><ul><li>1.获取数据 (<em>From, Join</em>)</li><li>2.过滤数据 (<em>Where</em>)</li><li>3.分组 (<em>Group by</em>)</li><li>4.分组过滤 (<em>Having</em>)</li><li>5.返回查询字段 (<em>Select</em>)</li><li>6.排序与分页 (<em>Order by &amp; Limit / Offset</em>)</li></ul><blockquote><p>尖叫提示：本文旨在说明通用的SQL执行底层原理，对于其优化技术不做考虑，比如谓词下推、投影下推等等。</p></blockquote><h3 id="执行的底层原理"><a href="#执行的底层原理" class="headerlink" title="执行的底层原理"></a>执行的底层原理</h3><p>其实上面所说的SQL执行顺序就是所谓的底层原理，当我们在执行SELECT语句时，每个步骤都会产生一张<strong>虚拟表(virtual table）</strong>，在执行下一步骤时，会将该虚拟表作为输入。指的注意的是，这些过程是对用户透明的。</p><p>你可以注意到，SELECT 是先从FROM 这一步开始执行的。在这个阶段，如果是多张表进行JOIN，还会经历下面的几个步骤：</p><h4 id="获取数据-From-Join"><a href="#获取数据-From-Join" class="headerlink" title="获取数据 (From, Join)"></a>获取数据 (<em>From, Join</em>)</h4><ul><li><p>首先会通过 CROSS JOIN 求笛卡尔积，相当于得到虚拟表 vt1-1；</p></li><li><p>接着通过ON 条件进行筛选，虚拟表 vt1-1 作为输入，输出虚拟表 vt1-2；</p></li><li><p>添加外部行。我们使用的是左连接、右链接或者全连接，就会涉及到外部行，也就是在虚拟表 vt1-2 的基础上增加外部行，得到虚拟表 vt1-3</p></li></ul><h4 id="过滤数据-Where"><a href="#过滤数据-Where" class="headerlink" title="过滤数据 (Where)"></a>过滤数据 (<em>Where</em>)</h4><p>经过上面的步骤，我们得到了一张最终的虚拟表vt1，在此表之上作用where过滤，通过筛选条件过滤掉不满足条件的数据，从而得到虚拟表vt2。</p><h4 id="分组-Group-by"><a href="#分组-Group-by" class="headerlink" title="分组 (Group by)"></a>分组 (<em>Group by</em>)</h4><p>经过where过滤操作之后，得到vt2。接下来进行GROUP BY操作，得到中间的虚拟表vt3。</p><h4 id="分组过滤-Having"><a href="#分组过滤-Having" class="headerlink" title="分组过滤 (Having)"></a>分组过滤 (<em>Having</em>)</h4><p>在虚拟表vt3的基础之上，使用having过滤掉不满足条件的聚合数据，得到vt4。</p><h4 id="返回查询字段-Select"><a href="#返回查询字段-Select" class="headerlink" title="返回查询字段 (Select)"></a>返回查询字段 (<em>Select</em>)</h4><p>当我们完成了条件筛选部分之后，就可以筛选表中提取的字段，也就是进入到 SELECT 和 DISTINCT 阶段。首先在 SELECT 阶段会提取目标字段，然后在 DISTINCT 阶段过滤掉重复的行，分别得到中间的虚拟表 vt5-1 和 vt5-2。</p><h4 id="排序与分页-Order-by-amp-Limit-Offset"><a href="#排序与分页-Order-by-amp-Limit-Offset" class="headerlink" title="排序与分页 (Order by &amp; Limit / Offset)"></a>排序与分页 (<em>Order by &amp; Limit / Offset</em>)</h4><p>当我们提取了想要的字段数据之后，就可以按照指定的字段进行排序，也就是 ORDER BY 阶段，得到虚拟表 vt6。最后在 vt6 的基础上，取出指定行的记录，也就是 LIMIT 阶段，得到最终的结果，对应的是虚拟表 vt7</p><h3 id="详细执行步骤分析"><a href="#详细执行步骤分析" class="headerlink" title="详细执行步骤分析"></a>详细执行步骤分析</h3><h4 id="Step-1-获取数据-From-Join"><a href="#Step-1-获取数据-From-Join" class="headerlink" title="Step 1:获取数据 (From, Join)"></a>Step 1:获取数据 (<em>From, Join</em>)</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">FROM citizen</span><br><span class="line">JOIN city</span><br></pre></td></tr></table></figure><p>该过程的第一步是执行From子句中的语句，然后执行Join子句。这些操作的结果是得到两个表的笛卡尔积。</p><table><thead><tr><th>name</th><th>city_id</th><th>city_id</th><th>city_name</th></tr></thead><tbody><tr><td>tom</td><td>3</td><td>1</td><td>上海</td></tr><tr><td>tom</td><td>3</td><td>2</td><td>北京</td></tr><tr><td>tom</td><td>3</td><td>3</td><td>杭州</td></tr><tr><td>jack</td><td>2</td><td>1</td><td>上海</td></tr><tr><td>jack</td><td>2</td><td>2</td><td>北京</td></tr><tr><td>jack</td><td>2</td><td>3</td><td>杭州</td></tr><tr><td>robin</td><td>1</td><td>1</td><td>上海</td></tr><tr><td>robin</td><td>1</td><td>2</td><td>北京</td></tr><tr><td>robin</td><td>1</td><td>3</td><td>杭州</td></tr><tr><td>jasper</td><td>3</td><td>1</td><td>上海</td></tr><tr><td>jasper</td><td>3</td><td>2</td><td>北京</td></tr><tr><td>jasper</td><td>3</td><td>3</td><td>杭州</td></tr><tr><td>kevin</td><td>1</td><td>1</td><td>上海</td></tr><tr><td>kevin</td><td>1</td><td>2</td><td>北京</td></tr><tr><td>kevin</td><td>1</td><td>3</td><td>杭州</td></tr><tr><td>rachel</td><td>2</td><td>1</td><td>上海</td></tr><tr><td>rachel</td><td>2</td><td>2</td><td>北京</td></tr><tr><td>rachel</td><td>2</td><td>3</td><td>杭州</td></tr><tr><td>trump</td><td>3</td><td>1</td><td>上海</td></tr><tr><td>trump</td><td>3</td><td>2</td><td>北京</td></tr><tr><td>trump</td><td>3</td><td>3</td><td>杭州</td></tr><tr><td>lilei</td><td>1</td><td>1</td><td>上海</td></tr><tr><td>lilei</td><td>1</td><td>2</td><td>北京</td></tr><tr><td>lilei</td><td>1</td><td>3</td><td>杭州</td></tr><tr><td>hanmeiei</td><td>1</td><td>1</td><td>上海</td></tr><tr><td>hanmeiei</td><td>1</td><td>2</td><td>北京</td></tr><tr><td>hanmeiei</td><td>1</td><td>3</td><td>杭州</td></tr></tbody></table><p>在FROM和JOIN执行结束之后，会按照JOIN的ON条件，筛选所需要的行</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ON citizen.city_id = city.city_id</span><br></pre></td></tr></table></figure><table><thead><tr><th>name</th><th>city_id</th><th>city_id</th><th>city_name</th></tr></thead><tbody><tr><td>tom</td><td>3</td><td>3</td><td>杭州</td></tr><tr><td>jack</td><td>2</td><td>2</td><td>北京</td></tr><tr><td>robin</td><td>1</td><td>1</td><td>上海</td></tr><tr><td>jasper</td><td>3</td><td>3</td><td>杭州</td></tr><tr><td>kevin</td><td>1</td><td>1</td><td>上海</td></tr><tr><td>rachel</td><td>2</td><td>2</td><td>北京</td></tr><tr><td>trump</td><td>3</td><td>3</td><td>杭州</td></tr><tr><td>lilei</td><td>1</td><td>1</td><td>上海</td></tr><tr><td>hanmeiei</td><td>1</td><td>1</td><td>上海</td></tr></tbody></table><h4 id="Step-2-过滤数据-Where"><a href="#Step-2-过滤数据-Where" class="headerlink" title="Step 2:过滤数据 (Where)"></a>Step 2:过滤数据 (<em>Where</em>)</h4><p>获得满足条件的行后，将传递给Where子句。这将使用条件表达式评估每一行。如果行的计算结果不为true，则会将其从集合中删除。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">WHERE city.city_name != '上海'</span><br></pre></td></tr></table></figure><table><thead><tr><th>name</th><th>city_id</th><th>city_id</th><th>city_name</th></tr></thead><tbody><tr><td>tom</td><td>3</td><td>3</td><td>杭州</td></tr><tr><td>jack</td><td>2</td><td>2</td><td>北京</td></tr><tr><td>jasper</td><td>3</td><td>3</td><td>杭州</td></tr><tr><td>rachel</td><td>2</td><td>2</td><td>北京</td></tr><tr><td>trump</td><td>3</td><td>3</td><td>杭州</td></tr></tbody></table><h4 id="Step-3-分组-Group-by"><a href="#Step-3-分组-Group-by" class="headerlink" title="Step 3:分组 (Group by)"></a>Step 3:分组 (<em>Group by</em>)</h4><p>下一步是执行Group by子句，它将具有相同值的行分为一组。此后，将按组对所有Select表达式进行评估，而不是按行进行评估。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">GROUP BY city.city_name</span><br></pre></td></tr></table></figure><table><thead><tr><th>GROUP_CONCAT(citizen.<code>name</code>)</th><th>city_id</th><th>city_name</th></tr></thead><tbody><tr><td>jack,rachel</td><td>2</td><td>北京</td></tr><tr><td>tom,jasper,trump</td><td>3</td><td>杭州</td></tr></tbody></table><h4 id="Step-4-分组过滤-Having"><a href="#Step-4-分组过滤-Having" class="headerlink" title="Step 4:分组过滤 (Having)"></a>Step 4:分组过滤 (<em>Having</em>)</h4><p>对分组后的数据使用Having子句所包含的谓词进行过滤</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">HAVING COUNT(*) &gt;= 2</span><br></pre></td></tr></table></figure><h4 id="Step-5-返回查询字段-Select"><a href="#Step-5-返回查询字段-Select" class="headerlink" title="Step 5:返回查询字段 (Select)"></a>Step 5:返回查询字段 (<em>Select</em>)</h4><p>在此步骤中，处理器将评估查询结果将要打印的内容，以及是否有一些函数要对数据运行，例如Distinct，Max，Sqrt，Date，Lower等等。本案例中，SELECT子句只会打印城市名称和其对应分组的count(*)值，并使用标识符“ City”作为city_name列的别名。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    city.city_name <span class="keyword">AS</span> <span class="string">"City"</span>,</span><br><span class="line"><span class="keyword">COUNT</span>(*) <span class="keyword">AS</span> <span class="string">"citizen_cnt"</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>city</th><th>citizen_cnt</th></tr></thead><tbody><tr><td>北京</td><td>2</td></tr><tr><td>杭州</td><td>3</td></tr></tbody></table><h4 id="Step-6-排序与分页-Order-by-amp-Limit-Offset"><a href="#Step-6-排序与分页-Order-by-amp-Limit-Offset" class="headerlink" title="Step 6:排序与分页 (Order by &amp; Limit / Offset)"></a>Step 6:排序与分页 (<em>Order by &amp; Limit / Offset</em>)</h4><p>查询的最后处理步骤涉及结果集的排序与输出大小。在我们的示例中，按照字母顺序升序排列，并输出两条数据结果。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">ORDER BY city.city_name ASC</span><br><span class="line">LIMIT 2</span><br></pre></td></tr></table></figure><table><thead><tr><th>city</th><th>citizen_cnt</th></tr></thead><tbody><tr><td>北京</td><td>2</td></tr><tr><td>杭州</td><td>3</td></tr></tbody></table><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要剖析了SQL语句的执行顺序和底层原理，基本的SQL查询会分为六大步骤。本文结合具体事例，给出了每一步骤的详细结果，这样会对其执行的底层原理有更加深刻的认识。</p>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>两阶段提交|Flink端到端的EXACTLY ONCE实现细节</title>
      <link href="/2020/08/18/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4-Flink%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84EXACTLY-ONCE%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82/"/>
      <url>/2020/08/18/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4-Flink%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84EXACTLY-ONCE%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>实时数仓|基于Flink1.11的SQL构建实时数仓探索实践</title>
      <link href="/2020/08/12/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-%E5%9F%BA%E4%BA%8EFlink1-11%E7%9A%84SQL%E6%9E%84%E5%BB%BA%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E6%8E%A2%E7%B4%A2%E5%AE%9E%E8%B7%B5/"/>
      <url>/2020/08/12/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-%E5%9F%BA%E4%BA%8EFlink1-11%E7%9A%84SQL%E6%9E%84%E5%BB%BA%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E6%8E%A2%E7%B4%A2%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>实时数仓主要是为了解决传统数仓数据时效性低的问题，实时数仓通常会用在实时的OLAP分析、实时的数据看板、业务指标实时监控等场景。虽然关于实时数仓的架构及技术选型与传统的离线数仓会存在差异，但是关于数仓建设的基本方法论是一致的。本文会分享基于Flink SQL从0到1搭建一个实时数仓的demo，涉及数据采集、存储、计算、可视化整个处理流程。通过本文你可以了解到：</p><ul><li>实时数仓的基本架构</li><li>实时数仓的数据处理流程</li><li>Flink1.11的SQL新特性</li><li>Flink1.11存在的bug</li><li>完整的操作案例</li></ul><blockquote><p>古人学问无遗力，少壮工夫老始成。</p><p>纸上得来终觉浅，绝知此事要躬行。</p></blockquote><h2 id="案例简介"><a href="#案例简介" class="headerlink" title="案例简介"></a>案例简介</h2><p>本文会以电商业务为例，展示实时数仓的数据处理流程。另外，本文旨在说明实时数仓的构建流程，所以不会涉及太复杂的数据计算。为了保证案例的可操作性和完整性，本文会给出详细的操作步骤。为了方便演示，本文的所有操作都是在Flink SQL Cli中完成的。</p><h2 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h2><p>具体的架构设计如图所示：首先通过canal解析MySQL的binlog日志，将数据存储在Kafka中。然后使用Flink SQL对原始数据进行清洗关联，并将处理之后的明细宽表写入kafka中。维表数据存储在MySQL中，通过Flink SQL对明细宽表与维表进行JOIN，将聚合后的数据写入MySQL，最后通过FineBI进行可视化展示。</p><img src="//jiamaoxiang.top/2020/08/12/实时数仓-基于Flink1-11的SQL构建实时数仓探索实践/实时数仓架构.png" style="zoom:80%;"><h2 id="业务数据准备"><a href="#业务数据准备" class="headerlink" title="业务数据准备"></a>业务数据准备</h2><ul><li>订单表（order_info）</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`order_info`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT <span class="keyword">COMMENT</span> <span class="string">'编号'</span>,</span><br><span class="line">  <span class="string">`consignee`</span> <span class="built_in">varchar</span>(<span class="number">100</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'收货人'</span>,</span><br><span class="line">  <span class="string">`consignee_tel`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'收件人电话'</span>,</span><br><span class="line">  <span class="string">`total_amount`</span> <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'总金额'</span>,</span><br><span class="line">  <span class="string">`order_status`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'订单状态'</span>,</span><br><span class="line">  <span class="string">`user_id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'用户id'</span>,</span><br><span class="line">  <span class="string">`payment_way`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'付款方式'</span>,</span><br><span class="line">  <span class="string">`delivery_address`</span> <span class="built_in">varchar</span>(<span class="number">1000</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'送货地址'</span>,</span><br><span class="line">  <span class="string">`order_comment`</span> <span class="built_in">varchar</span>(<span class="number">200</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'订单备注'</span>,</span><br><span class="line">  <span class="string">`out_trade_no`</span> <span class="built_in">varchar</span>(<span class="number">50</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'订单交易编号（第三方支付用)'</span>,</span><br><span class="line">  <span class="string">`trade_body`</span> <span class="built_in">varchar</span>(<span class="number">200</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'订单描述(第三方支付用)'</span>,</span><br><span class="line">  <span class="string">`create_time`</span> datetime <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'创建时间'</span>,</span><br><span class="line">  <span class="string">`operate_time`</span> datetime <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'操作时间'</span>,</span><br><span class="line">  <span class="string">`expire_time`</span> datetime <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'失效时间'</span>,</span><br><span class="line">  <span class="string">`tracking_no`</span> <span class="built_in">varchar</span>(<span class="number">100</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'物流单编号'</span>,</span><br><span class="line">  <span class="string">`parent_order_id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'父订单编号'</span>,</span><br><span class="line">  <span class="string">`img_url`</span> <span class="built_in">varchar</span>(<span class="number">200</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'图片路径'</span>,</span><br><span class="line">  <span class="string">`province_id`</span> <span class="built_in">int</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'地区'</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> AUTO_INCREMENT=<span class="number">1</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 <span class="keyword">COMMENT</span>=<span class="string">'订单表'</span>;</span><br></pre></td></tr></table></figure><ul><li>订单详情表（order_detail）</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`order_detail`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT <span class="keyword">COMMENT</span> <span class="string">'编号'</span>,</span><br><span class="line">  <span class="string">`order_id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'订单编号'</span>,</span><br><span class="line">  <span class="string">`sku_id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'sku_id'</span>,</span><br><span class="line">  <span class="string">`sku_name`</span> <span class="built_in">varchar</span>(<span class="number">200</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'sku名称（冗余)'</span>,</span><br><span class="line">  <span class="string">`img_url`</span> <span class="built_in">varchar</span>(<span class="number">200</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'图片名称（冗余)'</span>,</span><br><span class="line">  <span class="string">`order_price`</span> <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'购买价格(下单时sku价格）'</span>,</span><br><span class="line">  <span class="string">`sku_num`</span> <span class="built_in">varchar</span>(<span class="number">200</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'购买个数'</span>,</span><br><span class="line">  <span class="string">`create_time`</span> datetime <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'创建时间'</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> AUTO_INCREMENT=<span class="number">1</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 <span class="keyword">COMMENT</span>=<span class="string">'订单详情表'</span>;</span><br></pre></td></tr></table></figure><ul><li>商品表（sku_info）</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`sku_info`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT <span class="keyword">COMMENT</span> <span class="string">'skuid(itemID)'</span>,</span><br><span class="line">  <span class="string">`spu_id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'spuid'</span>,</span><br><span class="line">  <span class="string">`price`</span> <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">0</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'价格'</span>,</span><br><span class="line">  <span class="string">`sku_name`</span> <span class="built_in">varchar</span>(<span class="number">200</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'sku名称'</span>,</span><br><span class="line">  <span class="string">`sku_desc`</span> <span class="built_in">varchar</span>(<span class="number">2000</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'商品规格描述'</span>,</span><br><span class="line">  <span class="string">`weight`</span> <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'重量'</span>,</span><br><span class="line">  <span class="string">`tm_id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'品牌(冗余)'</span>,</span><br><span class="line">  <span class="string">`category3_id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'三级分类id（冗余)'</span>,</span><br><span class="line">  <span class="string">`sku_default_img`</span> <span class="built_in">varchar</span>(<span class="number">200</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'默认显示图片(冗余)'</span>,</span><br><span class="line">  <span class="string">`create_time`</span> datetime <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'创建时间'</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> AUTO_INCREMENT=<span class="number">1</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 <span class="keyword">COMMENT</span>=<span class="string">'商品表'</span>;</span><br></pre></td></tr></table></figure><ul><li>商品一级类目表（base_category1）</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`base_category1`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT <span class="keyword">COMMENT</span> <span class="string">'编号'</span>,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">10</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'分类名称'</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> AUTO_INCREMENT=<span class="number">1</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 <span class="keyword">COMMENT</span>=<span class="string">'一级分类表'</span>;</span><br></pre></td></tr></table></figure><ul><li>商品二级类目表（base_category2）</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`base_category2`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT <span class="keyword">COMMENT</span> <span class="string">'编号'</span>,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">200</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'二级分类名称'</span>,</span><br><span class="line">  <span class="string">`category1_id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'一级分类编号'</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> AUTO_INCREMENT=<span class="number">1</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 <span class="keyword">COMMENT</span>=<span class="string">'二级分类表'</span>;</span><br></pre></td></tr></table></figure><ul><li>商品三级类目表（base_category3）</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`base_category3`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT <span class="keyword">COMMENT</span> <span class="string">'编号'</span>,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">200</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'三级分类名称'</span>,</span><br><span class="line">  <span class="string">`category2_id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'二级分类编号'</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> AUTO_INCREMENT=<span class="number">1</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 <span class="keyword">COMMENT</span>=<span class="string">'三级分类表'</span>;</span><br></pre></td></tr></table></figure><ul><li>省份表（base_province）</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`base_province`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">int</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'id'</span>,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'省名称'</span>,</span><br><span class="line">  <span class="string">`region_id`</span> <span class="built_in">int</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'大区id'</span>,</span><br><span class="line">  <span class="string">`area_code`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'行政区位码'</span></span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8;</span><br></pre></td></tr></table></figure><ul><li>区域表（base_region）</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`base_region`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">int</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'大区id'</span>,</span><br><span class="line">  <span class="string">`region_name`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'大区名称'</span>,</span><br><span class="line">   PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8;</span><br></pre></td></tr></table></figure><blockquote><p>注意：以上的建表语句是在MySQL中完成的，完整的建表及模拟数据生成脚本见：</p><p>链接：<a href="https://pan.baidu.com/s/1fcMgDHGKedOpzqLbSRUGwA" target="_blank" rel="noopener">https://pan.baidu.com/s/1fcMgDHGKedOpzqLbSRUGwA</a> 提取码：zuqw</p></blockquote><h2 id="数据处理流程"><a href="#数据处理流程" class="headerlink" title="数据处理流程"></a>数据处理流程</h2><h3 id="ODS层数据同步"><a href="#ODS层数据同步" class="headerlink" title="ODS层数据同步"></a>ODS层数据同步</h3><p>关于ODS层的数据同步参见我的另一篇文章<a href="https://mp.weixin.qq.com/s/ooPAScXAw2soqlgEoSbRAw" target="_blank" rel="noopener">基于Canal与Flink实现数据实时增量同步(一)</a>。主要使用canal解析MySQL的binlog日志，然后将其写入到Kafka对应的topic中。由于篇幅限制，不会对具体的细节进行说明。同步之后的结果如下图所示：</p><p><img src="//jiamaoxiang.top/2020/08/12/实时数仓-基于Flink1-11的SQL构建实时数仓探索实践/ods.png" alt></p><h3 id="DIM层维表数据准备"><a href="#DIM层维表数据准备" class="headerlink" title="DIM层维表数据准备"></a>DIM层维表数据准备</h3><p>本案例中将维表存储在了MySQL中，实际生产中会用HBase存储维表数据。我们主要用到两张维表：<strong>区域维表</strong>和<strong>商品维表</strong>。处理过程如下：</p><ul><li>区域维表</li></ul><p>首先将<code>mydw.base_province</code>和<code>mydw.base_region</code>这个主题对应的数据抽取到MySQL中，主要使用Flink SQL的Kafka数据源对应的canal-json格式，注意：在执行装载之前，需要先在MySQL中创建对应的表，本文使用的MySQL数据库的名字为<strong>dim</strong>，用于存放维表数据。如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--   省份</span></span><br><span class="line"><span class="comment">--   kafka Source</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`ods_base_province`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`ods_base_province`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">INT</span>,</span><br><span class="line">  <span class="string">`name`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`region_id`</span> <span class="built_in">INT</span> ,</span><br><span class="line">  <span class="string">`area_code`</span><span class="keyword">STRING</span></span><br><span class="line">) <span class="keyword">WITH</span>(</span><br><span class="line"><span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line"> <span class="string">'topic'</span> = <span class="string">'mydw.base_province'</span>,</span><br><span class="line"> <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-3:9092'</span>,</span><br><span class="line"> <span class="string">'properties.group.id'</span> = <span class="string">'testGroup'</span>,</span><br><span class="line"> <span class="string">'format'</span> = <span class="string">'canal-json'</span> ,</span><br><span class="line"> <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span> </span><br><span class="line">) ; </span><br><span class="line"></span><br><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--   省份</span></span><br><span class="line"><span class="comment">--   MySQL Sink</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`base_province`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`base_province`</span> (</span><br><span class="line">    <span class="string">`id`</span> <span class="built_in">INT</span>,</span><br><span class="line">    <span class="string">`name`</span> <span class="keyword">STRING</span>,</span><br><span class="line">    <span class="string">`region_id`</span> <span class="built_in">INT</span> ,</span><br><span class="line">    <span class="string">`area_code`</span><span class="keyword">STRING</span>,</span><br><span class="line">    PRIMARY <span class="keyword">KEY</span> (<span class="keyword">id</span>) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'jdbc'</span>,</span><br><span class="line">    <span class="string">'url'</span> = <span class="string">'jdbc:mysql://kms-1:3306/dim'</span>,</span><br><span class="line">    <span class="string">'table-name'</span> = <span class="string">'base_province'</span>, <span class="comment">-- MySQL中的待插入数据的表</span></span><br><span class="line">    <span class="string">'driver'</span> = <span class="string">'com.mysql.jdbc.Driver'</span>,</span><br><span class="line">    <span class="string">'username'</span> = <span class="string">'root'</span>,</span><br><span class="line">    <span class="string">'password'</span> = <span class="string">'123qwe'</span>,</span><br><span class="line">    <span class="string">'sink.buffer-flush.interval'</span> = <span class="string">'1s'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--   省份</span></span><br><span class="line"><span class="comment">--   MySQL Sink Load Data</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> base_province</span><br><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> ods_base_province;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--   区域</span></span><br><span class="line"><span class="comment">--   kafka Source</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`ods_base_region`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`ods_base_region`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">INT</span>,</span><br><span class="line">  <span class="string">`region_name`</span> <span class="keyword">STRING</span></span><br><span class="line">) <span class="keyword">WITH</span>(</span><br><span class="line"><span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line"> <span class="string">'topic'</span> = <span class="string">'mydw.base_region'</span>,</span><br><span class="line"> <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-3:9092'</span>,</span><br><span class="line"> <span class="string">'properties.group.id'</span> = <span class="string">'testGroup'</span>,</span><br><span class="line"> <span class="string">'format'</span> = <span class="string">'canal-json'</span> ,</span><br><span class="line"> <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span> </span><br><span class="line">) ; </span><br><span class="line"></span><br><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--   区域</span></span><br><span class="line"><span class="comment">--   MySQL Sink</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`base_region`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`base_region`</span> (</span><br><span class="line">    <span class="string">`id`</span> <span class="built_in">INT</span>,</span><br><span class="line">    <span class="string">`region_name`</span> <span class="keyword">STRING</span>,</span><br><span class="line">     PRIMARY <span class="keyword">KEY</span> (<span class="keyword">id</span>) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'jdbc'</span>,</span><br><span class="line">    <span class="string">'url'</span> = <span class="string">'jdbc:mysql://kms-1:3306/dim'</span>,</span><br><span class="line">    <span class="string">'table-name'</span> = <span class="string">'base_region'</span>, <span class="comment">-- MySQL中的待插入数据的表</span></span><br><span class="line">    <span class="string">'driver'</span> = <span class="string">'com.mysql.jdbc.Driver'</span>,</span><br><span class="line">    <span class="string">'username'</span> = <span class="string">'root'</span>,</span><br><span class="line">    <span class="string">'password'</span> = <span class="string">'123qwe'</span>,</span><br><span class="line">    <span class="string">'sink.buffer-flush.interval'</span> = <span class="string">'1s'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--   区域</span></span><br><span class="line"><span class="comment">--   MySQL Sink Load Data</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> base_region</span><br><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> ods_base_region;</span><br></pre></td></tr></table></figure><p>经过上面的步骤，将创建维表所需要的原始数据已经存储到了MySQL中，接下来就需要在MySQL中创建维表，我们使用上面的两张表，创建一张视图：<code>dim_province</code>作为维表：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="comment">-- DIM层,区域维表,</span></span><br><span class="line"><span class="comment">-- 在MySQL中创建视图</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">VIEW</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> dim_province;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> dim_province <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  bp.id <span class="keyword">AS</span> province_id,</span><br><span class="line">  bp.name <span class="keyword">AS</span> province_name,</span><br><span class="line">  br.id <span class="keyword">AS</span> region_id,</span><br><span class="line">  br.region_name <span class="keyword">AS</span> region_name,</span><br><span class="line">  bp.area_code <span class="keyword">AS</span> area_code</span><br><span class="line"><span class="keyword">FROM</span> base_region br </span><br><span class="line">     <span class="keyword">JOIN</span> base_province bp <span class="keyword">ON</span> br.id= bp.region_id</span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>这样我们所需要的维表：dim_province就创建好了，只需要在维表join时，使用Flink SQL创建JDBC的数据源，就可以使用该维表了。同理，我们使用相同的方法创建商品维表，具体如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--  一级类目表</span></span><br><span class="line"><span class="comment">--   kafka Source</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`ods_base_category1`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`ods_base_category1`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`name`</span> <span class="keyword">STRING</span></span><br><span class="line">)<span class="keyword">WITH</span>(</span><br><span class="line"> <span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line"> <span class="string">'topic'</span> = <span class="string">'mydw.base_category1'</span>,</span><br><span class="line"> <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-3:9092'</span>,</span><br><span class="line"> <span class="string">'properties.group.id'</span> = <span class="string">'testGroup'</span>,</span><br><span class="line"> <span class="string">'format'</span> = <span class="string">'canal-json'</span> ,</span><br><span class="line"> <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span> </span><br><span class="line">) ;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--  一级类目表</span></span><br><span class="line"><span class="comment">--   MySQL Sink</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`base_category1`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`base_category1`</span> (</span><br><span class="line">    <span class="string">`id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">    <span class="string">`name`</span> <span class="keyword">STRING</span>,</span><br><span class="line">     PRIMARY <span class="keyword">KEY</span> (<span class="keyword">id</span>) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'jdbc'</span>,</span><br><span class="line">    <span class="string">'url'</span> = <span class="string">'jdbc:mysql://kms-1:3306/dim'</span>,</span><br><span class="line">    <span class="string">'table-name'</span> = <span class="string">'base_category1'</span>, <span class="comment">-- MySQL中的待插入数据的表</span></span><br><span class="line">    <span class="string">'driver'</span> = <span class="string">'com.mysql.jdbc.Driver'</span>,</span><br><span class="line">    <span class="string">'username'</span> = <span class="string">'root'</span>,</span><br><span class="line">    <span class="string">'password'</span> = <span class="string">'123qwe'</span>,</span><br><span class="line">    <span class="string">'sink.buffer-flush.interval'</span> = <span class="string">'1s'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--  一级类目表</span></span><br><span class="line"><span class="comment">--   MySQL Sink Load Data</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> base_category1</span><br><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> ods_base_category1;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--  二级类目表</span></span><br><span class="line"><span class="comment">--   kafka Source</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`ods_base_category2`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`ods_base_category2`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`name`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`category1_id`</span> <span class="built_in">BIGINT</span></span><br><span class="line">)<span class="keyword">WITH</span>(</span><br><span class="line"><span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line"> <span class="string">'topic'</span> = <span class="string">'mydw.base_category2'</span>,</span><br><span class="line"> <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-3:9092'</span>,</span><br><span class="line"> <span class="string">'properties.group.id'</span> = <span class="string">'testGroup'</span>,</span><br><span class="line"> <span class="string">'format'</span> = <span class="string">'canal-json'</span> ,</span><br><span class="line"> <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span> </span><br><span class="line">) ;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--  二级类目表</span></span><br><span class="line"><span class="comment">--   MySQL Sink</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`base_category2`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`base_category2`</span> (</span><br><span class="line">    <span class="string">`id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">    <span class="string">`name`</span> <span class="keyword">STRING</span>,</span><br><span class="line">    <span class="string">`category1_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">     PRIMARY <span class="keyword">KEY</span> (<span class="keyword">id</span>) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'jdbc'</span>,</span><br><span class="line">    <span class="string">'url'</span> = <span class="string">'jdbc:mysql://kms-1:3306/dim'</span>,</span><br><span class="line">    <span class="string">'table-name'</span> = <span class="string">'base_category2'</span>, <span class="comment">-- MySQL中的待插入数据的表</span></span><br><span class="line">    <span class="string">'driver'</span> = <span class="string">'com.mysql.jdbc.Driver'</span>,</span><br><span class="line">    <span class="string">'username'</span> = <span class="string">'root'</span>,</span><br><span class="line">    <span class="string">'password'</span> = <span class="string">'123qwe'</span>,</span><br><span class="line">    <span class="string">'sink.buffer-flush.interval'</span> = <span class="string">'1s'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--  二级类目表</span></span><br><span class="line"><span class="comment">--   MySQL Sink Load Data</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> base_category2</span><br><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> ods_base_category2;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">-- 三级类目表</span></span><br><span class="line"><span class="comment">--   kafka Source</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`ods_base_category3`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`ods_base_category3`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`name`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`category2_id`</span> <span class="built_in">BIGINT</span></span><br><span class="line">)<span class="keyword">WITH</span>(</span><br><span class="line"><span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line"> <span class="string">'topic'</span> = <span class="string">'mydw.base_category3'</span>,</span><br><span class="line"> <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-3:9092'</span>,</span><br><span class="line"> <span class="string">'properties.group.id'</span> = <span class="string">'testGroup'</span>,</span><br><span class="line"> <span class="string">'format'</span> = <span class="string">'canal-json'</span> ,</span><br><span class="line"> <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span> </span><br><span class="line">) ; </span><br><span class="line"></span><br><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--  三级类目表</span></span><br><span class="line"><span class="comment">--   MySQL Sink</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`base_category3`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`base_category3`</span> (</span><br><span class="line">    <span class="string">`id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">    <span class="string">`name`</span> <span class="keyword">STRING</span>,</span><br><span class="line">    <span class="string">`category2_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">    PRIMARY <span class="keyword">KEY</span> (<span class="keyword">id</span>) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'jdbc'</span>,</span><br><span class="line">    <span class="string">'url'</span> = <span class="string">'jdbc:mysql://kms-1:3306/dim'</span>,</span><br><span class="line">    <span class="string">'table-name'</span> = <span class="string">'base_category3'</span>, <span class="comment">-- MySQL中的待插入数据的表</span></span><br><span class="line">    <span class="string">'driver'</span> = <span class="string">'com.mysql.jdbc.Driver'</span>,</span><br><span class="line">    <span class="string">'username'</span> = <span class="string">'root'</span>,</span><br><span class="line">    <span class="string">'password'</span> = <span class="string">'123qwe'</span>,</span><br><span class="line">    <span class="string">'sink.buffer-flush.interval'</span> = <span class="string">'1s'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--  三级类目表</span></span><br><span class="line"><span class="comment">--   MySQL Sink Load Data</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> base_category3</span><br><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> ods_base_category3;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--   商品表</span></span><br><span class="line"><span class="comment">--   Kafka Source</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`ods_sku_info`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`ods_sku_info`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`spu_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`price`</span> <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">0</span>),</span><br><span class="line">  <span class="string">`sku_name`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`sku_desc`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`weight`</span> <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">  <span class="string">`tm_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`category3_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`sku_default_img`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`create_time`</span> <span class="built_in">TIMESTAMP</span>(<span class="number">0</span>)</span><br><span class="line">) <span class="keyword">WITH</span>(</span><br><span class="line"> <span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line"> <span class="string">'topic'</span> = <span class="string">'mydw.sku_info'</span>,</span><br><span class="line"> <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-3:9092'</span>,</span><br><span class="line"> <span class="string">'properties.group.id'</span> = <span class="string">'testGroup'</span>,</span><br><span class="line"> <span class="string">'format'</span> = <span class="string">'canal-json'</span> ,</span><br><span class="line"> <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span> </span><br><span class="line">) ; </span><br><span class="line"></span><br><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--   商品表</span></span><br><span class="line"><span class="comment">--   MySQL Sink</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`sku_info`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`sku_info`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`spu_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`price`</span> <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">0</span>),</span><br><span class="line">  <span class="string">`sku_name`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`sku_desc`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`weight`</span> <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">  <span class="string">`tm_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`category3_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`sku_default_img`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`create_time`</span> <span class="built_in">TIMESTAMP</span>(<span class="number">0</span>),</span><br><span class="line">   PRIMARY <span class="keyword">KEY</span> (tm_id) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'jdbc'</span>,</span><br><span class="line">    <span class="string">'url'</span> = <span class="string">'jdbc:mysql://kms-1:3306/dim'</span>,</span><br><span class="line">    <span class="string">'table-name'</span> = <span class="string">'sku_info'</span>, <span class="comment">-- MySQL中的待插入数据的表</span></span><br><span class="line">    <span class="string">'driver'</span> = <span class="string">'com.mysql.jdbc.Driver'</span>,</span><br><span class="line">    <span class="string">'username'</span> = <span class="string">'root'</span>,</span><br><span class="line">    <span class="string">'password'</span> = <span class="string">'123qwe'</span>,</span><br><span class="line">    <span class="string">'sink.buffer-flush.interval'</span> = <span class="string">'1s'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--   商品</span></span><br><span class="line"><span class="comment">--   MySQL Sink Load Data</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sku_info</span><br><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> ods_sku_info;</span><br></pre></td></tr></table></figure><p>经过上面的步骤，我们可以将创建商品维表的基础数据表同步到MySQL中，同样需要提前创建好对应的数据表。接下来我们使用上面的基础表在mySQL的dim库中创建一张视图：<code>dim_sku_info</code>，用作后续使用的维表。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="comment">-- DIM层,商品维表,</span></span><br><span class="line"><span class="comment">-- 在MySQL中创建视图</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> dim_sku_info <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  si.id <span class="keyword">AS</span> <span class="keyword">id</span>,</span><br><span class="line">  si.sku_name <span class="keyword">AS</span> sku_name,</span><br><span class="line">  si.category3_id <span class="keyword">AS</span> c3_id,</span><br><span class="line">  si.weight <span class="keyword">AS</span> weight,</span><br><span class="line">  si.tm_id <span class="keyword">AS</span> tm_id,</span><br><span class="line">  si.price <span class="keyword">AS</span> price,</span><br><span class="line">  si.spu_id <span class="keyword">AS</span> spu_id,</span><br><span class="line">  c3.name <span class="keyword">AS</span> c3_name,</span><br><span class="line">  c2.id <span class="keyword">AS</span> c2_id,</span><br><span class="line">  c2.name <span class="keyword">AS</span> c2_name,</span><br><span class="line">  c3.id <span class="keyword">AS</span> c1_id,</span><br><span class="line">  c3.name <span class="keyword">AS</span> c1_name</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">(</span><br><span class="line">  sku_info si </span><br><span class="line">  <span class="keyword">JOIN</span> base_category3 c3 <span class="keyword">ON</span> si.category3_id = c3.id</span><br><span class="line">  <span class="keyword">JOIN</span> base_category2 c2 <span class="keyword">ON</span> c3.category2_id =c2.id</span><br><span class="line">  <span class="keyword">JOIN</span> base_category1 c1 <span class="keyword">ON</span> c2.category1_id = c1.id</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>至此，我们所需要的维表数据已经准备好了，接下来开始处理DWD层的数据。</p><h3 id="DWD层数据处理"><a href="#DWD层数据处理" class="headerlink" title="DWD层数据处理"></a>DWD层数据处理</h3><p>经过上面的步骤，我们已经将所用的维表已经准备好了。接下来我们将对ODS的原始数据进行处理，加工成DWD层的明细宽表。具体过程如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--   订单详情</span></span><br><span class="line"><span class="comment">--   Kafka Source</span></span><br><span class="line"><span class="comment">-- ------------------------- </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`ods_order_detail`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`ods_order_detail`</span>(</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`order_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`sku_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`sku_name`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`img_url`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`order_price`</span> <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">  <span class="string">`sku_num`</span> <span class="built_in">INT</span>,</span><br><span class="line">  <span class="string">`create_time`</span> <span class="built_in">TIMESTAMP</span>(<span class="number">0</span>)</span><br><span class="line">) <span class="keyword">WITH</span>(</span><br><span class="line"> <span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line"> <span class="string">'topic'</span> = <span class="string">'mydw.order_detail'</span>,</span><br><span class="line"> <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-3:9092'</span>,</span><br><span class="line"> <span class="string">'properties.group.id'</span> = <span class="string">'testGroup'</span>,</span><br><span class="line"> <span class="string">'format'</span> = <span class="string">'canal-json'</span> ,</span><br><span class="line"> <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span> </span><br><span class="line">) ; </span><br><span class="line"></span><br><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="comment">--   订单信息</span></span><br><span class="line"><span class="comment">--   Kafka Source</span></span><br><span class="line"><span class="comment">-- -------------------------</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`ods_order_info`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`ods_order_info`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`consignee`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`consignee_tel`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`total_amount`</span> <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">  <span class="string">`order_status`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`user_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`payment_way`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`delivery_address`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`order_comment`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`out_trade_no`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`trade_body`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`create_time`</span> <span class="built_in">TIMESTAMP</span>(<span class="number">0</span>) ,</span><br><span class="line">  <span class="string">`operate_time`</span> <span class="built_in">TIMESTAMP</span>(<span class="number">0</span>) ,</span><br><span class="line">  <span class="string">`expire_time`</span> <span class="built_in">TIMESTAMP</span>(<span class="number">0</span>) ,</span><br><span class="line">  <span class="string">`tracking_no`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`parent_order_id`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="string">`img_url`</span> <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="string">`province_id`</span> <span class="built_in">INT</span></span><br><span class="line">) <span class="keyword">WITH</span>(</span><br><span class="line"><span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line"> <span class="string">'topic'</span> = <span class="string">'mydw.order_info'</span>,</span><br><span class="line"> <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-3:9092'</span>,</span><br><span class="line"> <span class="string">'properties.group.id'</span> = <span class="string">'testGroup'</span>,</span><br><span class="line"> <span class="string">'format'</span> = <span class="string">'canal-json'</span> ,</span><br><span class="line"> <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span> </span><br><span class="line">) ; </span><br><span class="line"></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="comment">-- DWD层,支付订单明细表dwd_paid_order_detail</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> dwd_paid_order_detail;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> dwd_paid_order_detail</span><br><span class="line">(</span><br><span class="line">  detail_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  order_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  user_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  province_id <span class="built_in">INT</span>,</span><br><span class="line">  sku_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  sku_name <span class="keyword">STRING</span>,</span><br><span class="line">  sku_num <span class="built_in">INT</span>,</span><br><span class="line">  order_price <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">0</span>),</span><br><span class="line">  create_time <span class="built_in">TIMESTAMP</span>(<span class="number">0</span>),</span><br><span class="line">  pay_time <span class="built_in">TIMESTAMP</span>(<span class="number">0</span>)</span><br><span class="line"> ) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line">    <span class="string">'topic'</span> = <span class="string">'dwd_paid_order_detail'</span>,</span><br><span class="line">    <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span>,</span><br><span class="line">    <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-3:9092'</span>,</span><br><span class="line">    <span class="string">'format'</span> = <span class="string">'changelog-json'</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="comment">-- DWD层,已支付订单明细表</span></span><br><span class="line"><span class="comment">-- 向dwd_paid_order_detail装载数据</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> dwd_paid_order_detail</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  od.id,</span><br><span class="line">  oi.id order_id,</span><br><span class="line">  oi.user_id,</span><br><span class="line">  oi.province_id,</span><br><span class="line">  od.sku_id,</span><br><span class="line">  od.sku_name,</span><br><span class="line">  od.sku_num,</span><br><span class="line">  od.order_price,</span><br><span class="line">  oi.create_time,</span><br><span class="line">  oi.operate_time</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">    (</span><br><span class="line">    <span class="keyword">SELECT</span> * </span><br><span class="line">    <span class="keyword">FROM</span> ods_order_info</span><br><span class="line">    <span class="keyword">WHERE</span> order_status = <span class="string">'2'</span> <span class="comment">-- 已支付</span></span><br><span class="line">    ) oi <span class="keyword">JOIN</span></span><br><span class="line">    (</span><br><span class="line">    <span class="keyword">SELECT</span> *</span><br><span class="line">    <span class="keyword">FROM</span> ods_order_detail</span><br><span class="line">    ) od </span><br><span class="line">    <span class="keyword">ON</span> oi.id = od.order_id;</span><br></pre></td></tr></table></figure><img src="//jiamaoxiang.top/2020/08/12/实时数仓-基于Flink1-11的SQL构建实时数仓探索实践/npm\mywebsite\source\_posts\实时数仓-基于Flink1-11的SQL构建实时数仓探索实践\任务1.png"><h3 id="ADS层数据"><a href="#ADS层数据" class="headerlink" title="ADS层数据"></a>ADS层数据</h3><p>经过上面的步骤，我们创建了一张dwd_paid_order_detail明细宽表，并将该表存储在了Kafka中。接下来我们将使用这张明细宽表与维表进行JOIN，得到我们ADS应用层数据。</p><ul><li><strong>ads_province_index</strong></li></ul><p>首先在MySQL中创建对应的ADS目标表：<strong>ads_province_index</strong></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> ads.ads_province_index(</span><br><span class="line">  province_id <span class="built_in">INT</span>(<span class="number">10</span>),</span><br><span class="line">  area_code <span class="built_in">VARCHAR</span>(<span class="number">100</span>),</span><br><span class="line">  province_name <span class="built_in">VARCHAR</span>(<span class="number">100</span>),</span><br><span class="line">  region_id <span class="built_in">INT</span>(<span class="number">10</span>),</span><br><span class="line">  region_name <span class="built_in">VARCHAR</span>(<span class="number">100</span>),</span><br><span class="line">  order_amount <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">  order_count <span class="built_in">BIGINT</span>(<span class="number">10</span>),</span><br><span class="line">  dt <span class="built_in">VARCHAR</span>(<span class="number">100</span>),</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (province_id, dt) </span><br><span class="line">) ;</span><br></pre></td></tr></table></figure><p>向MySQL的ADS层目标装载数据：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- Flink SQL Cli操作</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="comment">-- 使用 DDL创建MySQL中的ADS层表</span></span><br><span class="line"><span class="comment">-- 指标：1.每天每个省份的订单数</span></span><br><span class="line"><span class="comment">--      2.每天每个省份的订单金额</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> ads_province_index(</span><br><span class="line">  province_id <span class="built_in">INT</span>,</span><br><span class="line">  area_code <span class="keyword">STRING</span>,</span><br><span class="line">  province_name <span class="keyword">STRING</span>,</span><br><span class="line">  region_id <span class="built_in">INT</span>,</span><br><span class="line">  region_name <span class="keyword">STRING</span>,</span><br><span class="line">  order_amount <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">  order_count <span class="built_in">BIGINT</span>,</span><br><span class="line">  dt <span class="keyword">STRING</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (province_id, dt) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span>  </span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'jdbc'</span>,</span><br><span class="line">    <span class="string">'url'</span> = <span class="string">'jdbc:mysql://kms-1:3306/ads'</span>,</span><br><span class="line">    <span class="string">'table-name'</span> = <span class="string">'ads_province_index'</span>, </span><br><span class="line">    <span class="string">'driver'</span> = <span class="string">'com.mysql.jdbc.Driver'</span>,</span><br><span class="line">    <span class="string">'username'</span> = <span class="string">'root'</span>,</span><br><span class="line">    <span class="string">'password'</span> = <span class="string">'123qwe'</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="comment">-- dwd_paid_order_detail已支付订单明细宽表</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> dwd_paid_order_detail</span><br><span class="line">(</span><br><span class="line">  detail_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  order_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  user_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  province_id <span class="built_in">INT</span>,</span><br><span class="line">  sku_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  sku_name <span class="keyword">STRING</span>,</span><br><span class="line">  sku_num <span class="built_in">INT</span>,</span><br><span class="line">  order_price <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">  create_time <span class="keyword">STRING</span>,</span><br><span class="line">  pay_time <span class="keyword">STRING</span></span><br><span class="line"> ) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line">    <span class="string">'topic'</span> = <span class="string">'dwd_paid_order_detail'</span>,</span><br><span class="line">    <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span>,</span><br><span class="line">    <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-3:9092'</span>,</span><br><span class="line">    <span class="string">'format'</span> = <span class="string">'changelog-json'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="comment">-- tmp_province_index</span></span><br><span class="line"><span class="comment">-- 订单汇总临时表</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> tmp_province_index(</span><br><span class="line">    province_id <span class="built_in">INT</span>,</span><br><span class="line">    order_count <span class="built_in">BIGINT</span>,<span class="comment">-- 订单数</span></span><br><span class="line">    order_amount <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>), <span class="comment">-- 订单金额</span></span><br><span class="line">    pay_date <span class="built_in">DATE</span></span><br><span class="line">)<span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line">    <span class="string">'topic'</span> = <span class="string">'tmp_province_index'</span>,</span><br><span class="line">    <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span>,</span><br><span class="line">    <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-3:9092'</span>,</span><br><span class="line">    <span class="string">'format'</span> = <span class="string">'changelog-json'</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="comment">-- tmp_province_index</span></span><br><span class="line"><span class="comment">-- 订单汇总临时表数据装载</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> tmp_province_index</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">      province_id,</span><br><span class="line">      <span class="keyword">count</span>(<span class="keyword">distinct</span> order_id) order_count,<span class="comment">-- 订单数</span></span><br><span class="line">      <span class="keyword">sum</span>(order_price * sku_num) order_amount, <span class="comment">-- 订单金额</span></span><br><span class="line">      <span class="keyword">TO_DATE</span>(pay_time,<span class="string">'yyyy-MM-dd'</span>) pay_date</span><br><span class="line"><span class="keyword">FROM</span> dwd_paid_order_detail</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> province_id,<span class="keyword">TO_DATE</span>(pay_time,<span class="string">'yyyy-MM-dd'</span>)</span><br><span class="line">;</span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="comment">-- tmp_province_index_source</span></span><br><span class="line"><span class="comment">-- 使用该临时汇总表，作为数据源</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> tmp_province_index_source(</span><br><span class="line">    province_id <span class="built_in">INT</span>,</span><br><span class="line">    order_count <span class="built_in">BIGINT</span>,<span class="comment">-- 订单数</span></span><br><span class="line">    order_amount <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>), <span class="comment">-- 订单金额</span></span><br><span class="line">    pay_date <span class="built_in">DATE</span>,</span><br><span class="line">    proctime <span class="keyword">as</span> PROCTIME()   <span class="comment">-- 通过计算列产生一个处理时间列</span></span><br><span class="line"> ) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line">    <span class="string">'topic'</span> = <span class="string">'tmp_province_index'</span>,</span><br><span class="line">    <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span>,</span><br><span class="line">    <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-3:9092'</span>,</span><br><span class="line">    <span class="string">'format'</span> = <span class="string">'changelog-json'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="comment">-- DIM层,区域维表,</span></span><br><span class="line"><span class="comment">-- 创建区域维表数据源</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`dim_province`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> dim_province (</span><br><span class="line">  province_id <span class="built_in">INT</span>,</span><br><span class="line">  province_name <span class="keyword">STRING</span>,</span><br><span class="line">  area_code <span class="keyword">STRING</span>,</span><br><span class="line">  region_id <span class="built_in">INT</span>,</span><br><span class="line">  region_name <span class="keyword">STRING</span> ,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (province_id) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'jdbc'</span>,</span><br><span class="line">    <span class="string">'url'</span> = <span class="string">'jdbc:mysql://kms-1:3306/dim'</span>,</span><br><span class="line">    <span class="string">'table-name'</span> = <span class="string">'dim_province'</span>, </span><br><span class="line">    <span class="string">'driver'</span> = <span class="string">'com.mysql.jdbc.Driver'</span>,</span><br><span class="line">    <span class="string">'username'</span> = <span class="string">'root'</span>,</span><br><span class="line">    <span class="string">'password'</span> = <span class="string">'123qwe'</span>,</span><br><span class="line">    <span class="string">'scan.fetch-size'</span> = <span class="string">'100'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="comment">-- 向ads_province_index装载数据</span></span><br><span class="line"><span class="comment">-- 维表JOIN</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> ads_province_index</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  pc.province_id,</span><br><span class="line">  dp.area_code,</span><br><span class="line">  dp.province_name,</span><br><span class="line">  dp.region_id,</span><br><span class="line">  dp.region_name,</span><br><span class="line">  pc.order_amount,</span><br><span class="line">  pc.order_count,</span><br><span class="line">  <span class="keyword">cast</span>(pc.pay_date <span class="keyword">as</span> <span class="built_in">VARCHAR</span>)</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">tmp_province_index_source pc</span><br><span class="line">  <span class="keyword">JOIN</span> dim_province <span class="keyword">FOR</span> SYSTEM_TIME <span class="keyword">AS</span> <span class="keyword">OF</span> pc.proctime <span class="keyword">as</span> dp </span><br><span class="line">  <span class="keyword">ON</span> dp.province_id = pc.province_id;</span><br></pre></td></tr></table></figure><p>当提交任务之后：观察Flink WEB UI：</p><img src="//jiamaoxiang.top/2020/08/12/实时数仓-基于Flink1-11的SQL构建实时数仓探索实践/任务2.png" style="zoom:50%;"><p>查看ADS层的ads_province_index表数据：</p><p><img src="//jiamaoxiang.top/2020/08/12/实时数仓-基于Flink1-11的SQL构建实时数仓探索实践/%E4%BB%BB%E5%8A%A12%E7%BB%93%E6%9E%9C.png" alt></p><ul><li><strong>ads_sku_index</strong></li></ul><p>首先在MySQL中创建对应的ADS目标表：<strong>ads_sku_index</strong></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> ads_sku_index</span><br><span class="line">(</span><br><span class="line">  sku_id <span class="built_in">BIGINT</span>(<span class="number">10</span>),</span><br><span class="line">  sku_name <span class="built_in">VARCHAR</span>(<span class="number">100</span>),</span><br><span class="line">  weight <span class="keyword">DOUBLE</span>,</span><br><span class="line">  tm_id <span class="built_in">BIGINT</span>(<span class="number">10</span>),</span><br><span class="line">  price <span class="keyword">DOUBLE</span>,</span><br><span class="line">  spu_id <span class="built_in">BIGINT</span>(<span class="number">10</span>),</span><br><span class="line">  c3_id <span class="built_in">BIGINT</span>(<span class="number">10</span>),</span><br><span class="line">  c3_name <span class="built_in">VARCHAR</span>(<span class="number">100</span>) ,</span><br><span class="line">  c2_id <span class="built_in">BIGINT</span>(<span class="number">10</span>),</span><br><span class="line">  c2_name <span class="built_in">VARCHAR</span>(<span class="number">100</span>),</span><br><span class="line">  c1_id <span class="built_in">BIGINT</span>(<span class="number">10</span>),</span><br><span class="line">  c1_name <span class="built_in">VARCHAR</span>(<span class="number">100</span>),</span><br><span class="line">  order_amount <span class="keyword">DOUBLE</span>,</span><br><span class="line">  order_count <span class="built_in">BIGINT</span>(<span class="number">10</span>),</span><br><span class="line">  sku_count <span class="built_in">BIGINT</span>(<span class="number">10</span>),</span><br><span class="line">  dt <span class="built_in">varchar</span>(<span class="number">100</span>),</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (sku_id,dt)</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>向MySQL的ADS层目标装载数据：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="comment">-- 使用 DDL创建MySQL中的ADS层表</span></span><br><span class="line"><span class="comment">-- 指标：1.每天每个商品对应的订单个数</span></span><br><span class="line"><span class="comment">--      2.每天每个商品对应的订单金额</span></span><br><span class="line"><span class="comment">--      3.每天每个商品对应的数量</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> ads_sku_index</span><br><span class="line">(</span><br><span class="line">  sku_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  sku_name <span class="built_in">VARCHAR</span>,</span><br><span class="line">  weight <span class="keyword">DOUBLE</span>,</span><br><span class="line">  tm_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  price <span class="keyword">DOUBLE</span>,</span><br><span class="line">  spu_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  c3_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  c3_name <span class="built_in">VARCHAR</span> ,</span><br><span class="line">  c2_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  c2_name <span class="built_in">VARCHAR</span>,</span><br><span class="line">  c1_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  c1_name <span class="built_in">VARCHAR</span>,</span><br><span class="line">  order_amount <span class="keyword">DOUBLE</span>,</span><br><span class="line">  order_count <span class="built_in">BIGINT</span>,</span><br><span class="line">  sku_count <span class="built_in">BIGINT</span>,</span><br><span class="line">  dt <span class="built_in">varchar</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (sku_id,dt) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'jdbc'</span>,</span><br><span class="line">    <span class="string">'url'</span> = <span class="string">'jdbc:mysql://kms-1:3306/ads'</span>,</span><br><span class="line">    <span class="string">'table-name'</span> = <span class="string">'ads_sku_index'</span>, </span><br><span class="line">    <span class="string">'driver'</span> = <span class="string">'com.mysql.jdbc.Driver'</span>,</span><br><span class="line">    <span class="string">'username'</span> = <span class="string">'root'</span>,</span><br><span class="line">    <span class="string">'password'</span> = <span class="string">'123qwe'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="comment">-- dwd_paid_order_detail已支付订单明细宽表</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> dwd_paid_order_detail</span><br><span class="line">(</span><br><span class="line">  detail_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  order_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  user_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  province_id <span class="built_in">INT</span>,</span><br><span class="line">  sku_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  sku_name <span class="keyword">STRING</span>,</span><br><span class="line">  sku_num <span class="built_in">INT</span>,</span><br><span class="line">  order_price <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">  create_time <span class="keyword">STRING</span>,</span><br><span class="line">  pay_time <span class="keyword">STRING</span></span><br><span class="line"> ) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line">    <span class="string">'topic'</span> = <span class="string">'dwd_paid_order_detail'</span>,</span><br><span class="line">    <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span>,</span><br><span class="line">    <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-3:9092'</span>,</span><br><span class="line">    <span class="string">'format'</span> = <span class="string">'changelog-json'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="comment">-- tmp_sku_index</span></span><br><span class="line"><span class="comment">-- 商品指标统计</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> tmp_sku_index(</span><br><span class="line">    sku_id <span class="built_in">BIGINT</span>,</span><br><span class="line">    order_count <span class="built_in">BIGINT</span>,<span class="comment">-- 订单数</span></span><br><span class="line">    order_amount <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>), <span class="comment">-- 订单金额</span></span><br><span class="line">order_sku_num <span class="built_in">BIGINT</span>,</span><br><span class="line">    pay_date <span class="built_in">DATE</span></span><br><span class="line">)<span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line">    <span class="string">'topic'</span> = <span class="string">'tmp_sku_index'</span>,</span><br><span class="line">    <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span>,</span><br><span class="line">    <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-3:9092'</span>,</span><br><span class="line">    <span class="string">'format'</span> = <span class="string">'changelog-json'</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="comment">-- tmp_sku_index</span></span><br><span class="line"><span class="comment">-- 数据装载</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> tmp_sku_index</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">      sku_id,</span><br><span class="line">      <span class="keyword">count</span>(<span class="keyword">distinct</span> order_id) order_count,<span class="comment">-- 订单数</span></span><br><span class="line">      <span class="keyword">sum</span>(order_price * sku_num) order_amount, <span class="comment">-- 订单金额</span></span><br><span class="line">  <span class="keyword">sum</span>(sku_num) order_sku_num,</span><br><span class="line">      <span class="keyword">TO_DATE</span>(pay_time,<span class="string">'yyyy-MM-dd'</span>) pay_date</span><br><span class="line"><span class="keyword">FROM</span> dwd_paid_order_detail</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> sku_id,<span class="keyword">TO_DATE</span>(pay_time,<span class="string">'yyyy-MM-dd'</span>)</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="comment">-- tmp_sku_index_source</span></span><br><span class="line"><span class="comment">-- 使用该临时汇总表，作为数据源</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> tmp_sku_index_source(</span><br><span class="line">    sku_id <span class="built_in">BIGINT</span>,</span><br><span class="line">    order_count <span class="built_in">BIGINT</span>,<span class="comment">-- 订单数</span></span><br><span class="line">    order_amount <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>), <span class="comment">-- 订单金额</span></span><br><span class="line">    order_sku_num <span class="built_in">BIGINT</span>,</span><br><span class="line">    pay_date <span class="built_in">DATE</span>,</span><br><span class="line">    proctime <span class="keyword">as</span> PROCTIME()   <span class="comment">-- 通过计算列产生一个处理时间列</span></span><br><span class="line"> ) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line">    <span class="string">'topic'</span> = <span class="string">'tmp_sku_index'</span>,</span><br><span class="line">    <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span>,</span><br><span class="line">    <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-3:9092'</span>,</span><br><span class="line">    <span class="string">'format'</span> = <span class="string">'changelog-json'</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="comment">-- DIM层,商品维表,</span></span><br><span class="line"><span class="comment">-- 创建商品维表数据源</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`dim_sku_info`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> dim_sku_info (</span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  sku_name <span class="keyword">STRING</span>,</span><br><span class="line">  c3_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  weight <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">  tm_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  price <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">  spu_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  c3_name <span class="keyword">STRING</span>,</span><br><span class="line">  c2_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  c2_name <span class="keyword">STRING</span>,</span><br><span class="line">  c1_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  c1_name <span class="keyword">STRING</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="keyword">id</span>) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'jdbc'</span>,</span><br><span class="line">    <span class="string">'url'</span> = <span class="string">'jdbc:mysql://kms-1:3306/dim'</span>,</span><br><span class="line">    <span class="string">'table-name'</span> = <span class="string">'dim_sku_info'</span>, </span><br><span class="line">    <span class="string">'driver'</span> = <span class="string">'com.mysql.jdbc.Driver'</span>,</span><br><span class="line">    <span class="string">'username'</span> = <span class="string">'root'</span>,</span><br><span class="line">    <span class="string">'password'</span> = <span class="string">'123qwe'</span>,</span><br><span class="line">    <span class="string">'scan.fetch-size'</span> = <span class="string">'100'</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="comment">-- 向ads_sku_index装载数据</span></span><br><span class="line"><span class="comment">-- 维表JOIN</span></span><br><span class="line"><span class="comment">-- ---------------------------------</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> ads_sku_index</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  sku_id ,</span><br><span class="line">  sku_name ,</span><br><span class="line">  weight ,</span><br><span class="line">  tm_id ,</span><br><span class="line">  price ,</span><br><span class="line">  spu_id ,</span><br><span class="line">  c3_id ,</span><br><span class="line">  c3_name,</span><br><span class="line">  c2_id ,</span><br><span class="line">  c2_name ,</span><br><span class="line">  c1_id ,</span><br><span class="line">  c1_name ,</span><br><span class="line">  sc.order_amount,</span><br><span class="line">  sc.order_count ,</span><br><span class="line">  sc.order_sku_num ,</span><br><span class="line">  <span class="keyword">cast</span>(sc.pay_date <span class="keyword">as</span> <span class="built_in">VARCHAR</span>)</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">tmp_sku_index_source sc </span><br><span class="line">  <span class="keyword">JOIN</span> dim_sku_info <span class="keyword">FOR</span> SYSTEM_TIME <span class="keyword">AS</span> <span class="keyword">OF</span> sc.proctime <span class="keyword">as</span> ds</span><br><span class="line">  <span class="keyword">ON</span> ds.id = sc.sku_id</span><br><span class="line">  ;</span><br></pre></td></tr></table></figure><p>当提交任务之后：观察Flink WEB UI：</p><p><img src="//jiamaoxiang.top/2020/08/12/实时数仓-基于Flink1-11的SQL构建实时数仓探索实践/%E4%BB%BB%E5%8A%A13.png" alt></p><p>查看ADS层的ads_sku_index表数据：</p><p><img src="//jiamaoxiang.top/2020/08/12/实时数仓-基于Flink1-11的SQL构建实时数仓探索实践/%E4%BB%BB%E5%8A%A13%E7%BB%93%E6%9E%9C.png" alt></p><h3 id="FineBI结果展示"><a href="#FineBI结果展示" class="headerlink" title="FineBI结果展示"></a>FineBI结果展示</h3><p><img src="//jiamaoxiang.top/2020/08/12/实时数仓-基于Flink1-11的SQL构建实时数仓探索实践/%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA.png" alt></p><h2 id="其他注意点"><a href="#其他注意点" class="headerlink" title="其他注意点"></a>其他注意点</h2><h3 id="Flink1-11-0存在的bug"><a href="#Flink1-11-0存在的bug" class="headerlink" title="Flink1.11.0存在的bug"></a>Flink1.11.0存在的bug</h3><p>当在代码中使用Flink1.11.0版本时，如果将一个change-log的数据源insert到一个upsert sink时，会报如下异常：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[ERROR] Could not execute SQL statement. Reason:</span><br><span class="line">org.apache.flink.table.api.TableException: Provided trait [BEFORE_AND_AFTER] can<span class="string">'t satisfy required trait [ONLY_UPDATE_AFTER]. This is a bug in planner, please file an issue. </span></span><br><span class="line"><span class="string">Current node is TableSourceScan(table=[[default_catalog, default_database, t_pick_order]], fields=[order_no, status])</span></span><br></pre></td></tr></table></figure><p>该bug目前已被修复，修复可以在Flink1.11.1中使用。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要分享了构建一个实时数仓的demo案例，通过本文可以了解实时数仓的数据处理流程，在此基础之上，对Flink SQL的CDC会有更加深刻的认识。另外，本文给出了非常详细的使用案例，你可以直接上手进行操作，在实践中探索实时数仓的构建流程。</p>]]></content>
      
      
      <categories>
          
          <category> 实时数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实时数仓 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink1.11中的CDC Connectors操作实践</title>
      <link href="/2020/08/12/Flink1-11%E4%B8%AD%E7%9A%84CDC-Connectors%E6%93%8D%E4%BD%9C%E5%AE%9E%E8%B7%B5/"/>
      <url>/2020/08/12/Flink1-11%E4%B8%AD%E7%9A%84CDC-Connectors%E6%93%8D%E4%BD%9C%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<p>Flink1.11引入了CDC的connector，通过这种方式可以很方便地捕获变化的数据，大大简化了数据处理的流程。Flink1.11的CDC connector主要包括：<code>MySQL CDC</code>和<code>Postgres CDC</code>,同时对Kafka的<strong>Connector</strong>支持<code>canal-json</code>和<code>debezium-json</code>以及<code>changelog-json</code>的format。本文主要分享以下内容：</p><ul><li>CDC简介</li><li>Flink提供的 table format</li><li>使用过程中的注意点</li><li>mysql-cdc的操作实践</li><li>canal-json的操作实践</li><li>changelog-json的操作实践</li></ul><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Flink CDC Connector 是ApacheFlink的一组数据源连接器，使用<strong>变化数据捕获change data capture (CDC)）</strong>从不同的数据库中提取变更数据。Flink CDC连接器将Debezium集成为引擎来捕获数据变更。因此，它可以充分利用Debezium的功能。</p><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul><li><p>支持读取数据库快照，并且能够持续读取数据库的变更日志，即使发生故障，也支持<strong>exactly-once</strong> 的处理语义</p></li><li><p>对于DataStream API的CDC connector，用户无需部署Debezium和Kafka，即可在单个作业中使用多个数据库和表上的变更数据。</p></li><li><p>对于Table/SQL API 的CDC connector，用户可以使用SQL DDL创建CDC数据源，来监视单个表上的数据变更。</p></li></ul><h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><ul><li>数据库之间的增量数据同步</li><li>审计日志</li><li>数据库之上的实时物化视图</li><li>基于CDC的维表join</li><li>…</li></ul><h2 id="Flink提供的-table-format"><a href="#Flink提供的-table-format" class="headerlink" title="Flink提供的 table format"></a>Flink提供的 table format</h2><p>Flink提供了一系列可以用于table connector的table format，具体如下：</p><table><thead><tr><th align="left">Formats</th><th align="left">Supported Connectors</th></tr></thead><tbody><tr><td align="left"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/formats/csv.html" target="_blank" rel="noopener">CSV</a></td><td align="left"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html" target="_blank" rel="noopener">Apache Kafka</a>, <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html" target="_blank" rel="noopener">Filesystem</a></td></tr><tr><td align="left"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/formats/json.html" target="_blank" rel="noopener">JSON</a></td><td align="left"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html" target="_blank" rel="noopener">Apache Kafka</a>, <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html" target="_blank" rel="noopener">Filesystem</a>, <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/elasticsearch.html" target="_blank" rel="noopener">Elasticsearch</a></td></tr><tr><td align="left"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/formats/avro.html" target="_blank" rel="noopener">Apache Avro</a></td><td align="left"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html" target="_blank" rel="noopener">Apache Kafka</a>, <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html" target="_blank" rel="noopener">Filesystem</a></td></tr><tr><td align="left"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/formats/debezium.html" target="_blank" rel="noopener">Debezium CDC</a></td><td align="left"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html" target="_blank" rel="noopener">Apache Kafka</a></td></tr><tr><td align="left"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/formats/canal.html" target="_blank" rel="noopener">Canal CDC</a></td><td align="left"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html" target="_blank" rel="noopener">Apache Kafka</a></td></tr><tr><td align="left"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/formats/parquet.html" target="_blank" rel="noopener">Apache Parquet</a></td><td align="left"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html" target="_blank" rel="noopener">Filesystem</a></td></tr><tr><td align="left"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/formats/orc.html" target="_blank" rel="noopener">Apache ORC</a></td><td align="left"><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html" target="_blank" rel="noopener">Filesystem</a></td></tr></tbody></table><h2 id="使用过程中的注意点"><a href="#使用过程中的注意点" class="headerlink" title="使用过程中的注意点"></a>使用过程中的注意点</h2><h3 id="使用MySQL-CDC的注意点"><a href="#使用MySQL-CDC的注意点" class="headerlink" title="使用MySQL CDC的注意点"></a>使用MySQL CDC的注意点</h3><p>如果要使用MySQL CDC connector，对于程序而言，需要添加如下依赖:</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba.ververica<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-mysql-cdc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果要使用Flink SQL Client，需要添加如下jar包：<strong><a href="https://repo1.maven.org/maven2/com/alibaba/ververica/flink-sql-connector-mysql-cdc/1.0.0/flink-sql-connector-mysql-cdc-1.0.0.jar" target="_blank" rel="noopener">flink-sql-connector-mysql-cdc-1.0.0.jar</a></strong>，将该jar包放在Flink安装目录的lib文件夹下即可。</p><h3 id="使用canal-json的注意点"><a href="#使用canal-json的注意点" class="headerlink" title="使用canal-json的注意点"></a>使用canal-json的注意点</h3><p>如果要使用Kafka的canal-json，对于程序而言，需要添加如下依赖:</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- universal --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.11.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果要使用Flink SQL Client，需要添加如下jar包：<strong><a href="https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka_2.11/1.11.0/flink-sql-connector-kafka_2.11-1.11.0.jar" target="_blank" rel="noopener">flink-sql-connector-kafka_2.11-1.11.0.jar</a></strong>，将该jar包放在Flink安装目录的lib文件夹下即可。由于Flink1.11的安装包 的lib目录下并没有提供该jar包，所以必须要手动添加依赖包，否则会报如下错误：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[ERROR] Could not execute SQL statement. Reason:</span><br><span class="line">org.apache.flink.table.api.ValidationException: Could not find any factory <span class="keyword">for</span> identifier <span class="string">'kafka'</span> that implements <span class="string">'org.apache.flink.table.factories.DynamicTableSourceFactory'</span> <span class="keyword">in</span> the classpath.</span><br><span class="line"></span><br><span class="line">Available factory identifiers are:</span><br><span class="line"></span><br><span class="line">datagen</span><br><span class="line">mysql-cdc</span><br></pre></td></tr></table></figure><h3 id="使用changelog-json的注意点"><a href="#使用changelog-json的注意点" class="headerlink" title="使用changelog-json的注意点"></a>使用changelog-json的注意点</h3><p>如果要使用Kafka的changelog-json Format，对于程序而言，需要添加如下依赖:</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba.ververica<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-format-changelog-json<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果要使用Flink SQL Client，需要添加如下jar包：<strong><a href="https://repo1.maven.org/maven2/com/alibaba/ververica/flink-format-changelog-json/1.0.0/flink-format-changelog-json-1.0.0.jar" target="_blank" rel="noopener">flink-format-changelog-json-1.0.0.jar</a></strong>，将该jar包放在Flink安装目录的lib文件夹下即可。</p><h2 id="mysql-cdc的操作实践"><a href="#mysql-cdc的操作实践" class="headerlink" title="mysql-cdc的操作实践"></a>mysql-cdc的操作实践</h2><h3 id="创建MySQL数据源表"><a href="#创建MySQL数据源表" class="headerlink" title="创建MySQL数据源表"></a>创建MySQL数据源表</h3><p>在创建MySQL CDC表之前，需要先创建MySQL的数据表，如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- MySQL</span></span><br><span class="line"><span class="comment">/*Table structure for table `order_info` */</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`order_info`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`order_info`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT <span class="keyword">COMMENT</span> <span class="string">'编号'</span>,</span><br><span class="line">  <span class="string">`consignee`</span> <span class="built_in">varchar</span>(<span class="number">100</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'收货人'</span>,</span><br><span class="line">  <span class="string">`consignee_tel`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'收件人电话'</span>,</span><br><span class="line">  <span class="string">`total_amount`</span> <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'总金额'</span>,</span><br><span class="line">  <span class="string">`order_status`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'订单状态,1表示下单，2表示支付'</span>,</span><br><span class="line">  <span class="string">`user_id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'用户id'</span>,</span><br><span class="line">  <span class="string">`payment_way`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'付款方式'</span>,</span><br><span class="line">  <span class="string">`delivery_address`</span> <span class="built_in">varchar</span>(<span class="number">1000</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'送货地址'</span>,</span><br><span class="line">  <span class="string">`order_comment`</span> <span class="built_in">varchar</span>(<span class="number">200</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'订单备注'</span>,</span><br><span class="line">  <span class="string">`out_trade_no`</span> <span class="built_in">varchar</span>(<span class="number">50</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'订单交易编号（第三方支付用)'</span>,</span><br><span class="line">  <span class="string">`trade_body`</span> <span class="built_in">varchar</span>(<span class="number">200</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'订单描述(第三方支付用)'</span>,</span><br><span class="line">  <span class="string">`create_time`</span> datetime <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'创建时间'</span>,</span><br><span class="line">  <span class="string">`operate_time`</span> datetime <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'操作时间'</span>,</span><br><span class="line">  <span class="string">`expire_time`</span> datetime <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'失效时间'</span>,</span><br><span class="line">  <span class="string">`tracking_no`</span> <span class="built_in">varchar</span>(<span class="number">100</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'物流单编号'</span>,</span><br><span class="line">  <span class="string">`parent_order_id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'父订单编号'</span>,</span><br><span class="line">  <span class="string">`img_url`</span> <span class="built_in">varchar</span>(<span class="number">200</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'图片路径'</span>,</span><br><span class="line">  <span class="string">`province_id`</span> <span class="built_in">int</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'地区'</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> AUTO_INCREMENT=<span class="number">1</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 <span class="keyword">COMMENT</span>=<span class="string">'订单表'</span>;</span><br><span class="line"><span class="comment">-- ----------------------------</span></span><br><span class="line"><span class="comment">-- Records of order_info</span></span><br><span class="line"><span class="comment">-- ----------------------------</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`order_info`</span> </span><br><span class="line"><span class="keyword">VALUES</span> (<span class="number">476</span>, <span class="string">'lAXjcL'</span>, <span class="string">'13408115089'</span>, <span class="number">433.00</span>, <span class="string">'2'</span>, <span class="number">10</span>, <span class="string">'2'</span>, <span class="string">'OYyAdSdLxedceqovndCD'</span>, <span class="string">'ihjAYsSjrgJMQVdFQnSy'</span>, <span class="string">'8728720206'</span>, <span class="string">''</span>, <span class="string">'2020-06-18 02:21:38'</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="number">9</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`order_info`</span></span><br><span class="line"><span class="keyword">VALUES</span> (<span class="number">477</span>, <span class="string">'QLiFDb'</span>, <span class="string">'13415139984'</span>, <span class="number">772.00</span>, <span class="string">'1'</span>, <span class="number">90</span>, <span class="string">'2'</span>, <span class="string">'OizYrQbKuWvrvdfpkeSZ'</span>, <span class="string">'wiBhhqhMndCCgXwmWVQq'</span>, <span class="string">'1679381473'</span>, <span class="string">''</span>, <span class="string">'2020-06-18 09:12:25'</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="number">3</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`order_info`</span></span><br><span class="line"><span class="keyword">VALUES</span> (<span class="number">478</span>, <span class="string">'iwKjQD'</span>, <span class="string">'13320383859'</span>, <span class="number">88.00</span>, <span class="string">'1'</span>, <span class="number">107</span>, <span class="string">'1'</span>, <span class="string">'cbXLKtNHWOcWzJVBWdAs'</span>, <span class="string">'njjsnknHxsxhuCCeNDDi'</span>, <span class="string">'0937074290'</span>, <span class="string">''</span>, <span class="string">'2020-06-18 15:56:34'</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="number">7</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">/*Table structure for table `order_detail` */</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`order_detail`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT <span class="keyword">COMMENT</span> <span class="string">'编号'</span>,</span><br><span class="line">  <span class="string">`order_id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'订单编号'</span>,</span><br><span class="line">  <span class="string">`sku_id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'sku_id'</span>,</span><br><span class="line">  <span class="string">`sku_name`</span> <span class="built_in">varchar</span>(<span class="number">200</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'sku名称（冗余)'</span>,</span><br><span class="line">  <span class="string">`img_url`</span> <span class="built_in">varchar</span>(<span class="number">200</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'图片名称（冗余)'</span>,</span><br><span class="line">  <span class="string">`order_price`</span> <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'购买价格(下单时sku价格）'</span>,</span><br><span class="line">  <span class="string">`sku_num`</span> <span class="built_in">varchar</span>(<span class="number">200</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'购买个数'</span>,</span><br><span class="line">  <span class="string">`create_time`</span> datetime <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'创建时间'</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> AUTO_INCREMENT=<span class="number">1</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 <span class="keyword">COMMENT</span>=<span class="string">'订单明细表'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- ----------------------------</span></span><br><span class="line"><span class="comment">-- Records of order_detail</span></span><br><span class="line"><span class="comment">-- ----------------------------</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`order_detail`</span> </span><br><span class="line"><span class="keyword">VALUES</span> (<span class="number">1329</span>, <span class="number">476</span>, <span class="number">8</span>, <span class="string">'Apple iPhone XS Max (A2104) 256GB 深空灰色 移动联通电信4G手机 双卡双待'</span>, <span class="string">'http://XLMByOyZDTJQYxphQHNTgYAFzJJCKTmCbzvEJIpz'</span>, <span class="number">8900.00</span>, <span class="string">'3'</span>, <span class="string">'2020-06-18 02:21:38'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`order_detail`</span> </span><br><span class="line"><span class="keyword">VALUES</span> (<span class="number">1330</span>, <span class="number">477</span>, <span class="number">9</span>, <span class="string">'荣耀10 GT游戏加速 AIS手持夜景 6GB+64GB 幻影蓝全网通 移动联通电信'</span>, <span class="string">'http://ixOCtlYmlxEEgUfPLiLdjMftzrleOEIBKSjrhMne'</span>, <span class="number">2452.00</span>, <span class="string">'4'</span>, <span class="string">'2020-06-18 09:12:25'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`order_detail`</span></span><br><span class="line"><span class="keyword">VALUES</span> (<span class="number">1331</span>, <span class="number">478</span>, <span class="number">4</span>, <span class="string">'小米Play 流光渐变AI双摄 4GB+64GB 梦幻蓝 全网通4G 双卡双待 小水滴全面屏拍照游戏智能手机'</span>, <span class="string">'http://RqfEFnAOqnqRnNZLFRvBuwXxwNBtptYJCILDKQYv'</span>, <span class="number">1442.00</span>, <span class="string">'1'</span>, <span class="string">'2020-06-18 15:56:34'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`order_detail`</span> </span><br><span class="line"><span class="keyword">VALUES</span> (<span class="number">1332</span>, <span class="number">478</span>, <span class="number">8</span>, <span class="string">'Apple iPhone XS Max (A2104) 256GB 深空灰色 移动联通电信4G手机 双卡双待'</span>, <span class="string">'http://IwhuCDlsiLenfKjPzbJrIoxswdfofKhJLMzlJAKV'</span>, <span class="number">8900.00</span>, <span class="string">'3'</span>, <span class="string">'2020-06-18 15:56:34'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`order_detail`</span> </span><br><span class="line"><span class="keyword">VALUES</span> (<span class="number">1333</span>, <span class="number">478</span>, <span class="number">8</span>, <span class="string">'Apple iPhone XS Max (A2104) 256GB 深空灰色 移动联通电信4G手机 双卡双待'</span>, <span class="string">'http://bbfwTbAzTWapywODzOtDJMJUEqNTeRTUQuCDkqXP'</span>, <span class="number">8900.00</span>, <span class="string">'1'</span>, <span class="string">'2020-06-18 15:56:34'</span>);</span><br></pre></td></tr></table></figure><h3 id="Flink-SQL-Cli创建CDC数据源"><a href="#Flink-SQL-Cli创建CDC数据源" class="headerlink" title="Flink SQL Cli创建CDC数据源"></a>Flink SQL Cli创建CDC数据源</h3><p>启动 Flink 集群，再启动 SQL CLI,执行下面命令：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建订单信息表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> order_info(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">    user_id <span class="built_in">BIGINT</span>,</span><br><span class="line">    create_time <span class="built_in">TIMESTAMP</span>(<span class="number">0</span>),</span><br><span class="line">    operate_time <span class="built_in">TIMESTAMP</span>(<span class="number">0</span>),</span><br><span class="line">    province_id <span class="built_in">INT</span>,</span><br><span class="line">    order_status <span class="keyword">STRING</span>,</span><br><span class="line">    total_amount <span class="built_in">DECIMAL</span>(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">  ) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'mysql-cdc'</span>,</span><br><span class="line">    <span class="string">'hostname'</span> = <span class="string">'kms-1'</span>,</span><br><span class="line">    <span class="string">'port'</span> = <span class="string">'3306'</span>,</span><br><span class="line">    <span class="string">'username'</span> = <span class="string">'root'</span>,</span><br><span class="line">    <span class="string">'password'</span> = <span class="string">'123qwe'</span>,</span><br><span class="line">    <span class="string">'database-name'</span> = <span class="string">'mydw'</span>,</span><br><span class="line">    <span class="string">'table-name'</span> = <span class="string">'order_info'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>在Flink SQL Cli中查询该表的数据：result-mode: tableau，+表示数据的insert</p><p><img src="//jiamaoxiang.top/2020/08/12/Flink1-11中的CDC-Connectors操作实践/%E5%9B%BE1.png" alt></p><p>在SQL CLI中创建订单详情表：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> order_detail(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">    order_id <span class="built_in">BIGINT</span>,</span><br><span class="line">    sku_id <span class="built_in">BIGINT</span>,</span><br><span class="line">    sku_name <span class="keyword">STRING</span>,</span><br><span class="line">    sku_num <span class="built_in">BIGINT</span>,</span><br><span class="line">    order_price <span class="built_in">DECIMAL</span>(<span class="number">10</span>, <span class="number">5</span>),</span><br><span class="line">create_time <span class="built_in">TIMESTAMP</span>(<span class="number">0</span>)</span><br><span class="line"> ) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'mysql-cdc'</span>,</span><br><span class="line">    <span class="string">'hostname'</span> = <span class="string">'kms-1'</span>,</span><br><span class="line">    <span class="string">'port'</span> = <span class="string">'3306'</span>,</span><br><span class="line">    <span class="string">'username'</span> = <span class="string">'root'</span>,</span><br><span class="line">    <span class="string">'password'</span> = <span class="string">'123qwe'</span>,</span><br><span class="line">    <span class="string">'database-name'</span> = <span class="string">'mydw'</span>,</span><br><span class="line">    <span class="string">'table-name'</span> = <span class="string">'order_detail'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>查询结果如下：</p><p><img src="//jiamaoxiang.top/2020/08/12/Flink1-11中的CDC-Connectors操作实践/%E5%9B%BE2.png" alt></p><p>执行JOIN操作：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    od.id,</span><br><span class="line">    oi.id order_id,</span><br><span class="line">    oi.user_id,</span><br><span class="line">    oi.province_id,</span><br><span class="line">    od.sku_id,</span><br><span class="line">    od.sku_name,</span><br><span class="line">    od.sku_num,</span><br><span class="line">    od.order_price,</span><br><span class="line">    oi.create_time,</span><br><span class="line">    oi.operate_time</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">   (</span><br><span class="line">    <span class="keyword">SELECT</span> * </span><br><span class="line">    <span class="keyword">FROM</span> order_info</span><br><span class="line">    <span class="keyword">WHERE</span> </span><br><span class="line">     order_status = <span class="string">'2'</span><span class="comment">-- 已支付</span></span><br><span class="line">   ) oi</span><br><span class="line">   <span class="keyword">JOIN</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">SELECT</span> *</span><br><span class="line">    <span class="keyword">FROM</span> order_detail</span><br><span class="line">  ) od </span><br><span class="line">  <span class="keyword">ON</span> oi.id = od.order_id;</span><br></pre></td></tr></table></figure><h2 id="canal-json的操作实践"><a href="#canal-json的操作实践" class="headerlink" title="canal-json的操作实践"></a>canal-json的操作实践</h2><p>关于cannal的使用方式，可以参考我的另一篇文章：<a href="https://mp.weixin.qq.com/s/ooPAScXAw2soqlgEoSbRAw" target="_blank" rel="noopener">基于Canal与Flink实现数据实时增量同步(一)</a>。我已经将下面的表通过canal同步到了kafka，具体格式为：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"data"</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"id"</span>:<span class="string">"1"</span>,</span><br><span class="line">            <span class="string">"region_name"</span>:<span class="string">"华北"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"id"</span>:<span class="string">"2"</span>,</span><br><span class="line">            <span class="string">"region_name"</span>:<span class="string">"华东"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"id"</span>:<span class="string">"3"</span>,</span><br><span class="line">            <span class="string">"region_name"</span>:<span class="string">"东北"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"id"</span>:<span class="string">"4"</span>,</span><br><span class="line">            <span class="string">"region_name"</span>:<span class="string">"华中"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"id"</span>:<span class="string">"5"</span>,</span><br><span class="line">            <span class="string">"region_name"</span>:<span class="string">"华南"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"id"</span>:<span class="string">"6"</span>,</span><br><span class="line">            <span class="string">"region_name"</span>:<span class="string">"西南"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"id"</span>:<span class="string">"7"</span>,</span><br><span class="line">            <span class="string">"region_name"</span>:<span class="string">"西北"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"database"</span>:<span class="string">"mydw"</span>,</span><br><span class="line">    <span class="string">"es"</span>:1597128441000,</span><br><span class="line">    <span class="string">"id"</span>:102,</span><br><span class="line">    <span class="string">"isDdl"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="string">"mysqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:<span class="string">"varchar(20)"</span>,</span><br><span class="line">        <span class="string">"region_name"</span>:<span class="string">"varchar(20)"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"old"</span>:null,</span><br><span class="line">    <span class="string">"pkNames"</span>:null,</span><br><span class="line">    <span class="string">"sql"</span>:<span class="string">""</span>,</span><br><span class="line">    <span class="string">"sqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:12,</span><br><span class="line">        <span class="string">"region_name"</span>:12</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"table"</span>:<span class="string">"base_region"</span>,</span><br><span class="line">    <span class="string">"ts"</span>:1597128441424,</span><br><span class="line">    <span class="string">"type"</span>:<span class="string">"INSERT"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 在SQL CLI中创建该canal-json格式的表：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> region (</span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  region_name <span class="keyword">STRING</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"> <span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line"> <span class="string">'topic'</span> = <span class="string">'mydw.base_region'</span>,</span><br><span class="line"> <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-3:9092'</span>,</span><br><span class="line"> <span class="string">'properties.group.id'</span> = <span class="string">'testGroup'</span>,</span><br><span class="line"> <span class="string">'format'</span> = <span class="string">'canal-json'</span> ,</span><br><span class="line"> <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span> </span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>查询结果如下：</p><p><img src="//jiamaoxiang.top/2020/08/12/Flink1-11中的CDC-Connectors操作实践/%E5%9B%BE3.png" alt></p><h2 id="changelog-json的操作实践"><a href="#changelog-json的操作实践" class="headerlink" title="changelog-json的操作实践"></a>changelog-json的操作实践</h2><h3 id="创建MySQL数据源"><a href="#创建MySQL数据源" class="headerlink" title="创建MySQL数据源"></a>创建MySQL数据源</h3><p>参见上面的<strong>order_info</strong></p><h3 id="Flink-SQL-Cli创建changelog-json表"><a href="#Flink-SQL-Cli创建changelog-json表" class="headerlink" title="Flink SQL Cli创建changelog-json表"></a>Flink SQL Cli创建changelog-json表</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> order_gmv2kafka (</span><br><span class="line">  day_str <span class="keyword">STRING</span>,</span><br><span class="line">  gmv <span class="built_in">DECIMAL</span>(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line">    <span class="string">'topic'</span> = <span class="string">'order_gmv_kafka'</span>,</span><br><span class="line">    <span class="string">'scan.startup.mode'</span> = <span class="string">'earliest-offset'</span>,</span><br><span class="line">    <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'kms-3:9092'</span>,</span><br><span class="line">    <span class="string">'format'</span> = <span class="string">'changelog-json'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> order_gmv2kafka</span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DATE_FORMAT</span>(create_time, <span class="string">'yyyy-MM-dd'</span>) <span class="keyword">as</span> day_str, <span class="keyword">SUM</span>(total_amount) <span class="keyword">as</span> gmv</span><br><span class="line"><span class="keyword">FROM</span> order_info</span><br><span class="line"><span class="keyword">WHERE</span> order_status = <span class="string">'2'</span> <span class="comment">-- 订单已支付</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">DATE_FORMAT</span>(create_time, <span class="string">'yyyy-MM-dd'</span>);</span><br></pre></td></tr></table></figure><p>查询表看一下结果：</p><p><img src="//jiamaoxiang.top/2020/08/12/Flink1-11中的CDC-Connectors操作实践/%E5%9B%BE4.png" alt></p><p>再查一下kafka的数据：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">"data"</span>:&#123;<span class="string">"day_str"</span>:<span class="string">"2020-06-18"</span>,<span class="string">"gmv"</span>:433&#125;,<span class="string">"op"</span>:<span class="string">"+I"</span>&#125;</span><br></pre></td></tr></table></figure><p>当将另外两个订单的状态order_status更新为2时，<code>总金额=443+772+88=1293</code>再观察数据：</p><p><img src="//jiamaoxiang.top/2020/08/12/Flink1-11中的CDC-Connectors操作实践/%E5%9B%BE5.png" alt></p><p>再看kafka中的数据：</p><p><img src="//jiamaoxiang.top/2020/08/12/Flink1-11中的CDC-Connectors操作实践/%E5%9B%BE6.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文基于Flink1.11的SQL，对新添加的CDC connector的使用方式进行了阐述。主要包括MySQL CDC connector、canal-json及changelog-json的format，并指出了使用过程中的注意点。另外本文给出了完整的使用示例，如果你有现成的环境，那么可以直接进行测试使用。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>内含面试|一文搞懂HBase的基本原理</title>
      <link href="/2020/08/07/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82HBase%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/"/>
      <url>/2020/08/07/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82HBase%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>本文会对HBase的基本原理进行剖析，通过本文你可以了解到：</p><ul><li>CAP理论</li><li>NoSQL出现的原因</li><li>HBase的特点及使用场景</li><li>HBase的数据模型和基本原理</li><li>客户端API的基本使用 </li><li>易混淆知识点面试总结</li></ul><p><code>温馨提示</code>:本文内容较长，如果觉得有用，建议收藏。另外记得分享、点赞、在看，素质三连哦！</p><h2 id="从BigTable说起"><a href="#从BigTable说起" class="headerlink" title="从BigTable说起"></a>从BigTable说起</h2><p>HBase是在谷歌BigTable的基础之上进行开源实现的，是一个高可靠、高性能、面向列、可伸缩的分布式数据库，可以用来存储非结构化和半结构化的稀疏数据。HBase支持超大规模数据存储，可以通过水平扩展的方式处理超过10亿行数据和百万列元素组成的数据表。</p><p>BigTable是一个分布式存储系统，利用谷歌提出的MapReduce分布式并行计算模型来处理海量数据，使用谷歌分布式文件系统GFS作为底层的数据存储，并采用Chubby提供协同服务管理，具备广泛的应用型、可扩展性、高可用性及高性能性等特点。关于BigTable与HBase的对比，见下表：</p><table><thead><tr><th>依赖</th><th>BigTbale</th><th>HBase</th></tr></thead><tbody><tr><td>数据存储</td><td>GFS</td><td>HDFS</td></tr><tr><td>数据处理</td><td>MapReduce</td><td>Hadoop的MapReduce</td></tr><tr><td>协同服务</td><td>Chubby</td><td>Zookeeper</td></tr><tr><td>## CAP理论</td><td></td><td></td></tr></tbody></table><p>2000年，Berkerly大学有位Eric Brewer教授提出了一个CAP理论，在2002年，麻省理工学院的<code>Seth Gilbert(赛斯·吉尔伯特)</code>和<code>Nancy Lynch(南希·林奇)</code>发表了布鲁尔猜想的证明，证明了CAP理论的正确性。所谓CAP理论，是指对于一个分布式计算系统来说，不可能同时满足以下三点：</p><ul><li><p>一致性（<strong>C</strong>onsistency）</p><p>等同于所有节点访问同一份最新的数据副本。即任何一个读操作总是能够读到之前完成的写操作的结果，也就是说，在分布式环境中，不同节点访问的数据是一致的。</p></li><li><p>可用性（<strong>A</strong>vailability）</p><p>每次请求都能获取到非错的响应——<strong>但是不保证获取的数据为最新数据</strong>。即快速获取数据，可以在确定的时间内返回操作结果。</p></li><li><p>分区容错性（<strong>P</strong>artition tolerance）</p><p>以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。即指当出现网络分区时(系统中的一部分节点无法与其他的节点进行通信)，分离的系统也能够正常运行，即可靠性。</p></li></ul><p><img src="//jiamaoxiang.top/2020/08/07/一文搞懂HBase的基本原理/CAP.png" alt></p><p>如上图所示：一个分布式的系统不可能同时满足一致性、可用性和分区容错性，最多同时满足两个。当处理CAP的问题时，可以有一下几个选择：</p><ul><li><p>满足CA，不满足P。将所有与事务相关的内容都放在同一个机器上，这样会影响系统的可扩展性。传统的关系型数据库。如MySQL、SQL Server 、PostgresSQL等都采用了此种设计原则。</p></li><li><p>满足AP，不满足C。不满足一致性(C)，即允许系统返回不一致的数据。其实，对于WEB2.0的网站而言，更加关注的是服务是否可用，而不是一致性。比如你发了一篇博客或者写一篇微博，你的一部分朋友立马看到了这篇文章或者微博，另一部分朋友却要等一段时间之后才能刷出这篇文章或者微博。虽然有延时，但是对于一个娱乐性质的Web 2.0网站而言，这几分钟的延时并不重要，不会影响用户体验。相反，当发布一篇文章或微博时，不能够立即发布(不满足可用性)，用户对此肯定不爽。所以呢，对于WEB2.0的网站而言，可用性和分区容错性的优先级要高于数据一致性，当然，并没有完全放弃一致性，而是最终的一致性(有延时)。如Dynamo、Cassandra、CouchDB等NoSQL数据库采用了此原则。</p></li><li><p>满足CP，不满足A。强调一致性性(C)和分区容错性(P)，放弃可用性性(A)。当出现网络分区时，受影响的服务需要等待数据一致，在等待期间无法对外提供服务。如Neo4J、HBase 、MongoDB、Redis等采用了此种设计原则。</p></li></ul><h2 id="为什么出现NoSQL"><a href="#为什么出现NoSQL" class="headerlink" title="为什么出现NoSQL"></a>为什么出现NoSQL</h2><p>所谓<strong>NoSQL</strong>，即<strong>Not Only SQL</strong>的缩写，意思是不只是SQL。上面提到的CAP理论正是NoSQL的设计原则。那么，为什么会兴起NoSQL数据库呢?因为WEB2.0以及大数据时代的到来，关系型数据库越来越不能满足需求。大数据、物联网、移动互联网和云计算的发展，使得非结构化的数据比例高达90%以上，关系型数据库由于模型不灵活以及扩展水平较差，在面对大数据时，暴露出了越来越多的缺陷。由此NoSQL数据库应运而生，更好地满足了大数据时代及WEB2.0的需求。</p><p>面对WEB2.0以及大数据的挑战，关系型数据库在以下几个方面表现欠佳：</p><ul><li><p>对于海量数据的处理性能较差</p><p>WEB2.0时代，尤其是移动互联网的发展，UGC(用户生成内容，User Generated Content)以及PGC(公众生成内容，Public Generated Content)占据了我们的日常。现如今，自媒体发展遍地开花，几乎每个人都成了内容的创造者，比如博文、评论、意见、新闻消息、视频等等，不一而足。可见，这些数据产生的速度之快，数据量之大。比如微博、公众号、抑或是淘宝，在一分钟内产生的数据可能就会非常的惊人，面对这些千万级、亿级的数据记录，关系型数据库的查询效率显然是不能接受的。</p></li><li><p>无法满足高并发需求</p><p>WEB1.0时代，大部分是静态网页(即提供什么就看什么)，从而在大规模用户访问时，可以实现较好的响应能力。但是，在WEB2.0时代，强调的是用户的交互性(用户创造内容)，所有信息都需要事实动态生成，会造成高并发的数据库访问，可能每秒上万次的读写请求，对于很多关系型数据库而言，这显示是难以承受的。</p></li><li><p>无法满足扩展性和高可用性的需求</p><p>在当今<code>娱乐至死</code>的时代，热点问题(吸引人眼球，满足猎奇心理)会引来一窝蜂的流量，比如微博曝出某明星出轨，热搜榜会迅速引来大批用户围观(俗称吃瓜群众)，从而产生大量的互动交流(蹭热点)，这些都会造成数据库的读写负荷急剧增加，从而需要数据库能够在短时间内迅速提升性能以应对突发需求(毕竟宕机会非常影响户体验)。但是关系型数据库通常难以水平扩展，不能够像网页服务器和应用服务器那样简单地通过增加更多的硬件和服务节点来扩展性能和负载能力。</p></li></ul><p>综上，NoSQL数据库应运而生，是IT发展的必然。</p><h2 id="HBase的特点及使用场景"><a href="#HBase的特点及使用场景" class="headerlink" title="HBase的特点及使用场景"></a>HBase的特点及使用场景</h2><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul><li><strong>强一致性读写</strong></li></ul><p>HBase 不是 <code>最终一致性(eventually consistent)</code> 数据存储. 这让它很适合高速计数聚合类任务</p><ul><li><strong>自动分片(Automatic sharding)</strong></li></ul><p>HBase 表通过region分布在集群中。数据增长时，region会自动分割并重新分布</p><ul><li><p><strong>RegionServer 自动故障转移</strong></p></li><li><p><strong>Hadoop/HDFS 集成</strong></p><p>HBase 支持本机外HDFS 作为它的分布式文件系统</p></li><li><p><strong>MapReduce集成</strong></p><p>HBase 通过MapReduce支持大并发处理， HBase 可以同时做源(Source)和汇(Sink)</p></li><li><p><strong>Java 客户端 API</strong></p><p>HBase 支持易于使用的 Java API 进行编程访问</p></li><li><p><strong>Thrift/REST API</strong></p></li></ul><p>支持Thrift 和 REST 的方式访问HBase</p><ul><li><strong>Block Cache 和 布隆过滤器（Bloom Filter）</strong></li></ul><p>HBase支持 Block Cache 和 布隆过滤器进行查询优化，提升查询性能</p><ul><li><strong>运维管理</strong></li></ul><p>HBase提供内置的用于运维的网页和JMX 指标</p><h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><p><strong>HBase并不适合所有场景</strong></p><p>首先，*<em>数据量方面 *</em>。确信有足够多数据，如果有上亿或十亿行数据，至少单表数据量超过千万，HBase会是一个很好的选择。 如果只有上千或上百万行，用传统的RDBMS可能是更好的选择。</p><p>其次，<strong>关系型数据库特性方面</strong>。确信可以不依赖所有RDBMS的额外特性 (如列数据类型、二级索引、事务、高级查询语言等) 。一个建立在RDBMS上应用，并不能通过简单的改变JDBC驱动就能迁移到HBase，需要一次完全的重新设计。</p><p>再次，<strong>硬件方面</strong>。 确信你有足够硬件。比如由于HDFS 的默认副本是3，所以一般至少5个数据节点才能够发挥其特性，另外 还要加上一个 NameNode节点。</p><p>最后，<strong>数据分析方面</strong>。数据分析是HBase的弱项，因为对于HBase乃至整个NoSQL生态圈来说，基本上都是不支持表关联的。如果主要需求是数据分析，比如做报表，显然HBase是不太合适的。</p><h2 id="HBase的数据模型"><a href="#HBase的数据模型" class="headerlink" title="HBase的数据模型"></a>HBase的数据模型</h2><h3 id="基本术语"><a href="#基本术语" class="headerlink" title="基本术语"></a>基本术语</h3><p>HBase是一个稀疏、多维、持久化存储的映射表，采用的row key、列族、列限定符合时间戳进行索引，每个cell的值都是字节数组byte[]。了解HBase需要先知道下面的一些概念：</p><ul><li><p><strong>Namespace</strong></p><p>Namespace，即命名空间，是表的逻辑分组，类似于关系型数据库管理系统的database。HBase存在两个预定义的特殊的命名空间：<strong>hbase</strong>和<strong>default</strong>，其中hbase属于系统命名空间，用来存储HBase的内部的表。default属于默认的命名空间，即如果建表时不指定命名空间，则默认使用default。</p></li><li><p><strong>表</strong></p><p>由行和列组成，列划分为若干个列族</p></li><li><p><strong>行</strong></p><p>row key是未解释的字节数组，在HBase内部，row key是按字典排序由低到高存储在表中的。每个HBase的表由若干行组成，每个行由行键(row key)标识。可以利用这一特性，将经常一起读取的行存储在一起。</p></li><li><p><strong>列族</strong></p><p>HBase中，列是由列族进行组织的。一个列族所有列成员是有着相同的前缀，比如，列<em>courses:history</em> 和 <em>courses:math</em>都是 列族 <em>courses</em>的成员。冒号(:)是列族的分隔符，用来区分前缀和列名。列族必须在表建立的时候声明，而列则可以在使用时进行声明。另外，存储在一个列族中的所有数据，通常都具有相同的数据类型，这可以极大提高数据的压缩率。在物理上，一个的列族成员在文件系统上都是存储在一起。</p></li><li><p><strong>列</strong></p><p>列族里面的数据通过列限定符来定位。列通常不需要在创建表时就去定义，也不需要在不同行之间保持一致。列没有明确的数据类型，总是被视为字节数组byte[]。</p></li><li><p><strong>cell</strong></p><p>单元格，即通过row key、列族、列确定的具体存储的数据。单元格中存储的数据也没有明确的数据类型，总被视为字节数组byte[]。另外，每个单元格的数据是多版本的，每个版本会对应一个时间戳。</p></li><li><p><strong>时间戳</strong></p><p>由于HBase的表数据是具有版本的，这些版本是通过时间戳进行标识的。每次对一个单元格进行修改或删除时，HBase会自动为其生成并存储一个时间戳。一个单元格的不同版本是根据时间戳降序的顺序进行存储的，即优先读取最新的数据。</p><p>关于HBase的数据模型，详见下图：</p><p><img src="//jiamaoxiang.top/2020/08/07/一文搞懂HBase的基本原理/%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B.png" alt></p></li></ul><h3 id="概念模型"><a href="#概念模型" class="headerlink" title="概念模型"></a>概念模型</h3><p>在HBase概念模型中，一个表可以被看做是一个稀疏的、多维的映射关系，如下图所示：</p><img src="//jiamaoxiang.top/2020/08/07/一文搞懂HBase的基本原理/概念模型.png" style="zoom:80%;"><p>如上表所示：</p><p>该表包含两行数据，分别为<code>com.cnn.www</code>和<code>com.example.www</code>;</p><p>三个列族，分别为：<code>contents</code>, <code>anchor</code> 和<code>people</code>。</p><p>对于第一行数据(对应的row key为<code>com.cnn.www</code>),列族<code>anchor</code>包含两列：<code>anchor:cssnsi.com</code>和<code>anchor:my.look.ca</code>;列族<code>contents</code>包含一列：<code>contents:html</code>;</p><p>对于第一行数据(对应的row key为<code>com.cnn.www</code>),包含5个版本的数据</p><p>对于第二行数据(对应的row key为<code>com.example.www</code>),包含1个版本的数据</p><p>上表中可以通过一个四维坐标定位一个单元格数据：[row key,列族,列,时间戳]，比如[<code>com.cnn.www</code>,<code>contents</code>,<code>contents:html</code>,<code>t6</code>]</p><h3 id="物理模型"><a href="#物理模型" class="headerlink" title="物理模型"></a>物理模型</h3><p>从概念模型上看，HBase的表是稀疏的。在物理存储的时候，是按照列族进行存储的。一个列限定符(<code>column_family:column_qualifier</code>)可以被随时添加到已经存在的列族上。</p><img src="//jiamaoxiang.top/2020/08/07/一文搞懂HBase的基本原理/物理模型.png" style="zoom: 80%;"><p>从物理模型上看，概念模型中存在的<code>空单元格</code>是不会被存储的。比如要访问<code>contents:html</code>，时间戳为<code>t8</code>,则不会返回值。值得注意的是，如果访问数据时没有指定时间戳，则默认访问最新版本的数据，因为数据是按照版本时间戳降序排列的。</p><p>如上表：如果访问行<code>com.cnn.www</code>，列<code>contents:html</code>，在没有指定时间戳的情况下，则返回<code>t6</code>对应的数据;同理如果访问<code>anchor:cnnsi.com</code>,则返回<code>t9</code>对应的数据。</p><h2 id="HBase的原理及运行机制"><a href="#HBase的原理及运行机制" class="headerlink" title="HBase的原理及运行机制"></a>HBase的原理及运行机制</h2><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p>通过上面的描述，应该对HBase有了一定的了解。现在我们在来看一下HBase的宏观架构，如下图：</p><img src="//jiamaoxiang.top/2020/08/07/一文搞懂HBase的基本原理/整体架构.png" style="zoom: 67%;"><p> 我们先从宏观的角度看一下HBase的整体架构。从HBase的部署架构上来说，HBase有两种服务器：<code>Master服务器</code>和<code>RegionServer服务器</code>。一般一个HBase集群有一个Master服务器和几个RegionServer服务器。</p><p><code>Master服务器</code>负责维护表结构信息，实际的数据都存储在RegionServer服务器上。在HBase的集群中，客户端获取数据由客户端直连RegionServer的，所以你会发现Master挂掉之后你依然可以查询数据，但是不能创建新的表了。</p><ul><li><strong>Master</strong></li></ul><p>我们都知道，在Hadoop采用的是master-slave架构，即namenode节点为主节点，datanode节点为从节点。namenode节点对于hadoop集群而言至关重要，如果namenode节点挂了，那么整个集群也就瘫痪了。</p><p>但是，在HBase集群中，Master服务的作用并没有那么的重要。虽然是Master节点，其实并不是一个leader的角色。Master服务更像是一个‘打杂’的，类似于一个辅助者的角色。因为当我们连接HBase集群时，客户端会直接从Zookeeper中获取RegionServer的地址，然后从RegionServer中获取想要的数据，不需要经过Master节点。除此之外，当我们向HBase表中<strong>插入数据</strong>、<strong>删除数据</strong>等操作时，也都是直接跟RegionServer交互的，不需要Master服务参与。</p><p>那么，Master服务有什么作用呢？Master只负责各种协调工作，比如<strong>建表</strong>、<strong>删表</strong>、<strong>移动Region</strong>、<strong>合并</strong>等操作。这些操作有一个共性的问题：<strong>就是需要跨RegionServer</strong>。所以，HBase就将这些工作分配给了Master服务。这种结构的好处是大大降低了集群对Master的依赖。而Master节点一般只有一个到两个，一旦宕机，如果集群对Master的依赖度很大，那么就会产生单点故障问题。在HBase中即使Master宕机了，集群依然可以正常地运行，依然可以存储和删除数据。</p><ul><li><strong>RegionServer</strong></li></ul><p>RegionServer就是存放Region的容器，直观上说就是服务器上的一个服务。RegionServer是真正存储数据的节点，最终存储在分布式文件系统HDFS。当客户端从ZooKeeper获取RegionServer的地址后，它会直接从RegionServer获取数据。对于HBase集群而言，其重要性要比Master服务大。</p><ul><li><strong>Zookeeper</strong></li></ul><p>RegionServer非常依赖ZooKeeper服务，ZooKeeper在HBase中扮演的角色类似一个管家。ZooKeeper管理了HBase所有RegionServer的信息，包括具体的数据段存放在哪个RegionServer上。客户端每次与HBase连接，其实都是先与ZooKeeper通信，查询出哪个RegionServer需要连接，然后再连接RegionServer。</p><p>我们可以通过zkCli访问hbase节点的数据，通过下面命名可以获取hbase:meta表的信息：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 17] get /hbase/meta-region-server</span><br></pre></td></tr></table></figure><p>简单总结Zookeeper在HBase集群中的作用如下：对于服务端，<strong>是实现集群协调与控制的重要依赖</strong>。对于客户端，<strong>是查询与操作数据必不可少的一部分</strong>。</p><p>需要注意的是：当Master服务挂掉时，依然可以进行能读能写操作；但是把ZooKeeper一旦挂掉，就不能读取数据了，因为读取数据所需要的元数据表<strong>hbase:meata</strong>的位置存储在ZooKeeper上。可见zookeeper对于HBase而言是至关重要的。</p><ul><li>Region</li></ul><p>Region就是一段数据的集合。HBase中的表一般拥有一个到多个Region。Region不能跨服务器，一个RegionServer上有一个或者多个Region。当开始创建表时，数据量小的时候，一个Region足以存储所有数据，等到数据量逐渐增加，会拆分为多个region；当HBase在进行负载均衡的时候，也有可能会从一台RegionServer上把Region移动到另一台RegionServer上。Region是存储在HDFS的，它的所有数据存取操作都是调用了HDFS的客户端接口来实现的。一个Region就相当于关系型数据库中分区表的一个分区。</p><h3 id="微观架构"><a href="#微观架构" class="headerlink" title="微观架构"></a>微观架构</h3><p>上一小节对HBase的整体架构进行了说明，接下来再看一下内部细节，如下图所示：展示了一台RegionServer的内部架构。</p><p><img src="//jiamaoxiang.top/2020/08/07/一文搞懂HBase的基本原理/%E5%86%85%E9%83%A8%E7%BB%86%E8%8A%82.png" alt></p><p>如上图所示：一个RegionServer可以存储多个region，Region相当于一个数据分片。每一个Region都有起<br>始rowkey和结束rowkey，代表了它所存储的row范围。在一个region内部，包括多个store，其中一个store对应一个列族，每个store的内部又包含一个<strong>MemStore</strong>，主要负责数据排序，等超过一定阈值之后将MemStore的数据刷到HFile文件，HFile文件时最终存储数据的地方。</p><p>值得注意的是：一台RegionServer共用一个WAL(Write-Ahead Log)预写日志，如果开启了WAL，那么当写数据时会先写进WAL，可以起到容错作用。WAL是一个保险机制，数据在写到Memstore之前，先被写到WAL了。这样当故障恢复的时候可以从WAL中恢复数据。另外，每个Store都有一个MemStore，用于数据排序。一台RegionServer也只有一个BlockCache，用于读数据是进行缓存。</p><ul><li><strong>WAL预写日志</strong></li></ul><p><strong>Write Ahead Log (WAL)</strong>会记录HBase中的所有数据，WAL起到容错恢复的作用，并不是必须的选项。在HDFS上，WAL的默认路径是<code>/hbase/WALs/</code>,用户可以通过<code>hbase.wal.dir</code>进行配置。</p><p>WAL默认是开启的，如果关闭，可以使用下面的命令<code>Mutation.setDurability(Durability.SKIP_WAL)</code>。WAL支持异步和同步的写入方式，异步方式通过调用下面的方法<code>Mutation.setDurability(Durability.ASYNC_WAL)</code>。同步方式通过调用下面的方法：<code>Mutation.setDurability(Durability.SYNC_WAL)</code>，其中同步方式是默认的方式。</p><p>关于异步WAL，当有Put、Delete、Append操作时，并不会立即触发同步数据。而是要等到一定的时间间隔，该时间间隔可以通过参数<code>hbase.regionserver.optionallogflushinterval</code>进行设定，默认是1000ms。</p><ul><li><strong>MemStore</strong></li></ul><p>每个Store中有一个MemStore实例。数据写入WAL之后就会被放入MemStore。MemStore是内存的存储对象，只有当MemStore满了的时候才会将数据刷写（flush）到HFile中。</p><p>为了让数据顺序存储从而提高读取效率，HBase使用了LSM树结构来存储数据。数据会先在Memstore中<br>整理成LSM树，最后再刷写到HFile上。</p><p>关于MemStore，很容易让人混淆。数据在被刷到HFile之前，已经被存储到了HDFS的WAL上了，那么为什么还要在放入MemStore呢？其实很简单，我们都知道HDFS是不能修改的，而HBase的数据又是按照Row Key进行排序的，其实这个排序的过程就是在MemStore中进行的。<strong>值得注意的是：MemStore的作用不是为了加快写速度，而是为了对Row Key进行排序。</strong></p><ul><li><strong>HFile</strong></li></ul><p>HFile是数据存储的实际载体，我们创建的所有表、列等数据都存储在HFile里面。当Memstore达到一定阀值，或者达到了刷写时间间隔阀值的时候，HBaes会被这个Memstore的内容刷写到HDFS系统上，称为一个存储在硬盘上的HFile文件。至此，我们数据真正地被持久化到硬盘上。</p><h3 id="Region的定位"><a href="#Region的定位" class="headerlink" title="Region的定位"></a>Region的定位</h3><p>在开始讲解HBase的数据读写流程之前，先来看一下Region是怎么定位的。我们知道Region是HBase非常重要的一个概念，Region存储在RegionServer中，那么客户端在读写操作时是如何定位到所需要的region呢？关于这个问题，老版本的HBase与新版本的HBase有所不同。</p><h4 id="老版本HBase-0-96-0之前"><a href="#老版本HBase-0-96-0之前" class="headerlink" title="老版本HBase(0.96.0之前)"></a>老版本HBase(0.96.0之前)</h4><p>老版本的HBase采用的是为三层查询架构，如下图所示：</p><img src="//jiamaoxiang.top/2020/08/07/一文搞懂HBase的基本原理/old定位.png" style="zoom: 67%;"><p>如上图：第一层定位是Zookeeper中的节点数据，记录了<code>-ROOT-</code>表的位置信息；</p><p>第二层<code>-ROOT-</code>表记录了<code>.META.</code>region位置信息，<code>-ROOT-</code>表只有一个region，通过<code>-ROOT-</code>表可以访问<code>.META.表中的数据</code></p><p>第三层<code>.META.</code>表，记录了用户数据表的region位置信息，<code>.META.</code>表可以有多个region。</p><p><strong>整个查询步骤</strong>如下：</p><p>第一步：用户通过查找zk（ZooKeeper）的/hbase/root-regionserver节点来知道<code>-ROOT-</code>表的RegionServer位置。</p><p>第二步：访问<code>-ROOT-</code>表，查找所需要的数据表的元数据信息存在哪个<code>.META.</code>表上，这个<code>.META.</code>表在哪个RegionServer上。</p><p>第四步：访问<code>.META.</code>表来看你要查询的行键在什么Region范围里面。</p><p>第五步：连接具体的数据所在的RegionServer，这个一步才开始在很正的查询数据。</p><h4 id="新版本HBase"><a href="#新版本HBase" class="headerlink" title="新版本HBase"></a>新版本HBase</h4><p>老版本的HBase寻址存在很多弊端，在新版本中进行了改进。采用的是二级寻址的方式，仅仅使用 <code>hbase:meta</code>表来定位region，那么 从哪里获取<code>hbase:meta</code>的信息呢，答案是zookeeper。在zookeeper中存在一个<code>/hbase/meta-region-server</code>节点，可以获取<code>hbase:meta</code>表的位置信息，然后通过<code>hbase:meta</code>表查询所需要数据所在的region位置信息。</p><p><strong>整个查询步骤</strong>如下：</p><p>第一步：客户端先通过ZooKeeper的<code>/hbase/meta-region-server</code>节点查询<code>hbase:meta</code>表的位置。</p><p>第二步：客户端连接<code>hbase:meta</code>表所在的RegionServer。<code>hbase:meta</code>表存储了所有Region的行键范围信息，通过这个表就可以查询出你要存取的rowkey属于哪个Region的范围里面，以及这个Region属于哪个<br>RegionServer。</p><p>第三步：获取这些信息后，客户端就可以直连拥有你要存取的rowkey的RegionServer，并直接对其操作。</p><p>第四步：客户端会把meta信息缓存起来，下次操作就不需要进行以上加载hbase:meta的步骤了。</p><h2 id="客户端API基本使用"><a href="#客户端API基本使用" class="headerlink" title="客户端API基本使用"></a>客户端API基本使用</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Example</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String TABLE_NAME = <span class="string">"MY_TABLE_NAME_TOO"</span>;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String CF_DEFAULT = <span class="string">"DEFAULT_COLUMN_FAMILY"</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createOrOverwrite</span><span class="params">(Admin admin, HTableDescriptor table)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (admin.tableExists(table.getTableName())) &#123;</span><br><span class="line">      admin.disableTable(table.getTableName());</span><br><span class="line">      admin.deleteTable(table.getTableName());</span><br><span class="line">    &#125;</span><br><span class="line">    admin.createTable(table);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createSchemaTables</span><span class="params">(Configuration config)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> (Connection connection = ConnectionFactory.createConnection(config);</span><br><span class="line">         Admin admin = connection.getAdmin()) &#123;</span><br><span class="line"></span><br><span class="line">      HTableDescriptor table = <span class="keyword">new</span> HTableDescriptor(TableName.valueOf(TABLE_NAME));</span><br><span class="line">      table.addFamily(<span class="keyword">new</span> HColumnDescriptor(CF_DEFAULT).setCompressionType(Algorithm.NONE));</span><br><span class="line"></span><br><span class="line">      System.out.print(<span class="string">"Creating table. "</span>);</span><br><span class="line">      createOrOverwrite(admin, table);</span><br><span class="line">      System.out.println(<span class="string">" Done."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">modifySchema</span> <span class="params">(Configuration config)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> (Connection connection = ConnectionFactory.createConnection(config);</span><br><span class="line">         Admin admin = connection.getAdmin()) &#123;</span><br><span class="line"></span><br><span class="line">      TableName tableName = TableName.valueOf(TABLE_NAME);</span><br><span class="line">      <span class="keyword">if</span> (!admin.tableExists(tableName)) &#123;</span><br><span class="line">        System.out.println(<span class="string">"Table does not exist."</span>);</span><br><span class="line">        System.exit(-<span class="number">1</span>);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      HTableDescriptor table = admin.getTableDescriptor(tableName);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 更新table</span></span><br><span class="line">      HColumnDescriptor newColumn = <span class="keyword">new</span> HColumnDescriptor(<span class="string">"NEWCF"</span>);</span><br><span class="line">      newColumn.setCompactionCompressionType(Algorithm.GZ);</span><br><span class="line">      newColumn.setMaxVersions(HConstants.ALL_VERSIONS);</span><br><span class="line">      admin.addColumn(tableName, newColumn);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 更新column family</span></span><br><span class="line">      HColumnDescriptor existingColumn = <span class="keyword">new</span> HColumnDescriptor(CF_DEFAULT);</span><br><span class="line">      existingColumn.setCompactionCompressionType(Algorithm.GZ);</span><br><span class="line">      existingColumn.setMaxVersions(HConstants.ALL_VERSIONS);</span><br><span class="line">      table.modifyFamily(existingColumn);</span><br><span class="line">      admin.modifyTable(tableName, table);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 禁用table</span></span><br><span class="line">      admin.disableTable(tableName);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 删除column family</span></span><br><span class="line">      admin.deleteColumn(tableName, CF_DEFAULT.getBytes(<span class="string">"UTF-8"</span>));</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 删除表，首先要禁用表</span></span><br><span class="line">      admin.deleteTable(tableName);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String... args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration config = HBaseConfiguration.create();</span><br><span class="line"></span><br><span class="line">    config.addResource(<span class="keyword">new</span> Path(System.getenv(<span class="string">"HBASE_CONF_DIR"</span>), <span class="string">"hbase-site.xml"</span>));</span><br><span class="line">    config.addResource(<span class="keyword">new</span> Path(System.getenv(<span class="string">"HADOOP_CONF_DIR"</span>), <span class="string">"core-site.xml"</span>));</span><br><span class="line">    createSchemaTables(config);</span><br><span class="line">    modifySchema(config);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="易混淆知识点总结"><a href="#易混淆知识点总结" class="headerlink" title="易混淆知识点总结"></a>易混淆知识点总结</h2><h3 id="Q1：MemStore的作用是什么？"><a href="#Q1：MemStore的作用是什么？" class="headerlink" title="Q1：MemStore的作用是什么？"></a>Q1：MemStore的作用是什么？</h3><p>在HBase中，一个表可以有多个列族，一个列族在物理上是存储在一起的，一个列族会对应一个store，在store的内部会存在一个MemStore，其作用并不是为了提升读写速度，而是为了对RowKey进行排序。我们知道，HBase的数据是存储在HDFS上的，而HDFS是不支持修改的，HBase为了按RowKey进行排序，首先会将数据写入MemStore，数据会先在Memstore中整理成LSM树，最后再刷写到HFile上。</p><p>总之一句话：<strong>Memstore的实现目的不是加速数据写入或读取，而是维持数据结构。</strong></p><h3 id="Q2：读取数据时会先从MemStore读取吗？"><a href="#Q2：读取数据时会先从MemStore读取吗？" class="headerlink" title="Q2：读取数据时会先从MemStore读取吗？"></a>Q2：读取数据时会先从MemStore读取吗？</h3><p>MemStore的作用是为了按RowKey进行排序，其作用不是为了提升读取速度的。读取数据的时候是有专门的缓存叫<code>BlockCache</code>，如果开启了BlockCache，就是先读BlockCache，然后才是读HFile+Memstore的数据。</p><h3 id="Q3：BlockCache有什么用"><a href="#Q3：BlockCache有什么用" class="headerlink" title="Q3：BlockCache有什么用?"></a>Q3：BlockCache有什么用?</h3><p><code>块缓存（BlockCache）</code>使用内存来记录数据，适用于提升读取性能。当开启了块缓存后，HBase会优先从块缓存中查询是否有记录，如果没有才去检索存储在硬盘上的HFile。</p><p>值得注意的是，<strong>一个RegionServer只有一个BlockCache</strong>。BlockCache不是数据存储的必须组成部分，只是用来优化读取性能的。</p><p>BlockCache的基本原理是：在读请求到HBase之后，会先尝试查询BlockCache，如果获取不到所需的数据，就去HFile和Memstore中去获取。如果获取到了，则在返回数据的同时把Block块缓存到BlockCache中。</p><h3 id="Q4：HBase是怎么删除数据的？"><a href="#Q4：HBase是怎么删除数据的？" class="headerlink" title="Q4：HBase是怎么删除数据的？"></a>Q4：HBase是怎么删除数据的？</h3><p>HBase删除记录并不是真的删除了数据，而是标识了一个墓碑标记（tombstone marker），把这个版本连同之前的版本都标记为不可见了。这是为了性能着想，这样HBase就可以定期去清理这些已经被删除的记录，而不用每次都进行删除操作。所谓定期清理，就是按照一定时间周期在HBase做自动合并（compaction，HBase整理存储文件时的一个操作，会把多个文件块合并成一个文件）。这样删除操作对于HBase的性能影响被降到了最低，即便是在很高的并发负载下大量删除记录也是OK的。</p><p>合并操作分为两种：<strong>Minor Compaction</strong>和<strong>Major Compaction</strong>。</p><p>其中<strong>Minor Compaction</strong>是将<strong>Store</strong>中<strong>多个HFile</strong>合并为一个HFile。在这个过程中达到TTL的数据会被移除，<strong>但是被手动删除的数据不会被移除</strong>。这种合并触发频率较高。</p><p>而<strong>Major Compaction</strong>合并<strong>Store</strong>中的<strong>所有HFile</strong>为一个HFile。在这个过程中<strong>被手动删除的数据会被真正地移除</strong>。同时被删除的还有单元格内超过MaxVersions的版本数据。这种合并触发频率较低，默认为7天一次。不过由于Major Compaction消耗的性能较大，一般建议手动控制MajorCompaction的时机。</p><p><strong>需要注意的是</strong>：Major Compaction删除的是那些带墓碑标记的数据，而Minor Compaction合并的时候直接会忽略过期数据文件，所以过期的这些文件会在Minor Compaction的时候就被删除。</p><h3 id="Q5：为什么HBase具有高性能的读写能力？"><a href="#Q5：为什么HBase具有高性能的读写能力？" class="headerlink" title="Q5：为什么HBase具有高性能的读写能力？"></a>Q5：为什么HBase具有高性能的读写能力？</h3><p>因为HBase使用了一种LSM的存储结构，在LSM树的实现方式中，会在数据存储之前先对数据进行排序。LSM树是Google BigTable和HBase的基本存储算法，它是传统关系型数据库的B+树的改进。算法的核心在于尽量保证数据是顺序存储到磁盘上的，并且会有频率地对数据进行整理，确保其顺序性。</p><p>LSM树就是一堆小树，在内存中的小树即memstore，每次flush，内存中的memstore变成磁盘上一个新的storefile。这种批量的读写操作使得HBase的性能较高。</p><h3 id="Q6：Store与列簇是什么关系？"><a href="#Q6：Store与列簇是什么关系？" class="headerlink" title="Q6：Store与列簇是什么关系？"></a>Q6：Store与列簇是什么关系？</h3><p>Region是HBase的核心模块，而Store则是Region的核心模块。每个Store对应了表中的一个列族存储。每个Store包含一个MemStore和若干个HFile。</p><h3 id="Q7：WAL是RegionServer共享的，还是Region级别共享的？"><a href="#Q7：WAL是RegionServer共享的，还是Region级别共享的？" class="headerlink" title="Q7：WAL是RegionServer共享的，还是Region级别共享的？"></a>Q7：WAL是RegionServer共享的，还是Region级别共享的？</h3><p>在HBase中，每个RegionServer只需要维护一个WAL，所有Region对象共用一个WAL，而不是每个Region都维护一个WAL。这种方式对于多个Region的更新操作所发生的的日志修改，只需要不断地追加到单个日志文件中，不需要同时打开并写入多个日志文件，这样可以减少磁盘寻址次数，提高写性能。</p><p>但是这种方式也存在一个缺点，如果RegionServer发生故障，为了恢复其上的Region对象，需要将RegionServer上的WAL按照其所属的Region对象进行拆分，然后分发到其他RegionServer上执行恢复操作。</p><h3 id="Q8：Master挂掉之后，还能查询数据吗？"><a href="#Q8：Master挂掉之后，还能查询数据吗？" class="headerlink" title="Q8：Master挂掉之后，还能查询数据吗？"></a>Q8：Master挂掉之后，还能查询数据吗？</h3><p>可以的。Master服务主要负责表和Region的管理工作。主要作用有：</p><ul><li>管理用户对表的增加、删除、修改操作</li><li>实现不同RegionServer之前的负载均衡</li><li>Region的分裂与合并</li><li>对发生故障的RegionServer的Region进行迁移</li></ul><p>客户端访问HBase时，不需要Master的参与，只需要连接zookeeper获取<strong>hbase:meta</strong>地址，然后直连RegionServer进行数据读写操作，Master仅仅维护表和Region的元数据信息，负载很小。但是Master节点也不能长时间的宕机。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先从谷歌的BigTable说起，然后介绍了CAP相关理论，并分析了NoSQL出现的原因。接着对HBase的数据模型进行了剖析，然后详细描述了HBase的原理和运行机制。最后给出了客户端API的基本使用，并对常见的、易混淆的知识点进行了解释。</p>]]></content>
      
      
      <categories>
          
          <category> HBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数仓面试|四个在工作后才知道的SQL密技</title>
      <link href="/2020/08/06/%E6%95%B0%E4%BB%93%E9%9D%A2%E8%AF%95-%E5%9B%9B%E4%B8%AA%E5%9C%A8%E5%B7%A5%E4%BD%9C%E5%90%8E%E6%89%8D%E7%9F%A5%E9%81%93%E7%9A%84SQL%E5%AF%86%E6%8A%80/"/>
      <url>/2020/08/06/%E6%95%B0%E4%BB%93%E9%9D%A2%E8%AF%95-%E5%9B%9B%E4%B8%AA%E5%9C%A8%E5%B7%A5%E4%BD%9C%E5%90%8E%E6%89%8D%E7%9F%A5%E9%81%93%E7%9A%84SQL%E5%AF%86%E6%8A%80/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>SQL是大数据从业者的必备技能，大部分的大数据技术框架也都提供了SQL的解决方案。可以说SQL是一种经久不衰、历久弥新的编程语言。尤其是在数仓领域，使用SQL更是家常便饭。本文会分享四个在面试和工作中常用的几个使用技巧，具体包括：</p><ul><li>日期与期间的使用</li><li>临时表与Common Table Expression (WITH)</li><li>Aggregation 与CASE WHEN的结合使用</li><li>Window Function的其他用途</li></ul><blockquote><p>数仓？不就是写写SQL吗… <img src="//jiamaoxiang.top/2020/08/06/数仓面试-四个在工作后才知道的SQL密技/0.jpg" alt="img"></p></blockquote><h2 id="第一：日期与期间的使用"><a href="#第一：日期与期间的使用" class="headerlink" title="第一：日期与期间的使用"></a>第一：日期与期间的使用</h2><p>日期与时间段的筛选在工作中是经常被用到的，因为在拉取报表、仪表板和各种分析时，周、月、季度、年度的表现往往是分析需要考量的重点。</p><h3 id="时间区段的提取：Extract"><a href="#时间区段的提取：Extract" class="headerlink" title="时间区段的提取：Extract"></a>时间区段的提取：Extract</h3><ul><li>语法</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- field可以是day、hour、minute, month, quarter等等</span></span><br><span class="line"><span class="comment">-- source可以是date、timestamp类型</span></span><br><span class="line">extract(field FROM source)</span><br></pre></td></tr></table></figure><ul><li>使用</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">extract</span>(<span class="keyword">year</span> <span class="keyword">FROM</span> <span class="string">'2020-08-05 09:30:08'</span>);   <span class="comment">-- 结果为 2020</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">extract</span>(<span class="keyword">quarter</span> <span class="keyword">FROM</span> <span class="string">'2020-08-05 09:30:08'</span>);   <span class="comment">-- 结果为 3</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">extract</span>(<span class="keyword">month</span> <span class="keyword">FROM</span> <span class="string">'2020-08-05 09:30:08'</span>);   <span class="comment">-- 结果为 8</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">extract</span>(<span class="keyword">week</span> <span class="keyword">FROM</span> <span class="string">'2020-08-05 09:30:08'</span>);   <span class="comment">-- 结果为 31,一年中的第几周</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">extract</span>(<span class="keyword">day</span> <span class="keyword">FROM</span> <span class="string">'2020-08-05 09:30:08'</span>);  <span class="comment">-- 结果为 5</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">extract</span>(<span class="keyword">hour</span> <span class="keyword">FROM</span> <span class="string">'2020-08-05 09:30:08'</span>);   <span class="comment">-- 结果为 9</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">extract</span>(<span class="keyword">minute</span> <span class="keyword">FROM</span> <span class="string">'2020-08-05 09:30:08'</span>);   <span class="comment">-- 结果为 30</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">extract</span>(<span class="keyword">second</span> <span class="keyword">FROM</span> <span class="string">'2020-08-05 09:30:08'</span>);   <span class="comment">-- 结果为 8</span></span><br></pre></td></tr></table></figure><p>上面可供提取的字段，不同的数据库存在些许的差异。以Hive为例，支持<code>day, dayofweek, hour, minute, month, quarter, second, week 和 year</code>。其中周、月、年使用最为广泛，因为无论是公司内部产品，还是商用的产品所提供的数据后台统计，周报和月报(比如近7天、近30天)最注重表现的周期。</p><blockquote><p>注意：</p><p>impala支持：YEAR, QUARTER, MONTH, DAY, HOUR, MINUTE, SECOND, MILLISECOND, EPOCH</p><p>Hive支持：day, dayofweek, hour, minute, month, quarter, second, week 和 year</p><p>Hive是从<strong>Hive2.2.0</strong>版本开始引入该函数</p></blockquote><h3 id="周的提取"><a href="#周的提取" class="headerlink" title="周的提取"></a>周的提取</h3><ul><li>语法</li></ul><p>在按照周的区间进行统计时，需要识别出周一的日期与周日的日期，这个时候经常会用到下面的函数：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">next_day(STRING start_date, STRING day_of_week)</span><br><span class="line"><span class="comment">-- 返回当前日期对应的下一个周几对应的日期</span></span><br><span class="line"><span class="comment">-- 2020-08-05为周三</span></span><br><span class="line"><span class="keyword">SELECT</span> next_day(<span class="string">'2020-08-05'</span>,<span class="string">'MO'</span>) <span class="comment">-- 下一个周一对应的日期：2020-08-10</span></span><br><span class="line"><span class="keyword">SELECT</span> next_day(<span class="string">'2020-08-05'</span>,<span class="string">'TU'</span>) <span class="comment">-- 下一个周二对应的日期：2020-08-11</span></span><br><span class="line"><span class="keyword">SELECT</span> next_day(<span class="string">'2020-08-05'</span>,<span class="string">'WE'</span>) <span class="comment">-- 下一个周三对应的日期：2020-08-12</span></span><br><span class="line"><span class="keyword">SELECT</span> next_day(<span class="string">'2020-08-05'</span>,<span class="string">'TH'</span>) <span class="comment">-- 下一个周四对应的日期：2020-08-06，即为本周四</span></span><br><span class="line"><span class="keyword">SELECT</span> next_day(<span class="string">'2020-08-05'</span>,<span class="string">'FR'</span>) <span class="comment">-- 下一个周五对应的日期：2020-08-07，即为本周五</span></span><br><span class="line"><span class="keyword">SELECT</span> next_day(<span class="string">'2020-08-05'</span>,<span class="string">'SA'</span>) <span class="comment">-- 下一个周六对应的日期：2020-08-08，即为本周六</span></span><br><span class="line"><span class="keyword">SELECT</span> next_day(<span class="string">'2020-08-05'</span>,<span class="string">'SU'</span>) <span class="comment">-- 下一个周日对应的日期：2020-08-09，即为本周日</span></span><br><span class="line"><span class="comment">-- 星期一到星期日的英文（Monday，Tuesday、Wednesday、Thursday、Friday、Saturday、Sunday）</span></span><br></pre></td></tr></table></figure><ul><li>使用</li></ul><p>那么该如何获取当前日期所在周的周一对应的日期呢？只需要先获取当前日期的下周一对应的日期，然后减去7天，即可获得：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">date_add</span>(next_day(<span class="string">'2020-08-05'</span>,<span class="string">'MO'</span>),<span class="number">-7</span>);</span><br></pre></td></tr></table></figure><p>同理，获取当前日期所在周的周日对应的日期，只需要先获取当前日期的下周一对应的日期，然后减去1天，即可获得：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_add</span>(next_day(<span class="string">'2020-08-05'</span>,<span class="string">'MO'</span>),<span class="number">-1</span>) </span><br><span class="line"><span class="comment">-- 2020-08-09</span></span><br></pre></td></tr></table></figure><h3 id="月的提取"><a href="#月的提取" class="headerlink" title="月的提取"></a>月的提取</h3><ul><li>语法</li></ul><p>至于怎么将月份从单一日期提取出来呢，LAST_DAY这个函数可以将每个月中的日期变成该月的最后一天(28号，29号，30号或31号)，如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">last_day(STRING date)</span><br></pre></td></tr></table></figure><ul><li>使用</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">last_day</span>(<span class="string">'2020-08-05'</span>); <span class="comment">-- 2020-08-31</span></span><br></pre></td></tr></table></figure><p>除了上面的方式，也可以使用date_format函数，比如：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">date_format</span>(<span class="string">'2020-08-05'</span>,<span class="string">'yyyy-MM'</span>);</span><br><span class="line"><span class="comment">-- 2020-08</span></span><br></pre></td></tr></table></figure><h3 id="日期的范围"><a href="#日期的范围" class="headerlink" title="日期的范围"></a>日期的范围</h3><p>月的Window：使用add_months加上trunc()的应用</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 返回加减月份之后对应的日期</span></span><br><span class="line"><span class="comment">-- 2020-07-05</span></span><br><span class="line"><span class="keyword">select</span> add_months(<span class="string">'2020-08-05'</span>, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 返回当前日期的月初日期</span></span><br><span class="line"><span class="comment">-- 2020-08-01</span></span><br><span class="line"><span class="keyword">select</span> trunc(<span class="string">"2020-08-05"</span>,<span class="string">'MM'</span>)</span><br></pre></td></tr></table></figure><p>由上面范例可见，单纯使用add_months，减N个月的用法，可以刚好取到整数月的数据，但如果加上trunc()函数，则会从前N个月的一号开始取值。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 选取2020-07-05到2020-08-05所有数据</span></span><br><span class="line">BETWEEN add_months('2020-08-05', -1) AND '2020-08-05' </span><br><span class="line"><span class="comment">-- 选取2020-07-01到2020-08-05之间所有数据</span></span><br><span class="line">BETWEEN add_months(trunc("2020-08-05",'MM'),-1) AND '2020-08-05'</span><br></pre></td></tr></table></figure><h2 id="第二：临时表与Common-Table-Expression-WITH"><a href="#第二：临时表与Common-Table-Expression-WITH" class="headerlink" title="第二：临时表与Common Table Expression (WITH)"></a>第二：临时表与Common Table Expression (WITH)</h2><p>这两种方法是日常工作中经常被使用到，对于一些比较复杂的计算任务，为了避免过多的JOIN，通常会先把一些需要提取的部分数据使用临时表或是CTE的形式在主要查询区块前进行提取。</p><p><strong>临时表的作法：</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TEMPORARY TABLE table_1 AS  </span><br><span class="line">    SELECT </span><br><span class="line">        columns</span><br><span class="line">    FROM table A;</span><br><span class="line">CREATE TEMPORARY table_2 AS </span><br><span class="line">    SELECT</span><br><span class="line">        columns</span><br><span class="line">    FROM table B;</span><br><span class="line"></span><br><span class="line">SELECT</span><br><span class="line">    table_1.columns,</span><br><span class="line">    table_2.columns, </span><br><span class="line">    c.columns </span><br><span class="line">FROM table C JOIN table_1</span><br><span class="line">     JOIN table_2;</span><br></pre></td></tr></table></figure><p><strong>CTE的作法：</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-- 注意Hive、Impala支持这种语法，低版本的MySQL不支持(高版本支持)</span><br><span class="line">WITH employee_by_title_count AS (</span><br><span class="line">    SELECT</span><br><span class="line">        t.name as job_title</span><br><span class="line">        , COUNT(e.id) as amount_of_employees</span><br><span class="line">    FROM employees e</span><br><span class="line">        JOIN job_titles t on e.job_title_id = t.id</span><br><span class="line">    GROUP BY 1</span><br><span class="line">),</span><br><span class="line">salaries_by_title AS (</span><br><span class="line">     SELECT</span><br><span class="line">         name as job_title</span><br><span class="line">         , salary</span><br><span class="line">     FROM job_titles</span><br><span class="line">)</span><br><span class="line">SELECT *</span><br><span class="line">FROM employee_by_title_count e</span><br><span class="line">    JOIN salaries_by_title s ON s.job_title = e.job_title</span><br></pre></td></tr></table></figure><p>可以看到TEMP TABLE和CTE WITH的用法其实非常类似，目的都是为了让你的Query更加一目了然且优雅简洁。很多人习惯将所有的Query写在单一的区块里面，用过多的JOIN或SUBQUERY，导致最后逻辑丢失且自己也搞不清楚写到哪里，适时的使用TEMP TABLE和CTE作为辅助，绝对是很加分的。</p><h2 id="第三：Aggregation-与CASE-WHEN的结合使用"><a href="#第三：Aggregation-与CASE-WHEN的结合使用" class="headerlink" title="第三：Aggregation 与CASE WHEN的结合使用"></a>第三：Aggregation 与CASE WHEN的结合使用</h2><p>将Aggregation function (SUM/COUNT/COUNT DISTINCT/MIN/MAX) 结合CASE WHEN是最强大且最有趣的使用方式。这样的使用创造出一种类似EXCEL中SUMIF/COUNTIF的效果，可以用这个方式做出很多高效的分析。</p><ul><li>Table Name: order</li><li>Column: register_date, order_date, user_id, country, order_sales, order_id</li></ul><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">order</span>(</span><br><span class="line">    register_date <span class="keyword">string</span>,</span><br><span class="line">    order_date <span class="keyword">string</span>,</span><br><span class="line">    user_id <span class="keyword">string</span>,</span><br><span class="line">    country <span class="keyword">string</span>,</span><br><span class="line">    order_sales <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">    order_id <span class="keyword">string</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> <span class="keyword">order</span> <span class="keyword">VALUES</span>(<span class="string">"2020-06-07"</span>,<span class="string">"2020-06-09"</span>,<span class="string">"001"</span>,<span class="string">'c0'</span>,<span class="number">210</span>,<span class="string">"o1"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> <span class="keyword">order</span> <span class="keyword">VALUES</span>(<span class="string">"2020-06-08"</span>,<span class="string">"2020-06-09"</span>,<span class="string">"002"</span>,<span class="string">'c1'</span>,<span class="number">220</span>,<span class="string">"o2"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> <span class="keyword">order</span> <span class="keyword">VALUES</span>(<span class="string">"2020-06-07"</span>,<span class="string">"2020-06-10"</span>,<span class="string">"003"</span>,<span class="string">'c2'</span>,<span class="number">230</span>,<span class="string">"o3"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> <span class="keyword">order</span> <span class="keyword">VALUES</span>(<span class="string">"2020-06-09"</span>,<span class="string">"2020-06-10"</span>,<span class="string">"004"</span>,<span class="string">'c3'</span>,<span class="number">200</span>,<span class="string">"o4"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> <span class="keyword">order</span> <span class="keyword">VALUES</span>(<span class="string">"2020-06-07"</span>,<span class="string">"2020-06-20"</span>,<span class="string">"005"</span>,<span class="string">'c4'</span>,<span class="number">300</span>,<span class="string">"o5"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> <span class="keyword">order</span> <span class="keyword">VALUES</span>(<span class="string">"2020-06-10"</span>,<span class="string">"2020-06-23"</span>,<span class="string">"006"</span>,<span class="string">'c5'</span>,<span class="number">400</span>,<span class="string">"o6"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> <span class="keyword">order</span> <span class="keyword">VALUES</span>(<span class="string">"2020-06-07"</span>,<span class="string">"2020-06-19"</span>,<span class="string">"007"</span>,<span class="string">'c6'</span>,<span class="number">600</span>,<span class="string">"o7"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> <span class="keyword">order</span> <span class="keyword">VALUES</span>(<span class="string">"2020-06-12"</span>,<span class="string">"2020-06-18"</span>,<span class="string">"008"</span>,<span class="string">'c7'</span>,<span class="number">700</span>,<span class="string">"o8"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> <span class="keyword">order</span> <span class="keyword">VALUES</span>(<span class="string">"2020-06-07"</span>,<span class="string">"2020-06-09"</span>,<span class="string">"009"</span>,<span class="string">'c8'</span>,<span class="number">100</span>,<span class="string">"o9"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> <span class="keyword">order</span> <span class="keyword">VALUES</span>(<span class="string">"2020-06-15"</span>,<span class="string">"2020-06-18"</span>,<span class="string">"0010"</span>,<span class="string">'c9'</span>,<span class="number">200</span>,<span class="string">"o10"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> <span class="keyword">order</span> <span class="keyword">VALUES</span>(<span class="string">"2020-06-15"</span>,<span class="string">"2020-06-19"</span>,<span class="string">"0011"</span>,<span class="string">'c10'</span>,<span class="number">250</span>,<span class="string">"o11"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> <span class="keyword">order</span> <span class="keyword">VALUES</span>(<span class="string">"2020-06-12"</span>,<span class="string">"2020-06-29"</span>,<span class="string">"0012"</span>,<span class="string">'c11'</span>,<span class="number">270</span>,<span class="string">"o12"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> <span class="keyword">order</span> <span class="keyword">VALUES</span>(<span class="string">"2020-06-16"</span>,<span class="string">"2020-06-19"</span>,<span class="string">"0013"</span>,<span class="string">'c12'</span>,<span class="number">230</span>,<span class="string">"o13"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> <span class="keyword">order</span> <span class="keyword">VALUES</span>(<span class="string">"2020-06-17"</span>,<span class="string">"2020-06-20"</span>,<span class="string">"0014"</span>,<span class="string">'c13'</span>,<span class="number">290</span>,<span class="string">"o14"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> <span class="keyword">order</span> <span class="keyword">VALUES</span>(<span class="string">"2020-06-20"</span>,<span class="string">"2020-06-29"</span>,<span class="string">"0015"</span>,<span class="string">'c14'</span>,<span class="number">203</span>,<span class="string">"o15"</span>);</span><br></pre></td></tr></table></figure><h3 id="CASE-WHEN-时间，进行留存率-使用率的分析"><a href="#CASE-WHEN-时间，进行留存率-使用率的分析" class="headerlink" title="CASE WHEN 时间，进行留存率/使用率的分析"></a>CASE WHEN 时间，进行留存率/使用率的分析</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 允许多列去重</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.skewindata = <span class="literal">false</span></span><br><span class="line"><span class="comment">-- 允许使用位置编号分组或排序</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.orderby.position.alias = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="keyword">date_add</span>(Next_day(register_date, <span class="string">'MO'</span>),<span class="number">-1</span>) <span class="keyword">AS</span> week_end,</span><br><span class="line">    <span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> <span class="keyword">CASE</span> <span class="keyword">WHEN</span> order_date <span class="keyword">BETWEEN</span> register_date <span class="keyword">AND</span> <span class="keyword">date_add</span>(register_date,<span class="number">6</span>) <span class="keyword">THEN</span> user_id <span class="keyword">END</span>) <span class="keyword">AS</span> first_week_order,</span><br><span class="line">    <span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> <span class="keyword">CASE</span> <span class="keyword">WHEN</span> order_date <span class="keyword">BETWEEN</span> <span class="keyword">date_add</span>(register_date ,<span class="number">7</span>) <span class="keyword">AND</span> <span class="keyword">date_add</span>(register_date,<span class="number">13</span>) <span class="keyword">THEN</span> user_id <span class="keyword">END</span>) <span class="keyword">AS</span> sencod_week_order,</span><br><span class="line">    <span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> <span class="keyword">CASE</span> <span class="keyword">WHEN</span> order_date <span class="keyword">BETWEEN</span> <span class="keyword">date_add</span>(register_date ,<span class="number">14</span>) <span class="keyword">AND</span> <span class="keyword">date_add</span>(register_date,<span class="number">20</span>) <span class="keyword">THEN</span> user_id <span class="keyword">END</span>) <span class="keyword">as</span> third_week_order</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">order</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>上面的示例可以得知到用户在注册之后，有没有创建订单的行为。比如注册后的第一周，第二周，第三周分别有多少下单用户，这样可以分析出用户的使用情况和留存情况。</p><blockquote><p>注意：上面的使用方式，需要配置两个参数：</p><p><strong>hive.groupby.skewindata = false</strong>：允许多列去重，否则报错：</p><p><code>SemanticException [Error 10022]: DISTINCT on different columns not supported with skew in data</code></p><p><strong>hive.groupby.orderby.position.alias = true</strong>：允许使用位置编号分组或排序,否则报错：</p><p><code>SemanticException [Error 10025]: line 79:13 Expression not in GROUP BY key &#39;&#39;MO&#39;&#39;</code></p></blockquote><h3 id="CASE-WHEN-时间，进行每个用户消费金额的分析"><a href="#CASE-WHEN-时间，进行每个用户消费金额的分析" class="headerlink" title="CASE WHEN 时间，进行每个用户消费金额的分析"></a>CASE WHEN 时间，进行每个用户消费金额的分析</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT</span><br><span class="line">    user_id,</span><br><span class="line">    SUM (CASE WHEN order_date BETWEEN register_date AND date_add(register_date,6) THEN order_sales END) AS first_week_amount,</span><br><span class="line">    SUM (CASE WHEN order_date BETWEEN date_add(register_date ,7) AND date_add(register_date,13) THEN order_sales END) AS second_week_amount</span><br><span class="line">    FROM order</span><br><span class="line">GROUP BY 1</span><br></pre></td></tr></table></figure><p>通过筛选出注册与消费的日期，并且进行消费金额统计，每个用户在每段时间段(注册后第一周、第二周…以此类推)的消费金额，可以观察用户是否有持续维持消费习惯或是消费金额变低等分析。</p><h3 id="CASE-WHEN数量，消费金额超过某一定额的数量分析"><a href="#CASE-WHEN数量，消费金额超过某一定额的数量分析" class="headerlink" title="CASE WHEN数量，消费金额超过某一定额的数量分析"></a>CASE WHEN数量，消费金额超过某一定额的数量分析</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT</span><br><span class="line">    user_id,</span><br><span class="line">    COUNT(DISTINCT CASE WHEN order_sales &gt;= 100 THEN order_id END) AS count_of_order_greateer_than_100</span><br><span class="line">FROM order</span><br><span class="line">GROUP BY 1</span><br></pre></td></tr></table></figure><p>上面的示例就是类似countif的用法，针对每个用户，统计其订单金额大于某个值的订单数量，分析去筛选出高价值的顾客。</p><h3 id="CASE-WHEN数量，加上时间的用法"><a href="#CASE-WHEN数量，加上时间的用法" class="headerlink" title="CASE WHEN数量，加上时间的用法"></a>CASE WHEN数量，加上时间的用法</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT</span><br><span class="line">    user_id,</span><br><span class="line">    MIN(CASE WHEN order_sales &gt; 100 THEN order_date END) AS first_order_date_over1000,</span><br><span class="line">    MAX(CASE WHEN order_sales &gt; 100 THEN order_date END) AS recent_order_date_over100</span><br><span class="line">FROM order</span><br><span class="line">GROUP BY 1</span><br></pre></td></tr></table></figure><p>CASE WHEN加上MIN/MAX时间，可以得出该用户在其整个使用过程中，首次购买超过一定金额的订单日期，以及最近一次购买超过一定金额的订单日期。</p><h2 id="第四：Window-Function的其他用途"><a href="#第四：Window-Function的其他用途" class="headerlink" title="第四：Window Function的其他用途"></a>第四：Window Function的其他用途</h2><p>Window Function既是工作中经常使用的函数，也是面试时经常被问到的问题。常见的使用场景是分组取topN。本文介绍的另外一个用法，使用开窗函数进行用户访问session分析。</p><p>session是指在指定的时间段内用户在网站上发生的一系列互动。例如，一次session可以包含多个网页浏览、事件、社交互动和电子商务交易。session就相当于一个容器，其中包含了用户在网站上执行的操作。</p><p><img src="https://lh3.googleusercontent.com/jYib9rNgrLOavCGfEaMPqJNOIf6cN5aHqsZpXAKPP1IVOUM3iFImIIxMW_AnWHlI5xKJ=w550" alt="Many interactions can happen within one visit."></p><p>session具有一个过期时间，比如30分钟，即不活动状态超过 30 分钟，该session就会过时。</p><p>假设张三访问了网站，从他到达网站的那一刻开始，就开始计时。如果过了 30 分钟，而张三仍然没有进行任何形式的互动，则视为本次session结束。但是，只要张三与某个元素进行了互动（例如发生了某个事件、社交互动或打开了新网页），就会在该次互动的时间基础上再增加 30 分钟，从而重置过期时间。</p><p><img src="https://lh3.googleusercontent.com/e4JuWb2416rLY5bXu93-bBeDtWh3UH3UCymcVeFylmOLH_pNp-UX6amLQkrH4e9o2Q=w550" alt="A series of standard interactions and the visit expiry."></p><h3 id="数据准备-1"><a href="#数据准备-1" class="headerlink" title="数据准备"></a>数据准备</h3><ul><li>Table Name: user_visit_action</li><li>Columns: user_id, session_id , page_url, action_time</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_visit_action( </span><br><span class="line">    user_id <span class="keyword">string</span>,</span><br><span class="line">    session_id <span class="keyword">string</span>,</span><br><span class="line">    page_url <span class="keyword">string</span>,</span><br><span class="line">    action_time <span class="keyword">string</span>);</span><br><span class="line">    </span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> user_visit_action <span class="keyword">VALUES</span>(<span class="string">"001"</span>,<span class="string">"ss001"</span>,<span class="string">"http://a.com"</span>,<span class="string">"2020-08-06 13:34:11.478"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> user_visit_action <span class="keyword">VALUES</span>(<span class="string">"001"</span>,<span class="string">"ss001"</span>,<span class="string">"http://b.com"</span>,<span class="string">"2020-08-06 13:35:11.478"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> user_visit_action <span class="keyword">VALUES</span>(<span class="string">"001"</span>,<span class="string">"ss001"</span>,<span class="string">"http://c.com"</span>,<span class="string">"2020-08-06 13:36:11.478"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> user_visit_action <span class="keyword">VALUES</span>(<span class="string">"001"</span>,<span class="string">"ss002"</span>,<span class="string">"http://a.com"</span>,<span class="string">"2020-08-06 14:30:11.478"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> user_visit_action <span class="keyword">VALUES</span>(<span class="string">"001"</span>,<span class="string">"ss002"</span>,<span class="string">"http://b.com"</span>,<span class="string">"2020-08-06 14:31:11.478"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> user_visit_action <span class="keyword">VALUES</span>(<span class="string">"001"</span>,<span class="string">"ss002"</span>,<span class="string">"http://e.com"</span>,<span class="string">"2020-08-06 14:33:11.478"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> user_visit_action <span class="keyword">VALUES</span>(<span class="string">"001"</span>,<span class="string">"ss002"</span>,<span class="string">"http://f.com"</span>,<span class="string">"2020-08-06 14:35:11.478"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> user_visit_action <span class="keyword">VALUES</span>(<span class="string">"002"</span>,<span class="string">"ss003"</span>,<span class="string">"http://u.com"</span>,<span class="string">"2020-08-06 18:34:11.478"</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> user_visit_action <span class="keyword">VALUES</span>(<span class="string">"002"</span>,<span class="string">"ss003"</span>,<span class="string">"http://k.com"</span>,<span class="string">"2020-08-06 18:38:11.478"</span>);</span><br></pre></td></tr></table></figure><h3 id="用户访问session分析"><a href="#用户访问session分析" class="headerlink" title="用户访问session分析"></a>用户访问session分析</h3><p>范例的资料表如上，有使用者、访次和页面的连结和时间。以下则使用partition by来表达每个使用者在不同访次之间的浏览行为。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    user_id,</span><br><span class="line">    session_id,</span><br><span class="line">    page_url,</span><br><span class="line">    <span class="keyword">DENSE_RANK</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> user_id, session_id <span class="keyword">ORDER</span> <span class="keyword">BY</span> action_time <span class="keyword">ASC</span>) <span class="keyword">AS</span> page_order,</span><br><span class="line">    <span class="keyword">MIN</span>(action_time) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> user_id, session_id) <span class="keyword">AS</span> session_start_time,</span><br><span class="line">    <span class="keyword">MAX</span>(action_time) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> user_id, session_id) <span class="keyword">AS</span> session_finisht_time</span><br><span class="line"><span class="keyword">FROM</span> user_visit_action</span><br></pre></td></tr></table></figure><p>上面的查询会返回针对每个用户、每次的到访，浏览页面行为的先后次序，以及该session开始与结束的时间，以此为基础就可以将这个结果存入TEMP TABLE或是CTE ，进行更进一步的分析。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要分享了四个在工作和面试中经常遇到的SQL使用技巧。当然，这些都与具体的分析业务息息相关。最后，不管你是SQL boy  or  SQL girl，只要是掌握一些技巧，相信都能够Happy SQL querying 😊。</p>]]></content>
      
      
      <categories>
          
          <category> 数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数仓 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第七篇|Spark平台下基于LDA的k-means算法实现</title>
      <link href="/2020/08/02/%E7%AC%AC%E4%B8%83%E7%AF%87-Spark%E5%B9%B3%E5%8F%B0%E4%B8%8B%E5%9F%BA%E4%BA%8ELDA%E7%9A%84k-means%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/"/>
      <url>/2020/08/02/%E7%AC%AC%E4%B8%83%E7%AF%87-Spark%E5%B9%B3%E5%8F%B0%E4%B8%8B%E5%9F%BA%E4%BA%8ELDA%E7%9A%84k-means%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>本文主要在Spark平台下实现一个机器学习应用，该应用主要涉及LDA主题模型以及K-means聚类。通过本文你可以了解到：</p><ul><li>文本挖掘的基本流程</li><li>LDA主题模型算法</li><li>K-means算法</li><li>Spark平台下LDA主题模型实现</li><li>Spark平台下基于LDA的K-means算法实现</li></ul><h2 id="1-文本挖掘模块设计"><a href="#1-文本挖掘模块设计" class="headerlink" title="1.文本挖掘模块设计"></a>1.文本挖掘模块设计</h2><h3 id="1-1文本挖掘流程"><a href="#1-1文本挖掘流程" class="headerlink" title="1.1文本挖掘流程"></a>1.1文本挖掘流程</h3><p>文本分析是机器学习中的一个很宽泛的领域，并且在情感分析、聊天机器人、垃圾邮件检测、推荐系统以及自然语言处理等方面得到了广泛应用。</p><p>文本聚类是信息检索领域的一个重要概念，在文本挖掘领域有着广泛的应用。文本聚类能够自动地将文本数据集划分为不同的类簇，从而更好地组织文本信息，可以实现高效的知识导航与浏览。</p><p>本文选择主题模型LDA(Latent Dirichlet Allocation)算法对文档进行分类处理，选择在Spark平台上通过Spark MLlib实现LDA算法，其中Spark Mllib是Spark提供的机器学习库，该库提供了常用的机器学习算法。其基本设计思路如下图所示：</p><p><img src="//jiamaoxiang.top/2020/08/02/第七篇-Spark平台下基于LDA的k-means算法实现/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E8%BF%87%E7%A8%8B.png" alt></p><h3 id="1-2文本挖掘流程分析"><a href="#1-2文本挖掘流程分析" class="headerlink" title="1.2文本挖掘流程分析"></a>1.2文本挖掘流程分析</h3><p>首先是数据源部分，主要的数据包括文档数据和互联网爬虫数据。然后是数据抽取部分，将已经收集好的数据通过同步工具上传至分布式文件系统HDFS，作为模型训练的数据源。其次是数据探索与预处理部分，该部分主要是对原始数据集进行预处理，包括分词、停用词过滤、特征向量提取等。再次是模型训练部分，主要包括训练与测试，从而得到一个模型。最后是模型评估，对学得模型进行评估之后，进行线上部署。</p><h2 id="2-文本挖掘模块算法研究"><a href="#2-文本挖掘模块算法研究" class="headerlink" title="2.文本挖掘模块算法研究"></a>2.文本挖掘模块算法研究</h2><h3 id="2-1LDA主题模型算法"><a href="#2-1LDA主题模型算法" class="headerlink" title="2.1LDA主题模型算法"></a>2.1LDA主题模型算法</h3><p>LDA（Latent Dirichlet allocation）由David M. Blei，Andrew Y. Ng，Michael I. Jordan于2003年提出的基于概率模型的主题模型算法，即隐含狄利克雷分布，它可以将文档集中每篇文档的主题以概率分布的形式给出，将文本向量投射到更容易分析处理的主题空间当中，去除文本中存在的噪声，是一种常用的文本分析技术，可以用来识别大规模文档集或语料库中潜在的主题信息，通常被用来对大规模文档数据进行建模。通过主题模型和设定的主题数，可以训练出文档集合中不同的主题所占的比例以及每个主题下的关键词语出现的概率。从文档集合中学习得到主题分布和主题比例，可以进一步在数据挖掘任务中使用。如果生成一篇文档，则其中每个词出现的概率都可以通过下述公式来计算。</p><p><img src="//jiamaoxiang.top/2020/08/02/第七篇-Spark平台下基于LDA的k-means算法实现/%E5%9B%BE1.png" alt></p><p>LDA借用词袋的思想，以某一概率选取某个主题，再以某一概率选出主题中的每个单词，通过不断重复该步骤产生文档中的所有语词。该方法对词汇进行了模糊聚类，聚集到一类的词可以间接地表示一个隐含的主题。LDA对文本信息进行了挖掘，能用来衡量不同文档之间的潜在关系，也能通过某一类词来表达文档中隐藏的主题。</p><p><img src="//jiamaoxiang.top/2020/08/02/第七篇-Spark平台下基于LDA的k-means算法实现/LDA%E5%9B%BE%E6%A8%A1%E5%9E%8B.png" alt></p><h3 id="2-2K均值算法"><a href="#2-2K均值算法" class="headerlink" title="2.2K均值算法"></a>2.2K均值算法</h3><p>聚类(Clustering)是一种将数据集划分为若干组或类的方法。通过聚类过程将一群抽象的对象分为若干组，每一组由相似的对象构成，称之为一个类别。与分类不同(将数据按照事先定义好的分类标准进行划分)，聚类是一种无监督学习(unsupervised learning)，训练数据集的标签信息是未知的，目标是通过对无标记训练样本按照特定的测度的形似性程度进行聚合，为进一步数据分析提供基础。</p><p>K均值(k-means)算法的基本思想是初始随机给定K 个簇中心，即从n个数据对象中选择k个任意对象作为初始的簇中心，按照最邻近原则把待分类样本点分到各个簇。然后按平均法重新计算各个簇的中心(该类别中的所有数据对象的均值)，从而确定新的簇心。一直迭代，直到簇心的移动距离小于某个给定的值。</p><p>K均值算法采用了贪心策略，通过迭代优化来近似求解上式E值，算法流程如下图所示</p><p><img src="//jiamaoxiang.top/2020/08/02/第七篇-Spark平台下基于LDA的k-means算法实现/Kmeans.png" alt> </p><h3 id="2-3文本挖掘算法优化"><a href="#2-3文本挖掘算法优化" class="headerlink" title="2.3文本挖掘算法优化"></a>2.3文本挖掘算法优化</h3><p>LDA主题模型算法应用于文档聚类，计算得出的主题可以看做是文档的聚类中心，利用主题模型进行文档聚类，可以有效地组织文档数据集。同时，由于LDA主题模型可以计算出每篇文档在不同主题下的概率分布，因此可以将此主题的概率分布作为文档的特征向量，从而将高维的文档向量投影到低维的特征空间中。</p><p>计算文本之间的距离是传统的K-means算法在进行文本聚类时的关键步骤，而文本通常是非结构化数据，构建的文本向量具有稀疏性和维度高的特点，同时，构建文本特征向量并未考虑到文字之间的语义关系，因此可能会造成位于同一类簇的文本之间具有非相似性。</p><p>因此本文基于LDA主题模型改进K-means算法，首先通过LDA主题模型对文档数据集进行建模，挖掘出每篇文档的主题概率分布，既能够达到文档降维和去除噪声的效果，又能弥补通过关键词构建文档特征向量容易造成丢失信息的缺陷。最后每篇文档的主题概率分布作为K-means算法的输入数据集。 </p><h2 id="3-实验分析"><a href="#3-实验分析" class="headerlink" title="3.实验分析"></a>3.实验分析</h2><h3 id="3-1基于Spark的LDA主题模型算法实现"><a href="#3-1基于Spark的LDA主题模型算法实现" class="headerlink" title="3.1基于Spark的LDA主题模型算法实现"></a>3.1基于Spark的LDA主题模型算法实现</h3><h4 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h4><p>选择Newsgroups数据集作为该实验的训练集和测试集。Newgroups是一个新闻数据集，该数据集包括大约20000篇新闻文档，总共分为6个大类别，每个大类别又分不同的小类别，小类别共计20个，如下表所示。该新闻数据集已经成为了学界和业界在机器学习的文本挖掘实验中常用的数据集，比如文本分类和文本聚类。</p><p>该数据集共包含7个文件，其中3个文件为训练数据(train.data、train.label、train.map)，共计11269篇，另外3个文件为测试数据(test.data、test.label、test.map)，共计7505篇，另外一个文件为词汇表(vocabulary.txt),其第i行表示编号为i的单词的名称。文件扩展名为.data的文件格式为[docIdx wordIdx count]，其中docIdx表示文档编号，wordIdx表示词语的编号，count表示该词语的词频统计。文件扩展名为.label的文件表示文档的主题分类，每行数据代表某篇文档的类别。文件扩展名为.map的文表示类别编号与类别名称的映射关系，其具体格式为[labelName labelId]。</p><p><img src="//jiamaoxiang.top/2020/08/02/第七篇-Spark平台下基于LDA的k-means算法实现/%E6%95%B0%E6%8D%AE%E9%9B%86%E7%B1%BB%E5%88%AB.png" alt></p><h4 id="原始数据集处理"><a href="#原始数据集处理" class="headerlink" title="原始数据集处理"></a>原始数据集处理</h4><p>原始的数据集格式为[docIdx wordIdx count]，例如[1,20,2]表示在编号为1的文档中，编号为20的词语的词频是2。LDA接受的参数格式为：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[label,(vector_ size, [wiIdx,wjIdx,···wnIdx ],[tfi,tfj,···tfn])]</span><br></pre></td></tr></table></figure><p>上述格式的数据代表一个带有标签的稀疏向量，其中label表示文档编号，vector_ size表示向量的维度，wnIdx表示词n的索引编号，tfn表示词n的词频。需要将原始数据转换成上述的格式，具体步骤如下：</p><ul><li><strong>Step1：将原始数据集上传至HDFS</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[kms@kms-1 ~]$ hdfs dfs -put /opt/modules/train_data/lda/train.data  /train/lda/</span><br></pre></td></tr></table></figure><ul><li><strong>Step2:初始化SparkSession并加载数据</strong></li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>           </span><br><span class="line">                       .builder           </span><br><span class="line">                       .appName(<span class="string">s"<span class="subst">$&#123;this.getClass.getSimpleName&#125;</span>"</span>)                                          .getOrCreate()         </span><br><span class="line"><span class="comment">//设置日志级别         </span></span><br><span class="line"><span class="type">Logger</span>.getLogger(<span class="string">"org.apache.spark"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)         <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.hadoop"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>) </span><br><span class="line"><span class="comment">//加载原始数据集</span></span><br><span class="line"><span class="keyword">val</span> rowDS = spark.read.textFile(<span class="string">"/train/lda/train.data"</span>)</span><br></pre></td></tr></table></figure><ul><li><strong>Step3:数据集矩阵变换处理</strong></li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建形如MatrixEntry(row_index, column_index, value)的MatrixEntry</span></span><br><span class="line"><span class="keyword">val</span> matrixEntry:<span class="type">RDD</span>[<span class="type">MatrixEntry</span>] = rowDS.rdd.map(_.split(<span class="string">" "</span>))</span><br><span class="line">                           .map(rowdata =&gt; <span class="type">MatrixEntry</span>(rowdata(<span class="number">0</span>).toLong,rowdata(<span class="number">1</span>).toLong,rowdata(<span class="number">2</span>).toDouble))</span><br><span class="line"><span class="comment">//创建稀疏矩阵</span></span><br><span class="line"><span class="keyword">val</span> sparseMatrix: <span class="type">CoordinateMatrix</span> = <span class="keyword">new</span>  <span class="type">CoordinateMatrix</span>(matrixEntry)</span><br><span class="line"><span class="comment">//创建LabeledPoint数据集</span></span><br><span class="line"><span class="keyword">val</span> labelPointData = sparseMatrix.toIndexedRowMatrix.rows.map(r =&gt; (r.index, r.vector.asML))</span><br><span class="line"><span class="keyword">val</span> corpusDF = spark.createDataFrame(labelPointData).toDF(<span class="string">"label"</span>,<span class="string">"features"</span>)</span><br><span class="line">corpusDF.saveAsTextFile(<span class="string">"/tarin/lda/labelPointData"</span>)</span><br></pre></td></tr></table></figure><p>处理之后的部分数据集如下所示，其中一行代表一篇文档的特征向量</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[4551,(53976,[23,27,29,30,44,45,48,314,425,748,767,825,930,969,995,1345,7033,13872,16798,19139,26846,26847,27081,29607,30801,31200,31201,31202],[2.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])]</span><br><span class="line">[2493,(53976,[80,133,754,3699,4066,5190,6138,7327,7361,10267,10344,10949,11390,11683,11759,16206,22708,22709],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0])]</span><br></pre></td></tr></table></figure><h4 id="k折交叉验证确定训练参数"><a href="#k折交叉验证确定训练参数" class="headerlink" title="k折交叉验证确定训练参数"></a>k折交叉验证确定训练参数</h4><p>交叉验证法(cross validation)是将数据集D划分为k个大小相似的互斥子集的一种方法，其中每个子集都尽可能地保持数据分布的一致性，即从数据集D中通过分层采样的方式得到。然后，每次再用k-1个子集的并集作为训练集，剩下的那个子集作为测试集；通过这样的处理，可以得到k组训练集和测试集，进而可以进行k次训练和测试，最终返回的是这k个测试结果的均值。交叉验证法评估结果的稳定性和保真性在很大程度上依赖于k的取值，为突出这一点，通常把交叉验证法称为<code>k折交叉验证(k-fold cross validation)</code>。K通常取值为10，称之为10折交叉验证,下图给出了10折交叉验证的示意图。</p><p><img src="//jiamaoxiang.top/2020/08/02/第七篇-Spark平台下基于LDA的k-means算法实现/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81.png" alt></p><p>困惑度(perplexity)指标是LDA 模型的原作者Blei 等提出的一种反应模型泛化能力的指标, 在评价模型的优劣上具有一定的代表性和普遍性。所谓困惑度就是文档在划分主题时确定性的评判, 反映的是模型对新样本的适用性。其计算公式如下所示。</p><p>​                           <img src="//jiamaoxiang.top/2020/08/02/第七篇-Spark平台下基于LDA的k-means算法实现/%E5%9B%B0%E6%83%91%E5%BA%A6.png" alt></p><p>十折交叉验证处理过程如下所示</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//将数据集分割为10份，每份占10%</span></span><br><span class="line">    <span class="keyword">val</span> splitData = labelPointData.randomSplit(<span class="type">Array</span>.fill(<span class="number">10</span>)(<span class="number">0.1</span>))</span><br><span class="line">   <span class="comment">//设定主题的个数为15-25</span></span><br><span class="line">    <span class="keyword">val</span> rangTopic = <span class="number">15</span> to <span class="number">25</span></span><br><span class="line">    rangTopic.foreach &#123; k =&gt;</span><br><span class="line">      <span class="keyword">var</span> perplexity = <span class="number">0.0</span></span><br><span class="line">      <span class="keyword">for</span> (i &lt;- <span class="number">0</span> to <span class="number">9</span>) &#123;</span><br><span class="line">        <span class="comment">//选择其中9份做训练集</span></span><br><span class="line">        <span class="keyword">val</span> trainIdx = (<span class="number">0</span> to <span class="number">9</span>).toArray.filter(_ != i)</span><br><span class="line">        <span class="keyword">var</span> trainData = spark.sparkContext.union(</span><br><span class="line">          splitData(trainIdx(<span class="number">0</span>)),</span><br><span class="line">          splitData(trainIdx(<span class="number">1</span>)),</span><br><span class="line">          splitData(trainIdx(<span class="number">2</span>)),</span><br><span class="line">          splitData(trainIdx(<span class="number">3</span>)),</span><br><span class="line">          splitData(trainIdx(<span class="number">4</span>)),</span><br><span class="line">          splitData(trainIdx(<span class="number">5</span>)),</span><br><span class="line">          splitData(trainIdx(<span class="number">6</span>)),</span><br><span class="line">          splitData(trainIdx(<span class="number">7</span>)),</span><br><span class="line">          splitData(trainIdx(<span class="number">8</span>)))</span><br><span class="line">        <span class="comment">//创建DataFrame</span></span><br><span class="line">        <span class="keyword">val</span> trainDF = spark.createDataFrame(trainData).toDF(<span class="string">"label"</span>,<span class="string">"features"</span>)</span><br><span class="line">        <span class="keyword">val</span> testDF = spark.createDataFrame(splitData(i)).toDF(<span class="string">"label"</span>,<span class="string">"features"</span>)</span><br><span class="line">        <span class="comment">//训练主题个数为k时的模型</span></span><br><span class="line">        <span class="keyword">val</span> lda = <span class="keyword">new</span> <span class="type">LDA</span>().setK(k).setMaxIter(<span class="number">50</span>)</span><br><span class="line">        <span class="keyword">val</span> ldaModel = lda.fit(trainDF)</span><br><span class="line">       perplexity = perplexity + ldaModel.logPerplexity(testDF)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">val</span> avePerplexity = perplexity / <span class="number">10</span></span><br><span class="line">      <span class="type">System</span>.out.println(<span class="string">"当主题个数为 "</span> + k + <span class="string">"时，"</span> + <span class="string">"交叉验证的平均困惑度为 "</span> + avePerplexity)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>经过十折交叉验证，验证在不同主题下(取值15-25)训练模型的平均困惑度，测试发现在主题k=20时，困惑度的值最小。由于困惑度值越小, 表示该模型具有较好的泛化能力，所以选择k=20作为主题个数。</p><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p>将主题个数设置为20，迭代次数设置为50，使用上述数据集训练LDA模型，具体步骤如下</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//训练LDA模型</span></span><br><span class="line">      <span class="keyword">val</span> lda = <span class="keyword">new</span> <span class="type">LDA</span>().setK(<span class="number">20</span>).setMaxIter(<span class="number">50</span>)</span><br><span class="line">      <span class="keyword">val</span> model = lda.fit(corpusDF)</span><br><span class="line">      <span class="keyword">val</span> ll = model.logLikelihood(corpusDF)</span><br><span class="line">      <span class="keyword">val</span> lp = model.logPerplexity(corpusDF)</span><br><span class="line">      println(<span class="string">"当主题个数为20时对数似然为: "</span> + ll)</span><br><span class="line">      println(<span class="string">"当主题个数为20时困惑度为: "</span> + lp)</span><br><span class="line">      <span class="comment">//描述主题</span></span><br><span class="line">      <span class="keyword">val</span> topics = model.describeTopics(<span class="number">5</span>)</span><br><span class="line">      println(<span class="string">"The topics described by their top-weighted terms:"</span>)</span><br><span class="line">      topics.show(<span class="literal">false</span>)</span><br><span class="line">      topics.rdd.saveAsTextFile(<span class="string">"/tarin/lda/topics"</span>)</span><br><span class="line">      <span class="comment">// 测试结果</span></span><br><span class="line">    <span class="keyword">val</span> transformed = model.transform(corpusDF)</span><br><span class="line">transformed.select(<span class="string">"label"</span>,<span class="string">"topicDistribution"</span>).rdd.saveAsTextFile(<span class="string">"/tarin/lda /testtopic"</span>)</span><br></pre></td></tr></table></figure><p>通过训练得到LDA模型，其中训练数据的主题-单词概率分布如下表所示，选择权重排名在前5的单词，其中topic表示主题编号，termIndices表示词语编号组成的集合，termWeights表示词语编号对应的权重的集合。</p><p><img src="//jiamaoxiang.top/2020/08/02/第七篇-Spark平台下基于LDA的k-means算法实现/%E4%B8%BB%E9%A2%98%E5%8D%95%E8%AF%8D%E6%A6%82%E7%8E%87.png" alt></p><p>每个主题对应的单词列表如下表所示，其中topic表示主题，termIndices表示词语编号组成的集合,vocabulary表示词汇。</p><p><img src="//jiamaoxiang.top/2020/08/02/第七篇-Spark平台下基于LDA的k-means算法实现/%E8%AF%8D%E6%B1%87%E8%A1%A8.png" alt></p><p>该模型的文档-主题分布如下表所示，由于文档较多，这里仅列出部分文档，其中label表示文档编号，topicDistribution表示文档的主题分布。</p><p><img src="//jiamaoxiang.top/2020/08/02/第七篇-Spark平台下基于LDA的k-means算法实现/%E6%96%87%E6%A1%A3%E6%A0%87%E7%AD%BE.png" alt></p><p>通过上面的分析，得到了文本的主题分布。每篇文档将对应一个主题的特征空间，从而达到降维的目的。主题-单词单词概率分布描述了主题的特征空间，其中主题表示聚类中心。</p><p>结合词汇表与训练集，将其处理成[word,count]的形式，其中word表示单词，count表示该次出现的频次，具体的处理过程如下。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.apache.ml</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setMaster(<span class="string">"local"</span>)</span><br><span class="line">      .setAppName(<span class="string">"wordcount"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rawRDD = sc.textFile(<span class="string">"e:///lda/train.data"</span>)</span><br><span class="line">    <span class="keyword">val</span> vocabulary = sc.textFile(<span class="string">"e:///lda/vocabulary.txt"</span>)</span><br><span class="line">    <span class="keyword">val</span> word_nums = rawRDD.map(_.split(<span class="string">" "</span>)).map(row =&gt; (row(<span class="number">1</span>).toLong, row(<span class="number">2</span>).toInt))</span><br><span class="line">    <span class="keyword">val</span> word_count = word_nums.reduceByKey(_ + _)</span><br><span class="line">    <span class="keyword">val</span> sort_wcnt = word_count.sortByKey(<span class="literal">false</span>)</span><br><span class="line">    <span class="comment">//处理词汇表</span></span><br><span class="line">    <span class="keyword">val</span> num_vocabulary = vocabulary.zipWithIndex().map(row =&gt; (row._2, row._1))</span><br><span class="line">    <span class="keyword">val</span> sort_combination = sort_wcnt.join(num_vocabulary)</span><br><span class="line">      .map(row =&gt; (row._2._1, row._2._2))</span><br><span class="line">      .sortByKey(<span class="literal">false</span>)</span><br><span class="line">      .map(row =&gt; (row._2, row._1))</span><br><span class="line">    sort_combination.saveAsTextFile(<span class="string">"e:///combination"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过使用R语言的wordcloud2包，进行可视化文档词云图展示，见下图</p><p><img src="//jiamaoxiang.top/2020/08/02/第七篇-Spark平台下基于LDA的k-means算法实现/%E8%AF%8D%E4%BA%91%E5%9B%BE.png" alt></p><h3 id="3-2Spark平台下基于LDA的k-means算法实现"><a href="#3-2Spark平台下基于LDA的k-means算法实现" class="headerlink" title="3.2Spark平台下基于LDA的k-means算法实现"></a>3.2Spark平台下基于LDA的k-means算法实现</h3><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><p>将通过LDA主题模型计算的文档-主题分布作为k-means的输入，文档-主题分布的形式为[label, features，topicDistribution]，其中features代表文档的特征向量，每一行数据代表一篇文档。由于k-means接受的特征向量输入的形式为[label，features]，所以需要将原始的数据集schema转化为[label，features]的形式，即将topicDistribution列名转为features。处理步骤为：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">Val</span> trainDF =   transformed.select(<span class="string">"label"</span>,<span class="string">"topicDistribution"</span>).toDF(<span class="string">"label"</span>,   <span class="string">"features"</span>)</span><br></pre></td></tr></table></figure><h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><p>Spark ML的K-means算法提供了如下的参数配置：</p><ul><li><p>setFeaturesCol(value: String)：设置输入的特征向量列，默认值为features</p></li><li><p>setK(value: Int)：设置类簇的个数 </p></li><li><p>setMaxIter(value: Int)：设置最大迭代次数</p></li><li><p>setPredictionCol(value: String)：设置输出列名称，默认为prediction</p></li><li><p>setSeed(value: Long)：设置随机数种子</p></li><li><p>setTol(value: Double)：设置收敛阈值</p></li></ul><p>设置最大迭代次数为200，随机数种子为123，类簇个数为2、4、6、8、10、12、14、16、18、20，其余选择默认值，分别观察评估指标的变化情况。具体代码如下：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">(<span class="number">2</span> to <span class="number">20</span> by <span class="number">2</span>).toList.map &#123;k =&gt;</span><br><span class="line">      <span class="keyword">val</span> kmeans = <span class="keyword">new</span> <span class="type">KMeans</span>().setK(k).setSeed(<span class="number">123</span>).setMaxIter(<span class="number">200</span>)</span><br><span class="line">      <span class="keyword">val</span> kmeansModel = kmeans.fit(kMeansTrain)</span><br><span class="line">      <span class="comment">// 预测结果</span></span><br><span class="line">      <span class="keyword">val</span> predictions = kmeansModel.transform(kMeansTrain)</span><br><span class="line">      <span class="comment">//计算误差平方和</span></span><br><span class="line">      <span class="keyword">val</span> wssse = kmeansModel.computeCost(kMeansTrain)</span><br><span class="line">      println(<span class="string">s"Within set sum of squared errors = <span class="subst">$wssse</span>"</span>)</span><br><span class="line">      <span class="comment">// 计算轮廓系数</span></span><br><span class="line">      <span class="keyword">val</span> evaluator = <span class="keyword">new</span> <span class="type">ClusteringEvaluator</span>()</span><br><span class="line">      <span class="keyword">val</span> silhouette = evaluator.evaluate(predictions)</span><br><span class="line">      println(<span class="string">s"Silhouette with squared euclidean distance = <span class="subst">$silhouette</span>"</span>)</span><br><span class="line">      <span class="comment">//显示聚类结果</span></span><br><span class="line">      println(<span class="string">"Cluster Centers: "</span>)</span><br><span class="line">      kmeansModel.clusterCenters.foreach(println)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>经过训练，得到当K为6时聚类效果最好，此时的聚类结果如下表所示：</p><p><img src="//jiamaoxiang.top/2020/08/02/第七篇-Spark平台下基于LDA的k-means算法实现/%E8%81%9A%E7%B1%BB%E7%BB%93%E6%9E%9C.png" alt></p><h4 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h4><ul><li>轮廓系数</li></ul><p>轮廓系数（Silhouette Coefficient）是评价聚类效果好坏的一种方法，用来衡量簇的密集与分散程度。它结合内聚度和分离度两种因素，可以用来在相同原始数据的基础上用来评价不同算法、或者算法不同运行方式对聚类结果所产生的影响。轮廓系数取值为[-1,1],其值越大表示同类中样本距离最近，不同类中样本距离最远，即该值越接近于1，簇越紧凑，聚类效果越好。</p><p>使用K-means算法，将待分类数据集分为了 k 个类簇，对于其中的一个点 i 来说，a(i)表示该向量到它所属类簇中其他点的平均距离，b(i)表示该向量到其他类簇中点的平均距离。对于一个样本集合，所有样本的轮廓系数的平均值即为该聚类结果总的轮廓系数。</p><ul><li>误差平方和</li></ul><p>误差平方和又称残差平方和、组内平方和等。根据n个观察值拟合适当的模型后，余下未能拟合部份(ei=yi-y平均)称为残差，其中y平均表示n个观察值的平均值，所有n个残差平方之和称误差平方和。</p><p>当K取不同值时，计算所得误差平方和如下所示：</p><p><img src="//jiamaoxiang.top/2020/08/02/第七篇-Spark平台下基于LDA的k-means算法实现/%E8%AF%AF%E5%B7%AE%E5%B9%B3%E6%96%B9%E5%92%8C.png" alt></p><p>计算所得的轮廓系数如下图所示，结合误差平方和和轮廓系数，当k=6时，有着较好的聚类效果。</p><p><img src="//jiamaoxiang.top/2020/08/02/第七篇-Spark平台下基于LDA的k-means算法实现/%E8%BD%AE%E5%BB%93%E7%B3%BB%E6%95%B0.png" alt></p><h3 id="3-3结果分析"><a href="#3-3结果分析" class="headerlink" title="3.3结果分析"></a>3.3结果分析</h3><p>首先，通过LDA主题模型，可以计算出文档数据集的文档-主题分布情况和主题-单词的分布情况，训练得出的主题数即为类簇数。</p><p>对LDA训练的文档-主题分布结果，即将文档表示成在不同主题上的分布所组成的向量，由于LDA考虑到了词之间的语义关系，所以该特征向量能够更好地反应文档的信息，因此可以将其作为K-means聚类算法的输入，从而弥补基于空间向量模型的K-means算法的缺点。经过实验发现，在类簇K为6时，轮廓系数为65.9661577458792，误差平方和为0.8266340036962969，聚类效果良好。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本章主要介绍Spark平台下基于LDA的k-means算法实现。对文本挖掘进行了详细设计，在公开数据集上训练LDA模型，并对文档-主题分布和主题-词语分布进行了详细说明。最后实现了基于LDA的K-means聚类算法，克服了传统K-means算法的缺陷。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第六篇|Spark MLLib机器学习(1)</title>
      <link href="/2020/07/31/%E7%AC%AC%E5%85%AD%E7%AF%87-Spark-MLLib%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
      <url>/2020/07/31/%E7%AC%AC%E5%85%AD%E7%AF%87-Spark-MLLib%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>MLlib是Spark提供的一个机器学习库，通过调用MLlib封装好的算法，可以轻松地构建机器学习应用。它提供了非常丰富的机器学习算法，比如分类、回归、聚类及推荐算法。除此之外，MLlib对用于机器学习算法的API进行了标准化，从而使将多种算法组合到单个Pipeline或工作流中变得更加容易。通过本文，你可以了解到：</p><ul><li>什么是机器学习</li><li>大数据与机器学习</li><li>机器学习分类</li><li>Spark MLLib介绍</li></ul><blockquote><p>机器学习是人工智能的一个分支，是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。</p><p>来源：Mitchell, T. (1997). Machine Learning. McGraw Hill.</p></blockquote><h2 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习"></a>什么是机器学习</h2><p><img src="//jiamaoxiang.top/2020/07/31/第六篇-Spark-MLLib机器学习/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.png" alt></p><p>机器学习的应用已遍及人工智能的各个分支，如专家系统、自动推理、自然语言理解、模式识别、计算机视觉、智能机器人等领域。机器学习是人工智能的一个分支学科，主要研究的是让机器从过去的经历中学习经验，对数据的不确定性进行建模，对未来进行预测。机器学习应用的领域很多，比如搜索、推荐系统、垃圾邮件过滤、人脸识别、语音识别等等。</p><h2 id="大数据与机器学习"><a href="#大数据与机器学习" class="headerlink" title="大数据与机器学习"></a>大数据与机器学习</h2><p>大数据时代，数据产生的速度是非常惊人的。互联网、移动互联网、物联网、GPS等等都会在无时无刻产生着数据。处理这些数据所需要的存储与计算的能力也在成几何级增长，由此诞生了一系列的以Hadoop为代表的大数据技术，这些大数据技术为处理和存储这些数据提供了可靠的保障。</p><p>数据、信息、知识是由大到小的三个层次。单纯的数据很难说明一些问题，需要加之人们的一些经验，将其转换为信息，所谓信息，也就是为了消除不确定性，我们常说信息不对称，指的就是在不能够获取足够的信息时，很难消除一些不确定的因素。而知识则是最高阶段，所以数据挖掘也叫知识发现。</p><p>机器学习的任务就是利用一些算法，作用于大数据，然后挖掘背后所蕴含的潜在的知识。训练的数据越多，机器学习就越能体现出优势，以前机器学习解决不了的问题，现在通过大数据技术可以得到很好的解决，性能也会大幅度提升，如语音识别、图像识别等等。</p><h2 id="机器学习分类"><a href="#机器学习分类" class="headerlink" title="机器学习分类"></a>机器学习分类</h2><p>机器学习主要分为下面几大类：</p><ul><li><p><strong>监督学习(supervised learning)</strong></p><p>基本上是分类的同义词。学习中的<code>监督</code>来自训练数据集中标记的实例。比如，在邮政编码识别问题中，一组手写邮政编码图像与其对应的机器可读的转换物用作训练实例，监督分类模型的学习。常见的监督学习算法包括：线性回归、逻辑回归、决策树、朴素贝叶斯、支持向量机等等。</p></li><li><p><strong>无监督学习(unsupervised learning)</strong></p><p>本质上是聚类的同义词。学习过程是无监督的，因为输入实例没有类标记。无监督学习的任务是从给定的数据集中，挖掘出潜在的结构。比如，把猫和狗的照片给机器，不给这些照片打任何标签，但是希望机器能够将这些照片分分类，最终机器会把这些照片分为两大类，但是并不知道哪些是猫的照片，哪些是狗的照片，对于机器来说，相当于分成了 A、B 两类。常见的无监督学习算法包括：K-means 聚类、主成分分析(PCA)等。</p></li><li><p><strong>半监督学习(Semi-supervised learning)</strong></p><p>半监督学习是一类机器学习技术，在学习模型时，它使用标记的和未标记的实例。让学习器不依赖外界交互、自动地利用未标记样本来提升学习性能，就是半监督学习。</p><p> 半监督学习的现实需求非常强烈，因为在现实应用中往往能容易地收集到大量未标记样本，而获取<code>标记</code>却需耗费人力、物力。例如，在进行计算机辅助医学影像分析时，可以从医院获得大量医学影像， 但若希望医学专家把影像中的病灶全都标识出来则是不现实的<code>有标记数据少，未标记数据多</code>这个现象在互联网应用中更明显，例如在进行网页推荐时需请用户标记出感兴趣的网页， 但很少有用户愿花很多时间来提供标记，因此，有标记网页样本少，但互联网上存在无数网页可作为未标记样本来使用。</p></li><li><p><strong>强化学习(reinforcement learning)</strong></p><p>又称再励学习、评价学习，是一种重要的机器学习方法，在智能控制机器人及分析预测等领域有许多应用。强化学习的常见模型是标准的马尔可夫决策过程（Markov Decision Process, MDP)。</p></li></ul><h2 id="Spark-MLLib介绍"><a href="#Spark-MLLib介绍" class="headerlink" title="Spark MLLib介绍"></a>Spark MLLib介绍</h2><p>MLlib是Spark的机器学习库，通过该库可以简化机器学习的工程实践工作。MLlib包含了非常丰富的机器学习算法：分类、回归、聚类、协同过滤、主成分分析等等。目前，MLlib分为两个代码包：<strong>spark.mllib</strong>与<strong>spark.ml</strong>。</p><h3 id="spark-mllib"><a href="#spark-mllib" class="headerlink" title="spark.mllib"></a>spark.mllib</h3><p>Spark MLlib是Spark的重要组成部分，是最初提供的一个机器学习库。该库有一个缺点：如果数据集非常复杂，需要做多次处理，或者是对新数据需要结合多个已经训练好的单个模型进行综合计算时，使用Spark MLlib会使程序结构变得复杂，甚至难以理解和实现。</p><p>spark.mllib是基于RDD的原始算法API，目前处于维护状态。该库下包含4类常见的机器学习算法：<strong>分类</strong>、<strong>回归</strong>、<strong>聚类</strong>、<strong>协同过滤</strong>。指的注意的是，基于RDD的API不会再添加新的功能。</p><h3 id="spark-ml"><a href="#spark-ml" class="headerlink" title="spark.ml"></a>spark.ml</h3><p>Spark1.2版本引入了ML Pipeline，经过多个版本的发展，Spark ML克服了MLlib处理机器学习问题的一些不足(复杂、流程不清晰)，向用户提供了基于DataFrame API的机器学习库，使得构建整个机器学习应用的过程变得简单高效。</p><p><code>Spark ML</code>不是正式名称，用于指代基于DataFrame API的MLlib库 。与RDD相比，DataFrame提供了更加友好的API。DataFrame的许多好处包括Spark数据源，SQL / DataFrame查询，Tungsten和Catalyst优化以及跨语言的统一API。</p><p>Spark ML API提供了很多数据特征处理函数，如特征选取、特征转换、类别数值化、正则化、降维等。另外基于DataFrame API的ml库支持构建机器学习的Pipeline，把机器学习过程一些任务有序地组织在一起，便于运行和迁移。Spark官方推荐使用spark.ml库。</p><h3 id="数据变换"><a href="#数据变换" class="headerlink" title="数据变换"></a>数据变换</h3><p>数据变换是数据预处理的一项重要工作，比如对数据进行规范化、离散化、衍生指标等等。Spark ML中提供了非常丰富的数据转换算法，详细可以参考官网，现归纳如下：</p><p><img src="//jiamaoxiang.top/2020/07/31/第六篇-Spark-MLLib机器学习/%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2%E7%AE%97%E6%B3%95.png" alt></p><p>上面的转换算法中，词频逆文档频率(TF-IDF)、Word2Vec、PCA是比较常见的，如果你做过文本挖掘处理，那么对此应该并不陌生。</p><h3 id="数据规约"><a href="#数据规约" class="headerlink" title="数据规约"></a>数据规约</h3><p>大数据是机器学习的基础，为机器学习提供充足的数据训练集。在数据量非常大的时候，需要通过数据规约技术删除或者减少冗余的维度属性以来达到精简数据集的目的，类似于抽样的思想，虽然缩小了数据容量，但是并没有改变数据的完整性。Spark ML提供的特征选择和降维的方法如下表所示：</p><p><img src="//jiamaoxiang.top/2020/07/31/第六篇-Spark-MLLib机器学习/%E6%95%B0%E6%8D%AE%E8%A7%84%E7%BA%A6.png" alt></p><p>选择特征和降维是机器学习中常用的手段，可以使用上述的方法减少特征的选择，消除噪声的同时还能够维持原始的数据结构特征。尤其是主成分分析法(PCA)，无论是在统计学领域还是机器学习领域，都起到了很重要的作用。                                                        </p><h3 id="机器学习算法"><a href="#机器学习算法" class="headerlink" title="机器学习算法"></a>机器学习算法</h3><p>Spark支持分类、回归、聚类、推荐等常用的机器学习算法。见下表：</p><p><img src="//jiamaoxiang.top/2020/07/31/第六篇-Spark-MLLib机器学习/%E7%AE%97%E6%B3%95.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文对机器学习进行了总体介绍，主要包括机器学习的基本概念、机器学习的基本分类、Spark机器学习库的介绍。通过本文或许已经对机器学习有了初步的了解，在下一篇，我将会分享基于Spark ML库构建一个机器学习的应用,主要涉及LDA主题模型以及K-means聚类。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第五篇|Spark-Streaming编程指南(2)</title>
      <link href="/2020/07/29/%E7%AC%AC%E4%BA%94%E7%AF%87-Spark-Streaming%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97-2/"/>
      <url>/2020/07/29/%E7%AC%AC%E4%BA%94%E7%AF%87-Spark-Streaming%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97-2/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p><a href="https://mp.weixin.qq.com/s/yzjnf9682gDFUwIfd4n4eQ" target="_blank" rel="noopener">第四篇|Spark-Streaming编程指南(1)</a>对Spark Streaming执行机制、Transformations与Output Operations、Spark Streaming数据源(Sources)、Spark Streaming 数据汇(Sinks)进行了讨论。本文将延续上篇内容，主要包括以下内容：</p><ul><li><strong>有状态的计算</strong></li><li><strong>基于时间的窗口操作</strong></li><li><strong>持久化</strong></li><li><strong>检查点Checkpoint</strong></li><li><strong>使用DataFrames &amp; SQL处理流数据</strong></li></ul><h2 id="有状态的计算"><a href="#有状态的计算" class="headerlink" title="有状态的计算"></a>有状态的计算</h2><h3 id="updateStateByKey"><a href="#updateStateByKey" class="headerlink" title="updateStateByKey"></a>updateStateByKey</h3><p>上一篇文章中介绍了常见的无状态的转换操作，比如在WordCount的例子中，输出的结果只与当前batch interval的数据有关，不会依赖于上一个batch interval的计算结果。spark Streaming也提供了有状态的操作： <code>updateStateByKey</code>，该算子会维护一个状态，同时进行信息更新 。该操作会读取上一个batch interval的计算结果，然后将其结果作用到当前的batch interval数据统计中。其源码如下：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateStateByKey</span></span>[<span class="type">S</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      updateFunc: (<span class="type">Seq</span>[<span class="type">V</span>], <span class="type">Option</span>[<span class="type">S</span>]) =&gt; <span class="type">Option</span>[<span class="type">S</span>]</span><br><span class="line">    ): <span class="type">DStream</span>[(<span class="type">K</span>, <span class="type">S</span>)] = ssc.withScope &#123;</span><br><span class="line">    updateStateByKey(updateFunc, defaultPartitioner())</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>该算子只能在key–value对的DStream上使用，需要接收一个状态更新函数 updateFunc作为参数。使用案例如下：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StateWordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">      .setAppName(<span class="type">StateWordCount</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="comment">// 必须开启checkpoint,否则会报错</span></span><br><span class="line">    ssc.checkpoint(<span class="string">"file:///e:/checkpoint"</span>)</span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 状态更新函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">updateFunc</span></span>(newValues: <span class="type">Seq</span>[<span class="type">Int</span>], stateValue: <span class="type">Option</span>[<span class="type">Int</span>]): <span class="type">Option</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">var</span> oldvalue = stateValue.getOrElse(<span class="number">0</span>) <span class="comment">// 获取状态值</span></span><br><span class="line">      <span class="comment">// 遍历当前数据，并更新状态</span></span><br><span class="line">      <span class="keyword">for</span> (newValue &lt;- newValues) &#123;</span><br><span class="line">        oldvalue += newValue</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 返回最新的状态</span></span><br><span class="line">      <span class="type">Option</span>(oldvalue)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> count = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map(w =&gt; (w, <span class="number">1</span>))</span><br><span class="line">      .updateStateByKey(updateFunc)</span><br><span class="line">    count.print()</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>尖叫提示：上面的代码必须要开启checkpoint，否则会报错：</p><p><strong>Exception in thread “main” java.lang.IllegalArgumentException: requirement failed: The checkpoint directory has not been set. Please set it by StreamingContext.checkpoint()</strong></p></blockquote><h3 id="updateStateByKey缺点"><a href="#updateStateByKey缺点" class="headerlink" title="updateStateByKey缺点"></a>updateStateByKey缺点</h3><p>运行上面的代码会发现一个现象：即便没有数据源输入，Spark也会为新的batch interval更新状态，即如果没有数据源输入，则会不断地输出之前的计算状态结果。</p><p>updateStateByKey可以在指定的批次间隔内返回之前的全部历史数据，包括新增的，改变的和没有改变的。由于updateStateByKey在使用的时候一定要做checkpoint，当数据量过大的时候，checkpoint会占据庞大的数据量，会影响性能，效率不高。</p><h3 id="mapwithState"><a href="#mapwithState" class="headerlink" title="mapwithState"></a>mapwithState</h3><p>mapwithState是Spark提供的另外一个有状态的算子，该操作克服了updateStateByKey的缺点，从Spark 1.5开始引入。源码如下：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapWithState</span></span>[<span class="type">StateType</span>: <span class="type">ClassTag</span>, <span class="type">MappedType</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      spec: <span class="type">StateSpec</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">StateType</span>, <span class="type">MappedType</span>]</span><br><span class="line">    ): <span class="type">MapWithStateDStream</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">StateType</span>, <span class="type">MappedType</span>] = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapWithStateDStreamImpl</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">StateType</span>, <span class="type">MappedType</span>](</span><br><span class="line">      self,</span><br><span class="line">      spec.asInstanceOf[<span class="type">StateSpecImpl</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">StateType</span>, <span class="type">MappedType</span>]]</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>mapWithState只返回发生变化的key的值，对于没有发生变化的Key，则不返回。这样做可以只关心那些已经发生的变化的key，对于没有数据输入，则不会返回那些没有变化的key 的数据。这样的话，即使数据量很大，checkpint也不会updateBykey那样，占用太多的存储，效率比较高（生产环境中建议使用）。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StatefulNetworkWordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">"StatefulNetworkWordCount"</span>)</span><br><span class="line">      .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    ssc.checkpoint(<span class="string">"file:///e:/checkpoint"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordDstream = words.map(x =&gt; (x, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * word：当前key的值</span></span><br><span class="line"><span class="comment">      * one：当前key对应的value值</span></span><br><span class="line"><span class="comment">      * state：状态值</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="keyword">val</span> mappingFunc = (batchTime: <span class="type">Time</span>, word: <span class="type">String</span>, one: <span class="type">Option</span>[<span class="type">Int</span>], state: <span class="type">State</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> sum = one.getOrElse(<span class="number">0</span>) + state.getOption.getOrElse(<span class="number">0</span>)</span><br><span class="line">      println(<span class="string">s"&gt;&gt;&gt; batchTime = <span class="subst">$batchTime</span>"</span>)</span><br><span class="line">      println(<span class="string">s"&gt;&gt;&gt; word      = <span class="subst">$word</span>"</span>)</span><br><span class="line">      println(<span class="string">s"&gt;&gt;&gt; one     = <span class="subst">$one</span>"</span>)</span><br><span class="line">      println(<span class="string">s"&gt;&gt;&gt; state     = <span class="subst">$state</span>"</span>)</span><br><span class="line">      <span class="keyword">val</span> output = (word, sum)</span><br><span class="line">      state.update(sum) <span class="comment">//更新当前key的状态值</span></span><br><span class="line">      <span class="type">Some</span>(output) <span class="comment">//返回结果</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 通过StateSpec.function构建StateSpec</span></span><br><span class="line">    <span class="keyword">val</span> spec = <span class="type">StateSpec</span>.function(mappingFunc)</span><br><span class="line">    <span class="keyword">val</span> stateDstream = wordDstream.mapWithState(spec)</span><br><span class="line">    stateDstream.print()</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="基于时间的窗口操作"><a href="#基于时间的窗口操作" class="headerlink" title="基于时间的窗口操作"></a>基于时间的窗口操作</h2><p>Spark Streaming提供了两种类型的窗口操作，分别是滚动窗口和滑动窗口。具体分析如下：</p><h3 id="滚动窗口-Tumbling-Windows"><a href="#滚动窗口-Tumbling-Windows" class="headerlink" title="滚动窗口(Tumbling Windows)"></a>滚动窗口(Tumbling Windows)</h3><p>滚动窗口的示意图如下：滚动窗口只需要传入一个固定的时间间隔，滚动窗口是不存在重叠的。</p><p><img src="//jiamaoxiang.top/2020/07/29/第五篇-Spark-Streaming编程指南-2/%E6%BB%9A%E5%8A%A8%E7%AA%97%E5%8F%A3.png" alt></p><p>源码如下：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * @param windowDuration:窗口的长度; 必须是batch interval的整数倍.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">window</span></span>(windowDuration: <span class="type">Duration</span>): <span class="type">DStream</span>[<span class="type">T</span>] = window(windowDuration, <span class="keyword">this</span>.slideDuration)</span><br></pre></td></tr></table></figure><h3 id="滑动窗口-Sliding-Windows"><a href="#滑动窗口-Sliding-Windows" class="headerlink" title="滑动窗口(Sliding Windows)"></a>滑动窗口(Sliding Windows)</h3><p>滑动窗口的示意图如下：滑动窗口只需要传入两个参数，一个为窗口的长度，一个是滑动时间间隔。可以看出：滑动窗口是存在重叠的。</p><p><img src="//jiamaoxiang.top/2020/07/29/第五篇-Spark-Streaming编程指南-2/%E6%BB%91%E5%8A%A8.png" alt></p><p>源码如下：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * @param windowDuration 窗口长度;必须是batching interval的整数倍</span></span><br><span class="line"><span class="comment">   *                       </span></span><br><span class="line"><span class="comment">   * @param slideDuration  滑动间隔;必须是batching interval的整数倍</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">window</span></span>(windowDuration: <span class="type">Duration</span>, slideDuration: <span class="type">Duration</span>): <span class="type">DStream</span>[<span class="type">T</span>] = ssc.withScope &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">WindowedDStream</span>(<span class="keyword">this</span>, windowDuration, slideDuration)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="窗口操作"><a href="#窗口操作" class="headerlink" title="窗口操作"></a>窗口操作</h3><ul><li><p><strong>window</strong>(<em>windowLength</em>, <em>slideInterval</em>)</p><ul><li><p>解释</p><blockquote><p>基于源DStream产生的窗口化的批数据，计算得到一个新的Dstream</p></blockquote></li><li><p>源码</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">window</span></span>(windowDuration: <span class="type">Duration</span>): <span class="type">DStream</span>[<span class="type">T</span>] = window(windowDuration, <span class="keyword">this</span>.slideDuration)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">window</span></span>(windowDuration: <span class="type">Duration</span>, slideDuration: <span class="type">Duration</span>): <span class="type">DStream</span>[<span class="type">T</span>] = ssc.withScope &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">WindowedDStream</span>(<span class="keyword">this</span>, windowDuration, slideDuration)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p><strong>countByWindow</strong>(<em>windowLength</em>, <em>slideInterval</em>)</p><ul><li>解释</li></ul><blockquote><p>返回一个滑动窗口的元素个数</p></blockquote><ul><li><p>源码</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * @param windowDuration window长度，必须是batch interval的倍数 </span></span><br><span class="line"><span class="comment">   * @param slideDuration  滑动的时间间隔，必须是batch interval的倍数</span></span><br><span class="line"><span class="comment">   * 底层调用的是reduceByWindow</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">countByWindow</span></span>(</span><br><span class="line">      windowDuration: <span class="type">Duration</span>,</span><br><span class="line">      slideDuration: <span class="type">Duration</span>): <span class="type">DStream</span>[<span class="type">Long</span>] = ssc.withScope &#123;</span><br><span class="line">    <span class="keyword">this</span>.map(_ =&gt; <span class="number">1</span>L).reduceByWindow(_ + _, _ - _, windowDuration, slideDuration)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p><strong>reduceByWindow</strong>(<em>func</em>, <em>windowLength</em>, <em>slideInterval</em>) </p><ul><li>解释</li></ul><blockquote><p>返回一个单元素流。利用函数func聚集滑动时间间隔的流的元素创建这个单元素流。函数func必须满足结合律，从而可以支持并行计算</p></blockquote><ul><li><p>源码</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByWindow</span></span>(</span><br><span class="line">    reduceFunc: (<span class="type">T</span>, <span class="type">T</span>) =&gt; <span class="type">T</span>,</span><br><span class="line">    windowDuration: <span class="type">Duration</span>,</span><br><span class="line">    slideDuration: <span class="type">Duration</span></span><br><span class="line">  ): <span class="type">DStream</span>[<span class="type">T</span>] = ssc.withScope &#123;</span><br><span class="line">  <span class="keyword">this</span>.reduce(reduceFunc).window(windowDuration, slideDuration).reduce(reduceFunc)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p><strong>reduceByKeyAndWindow</strong>(<em>func</em>, <em>windowLength</em>, <em>slideInterval</em>, [<em>numTasks</em>])</p><ul><li>解释</li></ul><blockquote><p>应用到一个(K,V)键值对组成的DStream上时，会返回一个由(K,V)键值对组成的新的DStream。每一个key的值均由给定的reduce函数(func函数)进行聚合计算。注意：在默认情况下，这个算子利用了Spark默认的并发任务数去分组。可以通过numTasks参数的设置来指定不同的任务数</p></blockquote><ul><li><p>源码</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKeyAndWindow</span></span>(</span><br><span class="line">    reduceFunc: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>,</span><br><span class="line">    windowDuration: <span class="type">Duration</span>,</span><br><span class="line">    slideDuration: <span class="type">Duration</span></span><br><span class="line">  ): <span class="type">DStream</span>[(<span class="type">K</span>, <span class="type">V</span>)] = ssc.withScope &#123;</span><br><span class="line">  reduceByKeyAndWindow(reduceFunc, windowDuration, slideDuration, defaultPartitioner())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p><strong>reduceByKeyAndWindow</strong>(<em>func</em>, <em>invFunc</em>, <em>windowLength</em>, <em>slideInterval</em>, [<em>numTasks</em>]) </p><ul><li>解释</li></ul><blockquote><p>更加高效的reduceByKeyAndWindow，每个窗口的reduce值，是基于先前窗口的reduce值进行增量计算得到的；它会对进入滑动窗口的新数据进行reduce操作，并对离开窗口的老数据进行<code>逆向reduce</code>操作。但是，只能用于<code>可逆reduce函数</code>，即那些reduce函数都有一个对应的<code>逆向reduce函数</code>（以InvFunc参数传入）注意：必须开启 checkpointing</p></blockquote><ul><li><p>源码</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKeyAndWindow</span></span>(</span><br><span class="line">      reduceFunc: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>,</span><br><span class="line">      invReduceFunc: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>,</span><br><span class="line">      windowDuration: <span class="type">Duration</span>,</span><br><span class="line">      slideDuration: <span class="type">Duration</span>,</span><br><span class="line">      partitioner: <span class="type">Partitioner</span>,</span><br><span class="line">      filterFunc: ((<span class="type">K</span>, <span class="type">V</span>)) =&gt; <span class="type">Boolean</span></span><br><span class="line">    ): <span class="type">DStream</span>[(<span class="type">K</span>, <span class="type">V</span>)] = ssc.withScope &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> cleanedReduceFunc = ssc.sc.clean(reduceFunc)</span><br><span class="line">    <span class="keyword">val</span> cleanedInvReduceFunc = ssc.sc.clean(invReduceFunc)</span><br><span class="line">    <span class="keyword">val</span> cleanedFilterFunc = <span class="keyword">if</span> (filterFunc != <span class="literal">null</span>) <span class="type">Some</span>(ssc.sc.clean(filterFunc)) <span class="keyword">else</span> <span class="type">None</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">ReducedWindowedDStream</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">      self, cleanedReduceFunc, cleanedInvReduceFunc, cleanedFilterFunc,</span><br><span class="line">      windowDuration, slideDuration, partitioner</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p><strong>countByValueAndWindow</strong>(<em>windowLength</em>, <em>slideInterval</em>, [<em>numTasks</em>])</p><ul><li><p>解释</p><blockquote><p>当应用到一个(K,V)键值对组成的DStream上，返回一个由(K,V)键值对组成的新的DStream。每个key的对应的value值都是它们在滑动窗口中出现的频率</p></blockquote></li><li><p>源码</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countByValueAndWindow</span></span>(</span><br><span class="line">      windowDuration: <span class="type">Duration</span>,</span><br><span class="line">      slideDuration: <span class="type">Duration</span>,</span><br><span class="line">      numPartitions: <span class="type">Int</span> = ssc.sc.defaultParallelism)</span><br><span class="line">      (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>)</span><br><span class="line">      : <span class="type">DStream</span>[(<span class="type">T</span>, <span class="type">Long</span>)] = ssc.withScope &#123;</span><br><span class="line">    <span class="keyword">this</span>.map((_, <span class="number">1</span>L)).reduceByKeyAndWindow(</span><br><span class="line">      (x: <span class="type">Long</span>, y: <span class="type">Long</span>) =&gt; x + y,</span><br><span class="line">      (x: <span class="type">Long</span>, y: <span class="type">Long</span>) =&gt; x - y,</span><br><span class="line">      windowDuration,</span><br><span class="line">      slideDuration,</span><br><span class="line">      numPartitions,</span><br><span class="line">      (x: (<span class="type">T</span>, <span class="type">Long</span>)) =&gt; x._2 != <span class="number">0</span>L</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> count = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map(w =&gt; (w, <span class="number">1</span>))</span><br><span class="line">      .reduceByKeyAndWindow((w1: <span class="type">Int</span>, w2: <span class="type">Int</span>) =&gt; w1 + w2, <span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>))</span><br><span class="line">      .print()</span><br><span class="line"><span class="comment">//滚动窗口</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*    lines.window(Seconds(20))</span></span><br><span class="line"><span class="comment">      .flatMap(_.split(" "))</span></span><br><span class="line"><span class="comment">      .map((_, 1))</span></span><br><span class="line"><span class="comment">      .reduceByKey(_ + _)</span></span><br><span class="line"><span class="comment">      .print()*/</span></span><br></pre></td></tr></table></figure><h2 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h2><p>持久化是提升Spark应用性能的一种方式，在<a href="https://mp.weixin.qq.com/s/z22uJwTnBxeZnYlCKIXzNQ" target="_blank" rel="noopener">第二篇|Spark core编程指南</a>一文中讲解了RDD持久化的使用方式。其实，DStream也是支持持久化的，同样是使用persist()与cache()方法，持久化通常在有状态的算子中使用，比如窗口操作，默认情况下，虽然没有显性地调用持久化方法，但是底层已经帮用户做了持久化操作，通过下面的源码可以看出。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[streaming]</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WindowedDStream</span>[<span class="type">T</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    parent: <span class="type">DStream</span>[<span class="type">T</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">    _windowDuration: <span class="type">Duration</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    _slideDuration: <span class="type">Duration</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">DStream</span>[<span class="type">T</span>](<span class="params">parent.ssc</span>) </span>&#123;</span><br><span class="line">  <span class="comment">// 省略代码...</span></span><br><span class="line">  <span class="comment">// Persist parent level by default, as those RDDs are going to be obviously reused.</span></span><br><span class="line">  parent.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：与RDD的持久化不同，DStream的默认持久性级别将数据序列化在内存中，通过下面的源码可以看出：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/** 给定一个持计划级别 */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(level: <span class="type">StorageLevel</span>): <span class="type">DStream</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.isInitialized) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnsupportedOperationException</span>(</span><br><span class="line">        <span class="string">"Cannot change storage level of a DStream after streaming context has started"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">this</span>.storageLevel = level</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** 默认的持久化级别为(MEMORY_ONLY_SER) */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="type">DStream</span>[<span class="type">T</span>] = persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="type">DStream</span>[<span class="type">T</span>] = persist()</span><br></pre></td></tr></table></figure><p>从上面的源码可以看出persist()与cache()的主要区别是：</p><ul><li>cache()方法底层调用的是persist()方法</li><li>persist()方法有两个重载的方法<ul><li>无参数的persist()，默认是内存</li><li>perisist(level: StorageLevel),可以选择与RDD持久化相同的持久化级别</li></ul></li></ul><h2 id="检查点Checkpoint"><a href="#检查点Checkpoint" class="headerlink" title="检查点Checkpoint"></a>检查点Checkpoint</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>流应用程序通常是24/7运行的，因此必须对与应用程序逻辑无关的故障（例如系统故障，JVM崩溃等）具有弹性的容错能力。为此，Spark Streaming需要将足够的信息<code>checkpoint</code>到容错存储系统(比如HDFS)，以便可以从故障中恢复。检查点包括两种类型：</p><ul><li><p><strong>元数据检查点</strong></p><p> 元数据检查点可以保证从Driver程序失败中恢复。即如果运行drive的节点失败时，可以查看最近的checkpoin数据获取最新的状态。典型的应用程序元数据包括：</p><ul><li><strong>配置</strong> ：用于创建流应用程序的配置。</li><li><strong>DStream操作</strong> ：定义流应用程序的DStream操作。</li><li><strong>未完成的batch</strong> ：当前运行batch对应的job在队列中排队，还没有计算到该batch的数据。</li></ul></li><li><p><strong>数据检查点</strong> </p><p> 将生成的RDD保存到可靠的存储中。在某些<em>有状态</em>转换中，需要合并多个批次中的数据，所以需要开启检查点。在此类转换中，生成的RDD依赖于先前批次的RDD，这导致依赖链的长度随时间不断增加。为了避免恢复时间无限制的增加（与依赖链成比例），有状态转换的中间RDD定期 <em>checkpoint</em>到可靠的存储（例如HDFS），以切断依赖链，功能类似于持久化，只需要从当前的状态恢复，而不需要重新计算整个lineage。</p></li></ul><p>总而言之，从Driver程序故障中恢复时，主要需要元数据检查点。而如果使用有状态转换，则需要数据或RDD检查点。</p><h3 id="什么时候启用检查点"><a href="#什么时候启用检查点" class="headerlink" title="什么时候启用检查点"></a>什么时候启用检查点</h3><p>必须为具有以下类型的应用程序启用检查点：</p><ul><li><p><strong>使用了有状态转换转换操作</strong> </p><p>如果在应用程序中使用<code>updateStateByKey</code>或<code>reduceByKeyAndWindow</code>，则必须提供检查点目录以允许定期进行RDD检查点。</p></li><li><p><strong>从运行应用程序的Driver程序故障中恢复</strong> </p><p>元数据检查点用于恢复进度信息。</p></li></ul><p>注意，没有前述状态转换的简单流应用程序可以在不启用检查点的情况下运行。在这种情况下，从驱动程序故障中恢复也将是部分的（某些丢失但未处理的数据可能会丢失）。这通常是可以接受的，并且许多都以这种方式运行Spark Streaming应用程序。预计将来会改善对非Hadoop环境的支持。</p><h3 id="如何配置检查点"><a href="#如何配置检查点" class="headerlink" title="如何配置检查点"></a>如何配置检查点</h3><p>可以通过具有容错的、可靠的文件系统（例如HDFS，S3等）中设置目录来启用检查点，将检查点信息保存到该目录中。开启检查点，需要开启下面的两个配置：</p><ul><li>streamingContext.checkpoint(<dir>)：配置检查点的目录，比如HDFS路径</dir></li><li>dstream.checkpoint(<duration>)：检查点的频率</duration></li></ul><p>其中配置检查点的时间间隔是可选的。如果不设置，会根据DStream的类型选择一个默认值。对于MapWithStateDStream，默认的检查点间隔是batch interval的10倍。对于其他的DStream，默认的检查点间隔是10S，或者是batch interval的间隔时间。<strong>需要注意的是：checkpoint的频率必须是 batch interval的整数倍，否则会报错</strong>。</p><p>此外，如果要使应用程序从Driver程序故障中恢复，则需要使用下面的方式创建StreamingContext：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createStreamingContext</span> </span>(conf: <span class="type">SparkConf</span>,checkpointPath: <span class="type">String</span>):</span><br><span class="line"><span class="type">StreamingContext</span> = &#123;</span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>( &lt;<span class="type">ConfInfo</span>&gt; )</span><br><span class="line"><span class="comment">// .... other code ...</span></span><br><span class="line">ssc.checkPoint(checkpointDirectory)</span><br><span class="line">ssc</span><br><span class="line">&#125;</span><br><span class="line">#创建一个新的<span class="type">StreamingContext</span>或者从最近的checkpoint获取</span><br><span class="line"><span class="keyword">val</span> context = <span class="type">StreamingContext</span>.getOrCreate(checkpointDirectory,</span><br><span class="line">createStreamingContext _)</span><br><span class="line">#启动</span><br><span class="line">context.start()</span><br><span class="line">context.awaitTermination()</span><br></pre></td></tr></table></figure><ul><li>程序首次启动时，它将创建一个新的StreamingContext，然后调用start（）。</li><li>失败后重新启动程序时，它将根据检查点目录中的检查点数据重新创建StreamingContext。</li></ul><blockquote><p><strong>注意：</strong></p><p>RDD的检查点需要将数据保存到可靠存储上，由此带来一些成本开销。这可能会导致RDD获得检查点的那些批次的处理时间增加。因此，需要设置一个合理的检查点的间隔。在batch interval较小时(例如1秒），每个batch interval都进行检查点可能会大大降低吞吐量。相反，检查点时间间隔太长会导致 lineage和任务规模增加，这可能会产生不利影响。对于需要RDD检查点的有状态转换，默认间隔为batch interval的倍数，至少应为10秒。可以使用 <strong>dstream.checkpoint(checkpointInterval)</strong>进行配置。通常，DStream的5-10个batch interval的检查点间隔是一个较好的选择。</p></blockquote><h3 id="检查点和持久化之间的区别"><a href="#检查点和持久化之间的区别" class="headerlink" title="检查点和持久化之间的区别"></a>检查点和持久化之间的区别</h3><ul><li><p>持久化</p><ul><li>当我们将RDD保持在DISK_ONLY存储级别时，RDD将存储在一个位置，该RDD的后续使用将不会重新计算lineage。</li><li>在调用persist（）之后，Spark会记住RDD的lineage，即使它没有调用它。</li><li>作业运行完成后，将清除缓存并销毁文件。</li></ul></li><li><p>检查点</p><ul><li>检查点将RDD存储在HDFS中，将会删除lineage血缘关系。</li><li>在完成作业运行后，与持计划不同，不会删除检查点文件。</li><li>当checkpoint一个RDD时，将导致双重计算。即该操作在完成实际的计算工作之前，首先会调用持久化方法，然后再将其写入检查点目录。</li></ul></li></ul><h2 id="使用DataFrames-amp-SQL处理流数据"><a href="#使用DataFrames-amp-SQL处理流数据" class="headerlink" title="使用DataFrames &amp; SQL处理流数据"></a>使用DataFrames &amp; SQL处理流数据</h2><p>在Spark Streaming应用中，可以轻松地对流数据使用DataFrames和SQL操作。使用案例如下：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SqlStreaming</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="type">SqlStreaming</span>.getClass.getSimpleName)</span><br><span class="line">      .setMaster(<span class="string">"local[4]"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    words.foreachRDD &#123; rdd =&gt;</span><br><span class="line">      <span class="comment">// 调用SparkSession单例方法,如果已经创建了，则直接返回</span></span><br><span class="line">      <span class="keyword">val</span> spark = <span class="type">SparkSessionSingleton</span>.getInstance(rdd.sparkContext.getConf)</span><br><span class="line">      <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> wordsDataFrame = rdd.toDF(<span class="string">"word"</span>)</span><br><span class="line">      wordsDataFrame.show()</span><br><span class="line"></span><br><span class="line">      wordsDataFrame.createOrReplaceTempView(<span class="string">"words"</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> wordCountsDataFrame =</span><br><span class="line">        spark.sql(<span class="string">"select word, count(*) as total from words group by word"</span>)</span><br><span class="line">      wordCountsDataFrame.show()</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/** SparkSession单例 */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSessionSingleton</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@transient</span> <span class="keyword">private</span> <span class="keyword">var</span> instance: <span class="type">SparkSession</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getInstance</span></span>(sparkConf: <span class="type">SparkConf</span>): <span class="type">SparkSession</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (instance == <span class="literal">null</span>) &#123;</span><br><span class="line">      instance = <span class="type">SparkSession</span></span><br><span class="line">        .builder</span><br><span class="line">        .config(sparkConf)</span><br><span class="line">        .getOrCreate()</span><br><span class="line">    &#125;</span><br><span class="line">    instance</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文是Spark Streaming编程指南的第二篇分享，主要包括有状态的计算、基于时间的窗口操作、检查点等内容。下一篇将分享<strong>Spark MLLib机器学习</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第四篇|Spark Streaming编程指南(1)</title>
      <link href="/2020/07/27/%E7%AC%AC%E5%9B%9B%E7%AF%87-Spark-Streaming%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/"/>
      <url>/2020/07/27/%E7%AC%AC%E5%9B%9B%E7%AF%87-Spark-Streaming%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>Spark Streaming是构建在Spark Core基础之上的流处理框架，是Spark非常重要的组成部分。Spark Streaming于2013年2月在Spark0.7.0版本中引入，发展至今已经成为了在企业中广泛使用的流处理平台。在2016年7月，Spark2.0版本中引入了Structured Streaming，并在Spark2.2版本中达到了生产级别，Structured Streaming是构建在Spark SQL之上的流处理引擎，用户可以使用DataSet/DataFreame API进行流处理，目前Structured Streaming在不同的版本中发展速度很快。值得注意的是，本文不会对Structured Streaming做过多讲解，主要针对Spark Streaming进行讨论，包括以下内容：</p><ul><li>Spark Streaming介绍</li><li>Transformations与Output Operations</li><li>Spark Streaming数据源(Sources)</li><li>Spark Streaming 数据汇(Sinks)</li></ul><h2 id="Spark-Streaming介绍"><a href="#Spark-Streaming介绍" class="headerlink" title="Spark Streaming介绍"></a>Spark Streaming介绍</h2><h3 id="什么是DStream"><a href="#什么是DStream" class="headerlink" title="什么是DStream"></a>什么是DStream</h3><p>Spark Streaming是构建在Spark Core的RDD基础之上的，与此同时Spark Streaming引入了一个新的概念：DStream（Discretized Stream，离散化数据流)，表示连续不断的数据流。DStream抽象是Spark Streaming的流处理模型，在内部实现上，Spark Streaming会对输入数据按照时间间隔（如1秒）分段，每一段数据转换为Spark中的RDD，这些分段就是Dstream，并且对DStream的操作都最终转变为对相应的RDD的操作。如下图所示：</p><p><img src="//jiamaoxiang.top/2020/07/27/第四篇-Spark-Streaming编程指南/%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B.png" alt></p><p>如上图，这些底层的RDD转换操作是由Spark引擎来完成的，DStream的操作屏蔽了许多底层的细节，为用户提供了比较方便使用的高级API。</p><h3 id="计算模型"><a href="#计算模型" class="headerlink" title="计算模型"></a>计算模型</h3><p>在Flink中，批处理是流处理的特例，所以Flink是天然的流处理引擎。而Spark Streaming则不然，Spark Streaming认为流处理是批处理的特例，即Spark Streaming并不是纯实时的流处理引擎，在其内部使用的是<code>microBatch</code>模型，即将流处理看做是在较小时间间隔内(batch interval)的一些列的批处理。关于时间间隔的设定，需要结合具体的业务延迟需求，可以实现秒级或者分钟级的间隔。</p><p>Spark Streaming会将每个短时间间隔内接收的数据存储在集群中，然后对其作用一系列的算子操作(map,reduce, groupBy等)。执行过程见下图：</p><p><img src="//jiamaoxiang.top/2020/07/27/第四篇-Spark-Streaming编程指南/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B.png" alt></p><p>如上图：Spark Streaming会将输入的数据流分割成一个个小的batch，每一个batch都代表这一些列的RDD，然后将这些batch存储在内存中。通过启动Spark作业来处理这些batch数据，从而实现一个流处理应用。</p><h3 id="Spark-Streaming的工作机制"><a href="#Spark-Streaming的工作机制" class="headerlink" title="Spark Streaming的工作机制"></a>Spark Streaming的工作机制</h3><h4 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h4><p><img src="//jiamaoxiang.top/2020/07/27/第四篇-Spark-Streaming编程指南/%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.png" alt></p><ul><li>在Spark Streaming中，会有一个组件Receiver，作为一个长期运行的task跑在一个Executor上</li><li>每个Receiver都会负责一个input DStream（比如从文件中读取数据的文件流，比如套接字流，或者从Kafka中读取的一个输入流等等）</li><li>Spark Streaming通过input DStream与外部数据源进行连接，读取相关数据</li></ul><h4 id="执行细节"><a href="#执行细节" class="headerlink" title="执行细节"></a>执行细节</h4><p><img src="//jiamaoxiang.top/2020/07/27/第四篇-Spark-Streaming编程指南/%E5%86%85%E9%83%A8%E7%BB%86%E8%8A%82.png" alt></p><ul><li>1.启动StreamingContext</li><li>2.StreamingContext启动receiver，该receiver会一直运行在Executor的task中。用于连续不断地接收数据源，有两种主要的reciver，一种是可靠的reciver，当数据被接收并且存储到spark，发送回执确认，另一种是不可靠的reciver，对于数据源不发送回执确认。接收的数据会被缓存到work节点内存中，也会被复制到其他executor的所在的节点内存中，用于容错处理。</li><li>3.Streaming context周期触发job(根据batch-interval时间间隔)进行数据处理。</li><li>4.将数据输出。</li></ul><h3 id="Spark-Streaming编程步骤"><a href="#Spark-Streaming编程步骤" class="headerlink" title="Spark Streaming编程步骤"></a>Spark Streaming编程步骤</h3><p>经过上面的分析，对Spark Streaming有了初步的认识。那么该如何编写一个Spark Streaming应用程序呢？一个Spark Streaming一般包括一下几个步骤：</p><ul><li><p>1.创建<code>StreamingContext</code></p></li><li><p>2.创建输入<code>DStream</code>来定义输入源</p></li><li><p>3.通过对DStream应用转换操作和输出操作来定义处理逻辑</p></li><li><p>4.用streamingContext.start()来开始接收数据和处理流程</p></li><li><p>5.streamingContext.awaitTermination()方法来等待处理结束</p></li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StartSparkStreaming</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">      .setAppName(<span class="string">"Streaming"</span>)</span><br><span class="line">    <span class="comment">// 1.创建StreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.spark"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.hadoop"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line">    <span class="comment">// 2.创建DStream</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="comment">// 3.定义流计算处理逻辑</span></span><br><span class="line">    <span class="keyword">val</span> count = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">      .reduceByKey(_ + _)</span><br><span class="line">    <span class="comment">// 4.输出结果</span></span><br><span class="line">    count.print()</span><br><span class="line">    <span class="comment">// 5.启动</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">// 6.等待执行</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Transformations与Output-Operations"><a href="#Transformations与Output-Operations" class="headerlink" title="Transformations与Output Operations"></a>Transformations与Output Operations</h2><p>DStream是不可变的， 这意味着不能直接改变它们的内容，而是通过对DStream进行一系列转换(Transformation)来实现预期的应用程序逻辑。 每次转换都会创建一个新的DStream，该DStream表示来自父DStream的转换后的数据。 DStream转换是惰性(lazy)的，这意味只有执行output操作之后，才会去执行转换操作，这些触发执行的操作称之为<code>output operation</code>。 </p><h3 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h3><p>Spark Streaming提供了丰富的transformation操作，这些transformation又分为了<strong>有状态的transformation</strong>和<strong>无状态的transformation</strong>。除此之外，Spark Streaming也提供了一些window操作，值得注意的是window操作也是有状态的。具体细节如下：</p><h4 id="无状态的transformation"><a href="#无状态的transformation" class="headerlink" title="无状态的transformation"></a>无状态的transformation</h4><p>无状态的transformation是指每一个micro-batch的处理是相互独立的，即当前的计算结果不受之前计算结果的影响，Spark Streaming的大部分算子都是无状态的，比如常见的map(),flatMap(),reduceByKey()等等。</p><ul><li><strong>map(func)</strong></li></ul><p>对源DStream的每个元素，采用func函数进行转换，得到一个新的Dstream</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/** Return a new DStream by applying a function to all elements of this DStream. */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](mapFunc: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">DStream</span>[<span class="type">U</span>] = ssc.withScope &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MappedDStream</span>(<span class="keyword">this</span>, context.sparkContext.clean(mapFunc))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>flatMap(func)</strong></li></ul><p>与map相似，但是每个输入项可用被映射为0个或者多个输出项</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new DStream by applying a function to all elements of this DStream,</span></span><br><span class="line"><span class="comment"> * and then flattening the results</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](flatMapFunc: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">DStream</span>[<span class="type">U</span>] = ssc.withScope &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">FlatMappedDStream</span>(<span class="keyword">this</span>, context.sparkContext.clean(flatMapFunc))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>filter(func)</strong></li></ul><p>返回一个新的DStream，仅包含源DStream中满足函数func的项</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/** Return a new DStream containing only the elements that satisfy a predicate. */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(filterFunc: <span class="type">T</span> =&gt; <span class="type">Boolean</span>): <span class="type">DStream</span>[<span class="type">T</span>] = ssc.withScope &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">FilteredDStream</span>(<span class="keyword">this</span>, context.sparkContext.clean(filterFunc))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>repartition(numPartitions)</strong></li></ul><p>通过创建更多或者更少的分区改变DStream的并行程度</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return a new DStream with an increased or decreased level of parallelism. Each RDD in the</span></span><br><span class="line"><span class="comment">   * returned DStream has exactly numPartitions partitions.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(numPartitions: <span class="type">Int</span>): <span class="type">DStream</span>[<span class="type">T</span>] = ssc.withScope &#123;</span><br><span class="line">    <span class="keyword">this</span>.transform(_.repartition(numPartitions))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li><strong>reduce(func)</strong></li></ul><p>利用函数func聚集源DStream中每个RDD的元素，返回一个包含单元素RDDs的新DStream</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new DStream in which each RDD has a single element generated by reducing each RDD</span></span><br><span class="line"><span class="comment"> * of this DStream.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(reduceFunc: (<span class="type">T</span>, <span class="type">T</span>) =&gt; <span class="type">T</span>): <span class="type">DStream</span>[<span class="type">T</span>] = ssc.withScope &#123;</span><br><span class="line">  <span class="keyword">this</span>.map((<span class="literal">null</span>, _)).reduceByKey(reduceFunc, <span class="number">1</span>).map(_._2)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>count()</strong></li></ul><p>统计源DStream中每个RDD的元素数量</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return a new DStream in which each RDD has a single element generated by counting each RDD</span></span><br><span class="line"><span class="comment">   * of this DStream.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">DStream</span>[<span class="type">Long</span>] = ssc.withScope &#123;</span><br><span class="line">    <span class="keyword">this</span>.map(_ =&gt; (<span class="literal">null</span>, <span class="number">1</span>L))</span><br><span class="line">        .transform(_.union(context.sparkContext.makeRDD(<span class="type">Seq</span>((<span class="literal">null</span>, <span class="number">0</span>L)), <span class="number">1</span>)))</span><br><span class="line">        .reduceByKey(_ + _)</span><br><span class="line">        .map(_._2)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li><strong>union(otherStream)</strong></li></ul><p>返回一个新的DStream，包含源DStream和其他DStream的元素</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return a new DStream by unifying data of another DStream with this DStream.</span></span><br><span class="line"><span class="comment">   * @param that Another DStream having the same slideDuration as this DStream.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">union</span></span>(that: <span class="type">DStream</span>[<span class="type">T</span>]): <span class="type">DStream</span>[<span class="type">T</span>] = ssc.withScope &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">UnionDStream</span>[<span class="type">T</span>](<span class="type">Array</span>(<span class="keyword">this</span>, that))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li><strong>countByValue()</strong></li></ul><p>应用于元素类型为K的DStream上，返回一个（K，V）键值对类型的新DStream，每个键的值是在原DStream的每个RDD中的出现次数,比如<code>lines.flatMap(_.split(&quot; &quot;)).countByValue().print()</code>,对于输入：<code>spark spark flink</code>,将输出：<code>(spark,2),(flink,1)</code>,即按照元素值进行分组，然后统计每个分组的元素个数。</p><p>从源码可以看出：底层实现为map((_,1L)).reduceByKey((x: Long, y: Long) =&gt; x + y, numPartitions)，即先按当前的元素映射为一个tuple，其中key即为当前元素的值，然后再按照key做汇总。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return a new DStream in which each RDD contains the counts of each distinct value in</span></span><br><span class="line"><span class="comment">   * each RDD of this DStream. Hash partitioning is used to generate</span></span><br><span class="line"><span class="comment">   * the RDDs with `numPartitions` partitions (Spark's default number of partitions if</span></span><br><span class="line"><span class="comment">   * `numPartitions` not specified).</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">countByValue</span></span>(numPartitions: <span class="type">Int</span> = ssc.sc.defaultParallelism)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>)</span><br><span class="line">      : <span class="type">DStream</span>[(<span class="type">T</span>, <span class="type">Long</span>)] = ssc.withScope &#123;</span><br><span class="line">    <span class="keyword">this</span>.map((_, <span class="number">1</span>L)).reduceByKey((x: <span class="type">Long</span>, y: <span class="type">Long</span>) =&gt; x + y, numPartitions)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li><strong>reduceByKey(func, [numTasks])</strong></li></ul><p>当在一个由(K,V)键值对组成的DStream上执行该操作时，返回一个新的由(K,V)键值对组成的DStream，每一个key的值均由给定的recuce函数（func）聚集起来</p><p>比如：<code>lines.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_ + _).print()</code></p><p>对于输入：spark spark flink，将输出：(spark,2),(flink,1)</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new DStream by applying `reduceByKey` to each RDD. The values for each key are</span></span><br><span class="line"><span class="comment"> * merged using the associative and commutative reduce function. Hash partitioning is used to</span></span><br><span class="line"><span class="comment"> * generate the RDDs with Spark's default number of partitions.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(reduceFunc: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">DStream</span>[(<span class="type">K</span>, <span class="type">V</span>)] = ssc.withScope &#123;</span><br><span class="line">  reduceByKey(reduceFunc, defaultPartitioner())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>join(otherStream, [numTasks])</strong></li></ul><p>当应用于两个DStream（一个包含（K,V）键值对,一个包含(K,W)键值对），返回一个包含(K, (V, W))键值对的新Dstream</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new DStream by applying 'join' between RDDs of `this` DStream and `other` DStream.</span></span><br><span class="line"><span class="comment"> * Hash partitioning is used to generate the RDDs with Spark's default number of partitions.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>: <span class="type">ClassTag</span>](other: <span class="type">DStream</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">DStream</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))] = ssc.withScope &#123;</span><br><span class="line">  join[<span class="type">W</span>](other, defaultPartitioner())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>cogroup(otherStream, [numTasks])</strong></li></ul><p>当应用于两个DStream（一个包含（K,V）键值对,一个包含(K,W)键值对），返回一个包含(K, Seq[V], Seq[W])的元组</p><blockquote><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">&gt;    <span class="comment">// 输入：spark</span></span><br><span class="line">&gt;    <span class="comment">// 输出：(spark,(CompactBuffer(1),CompactBuffer(1)))</span></span><br><span class="line">&gt;    <span class="keyword">val</span> <span class="type">DS1</span> = lines.flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>))</span><br><span class="line">&gt;    <span class="keyword">val</span> <span class="type">DS2</span> = lines.flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>))</span><br><span class="line">&gt;    <span class="type">DS1</span>.cogroup(<span class="type">DS2</span>).print()</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new DStream by applying 'cogroup' between RDDs of `this` DStream and `other` DStream.</span></span><br><span class="line"><span class="comment"> * Hash partitioning is used to generate the RDDs with Spark's default number</span></span><br><span class="line"><span class="comment"> * of partitions.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    other: <span class="type">DStream</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">DStream</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W</span>]))] = ssc.withScope &#123;</span><br><span class="line">  cogroup(other, defaultPartitioner())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>transform(func)</strong></li></ul><p>通过对源DStream的每个RDD应用RDD-to-RDD函数，创建一个新的DStream。支持在新的DStream中做任何RDD操作</p><blockquote><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">&gt;<span class="comment">// 输入：spark spark flink</span></span><br><span class="line">&gt;<span class="comment">// 输出：(spark,2)、(flink,1)</span></span><br><span class="line">&gt;<span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">&gt;<span class="keyword">val</span> resultDStream = lines.transform(rdd =&gt; &#123;</span><br><span class="line">&gt;  rdd.flatMap(_.split(<span class="string">"\\W"</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">&gt;&#125;)</span><br><span class="line">&gt;resultDStream.print()</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new DStream in which each RDD is generated by applying a function</span></span><br><span class="line"><span class="comment"> * on each RDD of 'this' DStream.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](transformFunc: <span class="type">RDD</span>[<span class="type">T</span>] =&gt; <span class="type">RDD</span>[<span class="type">U</span>]): <span class="type">DStream</span>[<span class="type">U</span>] = ssc.withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanedF = context.sparkContext.clean(transformFunc, <span class="literal">false</span>)</span><br><span class="line">  transform((r: <span class="type">RDD</span>[<span class="type">T</span>], _: <span class="type">Time</span>) =&gt; cleanedF(r))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="有状态的transformation"><a href="#有状态的transformation" class="headerlink" title="有状态的transformation"></a>有状态的transformation</h4><p>有状态的transformation是指每个micro-batch的处理不是相互独立的，即当前的micro-batch处理依赖于之前的micro-batch计算结果。常见的有状态的transformation主要有countByValueAndWindow, reduceByKeyAndWindow , mapWithState, updateStateByKey等等。其实所有的基于window的操作都是有状态的，因为追踪整个窗口内的数据。</p><p>关于有状态的transformation和Window Operations，参见下文。</p><h3 id="Output-Operations"><a href="#Output-Operations" class="headerlink" title="Output Operations"></a>Output Operations</h3><p>使用Output operations可以将DStream写入多外部存储设备或打印到控制台。上文提到，Spark Streaming的transformation是lazy的，因此需要Output Operation进行触发计算，其功能类似于RDD的action操作。具体详见下文Spark Streaming 数据汇(Sinks)。</p><h2 id="Spark-Streaming数据源"><a href="#Spark-Streaming数据源" class="headerlink" title="Spark Streaming数据源"></a>Spark Streaming数据源</h2><p>Spark Streaming的目的是成为一个通用的流处理框架，为了实现这一目标，Spark Streaming使用<strong>Receiver</strong>来集成各种各样的数据源。但是，对于有些数据源(如kafka),Spark Streaming支持使用<strong>Direct</strong>的方式去接收数据，这种方式比Receiver方式性能要好。</p><h3 id="基于Receiver的方式"><a href="#基于Receiver的方式" class="headerlink" title="基于Receiver的方式"></a>基于Receiver的方式</h3><p><img src="//jiamaoxiang.top/2020/07/27/第四篇-Spark-Streaming编程指南/receiver.png" alt></p><p>Receiver的作用是从数据源收集数据，然后将数据传送给Spark Streaming。基本原理是：随着数据的不断到来，在相对应的batch interval时间间隔内，这些数据会被收集并且打包成block，只要等到batch interval时间完成了，收集的数据block会被发送给spark进行处理。</p><p>如上图：当Spark Streaming启动时，receiver开始收集数据。在<code>t0</code>的batch interval结束时(即收集完了该时间段内的数据)，收集到的block <strong>#0</strong>会被发送到Spark进行处理。在<code>t2</code>时刻，Spark会处理<code>t1</code>的batch interval的数据block，与此同时会不停地收集<code>t2</code>的batch interval对应的block<strong>#2</strong>。</p><p>常见的基于Receiver的数据源包括：Kafka, Kinesis, Flume,Twitter。除此之外，用户也可以通过继承 <strong>Receiver</strong>抽象类，实现<code>onStart()</code>与<code>onStop()</code>两个方法，进行自定义Receiver。本文不会对基于Receiver的数据源做过多讨论，主要针对基于Direct的Kafka数据源进行详细解释。</p><h3 id="基于Direct的方式"><a href="#基于Direct的方式" class="headerlink" title="基于Direct的方式"></a>基于Direct的方式</h3><p>Spark 1.3中引入了这种新的无Receiver的Direct方法，以确保更强的端到端保证。该方法不是使用Receiver来接收数据，而是定期查询Kafka每个topic+partition中的最新偏移量，并相应地定义要在每个批次中处理的偏移量范围。启动用于处理数据的作业时，Kafka的简单consumer API用于读取Kafka定义的偏移量范围（类似于从文件系统读取文件）。请注意，此功能是在Scala和Java API的Spark 1.3引入的，在Python API的Spark 1.4中引入的。</p><p>基于Direct的方式具有以下优点：</p><ul><li><strong>简化并行读取</strong></li></ul><p>如果要读取多个partition，不需要创建多个输入DStream然后对他们进行union操作。Spark会创建跟Kafka partition一样多的RDD partition，并且会并行从kafka中读取数据。所以在kafka partition和RDD partition之间，有一一对应的关系。</p><ul><li><strong>高性能</strong></li></ul><p>如果要保证数据零丢失，在基于Receiver的方式中，需要开启WAL机制。这种方式其实效率很低，因为数据实际被复制了两份，kafka自己本身就有高可靠的机制，会对数据复制一份，而这里又会复制一份到WAL中。而基于Direct的方式，不依赖于Receiver，不需要开启WAL机制，只要kafka中做了数据的复制，那么就可以通过kafka的副本进行恢复。</p><ul><li><strong>Exactly-once语义</strong></li></ul><p>基于Receiver的方式，使用kafka的高阶API来在Zookeeper中保存消费过的offset。这是消费kafka数据的传统方式。这种方式配合WAL机制，可以保证数据零丢失的高可靠性，但是却无法保证Exactly-once语义(Spark和Zookeeper之间可能是不同步的)。基于Direct的方式，使用kafka的简单API，Spark Streaming自己就负责追踪消费的offset，并保存在checkpoint中。Spark自己一定是同步的，因此可以保证数据时消费一次且仅消费一次。</p><h3 id="Spark-Streaming集成kafka"><a href="#Spark-Streaming集成kafka" class="headerlink" title="Spark Streaming集成kafka"></a>Spark Streaming集成kafka</h3><h4 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h4><p>使用<strong>KafkaUtils</strong>添加Kafka数据源，源码如下：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDirectStream</span></span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">    ssc: <span class="type">StreamingContext</span>,</span><br><span class="line">    locationStrategy: <span class="type">LocationStrategy</span>,</span><br><span class="line">    consumerStrategy: <span class="type">ConsumerStrategy</span>[<span class="type">K</span>, <span class="type">V</span>]</span><br><span class="line">  ): <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">K</span>, <span class="type">V</span>]] = &#123;</span><br><span class="line">  <span class="keyword">val</span> ppc = <span class="keyword">new</span> <span class="type">DefaultPerPartitionConfig</span>(ssc.sparkContext.getConf)</span><br><span class="line">  createDirectStream[<span class="type">K</span>, <span class="type">V</span>](ssc, locationStrategy, consumerStrategy, ppc)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>具体参数解释：</p><ul><li><p><strong>K</strong>：Kafka消息key的类型</p></li><li><p><strong>V</strong>：Kafka消息value的类型</p></li><li><p><strong>ssc</strong>：StreamingContext</p></li><li><p><strong>locationStrategy</strong>: LocationStrategy，根据Executor中的主题的分区来调度consumer，即尽可能地让consumer靠近leader partition。该配置可以提升性能，但对于location的选择只是一种参考，并不是绝对的。可以选择如下方式：</p><ul><li>PreferBrokers：Spark和Kafka运行在同一个节点上，可以使用此种方式</li><li>PreferConsistent：大部分情况使用此方式，它将一致地在所有Executor之间分配分区</li><li>PreferFixed：将特定的主题分区放置到特定的主机上，在数据负载不均衡时使用</li></ul><p><strong>注意</strong>：多数情况下使用PreferConsisten，其他两种方式只是在特定的场景使用。这种配置只是一种参考，具体的情况还是会根据集群的资源自动调整。</p></li><li><p><strong>consumerStrategy</strong>：消费策略，主要有下面三种方式：</p><ul><li>Subscribe：订阅指定主题名称的主题集合</li><li>SubscribePattern：通过正则匹配，订阅相匹配的主题数据</li><li>Assign：订阅一个主题+分区的集合</li></ul><p><strong>注意</strong>：大多数情况下使用Subscribe方式。</p></li></ul><h4 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h4>  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TolerateWCTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createContext</span></span>(checkpointDirectory: <span class="type">String</span>): <span class="type">StreamingContext</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .set(<span class="string">"spark.streaming.backpressure.enabled"</span>, <span class="string">"true"</span>)</span><br><span class="line">      <span class="comment">//每秒钟从kafka分区中读取的records数量,默认not set</span></span><br><span class="line">      .set(<span class="string">"spark.streaming.kafka.maxRatePerPartition"</span>, <span class="string">"1000"</span>) <span class="comment">//</span></span><br><span class="line">      <span class="comment">//Driver为了获取每个leader分区的最近offsets，连续进行重试的次数，</span></span><br><span class="line">      <span class="comment">//默认是1，表示最多重试2次，仅仅适用于 new Kafka direct stream API</span></span><br><span class="line">      .set(<span class="string">"spark.streaming.kafka.maxRetries"</span>, <span class="string">"2"</span>)</span><br><span class="line">      .setAppName(<span class="string">"TolerateWCTest"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    ssc.checkpoint(checkpointDirectory)</span><br><span class="line">    <span class="keyword">val</span> topic = <span class="type">Array</span>(<span class="string">"testkafkasource2"</span>)</span><br><span class="line">    <span class="keyword">val</span> kafkaParam = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="string">"bootstrap.servers"</span> -&gt; <span class="string">"kms-1:9092"</span>,</span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">"group.id"</span> -&gt; <span class="string">"group0"</span>,</span><br><span class="line">      <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"latest"</span>, <span class="comment">//默认latest，</span></span><br><span class="line">      <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)) <span class="comment">//默认true,false:手动提交</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lines = <span class="type">KafkaUtils</span>.createDirectStream(</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topic, kafkaParam))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.value().split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordDstream = words.map(x =&gt; (x, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> stateDstream = wordDstream.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    stateDstream.cache()</span><br><span class="line">    <span class="comment">//参照batch interval设置，</span></span><br><span class="line">    <span class="comment">//不得低于batch interval，否则会报错，</span></span><br><span class="line">    <span class="comment">//设为batch interval的2倍</span></span><br><span class="line">    stateDstream.checkpoint(<span class="type">Seconds</span>(<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把DStream保存到MySQL数据库中</span></span><br><span class="line">    stateDstream.foreachRDD(rdd =&gt;</span><br><span class="line">      rdd.foreachPartition &#123; record =&gt;</span><br><span class="line">        <span class="keyword">var</span> conn: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> stmt: <span class="type">PreparedStatement</span> = <span class="literal">null</span></span><br><span class="line">        <span class="comment">// 给每个partition，获取一个连接</span></span><br><span class="line">        conn = <span class="type">ConnectionPool</span>.getConnection</span><br><span class="line">        <span class="comment">// 遍历partition中的数据，使用一个连接，插入数据库</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (record.hasNext) &#123;</span><br><span class="line">          <span class="keyword">val</span> wordcounts = record.next()</span><br><span class="line">          <span class="keyword">val</span> sql = <span class="string">"insert into wctbl(word,count) values (?,?)"</span></span><br><span class="line">          stmt = conn.prepareStatement(sql);</span><br><span class="line">          stmt.setString(<span class="number">1</span>, wordcounts._1.trim)</span><br><span class="line">          stmt.setInt(<span class="number">2</span>, wordcounts._2.toInt)</span><br><span class="line">          stmt.executeUpdate()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 用完以后，将连接还回去</span></span><br><span class="line">        <span class="type">ConnectionPool</span>.returnConnection(conn)</span><br><span class="line">      &#125;)</span><br><span class="line">    ssc</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> checkpointDirectory = <span class="string">"hdfs://kms-1:8020/docheckpoint"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="type">StreamingContext</span>.getOrCreate(</span><br><span class="line">      checkpointDirectory,</span><br><span class="line">      () =&gt; createContext(checkpointDirectory))</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Spark-Streaming-数据汇-Sinks"><a href="#Spark-Streaming-数据汇-Sinks" class="headerlink" title="Spark Streaming 数据汇(Sinks)"></a>Spark Streaming 数据汇(Sinks)</h2><h3 id="Output-Operation介绍"><a href="#Output-Operation介绍" class="headerlink" title="Output Operation介绍"></a>Output Operation介绍</h3><p>Spark Streaming提供了下面内置的Output Operation，如下：</p><ul><li><strong>print</strong>()</li></ul><p>打印数据数据到标准输出，如果不传递参数，默认打印前10个元素</p><ul><li><strong>saveAsTextFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</li></ul><p>将DStream内容存储到文件系统，每个batch interval的文件名称为`<em>prefix-TIME_IN_MS[.suffix]</em></p><ul><li><strong>saveAsObjectFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</li></ul><p>将DStream的内容保存为序列化的java对象的SequenceFile，每个batch interval的文件名称为<code>prefix-TIME_IN_MS[.suffix]</code>,Python API不支持此方法。</p><ul><li><strong>saveAsHadoopFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</li></ul><p>将DStream内容保存为Hadoop文件，每个batch interval的文件名称为<code>prefix-TIME_IN_MS[.suffix]</code>,Python API不支持此方法。</p><ul><li><strong>foreachRDD</strong>(<em>func</em>)</li></ul><p>通用的数据输出算子，func函数将每个RDD的数据输出到外部存储设备，比如将RDD写入到文件或者数据库。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Apply a function to each RDD in this DStream. This is an output operator, so</span></span><br><span class="line"><span class="comment">  * 'this' DStream will be registered as an output stream and therefore materialized.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">foreachRDD</span></span>(foreachFunc: <span class="type">RDD</span>[<span class="type">T</span>] =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = ssc.withScope &#123;</span><br><span class="line">   <span class="keyword">val</span> cleanedF = context.sparkContext.clean(foreachFunc, <span class="literal">false</span>)</span><br><span class="line">   foreachRDD((r: <span class="type">RDD</span>[<span class="type">T</span>], _: <span class="type">Time</span>) =&gt; cleanedF(r), displayInnerRDDOps = <span class="literal">true</span>)</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Apply a function to each RDD in this DStream. This is an output operator, so</span></span><br><span class="line"><span class="comment">  * 'this' DStream will be registered as an output stream and therefore materialized.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">foreachRDD</span></span>(foreachFunc: (<span class="type">RDD</span>[<span class="type">T</span>], <span class="type">Time</span>) =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = ssc.withScope &#123;</span><br><span class="line">   <span class="comment">// because the DStream is reachable from the outer object here, and because</span></span><br><span class="line">   <span class="comment">// DStreams can't be serialized with closures, we can't proactively check</span></span><br><span class="line">   <span class="comment">// it for serializability and so we pass the optional false to SparkContext.clean</span></span><br><span class="line">   foreachRDD(foreachFunc, displayInnerRDDOps = <span class="literal">true</span>)</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">foreachRDD</span></span>(</span><br><span class="line">     foreachFunc: (<span class="type">RDD</span>[<span class="type">T</span>], <span class="type">Time</span>) =&gt; <span class="type">Unit</span>,</span><br><span class="line">     displayInnerRDDOps: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">   <span class="keyword">new</span> <span class="type">ForEachDStream</span>(<span class="keyword">this</span>,</span><br><span class="line">     context.sparkContext.clean(foreachFunc, <span class="literal">false</span>), displayInnerRDDOps).register()</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p><strong>foreachRDD</strong>是一个非常重要的操作，用户可以使用它将处理的数据输出到外部存储设备。关于foreachRDD的使用，需要特点别注意一些细节问题。具体分析如下：</p><p>如果将数据写入到MySQL，需要获取连接Connection。用户可能不经意的在Spark Driver中创建一个连接对象，然后在Work中使用它将数据写入外部设备，代码如下：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">dstream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  <span class="keyword">val</span> connection = createNewConnection()  <span class="comment">// ①注意：该段代码在driver上执行</span></span><br><span class="line">  rdd.foreach &#123; record =&gt;</span><br><span class="line">    connection.send(record) <span class="comment">// ②注意：该段代码在worker上执行</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>尖叫提示：上面的使用方式是错误的，因为需要将connection对象进行序列化，然后发送到driver节点，而这种connection对象是不能被序列化，所以不能跨节点传输。上面代码会报序列化错误，正确的使用方式是在worker节点创建connection，即在<code>rdd.foreach</code>内部创建connection。方式如下：</p></blockquote><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">dstream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  rdd.foreach &#123; record =&gt;</span><br><span class="line">    <span class="keyword">val</span> connection = createNewConnection()</span><br><span class="line">    connection.send(record)</span><br><span class="line">    connection.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的方式解决了不能序列化的问题，但是会为每个RDD的record创建一个connection，通常创建一个connection对象是会存在一定性能开销的，所以频繁创建和销毁connection对象会造成整体的吞吐量降低。一个比较好的做法是将<code>rdd.foreach</code>替换为``rdd.foreachPartition<code></code>,这样就不用频繁为每个record创建connection，而是为RDD的partition创建connection，大大减少了创建connection带来的开销。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">dstream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  rdd.foreachPartition &#123; partitionOfRecords =&gt;</span><br><span class="line">    <span class="keyword">val</span> connection = createNewConnection()</span><br><span class="line">    partitionOfRecords.foreach(record =&gt; connection.send(record))</span><br><span class="line">    connection.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其实上面的使用方式还可以进一步优化，可以通过在多个RDD或者批数据间重用连接对象。用户可以维护一个静态的连接对象池，重复使用池中的对象将多批次的RDD推送到外部系统，以进一步节省开销：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">dstream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  rdd.foreachPartition &#123; partitionOfRecords =&gt;</span><br><span class="line">    <span class="keyword">val</span> connection = <span class="type">ConnectionPool</span>.getConnection()</span><br><span class="line">    partitionOfRecords.foreach(record =&gt; connection.send(record))</span><br><span class="line">    <span class="type">ConnectionPool</span>.returnConnection(connection)  </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="使用案例-1"><a href="#使用案例-1" class="headerlink" title="使用案例"></a>使用案例</h3><ul><li>模拟数据库连接池</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 简易版的连接池</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConnectionPool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 静态的Connection队列</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> LinkedList&lt;Connection&gt; connectionQueue;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 加载驱动</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Class.forName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ClassNotFoundException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取连接，多线程访问并发控制</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (connectionQueue == <span class="keyword">null</span>) &#123;</span><br><span class="line">                connectionQueue = <span class="keyword">new</span> LinkedList&lt;Connection&gt;();</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">                    Connection conn = DriverManager.getConnection(<span class="string">"jdbc:mysql://localhost:3306/wordcount"</span>, <span class="string">"root"</span>,</span><br><span class="line">                            <span class="string">"123qwe"</span>);</span><br><span class="line">                    connectionQueue.push(conn);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> connectionQueue.poll();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 用完之后，返回一个连接</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">returnConnection</span><span class="params">(Connection conn)</span> </span>&#123;</span><br><span class="line">        connectionQueue.push(conn);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>实时统计写入MySQL</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"NetworkWordCount"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">    wordCounts.print()</span><br><span class="line">    <span class="comment">// 存储到MySQL</span></span><br><span class="line">    wordCounts.foreachRDD &#123; rdd =&gt;</span><br><span class="line">      rdd.foreachPartition &#123; partition =&gt;</span><br><span class="line">        <span class="keyword">var</span> conn: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> stmt: <span class="type">PreparedStatement</span> = <span class="literal">null</span></span><br><span class="line">        <span class="comment">// 给每个partition，获取一个连接</span></span><br><span class="line">        conn = <span class="type">ConnectionPool</span>.getConnection</span><br><span class="line">        <span class="comment">// 遍历partition中的数据，使用一个连接，插入数据库</span></span><br><span class="line">        <span class="keyword">while</span> (partition.hasNext) &#123;</span><br><span class="line">          <span class="keyword">val</span> wordcounts = partition.next()</span><br><span class="line">          <span class="keyword">val</span> sql = <span class="string">"insert into wctbl(word,count) values (?,?)"</span></span><br><span class="line">          stmt = conn.prepareStatement(sql);</span><br><span class="line">          stmt.setString(<span class="number">1</span>, wordcounts._1.trim)</span><br><span class="line">          stmt.setInt(<span class="number">2</span>, wordcounts._2.toInt)</span><br><span class="line">          stmt.executeUpdate()</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 用完以后，将连接还回去</span></span><br><span class="line">        <span class="type">ConnectionPool</span>.returnConnection(conn)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>由于篇幅限制，本文主要对Spark Streaming执行机制、Transformations与Output Operations、Spark Streaming数据源(Sources)、Spark Streaming 数据汇(Sinks)进行了讨论。下一篇将分享<strong>基于时间的窗口操作</strong>、<strong>有状态的计算</strong>、<strong>检查点Checkpoint</strong>、<strong>性能调优</strong>等内容。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第三篇|Spark SQL编程指南</title>
      <link href="/2020/07/23/%E7%AC%AC%E4%B8%89%E7%AF%87-Spark-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/"/>
      <url>/2020/07/23/%E7%AC%AC%E4%B8%89%E7%AF%87-Spark-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>在<a href="https://mp.weixin.qq.com/s/z22uJwTnBxeZnYlCKIXzNQ" target="_blank" rel="noopener">《第二篇|Spark Core编程指南》</a>一文中，对Spark的核心模块进行了讲解。本文将讨论Spark的另外一个重要模块–Spark SQL，Spark SQL是在Shark的基础之上构建的，于2014年5月发布。从名称上可以看出，该模块是Spark提供的关系型操作API，实现了SQL-on-Spark的功能。对于一些熟悉SQL的用户，可以直接使用SQL在Spark上进行复杂的数据处理。通过本文，你可以了解到：</p><ul><li>Spark SQL简介</li><li>DataFrame API&amp;DataSet API</li><li>Catalyst Optimizer优化器</li><li>Spark SQL基本操作</li><li>Spark SQL的数据源</li><li>RDD与DataFrame相互转换</li><li>Thrift  server与Spark SQL CLI</li></ul><h2 id="Spark-SQL简介"><a href="#Spark-SQL简介" class="headerlink" title="Spark SQL简介"></a>Spark SQL简介</h2><p>Spark SQL是Spark的其中一个模块，用于结构化数据处理。与基本的Spark RDD API不同，Spark SQL提供的接口为Spark提供了有关数据结构和正在执行的计算的更多信息，Spark SQL会使用这些额外的信息来执行额外的优化。使用SparkSQL的方式有很多种，包括SQL、DataFrame API以及Dataset API。值得注意的是，无论使用何种方式何种语言，其执行引擎都是相同的。实现这种统一，意味着开发人员可以轻松地在不同的API之间来回切换，从而使数据处理更加地灵活。</p><h2 id="DataFrame-API-amp-DataSet-API"><a href="#DataFrame-API-amp-DataSet-API" class="headerlink" title="DataFrame API&amp;DataSet API"></a>DataFrame API&amp;DataSet API</h2><h3 id="DataFrame-API"><a href="#DataFrame-API" class="headerlink" title="DataFrame API"></a>DataFrame API</h3><p>DataFrame代表一个不可变的分布式数据集合，其核心目的是让开发者面对数据处理时，只关心要做什么，而不用关心怎么去做，将一些优化的工作交由Spark框架本身去处理。DataFrame是具有Schema信息的，也就是说可以被看做具有字段名称和类型的数据，类似于关系型数据库中的表，但是底层做了很多的优化。创建了DataFrame之后，就可以使用SQL进行数据处理。</p><p>用户可以从多种数据源中构造DataFrame，例如：结构化数据文件，Hive中的表，外部数据库或现有RDD。DataFrame API支持Scala，Java，Python和R，在Scala和Java中，row类型的DataSet代表DataFrame，即<code>Dataset[Row]</code>等同于DataFrame。</p><h3 id="DataSet-API"><a href="#DataSet-API" class="headerlink" title="DataSet API"></a>DataSet API</h3><p>DataSet是Spark 1.6中添加的新接口，是DataFrame的扩展，它具有RDD的优点（强类型输入，支持强大的lambda函数）以及Spark SQL的优化执行引擎的优点。可以通过JVM对象构建DataSet，然后使用函数转换（map<code>，</code>flatMap<code>，</code>filter)。值得注意的是，Dataset API在Scala和 Java中可用，Python不支持Dataset API。</p><p>另外，DataSet API可以减少内存的使用，由于Spark框架知道DataSet的数据结构，因此在持久化DataSet时可以节省很多的内存空间。</p><h2 id="Catalyst-Optimizer优化器"><a href="#Catalyst-Optimizer优化器" class="headerlink" title="Catalyst Optimizer优化器"></a>Catalyst Optimizer优化器</h2><p>在Catalyst中，存在两种类型的计划：</p><ul><li><strong>逻辑计划（Logical Plan）</strong>：定义数据集上的计算，尚未定义如何去执行计算。每个逻辑计划定义了一系列的用户代码所需要的属性(查询字段)和约束(where条件)，但是不定义该如何执行。具体如下图所示：</li></ul><p><img src="//jiamaoxiang.top/2020/07/23/第三篇-Spark-SQL编程指南/%E9%80%BB%E8%BE%91%E8%AE%A1%E5%88%92.png" alt></p><ul><li><strong>物理计划(Physical Plan)</strong>:物理计划是从逻辑计划生成的，定义了如何执行计算，是可执行的。举个栗子：逻辑计划中的JOIN会被转换为物理计划中的sort merge JOIN。需要注意，Spark会生成多个物理计划，然后选择成本最低的物理计划。具体如下图所示：</li></ul><p><img src="//jiamaoxiang.top/2020/07/23/第三篇-Spark-SQL编程指南/%E7%89%A9%E7%90%86%E8%AE%A1%E5%88%92.png" alt></p><p>在Spark SQL中，所有的算子操作会被转换成AST(abstract syntax tree,抽象语法树)，然后将其传递给Catalyst优化器。该优化器是在Scala的函数式编程基础会上构建的，Catalyst支持基于规则的(rule-based)和基于成本的(cost-based)优化策略。</p><p>Spark SQL的查询计划包括4个阶段(见下图)：</p><p><img src="//jiamaoxiang.top/2020/07/23/第三篇-Spark-SQL编程指南/%E6%89%A7%E8%A1%8C.png" alt></p><ul><li>1.分析</li><li>2.逻辑优化</li><li>3.物理计划</li><li>4.生成代码，将查询部分编译成Java字节码</li></ul><p><strong>注意：</strong>在物理计划阶段，Catalyst会生成多个计划，并且会计算每个计划的成本，然后比较这些计划的成本的大小，即基于成本的策略。在其他阶段，都是基于规则的的优化策略。</p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p><strong>Unresolved Logical plan –&gt; Logical plan</strong>。Spark SQL的查询计划首先起始于由SQL解析器返回的AST，或者是由API构建的DataFrame对象。在这两种情况下，都会存在未处理的属性引用(某个查询字段可能不存在，或者数据类型错误)，比如查询语句:<code>SELECT col FROM sales</code>,关于字段<code>col</code>的类型，或者该字段是否是一个有效的字段，只有等到查看该<code>sales</code>表时才会清楚。当不能确定一个属性字段的类型或者没能够与输入表进行匹配时，称之为<code>未处理的</code>。Spark SQL使用Catalyst的规则以及Catalog对象(能够访问数据源的表信息)来处理这些属性。首先会构建一个<strong>Unresolved Logical Plan</strong>树，然后作用一系列的规则，最后生成Logical Plan。</p><h3 id="逻辑优化"><a href="#逻辑优化" class="headerlink" title="逻辑优化"></a>逻辑优化</h3><p><strong>Logical plan –&gt; Optimized Logical Plan</strong>。逻辑优化阶段使用基于规则的优化策略，比如谓词下推、投影裁剪等。经过一些列优化过后，生成优化的逻辑计划Optimized Logical Plan。</p><h3 id="物理计划"><a href="#物理计划" class="headerlink" title="物理计划"></a>物理计划</h3><p><strong>Optimized Logical Plan –&gt;physical Plan</strong>。在物理计划阶段，Spark SQL会将优化的逻辑计划生成多个物理执行计划，然后使用Cost Model计算每个物理计划的成本，最终选择一个物理计划。在这个阶段，如果确定一张表很小(可以持久化到内存)，Spark SQL会使用broadcast join。</p><p>需要注意的是，物理计划器也会使用基于规则的优化策略，比如将投影、过滤操作管道化一个Spark的map算子。此外，还会将逻辑计划阶段的操作推到数据源端(支持谓词下推、投影下推)。</p><h3 id="代码生成"><a href="#代码生成" class="headerlink" title="代码生成"></a>代码生成</h3><p>查询优化的最终阶段是生成Java字节码，使用<strong>Quasi quotes</strong>来完成这项工作的。</p><p>经过上面的分析，对Catalyst Optimizer有了初步的了解。关于Spark的其他组件是如何与Catalyst Optimizer交互的呢？具体如下图所示：</p><p><img src="//jiamaoxiang.top/2020/07/23/第三篇-Spark-SQL编程指南/%E5%85%B6%E4%BB%96%E7%BB%84%E4%BB%B6.png" alt></p><p>如上图所示：ML Pipelines, Structured streaming以及 GraphFrames都使用了DataFrame/Dataset<br>APIs，并且都得益于 Catalyst optimiser。</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="创建SparkSession"><a href="#创建SparkSession" class="headerlink" title="创建SparkSession"></a>创建SparkSession</h3><p>SparkSession是Dataset与DataFrame API的编程入口，从Spark2.0开始支持。用于统一原来的HiveContext和SQLContext，为了兼容两者，仍然保留这两个入口。通过一个SparkSession入口，提高了Spark的易用性。下面的代码展示了如何创建一个SparkSession：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .appName(<span class="string">"Spark SQL basic example"</span>)</span><br><span class="line">  .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">  .getOrCreate()</span><br><span class="line"><span class="comment">//导入隐式转换，比如将RDD转为DataFrame</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure><h3 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h3><p>创建完SparkSession之后，可以使用SparkSession从已经存在的RDD、Hive表或者其他数据源中创建DataFrame。下面的示例使用的是从一个JSON文件数据源中创建DataFrame：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* &#123;"name":"Michael"&#125;</span></span><br><span class="line"><span class="comment">* &#123;"name":"Andy", "age":30&#125;</span></span><br><span class="line"><span class="comment">* &#123;"name":"Justin", "age":19&#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">"E://people.json"</span>)</span><br><span class="line"><span class="comment">//输出DataFrame的内容</span></span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure><h3 id="DataFrame基本操作"><a href="#DataFrame基本操作" class="headerlink" title="DataFrame基本操作"></a>DataFrame基本操作</h3><p>创建完DataFrame之后，可以对其进行一些列的操作，具体如下面代码所示：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 打印该DataFrame的信息</span></span><br><span class="line">df.printSchema()</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">// |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 查询name字段</span></span><br><span class="line">df.select(<span class="string">"name"</span>).show()</span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"><span class="comment">// |   name|</span></span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"><span class="comment">// |Michael|</span></span><br><span class="line"><span class="comment">// |   Andy|</span></span><br><span class="line"><span class="comment">// | Justin|</span></span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 将每个人的age + 1</span></span><br><span class="line">df.select($<span class="string">"name"</span>, $<span class="string">"age"</span> + <span class="number">1</span>).show()</span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |   name|(age + 1)|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |Michael|     null|</span></span><br><span class="line"><span class="comment">// |   Andy|       31|</span></span><br><span class="line"><span class="comment">// | Justin|       20|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 查找age大于21的人员信息</span></span><br><span class="line">df.filter($<span class="string">"age"</span> &gt; <span class="number">21</span>).show()</span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// |age|name|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// | 30|Andy|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 按照age分组，统计每种age的个数</span></span><br><span class="line">df.groupBy(<span class="string">"age"</span>).count().show()</span><br><span class="line"><span class="comment">// +----+-----+</span></span><br><span class="line"><span class="comment">// | age|count|</span></span><br><span class="line"><span class="comment">// +----+-----+</span></span><br><span class="line"><span class="comment">// |  19|    1|</span></span><br><span class="line"><span class="comment">// |null|    1|</span></span><br><span class="line"><span class="comment">// |  30|    1|</span></span><br><span class="line"><span class="comment">// +----+-----+</span></span><br></pre></td></tr></table></figure><h3 id="在程序中使用SQL查询"><a href="#在程序中使用SQL查询" class="headerlink" title="在程序中使用SQL查询"></a>在程序中使用SQL查询</h3><p>上面的操作使用的是<strong>DSL(domain-specific language)</strong>方式，还可以直接使用SQL对DataFrame进行操作，具体如下所示：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 将DataFrame注册为SQL的临时视图</span></span><br><span class="line"><span class="comment">// 该方法创建的是一个本地的临时视图，生命周期与其绑定的SparkSession会话相关</span></span><br><span class="line"><span class="comment">// 即如果创建该view的session结束了，该view也就消失了</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"><span class="keyword">val</span> sqlDF = spark.sql(<span class="string">"SELECT * FROM people"</span>)</span><br><span class="line">sqlDF.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure><h3 id="Global-Temporary-View"><a href="#Global-Temporary-View" class="headerlink" title="Global Temporary View"></a>Global Temporary View</h3><p>上面使用的是Temporary views的方式，该方式是Spark Session范围的。如果将创建的view可以在所有session之间共享，可以使用Global Temporary View的方式创建view，具体如下：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 将DataFrame注册为全局临时视图(global temporary view)</span></span><br><span class="line"><span class="comment">// 该方法创建的是一个全局的临时视图，生命周期与其绑定的Spark应用程序相关，</span></span><br><span class="line"><span class="comment">// 即如果应用程序结束，会自动被删除</span></span><br><span class="line"><span class="comment">// 全局临时视图是可以跨Spark Session的，系统保留的数据库名为`global_temp`</span></span><br><span class="line"><span class="comment">// 当查询时，必须要加上全限定名，如`SELECT * FROM global_temp.view1`</span></span><br><span class="line">df.createGlobalTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 全局临时视图默认的保留数据库为:`global_temp` </span></span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 全局临时视图支持跨Spark Session会话</span></span><br><span class="line">spark.newSession().sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure><h3 id="创建DataSet"><a href="#创建DataSet" class="headerlink" title="创建DataSet"></a>创建DataSet</h3><p>DataSet与RDD很类似，但是，RDD使用的Java的序列化器或者Kyro序列化，而DataSet使用的是Encoder对在网络间传输的对象进行序列化的。创建DataSet的示例如下：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">创建DataSet</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">caseClassDS</span> </span>= <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Andy"</span>, <span class="number">32</span>)).toDS()</span><br><span class="line">caseClassDS.show()</span><br><span class="line"><span class="comment">// +----+---+</span></span><br><span class="line"><span class="comment">// |name|age|</span></span><br><span class="line"><span class="comment">// +----+---+</span></span><br><span class="line"><span class="comment">// |Andy| 32|</span></span><br><span class="line"><span class="comment">// +----+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过导入Spark的隐式转换spark.implicits._</span></span><br><span class="line"><span class="comment">// 可以自动识别数据类型</span></span><br><span class="line"><span class="keyword">val</span> primitiveDS = <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).toDS()</span><br><span class="line">primitiveDS.map(_ + <span class="number">1</span>).collect() <span class="comment">// 返回: Array(2, 3, 4)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过调用as方法，DataFrame可以转为DataSet，</span></span><br><span class="line"><span class="keyword">val</span> path = <span class="string">"E://people.json"</span></span><br><span class="line"><span class="keyword">val</span> peopleDS = spark.read.json(path).as[<span class="type">Person</span>]</span><br><span class="line">peopleDS.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure><h2 id="RDD与DataFrame相互转换"><a href="#RDD与DataFrame相互转换" class="headerlink" title="RDD与DataFrame相互转换"></a>RDD与DataFrame相互转换</h2><p>Spark SQL支持两种不同的方式将RDD转换为DataFrame。第一种是使用反射来推断包含特定类型对象的RDD的模式，这种基于反射的方式可以提供更简洁的代码，如果在编写Spark应用程序时，已经明确了schema，可以使用这种方式。第二种方式是通过可编程接口来构建schema，然后将其应用于现有的RDD。此方式编写的代码更冗长，此种方式创建的DataFrame，直到运行时才知道该DataFrame的列及其类型。</p><p>下面案例的数据集如下people.txt：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Tom, 29</span><br><span class="line">Bob, 30</span><br><span class="line">Jack, 19</span><br></pre></td></tr></table></figure><h3 id="通过反射的方式"><a href="#通过反射的方式" class="headerlink" title="通过反射的方式"></a>通过反射的方式</h3><p>Spark SQL的Scala接口支持自动将包含样例类的RDD转换为DataFrame。样例类定义表的schema。通过反射读取样例类的参数名称，并映射成column的名称。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">object RDD2DF_m1 &#123;</span><br><span class="line">  <span class="comment">//创建样例类</span></span><br><span class="line">  <span class="function"><span class="keyword">case</span> class  <span class="title">Person</span><span class="params">(name: String, age: Int)</span></span></span><br><span class="line"><span class="function">  def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"RDD2DF_m1"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    Logger.getLogger(<span class="string">"org.apache.spark"</span>).setLevel(Level.OFF)</span><br><span class="line">    Logger.getLogger(<span class="string">"org.apache.hadoop"</span>).setLevel(Level.OFF)</span><br><span class="line">    runRDD2DF(spark)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> def <span class="title">runRDD2DF</span><span class="params">(spark: SparkSession)</span> </span>= &#123;</span><br><span class="line">    <span class="comment">//导入隐式转换,用于RDD转为DataFrame</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">//从文本文件中创建RDD，并将其转换为DataFrame</span></span><br><span class="line">    val peopleDF = spark.sparkContext</span><br><span class="line">      .textFile(<span class="string">"file:///E:/people.txt"</span>)</span><br><span class="line">      .map(_.split(<span class="string">","</span>))</span><br><span class="line">      .map(attributes =&gt; Person(attributes(<span class="number">0</span>), attributes(<span class="number">1</span>).trim.toInt))</span><br><span class="line">      .toDF()</span><br><span class="line">    <span class="comment">//将DataFrame注册成临时视图</span></span><br><span class="line">    peopleDF.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line">    <span class="comment">// 运行SQL语句</span></span><br><span class="line">    val teenagersDF = spark.sql(<span class="string">"SELECT name, age FROM people WHERE age BETWEEN 13 AND 19"</span>)</span><br><span class="line">    <span class="comment">// 使用字段索引访问列</span></span><br><span class="line">    teenagersDF.map(teenager =&gt; <span class="string">"Name: "</span> + teenager(<span class="number">0</span>)).show()</span><br><span class="line">    <span class="comment">// +----------+</span></span><br><span class="line">    <span class="comment">// |     value|</span></span><br><span class="line">    <span class="comment">// +----------+</span></span><br><span class="line">    <span class="comment">// |Name: Jack|</span></span><br><span class="line">    <span class="comment">// +----------+</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 通过字段名访问列</span></span><br><span class="line">    teenagersDF.map(teenager =&gt; <span class="string">"Name: "</span> + teenager.getAs[String](<span class="string">"name"</span>)).show()</span><br><span class="line">    <span class="comment">// +------------+</span></span><br><span class="line">    <span class="comment">// |       value|</span></span><br><span class="line">    <span class="comment">// +------------+</span></span><br><span class="line">    <span class="comment">// |Name: Jack|</span></span><br><span class="line">    <span class="comment">// +------------+</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="通过构建schema的方式"><a href="#通过构建schema的方式" class="headerlink" title="通过构建schema的方式"></a>通过构建schema的方式</h3><p>通过构建schema的方式创建DataFrame主要包括三步：</p><ul><li>1.从原始RDD创建Row类型的RDD</li><li>2.使用StructType，创建schema</li><li>3.通过createDataFrame方法将schema应用于Row类型的RDD </li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">object RDD2DF_m2 &#123;</span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"RDD2DF_m1"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    Logger.getLogger(<span class="string">"org.apache.spark"</span>).setLevel(Level.OFF)</span><br><span class="line">    Logger.getLogger(<span class="string">"org.apache.hadoop"</span>).setLevel(Level.OFF)</span><br><span class="line">    runRDD2DF(spark)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> def <span class="title">runRDD2DF</span><span class="params">(spark: SparkSession)</span> </span>= &#123;</span><br><span class="line">    <span class="comment">//导入隐式转换,用于RDD转为DataFrame</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">//创建原始RDD</span></span><br><span class="line">    val peopleRDD = spark.sparkContext.textFile(<span class="string">"E:/people.txt"</span>)</span><br><span class="line">    <span class="comment">//step 1 将原始RDD转换为ROW类型的RDD</span></span><br><span class="line">    val rowRDD = peopleRDD</span><br><span class="line">      .map(_.split(<span class="string">","</span>))</span><br><span class="line">      .map(attributes =&gt; Row(attributes(<span class="number">0</span>), attributes(<span class="number">1</span>).trim.toInt))</span><br><span class="line">    <span class="comment">//step 2 创建schema</span></span><br><span class="line">    val schema = StructType(Array(</span><br><span class="line">      StructField(<span class="string">"name"</span>, StringType, <span class="keyword">true</span>),</span><br><span class="line">      StructField(<span class="string">"age"</span>, IntegerType, <span class="keyword">true</span>)</span><br><span class="line">    ))</span><br><span class="line">    <span class="comment">//step 3 创建DF</span></span><br><span class="line">    val peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line">    <span class="comment">// 将DataFrame注册成临时视图</span></span><br><span class="line">    peopleDF.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line">    <span class="comment">// 运行SQL语句</span></span><br><span class="line">    val results = spark.sql(<span class="string">"SELECT name FROM people"</span>)</span><br><span class="line">    <span class="comment">// 使用字段索引访问列</span></span><br><span class="line">    results.map(attributes =&gt; <span class="string">"Name: "</span> + attributes(<span class="number">0</span>)).show()</span><br><span class="line">    <span class="comment">// +----------+</span></span><br><span class="line">    <span class="comment">// |     value|</span></span><br><span class="line">    <span class="comment">// +----------+</span></span><br><span class="line">    <span class="comment">// | Name: Tom|</span></span><br><span class="line">    <span class="comment">// | Name: Bob|</span></span><br><span class="line">    <span class="comment">// | Name: Jack|</span></span><br><span class="line">    <span class="comment">// +----------+</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Spark-SQL的数据源"><a href="#Spark-SQL的数据源" class="headerlink" title="Spark SQL的数据源"></a>Spark SQL的数据源</h2><p>Spark SQL支持通过DataFrame接口对各种数据源进行操作，可以使用关系转换以及临时视图对DataFrame进行操作。常见的数据源包括以下几种：</p><h3 id="文件数据源"><a href="#文件数据源" class="headerlink" title="文件数据源"></a>文件数据源</h3><ul><li><p>Parquet文件</p></li><li><p>JSON文件</p></li><li><p>CSV文件</p></li><li><p>ORC文件</p></li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runBasicDataSourceExample</span></span>(spark: <span class="type">SparkSession</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 读取parquet文件数据源,并将结果写入到parquet文件</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> usersDF = spark</span><br><span class="line">      .read</span><br><span class="line">      .load(<span class="string">"E://users.parquet"</span>)</span><br><span class="line">    usersDF.show()</span><br><span class="line">    <span class="comment">// 将DF保存到parquet文件</span></span><br><span class="line">    usersDF</span><br><span class="line">      .select(<span class="string">"name"</span>, <span class="string">"favorite_color"</span>)</span><br><span class="line">      .write</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">      .save(<span class="string">"E://namesAndFavColors.parquet"</span>)</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 读取json文件数据源,并将结果写入到parquet文件</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> peopleDF = spark</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">"json"</span>)</span><br><span class="line">      .load(<span class="string">"E://people.json"</span>)</span><br><span class="line">    peopleDF.show()</span><br><span class="line">    <span class="comment">// 将DF保存到parquet文件</span></span><br><span class="line">    peopleDF</span><br><span class="line">      .select(<span class="string">"name"</span>, <span class="string">"age"</span>)</span><br><span class="line">      .write</span><br><span class="line">      .format(<span class="string">"parquet"</span>)</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">      .save(<span class="string">"E://namesAndAges.parquet"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 读取CSV文件数据源</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    <span class="keyword">val</span> peopleDFCsv = spark.read.format(<span class="string">"csv"</span>)</span><br><span class="line">      .option(<span class="string">"sep"</span>, <span class="string">";"</span>)</span><br><span class="line">      .option(<span class="string">"inferSchema"</span>, <span class="string">"true"</span>)</span><br><span class="line">      .option(<span class="string">"header"</span>, <span class="string">"true"</span>)</span><br><span class="line">      .load(<span class="string">"E://people.csv"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 将usersDF写入到ORC文件</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    usersDF.write.format(<span class="string">"orc"</span>)</span><br><span class="line">      .option(<span class="string">"orc.bloom.filter.columns"</span>, <span class="string">"favorite_color"</span>)</span><br><span class="line">      .option(<span class="string">"orc.dictionary.key.threshold"</span>, <span class="string">"1.0"</span>)</span><br><span class="line">      .option(<span class="string">"orc.column.encoding.direct"</span>, <span class="string">"name"</span>)</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">      .save(<span class="string">"E://users_with_options.orc"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 将peopleDF保存为持久化表，一般保存为Hive中</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    peopleDF</span><br><span class="line">      .write</span><br><span class="line">      .option(<span class="string">"path"</span>,<span class="string">"E://warehouse/people_bucketed"</span>) <span class="comment">// 保存路径</span></span><br><span class="line">      .bucketBy(<span class="number">42</span>, <span class="string">"name"</span>)           <span class="comment">// 按照name字段分桶</span></span><br><span class="line">      .sortBy(<span class="string">"age"</span>)                  <span class="comment">// 按照age字段排序</span></span><br><span class="line">      .saveAsTable(<span class="string">"people_bucketed"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 将userDF保存为分区文件，类似于Hive分区表</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    usersDF</span><br><span class="line">      .write</span><br><span class="line">      .partitionBy(<span class="string">"favorite_color"</span>)  <span class="comment">// 分区字段</span></span><br><span class="line">      .format(<span class="string">"parquet"</span>)        <span class="comment">// 文件格式</span></span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>) <span class="comment">// 保存模式</span></span><br><span class="line">      .save(<span class="string">"E://namesPartByColor.parquet"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      *</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    usersDF</span><br><span class="line">      .write</span><br><span class="line">      .option(<span class="string">"path"</span>,<span class="string">"E://warehouse/users_partitioned_bucketed"</span>) <span class="comment">// 保存路径</span></span><br><span class="line">      .partitionBy(<span class="string">"favorite_color"</span>)  <span class="comment">// 分区</span></span><br><span class="line">      .bucketBy(<span class="number">42</span>, <span class="string">"name"</span>)           <span class="comment">// 分桶</span></span><br><span class="line">      .saveAsTable(<span class="string">"users_partitioned_bucketed"</span>)</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">"DROP TABLE IF EXISTS people_bucketed"</span>)</span><br><span class="line">    spark.sql(<span class="string">"DROP TABLE IF EXISTS users_partitioned_bucketed"</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p><strong>保存模式</strong></p><table><thead><tr><th align="left">Scala/Java</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><code>SaveMode.ErrorIfExists</code>(default)</td><td align="left">如果目标文件已经存在，则报异常</td></tr><tr><td align="left"><code>SaveMode.Append</code></td><td align="left">如果目标文件或表已经存在，则将结果追加进去</td></tr><tr><td align="left"><code>SaveMode.Overwrite</code></td><td align="left">如果目标文件或表已经存在，则覆盖原有的内容</td></tr><tr><td align="left"><code>SaveMode.Ignore</code></td><td align="left">类似于SQL中的CREATE TABLE IF NOT EXISTS，如果目标文件或表已经存在，则不做任何操作</td></tr></tbody></table><p><strong>保存为持久化表</strong></p><p>DataFrame可以被保存为Hive的持久化表，值得注意的是，这种方式并不依赖与Hive的部署，也就是说Spark会使用Derby创建一个默认的本地Hive metastore，与createOrReplaceTempView不同，该方式会直接将结果物化。</p><p>对于基于文件的数据源( text, parquet, json等)，在保存的时候可以指定一个具体的路径，比如 df.write.option(“path”, “/some/path”).saveAsTable(“t”)(存储在指定路径下的文件格式为parquet)<code>。</code>当表被删除时，自定义的表的路径和表数据不会被移除。如果没有指定具体的路径，spark默认的是warehouse的目录(/user/hive/warehouse),当表被删除时，默认的表路径也会被删除。</p><h3 id="Hive数据源"><a href="#Hive数据源" class="headerlink" title="Hive数据源"></a>Hive数据源</h3><p>见下面小节：Spark SQL集成Hive</p><h3 id="JDBC数据源"><a href="#JDBC数据源" class="headerlink" title="JDBC数据源"></a>JDBC数据源</h3><p>Spark SQL还包括一个可以使用JDBC从其他数据库读取数据的数据源。与使用JdbcRDD相比，应优先使用此功能。这是因为结果作为DataFrame返回，它们可以在Spark SQL中轻松处理或与其他数据源连接。JDBC数据源也更易于使用Java或Python，因为它不需要用户提供ClassTag。</p><p> 可以使用Data Sources API将远程数据库中的表加载为DataFrame或Spark SQL临时视图。用户可以在数据源选项中指定JDBC连接属性。 user并且password通常作为用于登录数据源的连接属性提供。除连接属性外，Spark还支持以下不区分大小写的选项：</p><table><thead><tr><th align="left">属性名称</th><th align="left">解释</th></tr></thead><tbody><tr><td align="left"><code>url</code></td><td align="left">要连接的JDBC URL</td></tr><tr><td align="left"><code>dbtable</code></td><td align="left">读取或写入的JDBC表</td></tr><tr><td align="left"><code>query</code></td><td align="left">指定查询语句</td></tr><tr><td align="left"><code>driver</code></td><td align="left">用于连接到该URL的JDBC驱动类名</td></tr><tr><td align="left"><code>partitionColumn, lowerBound, upperBound</code></td><td align="left">如果指定了这些选项，则必须全部指定。另外， <code>numPartitions</code>必须指定</td></tr><tr><td align="left"><code>numPartitions</code></td><td align="left">表读写中可用于并行处理的最大分区数。这也确定了并发JDBC连接的最大数量。如果要写入的分区数超过此限制，我们可以通过<code>coalesce(numPartitions)</code>在写入之前进行调用将其降低到此限制</td></tr><tr><td align="left"><code>queryTimeout</code></td><td align="left">默认为<code>0</code>，查询超时时间</td></tr><tr><td align="left"><code>fetchsize</code></td><td align="left">JDBC的获取大小，它确定每次要获取多少行。这可以帮助提高JDBC驱动程序的性能</td></tr><tr><td align="left"><code>batchsize</code></td><td align="left">默认为1000，JDBC批处理大小，这可以帮助提高JDBC驱动程序的性能。</td></tr><tr><td align="left"><code>isolationLevel</code></td><td align="left">事务隔离级别，适用于当前连接。它可以是一个<code>NONE</code>，<code>READ_COMMITTED</code>，<code>READ_UNCOMMITTED</code>，<code>REPEATABLE_READ</code>，或<code>SERIALIZABLE</code>，对应于由JDBC的连接对象定义，缺省值为标准事务隔离级别<code>READ_UNCOMMITTED</code>。此选项仅适用于写作。</td></tr><tr><td align="left"><code>sessionInitStatement</code></td><td align="left">在向远程数据库打开每个数据库会话之后，在开始读取数据之前，此选项将执行自定义SQL语句，使用它来实现会话初始化代码。</td></tr><tr><td align="left"><code>truncate</code></td><td align="left">这是与JDBC writer相关的选项。当<code>SaveMode.Overwrite</code>启用时，就会清空目标表的内容，而不是删除和重建其现有的表。默认为<code>false</code></td></tr><tr><td align="left"><code>pushDownPredicate</code></td><td align="left">用于启用或禁用谓词下推到JDBC数据源的选项。默认值为true，在这种情况下，Spark将尽可能将过滤器下推到JDBC数据源。</td></tr></tbody></table><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JdbcDatasetExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"JdbcDatasetExample"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>) <span class="comment">//设置为本地运行</span></span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.spark"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.hadoop"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line">    runJdbcDatasetExample(spark)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runJdbcDatasetExample</span></span>(spark: <span class="type">SparkSession</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//注意：从JDBC源加载数据</span></span><br><span class="line">    <span class="keyword">val</span> jdbcPersonDF = spark.read</span><br><span class="line">      .format(<span class="string">"jdbc"</span>)</span><br><span class="line">      .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://localhost/mydb"</span>)</span><br><span class="line">      .option(<span class="string">"dbtable"</span>, <span class="string">"person"</span>)</span><br><span class="line">      .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">      .option(<span class="string">"password"</span>, <span class="string">"123qwe"</span>)</span><br><span class="line">      .load()</span><br><span class="line">    <span class="comment">//打印jdbcDF的schema</span></span><br><span class="line">    jdbcPersonDF.printSchema()</span><br><span class="line">    <span class="comment">//打印数据</span></span><br><span class="line">    jdbcPersonDF.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> connectionProperties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    connectionProperties.put(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">    connectionProperties.put(<span class="string">"password"</span>, <span class="string">"123qwe"</span>)</span><br><span class="line">    <span class="comment">//通过.jdbc的方式加载数据</span></span><br><span class="line">    <span class="keyword">val</span> jdbcStudentDF = spark</span><br><span class="line">      .read</span><br><span class="line">      .jdbc(<span class="string">"jdbc:mysql://localhost/mydb"</span>, <span class="string">"student"</span>, connectionProperties)</span><br><span class="line">    <span class="comment">//打印jdbcDF的schema</span></span><br><span class="line">    jdbcStudentDF.printSchema()</span><br><span class="line">    <span class="comment">//打印数据</span></span><br><span class="line">    jdbcStudentDF.show()</span><br><span class="line">    <span class="comment">// 保存数据到JDBC源</span></span><br><span class="line">    jdbcStudentDF.write</span><br><span class="line">      .format(<span class="string">"jdbc"</span>)</span><br><span class="line">      .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://localhost/mydb"</span>)</span><br><span class="line">      .option(<span class="string">"dbtable"</span>, <span class="string">"student2"</span>)</span><br><span class="line">      .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">      .option(<span class="string">"password"</span>, <span class="string">"123qwe"</span>)</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Append</span>)</span><br><span class="line">      .save()</span><br><span class="line"></span><br><span class="line">    jdbcStudentDF</span><br><span class="line">      .write</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Append</span>)</span><br><span class="line">      .jdbc(<span class="string">"jdbc:mysql://localhost/mydb"</span>, <span class="string">"student2"</span>, connectionProperties)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Spark-SQL集成Hive"><a href="#Spark-SQL集成Hive" class="headerlink" title="Spark SQL集成Hive"></a>Spark SQL集成Hive</h2><p>Spark SQL还支持读取和写入存储在Apache Hive中的数据。但是，由于Hive具有大量依赖项，因此这些依赖项不包含在默认的Spark发布包中。如果可以在类路径上找到Hive依赖项，Spark将自动加载它们。请注意，这些Hive依赖项也必须存在于所有工作节点(worker nodes)上，因为它们需要访问Hive序列化和反序列化库（SerDes）才能访问存储在Hive中的数据。</p><p><strong>将hive-site.xml，core-site.xml以及hdfs-site.xml文件放在conf/下</strong>。</p><p>在使用Hive时，必须实例化一个支持Hive的SparkSession，包括连接到持久性Hive Metastore，支持Hive 的序列化、反序列化（serdes）和Hive用户定义函数。没有部署Hive的用户仍可以启用Hive支持。如果未配置hive-site.xml，则上下文(context)会在当前目录中自动创建metastore_db，并且会创建一个由spark.sql.warehouse.dir配置的目录，其默认目录为spark-warehouse，位于启动Spark应用程序的当前目录中。请注意，自Spark 2.0.0以来，该在hive-site.xml中的hive.metastore.warehouse.dir属性已被标记过时(deprecated)。使用spark.sql.warehouse.dir用于指定warehouse中的默认位置。可能需要向启动Spark应用程序的用户授予写入的权限。</p><p>​      下面的案例为在本地运行(为了方便查看打印的结果)，运行结束之后会发现在项目的目录下E:\IdeaProjects\myspark创建了spark-warehouse和metastore_db的文件夹。可以看出没有部署Hive的用户仍可以启用Hive支持，同时也可以将代码打包，放在集群上运行。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkHiveExample</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span>(<span class="params">key: <span class="type">Int</span>, value: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">def</span> <span class="title">main</span>(<span class="params">args: <span class="type">Array</span>[<span class="type">String</span>]</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"Spark Hive Example"</span>)</span><br><span class="line">      .config(<span class="string">"spark.sql.warehouse.dir"</span>, <span class="string">"e://warehouseLocation"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)<span class="comment">//设置为本地运行</span></span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.spark"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.hadoop"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">import</span> spark.sql</span><br><span class="line">    <span class="comment">//使用Spark SQL 的语法创建Hive中的表</span></span><br><span class="line">    sql(<span class="string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive"</span>)</span><br><span class="line">    sql(<span class="string">"LOAD DATA LOCAL INPATH 'file:///e:/kv1.txt' INTO TABLE src"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用HiveQL查询</span></span><br><span class="line">    sql(<span class="string">"SELECT * FROM src"</span>).show()</span><br><span class="line">    <span class="comment">// +---+-------+</span></span><br><span class="line">    <span class="comment">// |key|  value|</span></span><br><span class="line">    <span class="comment">// +---+-------+</span></span><br><span class="line">    <span class="comment">// |238|val_238|</span></span><br><span class="line">    <span class="comment">// | 86| val_86|</span></span><br><span class="line">    <span class="comment">// |311|val_311|</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 支持使用聚合函数</span></span><br><span class="line">    sql(<span class="string">"SELECT COUNT(*) FROM src"</span>).show()</span><br><span class="line">    <span class="comment">// +--------+</span></span><br><span class="line">    <span class="comment">// |count(1)|</span></span><br><span class="line">    <span class="comment">// +--------+</span></span><br><span class="line">    <span class="comment">// |    500 |</span></span><br><span class="line">    <span class="comment">// +--------+</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// SQL查询的结果是一个DataFrame，支持使用所有的常规的函数</span></span><br><span class="line">    <span class="keyword">val</span> sqlDF = sql(<span class="string">"SELECT key, value FROM src WHERE key &lt; 10 AND key &gt; 0 ORDER BY key"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// DataFrames是Row类型的, 允许你按顺序访问列.</span></span><br><span class="line">    <span class="keyword">val</span> stringsDS = sqlDF.map &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Row</span>(key: <span class="type">Int</span>, value: <span class="type">String</span>) =&gt; <span class="string">s"Key: <span class="subst">$key</span>, Value: <span class="subst">$value</span>"</span></span><br><span class="line">    &#125;</span><br><span class="line">    stringsDS.show()</span><br><span class="line">    <span class="comment">// +--------------------+</span></span><br><span class="line">    <span class="comment">// |               value|</span></span><br><span class="line">    <span class="comment">// +--------------------+</span></span><br><span class="line">    <span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line">    <span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line">    <span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//可以通过SparkSession使用DataFrame创建一个临时视图</span></span><br><span class="line">    <span class="keyword">val</span> recordsDF = spark.createDataFrame((<span class="number">1</span> to <span class="number">100</span>).map(i =&gt; <span class="type">Record</span>(i, <span class="string">s"val_<span class="subst">$i</span>"</span>)))</span><br><span class="line">    recordsDF.createOrReplaceTempView(<span class="string">"records"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//可以用DataFrame与Hive中的表进行join查询</span></span><br><span class="line">    sql(<span class="string">"SELECT * FROM records r JOIN src s ON r.key = s.key"</span>).show()</span><br><span class="line">    <span class="comment">// +---+------+---+------+</span></span><br><span class="line">    <span class="comment">// |key| value|key| value|</span></span><br><span class="line">    <span class="comment">// +---+------+---+------+</span></span><br><span class="line">    <span class="comment">// |  2| val_2|  2| val_2|</span></span><br><span class="line">    <span class="comment">// |  4| val_4|  4| val_4|</span></span><br><span class="line">    <span class="comment">// |  5| val_5|  5| val_5|</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建一个Parquet格式的hive托管表，使用的是HQL语法，没有使用Spark SQL的语法("USING hive")</span></span><br><span class="line">    sql(<span class="string">"CREATE TABLE IF NOT EXISTS hive_records(key int, value string) STORED AS PARQUET"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//读取Hive中的表，转换成了DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.table(<span class="string">"src"</span>)</span><br><span class="line">    <span class="comment">//将该DataFrame保存为Hive中的表，使用的模式(mode)为复写模式(Overwrite)</span></span><br><span class="line">    <span class="comment">//即如果保存的表已经存在，则会覆盖掉原来表中的内容</span></span><br><span class="line">    df.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).saveAsTable(<span class="string">"hive_records"</span>)</span><br><span class="line">    <span class="comment">// 查询表中的数据</span></span><br><span class="line">    sql(<span class="string">"SELECT * FROM hive_records"</span>).show()</span><br><span class="line">    <span class="comment">// +---+-------+</span></span><br><span class="line">    <span class="comment">// |key|  value|</span></span><br><span class="line">    <span class="comment">// +---+-------+</span></span><br><span class="line">    <span class="comment">// |238|val_238|</span></span><br><span class="line">    <span class="comment">// | 86| val_86|</span></span><br><span class="line">    <span class="comment">// |311|val_311|</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置Parquet数据文件路径</span></span><br><span class="line">    <span class="keyword">val</span> dataDir = <span class="string">"/tmp/parquet_data"</span></span><br><span class="line">    <span class="comment">//spark.range(10)返回的是DataSet[Long]</span></span><br><span class="line">    <span class="comment">//将该DataSet直接写入parquet文件</span></span><br><span class="line">    spark.range(<span class="number">10</span>).write.parquet(dataDir)</span><br><span class="line">    <span class="comment">// 在Hive中创建一个Parquet格式的外部表</span></span><br><span class="line">    sql(<span class="string">s"CREATE EXTERNAL TABLE IF NOT EXISTS hive_ints(key int) STORED AS PARQUET LOCATION '<span class="subst">$dataDir</span>'"</span>)</span><br><span class="line">    <span class="comment">// 查询上面创建的表</span></span><br><span class="line">    sql(<span class="string">"SELECT * FROM hive_ints"</span>).show()</span><br><span class="line">    <span class="comment">// +---+</span></span><br><span class="line">    <span class="comment">// |key|</span></span><br><span class="line">    <span class="comment">// +---+</span></span><br><span class="line">    <span class="comment">// |  0|</span></span><br><span class="line">    <span class="comment">// |  1|</span></span><br><span class="line">    <span class="comment">// |  2|</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 开启Hive动态分区</span></span><br><span class="line">    spark.sqlContext.setConf(<span class="string">"hive.exec.dynamic.partition"</span>, <span class="string">"true"</span>)</span><br><span class="line">    spark.sqlContext.setConf(<span class="string">"hive.exec.dynamic.partition.mode"</span>, <span class="string">"nonstrict"</span>)</span><br><span class="line">    <span class="comment">// 使用DataFrame API创建Hive的分区表</span></span><br><span class="line">    df.write.partitionBy(<span class="string">"key"</span>).format(<span class="string">"hive"</span>).saveAsTable(<span class="string">"hive_part_tbl"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分区键‘key’将会在最终的schema中被移除</span></span><br><span class="line">    sql(<span class="string">"SELECT * FROM hive_part_tbl"</span>).show()</span><br><span class="line">    <span class="comment">// +-------+---+</span></span><br><span class="line">    <span class="comment">// |  value|key|</span></span><br><span class="line">    <span class="comment">// +-------+---+</span></span><br><span class="line">    <span class="comment">// |val_238|238|</span></span><br><span class="line">    <span class="comment">// | val_86| 86|</span></span><br><span class="line">    <span class="comment">// |val_311|311|</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Thrift-server与Spark-SQL-CLI"><a href="#Thrift-server与Spark-SQL-CLI" class="headerlink" title="Thrift  server与Spark SQL CLI"></a>Thrift  server与Spark SQL CLI</h2><p>可以使用JDBC/ODBC或者命令行访问Spark SQL，通过这种方式，用户可以直接使用SQL运行查询，而不用编写代码。</p><h3 id="Thrift-JDBC-ODBC-server"><a href="#Thrift-JDBC-ODBC-server" class="headerlink" title="Thrift JDBC/ODBC server"></a>Thrift JDBC/ODBC server</h3><p>Thrift JDBC/ODBC server与Hive的HiveServer2向对应，可以使用Beeline访问JDBC服务器。在Spark的sbin目录下存在start-thriftserver.sh脚本，使用此脚本启动JDBC/ODBC服务器：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./sbin/start-thriftserver.sh</span><br></pre></td></tr></table></figure><p>使用beeline访问JDBC/ODBC服务器,Beeline会要求提供用户名和密码,在非安全模式下，只需输入用户名和空白密码即可</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">beeline&gt; !connect jdbc:hive2://localhost:10000</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2020/07/23/第三篇-Spark-SQL编程指南/thrift.png" alt></p><h3 id="Spark-SQL-CLI"><a href="#Spark-SQL-CLI" class="headerlink" title="Spark SQL CLI"></a>Spark SQL CLI</h3><p>Spark SQL CLI是在本地模式下运行Hive Metastore服务并执行从命令行输入的查询的便捷工具。请注意，Spark SQL CLI无法与Thrift JDBC服务器通信。</p><p>要启动Spark SQL CLI，只需要在Spark的bin目录中运行以下命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./spark-sql</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2020/07/23/第三篇-Spark-SQL编程指南/sparkSQL.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要对Spark SQL进行了阐述，主要包括Spark SQL的介绍、DataFrame&amp;DataSet API基本使用、Catalyst Optimizer优化器的基本原理、Spark SQL编程、Spark SQL数据源以及与Hive集成、Thrift  server与Spark SQL CLI。下一篇将分享Spark Streaming编程指南。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第二篇|Spark core编程指南</title>
      <link href="/2020/07/18/%E7%AC%AC%E4%BA%8C%E7%AF%87-Spark-core%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/"/>
      <url>/2020/07/18/%E7%AC%AC%E4%BA%8C%E7%AF%87-Spark-core%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>在<a href="https://mp.weixin.qq.com/s/e37gLo-Mq_93U12DmFDyPQ" target="_blank" rel="noopener">《第一篇|Spark概览》</a>一文中，对Spark的整体面貌进行了阐述。本文将深入探究Spark的核心组件–<strong>Spark core</strong>，Spark Core是Spark平台的基础通用执行引擎，所有其他功能均建立在该引擎之上。它不仅提供了内存计算功能来提高速度，而且还提供了通用的执行模型以支持各种应用程序，另外，用户可以使用Java，Scala和Python API开发应用程序。Spark core是建立在统一的抽象RDD之上的，这使得Spark的各个组件可以随意集成，可以在同一个应用程序中使用不同的组件以完成复杂的大数据处理任务。本文主要讨论的内容有：</p><ul><li>什么是RDD<ul><li>RDD的设计初衷</li><li>RDD的基本概念与主要特点</li><li>宽依赖与窄依赖</li><li>stage划分与作业调度</li></ul></li><li>RDD操作算子<ul><li>Transformations</li><li>Actions</li></ul></li><li>共享变量<ul><li>广播变量</li><li>累加器</li></ul></li><li>持久化</li><li>综合案例</li></ul><h2 id="什么是RDD"><a href="#什么是RDD" class="headerlink" title="什么是RDD"></a>什么是RDD</h2><h3 id="设计初衷"><a href="#设计初衷" class="headerlink" title="设计初衷"></a>设计初衷</h3><p>RDD(Resilient Distributed Datasets)的设计之初是为了解决目前存在的一些计算框架对于两类应用场景的处理效率不高的问题，这两类应用场景是<strong>迭代式算法</strong>和<strong>交互式数据挖掘</strong>。在这两种应用场景中，通过将数据保存在内存中，可以将性能提高到几个数量级。对于<strong>迭代式算法</strong>而言，比如PageRank、K-means聚类、逻辑回归等，经常需要重用中间结果。另一种应用场景是<strong>交互式数据挖掘</strong>，比如在同一份数据集上运行多个即席查询。大部分的计算框架(比如Hadoop)，使用中间计算结果的方式是将其写入到一个外部存储设备(比如HDFS)，这会增加额外的负载(数据复制、磁盘IO和序列化)，由此会增加应用的执行时间。</p><p>RDD可以有效地支持多数应用中的数据重用，它是一种容错的、并行的数据结构，可以让用户显性地将中间结果持久化到内存中，并且可以通过分区来优化数据的存放，另外，RDD支持丰富的算子操作，用户可以很容易地使用这些算子对RDD进行操作。</p><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>一个RDD是一个分布式对象集合，其本质是一个只读的、分区的记录集合。每个RDD可以分成多个分区，不同的分区保存在不同的集群节点上(<strong>具体如下图所示</strong>)。RDD是一种高度受限的共享内存模型，即RDD是只读的分区记录集合，所以也就不能对其进行修改。只能通过两种方式创建RDD，一种是基于物理存储的数据创建RDD，另一种是通过在其他RDD上作用转换操作(transformation，比如map、filter、join等)得到新的RDD。</p><p><img src="//jiamaoxiang.top/2020/07/18/第二篇-Spark-core编程指南/RDD%E5%88%86%E5%B8%83.png" alt></p><p>RDD不需要被物化，它通过血缘关系(lineage)来确定其是从RDD计算得来的。另外，用户可以控制RDD的<strong>持久化</strong>和<strong>分区</strong>，用户可以将需要被重用的RDD进行持久化操作(比如内存、或者磁盘)以提高计算效率。也可以按照记录的key将RDD的元素分布在不同的机器上，比如在对两个数据集进行JOIN操作时，可以确保以相同的方式进行hash分区。</p><h3 id="主要特点"><a href="#主要特点" class="headerlink" title="主要特点"></a>主要特点</h3><ul><li><p><strong>基于内存</strong></p><p>RDD是位于内存中的对象集合。RDD可以存储在内存、磁盘或者内存加磁盘中，但是，Spark之所以速度快，是基于这样一个事实：数据存储在内存中，并且每个算子不会从磁盘上提取数据。</p></li><li><p><strong>分区</strong></p><p>分区是对逻辑数据集划分成不同的独立部分，分区是分布式系统性能优化的一种技术手段，可以减少网络流量传输，将相同的key的元素分布在相同的分区中可以减少shuffle带来的影响。RDD被分成了多个分区，这些分区分布在集群中的不同节点。</p></li><li><p><strong>强类型</strong></p><p>RDD中的数据是强类型的，当创建RDD的时候，所有的元素都是相同的类型，该类型依赖于数据集的数据类型。</p></li><li><p><strong>懒加载</strong></p><p>Spark的转换操作是懒加载模式，这就意味着只有在执行了action(比如count、collect等)操作之后，才会去执行一些列的算子操作。</p></li><li><p><strong>不可修改</strong></p><p>RDD一旦被创建，就不能被修改。只能从一个RDD转换成另外一个RDD。</p></li><li><p><strong>并行化</strong></p><p>RDD是可以被并行操作的，由于RDD是分区的，每个分区分布在不同的机器上，所以每个分区可以被并行操作。</p></li><li><p><strong>持久化</strong></p><p>由于RDD是懒加载的，只有action操作才会导致RDD的转换操作被执行，进而创建出相对应的RDD。对于一些被重复使用的RDD，可以对其进行持久化操作(比如将其保存在内存或磁盘中，Spark支持多种持久化策略)，从而提高计算效率。</p></li></ul><h3 id="宽依赖和窄依赖"><a href="#宽依赖和窄依赖" class="headerlink" title="宽依赖和窄依赖"></a>宽依赖和窄依赖</h3><p>RDD中不同的操作会使得不同RDD中的分区产不同的依赖，主要有两种依赖：<strong>宽依赖</strong>和<strong>窄依赖</strong>。宽依赖是指一个父RDD的一个分区对应一个子RDD的多个分区，窄依赖是指一个父RDD的分区对应与一个子RDD的分区，或者多个父RDD的分区对应一个子RDD分区。关于宽依赖与窄依赖，如下图所示：</p><p><img src="//jiamaoxiang.top/2020/07/18/第二篇-Spark-core编程指南/%E4%BE%9D%E8%B5%961.png" alt></p><h3 id="Stage划分"><a href="#Stage划分" class="headerlink" title="Stage划分"></a>Stage划分</h3><p>窄依赖会被划分到同一个stage中，这样可以以管道的形式迭代执行。宽依赖所依赖的分区一般有多个，所以需要跨节点传输数据。从容灾方面看，两种依赖的计算结果恢复的方式是不同的，窄依赖只需要恢复父RDD丢失的分区即可，而宽依赖则需要考虑恢复所有父RDD丢失的分区。</p><p>DAGScheduler会将Job的RDD划分到不同的stage中，并构建一个stage的依赖关系，即DAG。这样划分的目的是既可以保障没有依赖关系的stage可以并行执行，又可以保证存在依赖关系的stage顺序执行。stage主要分为两种类型，一种是<strong>ShuffleMapStage</strong>，另一种是<strong>ResultStage</strong>。其中ShuffleMapStage是属于上游的stage，而ResulStage属于最下游的stage，这意味着上游的stage先执行，最后执行ResultStage。</p><ul><li>ShuffleMapStage</li></ul><p>ShuffleMapStage是DAG调度流程的中间stage，它可以包含一个或者多个ShuffleMapTask，用与生成Shuffle的数据，ShuffleMapStage可以是ShuffleMapStage的前置stage，但一定是ResultStage的前置stage。部分源码如下：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">ShuffleMapStage</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    id: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    rdd: <span class="type">RDD</span>[_],</span></span></span><br><span class="line"><span class="class"><span class="params">    numTasks: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    parents: <span class="type">List</span>[<span class="type">Stage</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">    firstJobId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    callSite: <span class="type">CallSite</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val shuffleDep: <span class="type">ShuffleDependency</span>[_, _, _],</span></span></span><br><span class="line"><span class="class"><span class="params">    mapOutputTrackerMaster: <span class="type">MapOutputTrackerMaster</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Stage</span>(<span class="params">id, rdd, numTasks, parents, firstJobId, callSite</span>) </span>&#123;</span><br><span class="line">      <span class="comment">// 省略代码</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>ResultStage</li></ul><p>ResultStage可以使用指定的函数对RDD中的分区进行计算并得到最终结果，ResultStage是最后执行的stage，比如打印数据到控制台，或者将数据写入到外部存储设备等。部分源码如下：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">ResultStage</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    id: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    rdd: <span class="type">RDD</span>[_],</span></span></span><br><span class="line"><span class="class"><span class="params">    val func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]</span>) <span class="title">=&gt;</span> <span class="title">_</span>,</span></span><br><span class="line"><span class="class">    <span class="title">val</span> <span class="title">partitions</span></span>: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">    parents: <span class="type">List</span>[<span class="type">Stage</span>],</span><br><span class="line">    firstJobId: <span class="type">Int</span>,</span><br><span class="line">    callSite: <span class="type">CallSite</span>)</span><br><span class="line">  <span class="keyword">extends</span> <span class="type">Stage</span>(id, rdd, partitions.length, parents, firstJobId, callSite) &#123;</span><br><span class="line"><span class="comment">// 省略代码</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面提到Spark通过分析各个RDD的依赖关系生成DAG，通过各个RDD中的分区之间的依赖关系来决定如何划分stage。具体的思路是：在DAG中进行反向解析，遇到宽依赖就断开、遇到窄依赖就把当前的RDD加入到当前的stage中。即将窄依赖划分到同一个stage中，从而形成一个pipeline，提升计算效率。所以一个DAG图可以划分为多个stage，每个stage都代表了一组关联的，相互之间没有shuffle依赖关系的任务组成的task集合，每个task集合会被提交到TaskScheduler进行调度处理，最终将任务分发到Executor中进行执行。</p><h3 id="Spark作业调度流程"><a href="#Spark作业调度流程" class="headerlink" title="Spark作业调度流程"></a>Spark作业调度流程</h3><p>Spark首先会对Job进行一系列的RDD转换操作，并通过RDD之间的依赖关系构建DAG(Direct Acyclic Graph,有向无环图)。然后根据RDD依赖关系将RDD划分到不同的stage中，每个stage按照partition的数量创建多个Task，最后将这些Task提交到集群的work节点上执行。具体流程如下图所示：</p><p><img src="//jiamaoxiang.top/2020/07/18/第二篇-Spark-core编程指南/%E8%B0%83%E5%BA%A6.png" alt></p><ul><li><p>1.构建DAG，将DAG提交到调度系统；</p></li><li><p>2.DAGScheduler负责接收DAG，并将DAG划分成多个stage，最后将每个stage中的Task以任务集合(TaskSet)的形式提交个TaskScheduler做下一步处理；</p></li><li><p>3.使用集群管理器分配资源与任务调度，对于失败的任务会有相应的重试机制。TaskScheduler负责从DAGScheduler接收TaskSet，然后会创建TaskSetManager对TaskSet进行管理，最后由SchedulerBackend对Task进行调度；</p></li><li><p>4.执行具体的任务，并将任务的中间结果和最终结果存入存储体系。</p></li></ul><h2 id="RDD操作算子"><a href="#RDD操作算子" class="headerlink" title="RDD操作算子"></a>RDD操作算子</h2><p>Spark提供了丰富的RDD操作算子，主要包括两大类：<strong>Transformation</strong>与<strong>Action</strong>，下面会对一些常见的算子进行说明。</p><h3 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h3><p>下面是一些常见的transformation操作，值得注意的是，对于普通的RDD，支持Scala、Java、Python和R的API，对于pairRDD，仅支持Scala和JavaAPI。下面将对一些常见的算子进行解释：</p><ul><li><strong>map</strong>(<em>func</em>)</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将每个元素传递到func函数中，并返回一个新的RDD</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.map(cleanF))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>filter</strong>(<em>func</em>)</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 筛选出满足func函数的元素，并返回一个新的RDD</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Boolean</span>): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">T</span>, <span class="type">T</span>](</span><br><span class="line">      <span class="keyword">this</span>,</span><br><span class="line">      (context, pid, iter) =&gt; iter.filter(cleanF),</span><br><span class="line">      preservesPartitioning = <span class="literal">true</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li><strong>flatMap</strong>(<em>func</em>)</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 首先对该RDD所有元素应用func函数，然后将结果打平，一个元素会映射到0或者多个元素，返回一个新RDD </span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.flatMap(cleanF))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li><strong>mapPartitions</strong>(<em>func</em>)</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 将func作用于该RDD的每个分区，返回一个新的RDD</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">      preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanedF = sc.clean(f)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>(</span><br><span class="line">      <span class="keyword">this</span>,</span><br><span class="line">      (context: <span class="type">TaskContext</span>, index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedF(iter),</span><br><span class="line">      preservesPartitioning)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li><strong>union</strong>(<em>otherDataset</em>)</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 返回一个新的RDD，包含两个RDD的元素，类似于SQL的UNION ALL</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">union</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    sc.union(<span class="keyword">this</span>, other)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li><strong>intersection</strong>(<em>otherDataset</em>)</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 返回一个新的RDD，包含两个RDD的交集</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">intersection</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">this</span>.map(v =&gt; (v, <span class="literal">null</span>)).cogroup(other.map(v =&gt; (v, <span class="literal">null</span>)))</span><br><span class="line">        .filter &#123; <span class="keyword">case</span> (_, (leftGroup, rightGroup)) =&gt; leftGroup.nonEmpty &amp;&amp; rightGroup.nonEmpty &#125;</span><br><span class="line">        .keys</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li><strong>distinct</strong>([<em>numPartitions</em>]))</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 返回一个新的RDD，对原RDD元素去重</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>(): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">   distinct(partitions.length)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><ul><li><strong>groupByKey</strong>([<em>numPartitions</em>])</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 将pairRDD按照key进行分组，该算子的性能开销较大，可以使用PairRDDFunctions.aggregateByKey</span></span><br><span class="line"><span class="comment">   *或者PairRDDFunctions.reduceByKey进行代替</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])] = self.withScope &#123;</span><br><span class="line">    groupByKey(defaultPartitioner(self))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li><strong>reduceByKey</strong>(<em>func</em>, [<em>numPartitions</em>])</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 使用reduce函数对每个key对应的值进行聚合，该算子会在本地先对每个mapper结果进行合并，然后再将结果发送到reducer，类似于MapReduce的combiner功能</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">    reduceByKey(defaultPartitioner(self), func)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li><strong>aggregateByKey</strong>(<em>zeroValue</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numPartitions</em>])</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 使用给定的聚合函数和初始值对每个key对应的value值进行聚合</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">aggregateByKey</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">V</span>) =&gt; <span class="type">U</span>,</span><br><span class="line">      combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)] = self.withScope &#123;</span><br><span class="line">    aggregateByKey(zeroValue, defaultPartitioner(self))(seqOp, combOp)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li><strong>sortByKey</strong>([<em>ascending</em>], [<em>numPartitions</em>])</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 按照key对RDD进行排序，所以每个分区的元素都是排序的</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sortByKey</span></span>(ascending: <span class="type">Boolean</span> = <span class="literal">true</span>, numPartitions: <span class="type">Int</span> = self.partitions.length)</span><br><span class="line">      : <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">val</span> part = <span class="keyword">new</span> <span class="type">RangePartitioner</span>(numPartitions, self, ascending)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](self, part)</span><br><span class="line">      .setKeyOrdering(<span class="keyword">if</span> (ascending) ordering <span class="keyword">else</span> ordering.reverse)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li><strong>join</strong>(<em>otherDataset</em>, [<em>numPartitions</em>])</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 将相同key的pairRDD JOIN在一起，返回(k, (v1, v2))tuple类型</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))] = self.withScope &#123;</span><br><span class="line">    join(other, defaultPartitioner(self, other))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numPartitions</em>])</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 将相同key的元素放在一组，返回的RDD类型为(K, (Iterable[V], Iterable[W1], Iterable[W2])</span></span><br><span class="line"><span class="comment">   * 第一个Iterable里面包含当前RDD的key对应的value值，第二个Iterable里面包含W1 RDD的key对应的    * value值，第三个Iterable里面包含W2 RDD的key对应的value值</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W1</span>, <span class="type">W2</span>](other1: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W1</span>)], other2: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W2</span>)], numPartitions: <span class="type">Int</span>)</span><br><span class="line">      : <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W1</span>], <span class="type">Iterable</span>[<span class="type">W2</span>]))] = self.withScope &#123;</span><br><span class="line">    cogroup(other1, other2, <span class="keyword">new</span> <span class="type">HashPartitioner</span>(numPartitions))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li><strong>coalesce</strong>(<em>numPartitions</em>)</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 该函数用于将RDD进行重分区，使用HashPartitioner。第一个参数为重分区的数目，第二个为是否进行      * shuffle，默认为false;</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coalesce</span></span>(numPartitions: <span class="type">Int</span>, shuffle: <span class="type">Boolean</span> = <span class="literal">false</span>,</span><br><span class="line">               partitionCoalescer: <span class="type">Option</span>[<span class="type">PartitionCoalescer</span>] = <span class="type">Option</span>.empty)</span><br><span class="line">              (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>)</span><br><span class="line">      : <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    require(numPartitions &gt; <span class="number">0</span>, <span class="string">s"Number of partitions (<span class="subst">$numPartitions</span>) must be positive."</span>)</span><br><span class="line">    <span class="keyword">if</span> (shuffle) &#123;</span><br><span class="line">      <span class="comment">/** Distributes elements evenly across output partitions, starting from a random partition. */</span></span><br><span class="line">      <span class="keyword">val</span> distributePartition = (index: <span class="type">Int</span>, items: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; &#123;</span><br><span class="line">        <span class="keyword">var</span> position = <span class="keyword">new</span> <span class="type">Random</span>(hashing.byteswap32(index)).nextInt(numPartitions)</span><br><span class="line">        items.map &#123; t =&gt;</span><br><span class="line">          <span class="comment">// Note that the hash code of the key will just be the key itself. The HashPartitioner</span></span><br><span class="line">          <span class="comment">// will mod it with the number of total partitions.</span></span><br><span class="line">          position = position + <span class="number">1</span></span><br><span class="line">          (position, t)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; : <span class="type">Iterator</span>[(<span class="type">Int</span>, <span class="type">T</span>)]</span><br><span class="line"></span><br><span class="line">      <span class="comment">// include a shuffle step so that our upstream tasks are still distributed</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">CoalescedRDD</span>(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">Int</span>, <span class="type">T</span>, <span class="type">T</span>](mapPartitionsWithIndex(distributePartition),</span><br><span class="line">        <span class="keyword">new</span> <span class="type">HashPartitioner</span>(numPartitions)),</span><br><span class="line">        numPartitions,</span><br><span class="line">        partitionCoalescer).values</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">CoalescedRDD</span>(<span class="keyword">this</span>, numPartitions, partitionCoalescer)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><ul><li><strong>repartition</strong>(<em>numPartitions</em>)</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 可以增加或者减少分区，底层调用的是coalesce方法。如果要减少分区，建议使用coalesce，因为可以避    * 免shuffle</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    coalesce(numPartitions, shuffle = <span class="literal">true</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h3><p>一些常见的action算子如下表所示</p><table><thead><tr><th><strong>操作</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td>count()</td><td>返回数据集中的元素个数</td></tr><tr><td>collect()</td><td>以数组的形式返回数据集中的所有元素</td></tr><tr><td>first()</td><td>返回数据集中的第一个元素</td></tr><tr><td>take(n)</td><td>以数组的形式返回数据集中的前n个元素</td></tr><tr><td>reduce(func)</td><td>通过函数func（输入两个参数并返回一个值）聚合数据集中的元素</td></tr><tr><td>foreach(func)</td><td>将数据集中的每个元素传递到函数func中运行</td></tr></tbody></table><h2 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h2><p>Spark提供了两种类型的共享变量：<strong>广播变量</strong>和<strong>累加器</strong>。广播变量(Broadcast variables)是一个只读的变量，并且在每个节点都保存一份副本，而不需要在集群中发送数据。累加器(Accumulators)可以将所有任务的数据累加到一个共享结果中。</p><h3 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h3><p>广播变量允许用户在集群中共享一个不可变的值，该共享的、不可变的值被持计划到集群的每台节点上。通常在需要将一份小数据集(比如维表)复制到集群中的每台节点时使用，比如日志分析的应用，web日志通常只包含pageId，而每个page的标题保存在一张表中，如果要分析日志(比如哪些page被访问的最多)，则需要将两者join在一起，这时就可以使用广播变量，将该表广播到集群的每个节点。具体如下图所示：</p><p><img src="//jiamaoxiang.top/2020/07/18/第二篇-Spark-core编程指南/%E5%B9%BF%E6%92%AD.png" alt></p><p>如上图，首先Driver将序列化对象分割成小的数据库，然后将这些数据块存储在Driver节点的BlockManager上。当ececutor中执行具体的task时，每个executor首先尝试从自己所在节点的BlockManager提取数据，如果之前已经提取的该广播变量的值，就直接使用它。如果没有找到，则会向远程的Driver或者其他的Executor中提取广播变量的值，一旦获取该值，就将其存储在自己节点的BlockManager中。这种机制可以避免Driver端向多个executor发送数据而造成的性能瓶颈。</p><p>基本使用方式如下：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 模拟一个数据集合</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> mockCollection = <span class="string">"Spark Flink Hadoop Hive"</span>.split(<span class="string">" "</span>)</span><br><span class="line">mockCollection: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="type">Spark</span>, <span class="type">Flink</span>, <span class="type">Hadoop</span>, <span class="type">Hive</span>)</span><br><span class="line"><span class="comment">// 构造RDD</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> words = sc.parallelize(mockCollection,<span class="number">2</span>)</span><br><span class="line">words: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">7</span>] at parallelize at &lt;console&gt;:<span class="number">29</span></span><br><span class="line"><span class="comment">// 模拟广播变量数据</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> mapData = <span class="type">Map</span>(<span class="string">"Spark"</span> -&gt; <span class="number">10</span>, <span class="string">"Flink"</span> -&gt; <span class="number">20</span>,<span class="string">"Hadoop"</span> -&gt; <span class="number">15</span>, <span class="string">"Hive"</span> -&gt; <span class="number">9</span>)</span><br><span class="line">mapData: scala.collection.immutable.<span class="type">Map</span>[<span class="type">String</span>,<span class="type">Int</span>] = <span class="type">Map</span>(<span class="type">Spark</span> -&gt; <span class="number">10</span>, <span class="type">Flink</span> -&gt; <span class="number">20</span>, <span class="type">Hadoop</span> -&gt; <span class="number">15</span>, <span class="type">Hive</span> -&gt; <span class="number">9</span>)</span><br><span class="line"><span class="comment">// 创建一个广播变量</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> broadCast = sc.broadcast(mapData)</span><br><span class="line">broadCast: org.apache.spark.broadcast.<span class="type">Broadcast</span>[scala.collection.immutable.<span class="type">Map</span>[<span class="type">String</span>,<span class="type">Int</span>]] = <span class="type">Broadcast</span>(<span class="number">4</span>)</span><br><span class="line"><span class="comment">// 在算子内部使用广播变量,根据key取出value值，按value升序排列</span></span><br><span class="line">scala&gt; words.map(word =&gt; (word,broadCast.value.getOrElse(word,<span class="number">0</span>))).sortBy(wordPair =&gt; wordPair._2).collect</span><br><span class="line">res5: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="type">Hive</span>,<span class="number">9</span>), (<span class="type">Spark</span>,<span class="number">10</span>), (<span class="type">Hadoop</span>,<span class="number">15</span>), (<span class="type">Flink</span>,<span class="number">20</span>))</span><br></pre></td></tr></table></figure><h3 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h3><p>累加器(Accumulator)是Spark提供的另外一个共享变量，与广播变量不同，累加器是可以被修改的，是可变的。每个transformation会将修改的累加器值传输到Driver节点，累加器可以实现一个累加的功能，类似于一个计数器。Spark本身支持数字类型的累加器，用户也可以自定义累加器的类型。</p><p><img src="//jiamaoxiang.top/2020/07/18/第二篇-Spark-core编程指南/%E7%B4%AF%E5%8A%A0.png" alt></p><h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><p>可以通过sparkContext.<code>longAccumulator()</code> 或者<code>SparkContext.doubleAccumulator()</code>分别创建Long和Double类型的累加器。运行在集群中的task可以调用add方法对该累加器变量进行累加，但是不能够读取累加器的值，只有Driver程序可以通过调用value方法读取累加器的值。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkAccumulator</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="type">SparkShareVariable</span>.getClass.getSimpleName)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.spark"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.hadoop"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line">    <span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>)</span><br><span class="line">    <span class="keyword">val</span> listRDD = sc.parallelize(list)</span><br><span class="line">    <span class="keyword">var</span> counter = <span class="number">0</span> <span class="comment">//外部变量</span></span><br><span class="line">    <span class="comment">//初始化一个accumulator，初始值默认为0</span></span><br><span class="line">    <span class="keyword">val</span> countAcc = sc.longAccumulator(<span class="string">"my accumulator"</span>)</span><br><span class="line">    <span class="keyword">val</span> mapRDD = listRDD.map(num =&gt; &#123;</span><br><span class="line">      counter += <span class="number">1</span> <span class="comment">//在算子内部使用了外部变量，这样操作不会改变外部变量的值</span></span><br><span class="line">      <span class="keyword">if</span> (num % <span class="number">3</span> == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">//遇到3的倍数，累加器+1</span></span><br><span class="line">        countAcc.add(<span class="number">1</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      num * <span class="number">2</span></span><br><span class="line">    &#125;)</span><br><span class="line">    mapRDD.foreach(println)</span><br><span class="line">    println(<span class="string">"counter = "</span> + counter) <span class="comment">// counter = 0</span></span><br><span class="line">    println(<span class="string">"countAcc = "</span> + countAcc.value) <span class="comment">// countAcc = 4</span></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>尖叫提示：</p><p>我们在dirver中声明的一些局部变量或者成员变量，可以直接在transformation中使用，但是经过transformation操作之后，是不会将最终的结果重新赋值给dirver中的对应的变量。因为通过action触发transformation操作之后，transformation的操作都是通过DAGScheduler将代码打包，然后序列化，最后交由TaskScheduler传送到各个Worker节点中的Executor去执行，在transformation中执行的这些变量，是自己节点上的变量，不是dirver上最初的变量，只不过是将driver上的对应的变量拷贝了一份而已。</p></blockquote><h3 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h3><p>Spark提供了一些默认类型的累加器，同时也支持自定义累加器。通过继承AccumulatorV2类即可实现自定义累加器，具体代码如下:</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">customAccumulator</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[<span class="type">BigInt</span>, <span class="type">BigInt</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> num:<span class="type">BigInt</span> = <span class="number">0</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 返回该accumulator是否为0值，比如一个计数器，0代表zero，如果是一个list，Nil代表zero </span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.num == <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 创建一个该accumulator副本</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">BigInt</span>, <span class="type">BigInt</span>] = &#123;</span><br><span class="line">    <span class="keyword">new</span> customAccumulator</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 重置accumulator的值, 该值为0，调用 `isZero` 必须返回true</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.num = <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 根据输入的值，进行累加，</span></span><br><span class="line">  <span class="comment">// 判断为偶数时，累加器加上该值</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(intVal: <span class="type">BigInt</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span>(intVal % <span class="number">2</span> == <span class="number">0</span>)&#123;</span><br><span class="line">      <span class="keyword">this</span>.num += intVal</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 合并其他的同一类型的accumulator，并更新该accumulator值</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">BigInt</span>, <span class="type">BigInt</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.num += other.value</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 定义当前accumulator的值</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: <span class="type">BigInt</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.num</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用该自定义累加器</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> acc = <span class="keyword">new</span> customAccumulator</span><br><span class="line"><span class="keyword">val</span> newAcc = sc.register(acc, <span class="string">"evenAcc"</span>)</span><br><span class="line">println(acc.value)</span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)).foreach(x =&gt; acc.add(x))</span><br><span class="line">println(acc.value)</span><br></pre></td></tr></table></figure><h2 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h2><h3 id="持久化方法"><a href="#持久化方法" class="headerlink" title="持久化方法"></a>持久化方法</h3><p>在Spark中，RDD采用惰性求值的机制，每次遇到action操作，都会从头开始执行计算。每次调用action操作，都会触发一次从头开始的计算。对于需要被重复使用的RDD，spark支持对其进行持久化，通过调用persist()或者cache()方法即可实现RDD的持计划。通过持久化机制可以避免重复计算带来的开销。值得注意的是，当调用持久化的方法时，只是对该RDD标记为了持久化，需要等到第一次执行action操作之后，才会把计算结果进行持久化。持久化后的RDD将会被保留在计算节点的内存中被后面的行动操作重复使用。</p><p>Spark提供的两个持久化方法的主要区别是：cache()方法默认使用的是内存级别，其底层调用的是persist()方法，具体源码片段如下：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (isLocallyCheckpointed) &#123;</span><br><span class="line">      persist(<span class="type">LocalRDDCheckpointData</span>.transformStorageLevel(newLevel), allowOverride = <span class="literal">true</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      persist(newLevel, allowOverride = <span class="literal">false</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 使用默认的存储级别持久化RDD (`MEMORY_ONLY`).</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 使用默认的存储级别持久化RDD (`MEMORY_ONLY`).</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist()</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 手动地把持久化的RDD从缓存中移除</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">unpersist</span></span>(blocking: <span class="type">Boolean</span> = <span class="literal">true</span>): <span class="keyword">this</span>.<span class="keyword">type</span> = &#123;</span><br><span class="line">    logInfo(<span class="string">"Removing RDD "</span> + id + <span class="string">" from persistence list"</span>)</span><br><span class="line">    sc.unpersistRDD(id, blocking)</span><br><span class="line">    storageLevel = <span class="type">StorageLevel</span>.<span class="type">NONE</span></span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="持计划存储级别"><a href="#持计划存储级别" class="headerlink" title="持计划存储级别"></a>持计划存储级别</h3><p>Spark的提供了多种持久化级别，比如内存、磁盘、内存+磁盘等。具体如下表所示：</p><table><thead><tr><th align="left">Storage Level</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left">MEMORY_ONLY</td><td align="left">默认，表示将RDD作为反序列化的Java对象存储于JVM中，如果内存不够用，则部分分区不会被持久化，等到使用到这些分区时，会重新计算。</td></tr><tr><td align="left">MEMORY_AND_DISK</td><td align="left">将RDD作为反序列化的Java对象存储在JVM中，如果内存不足，超出的分区将会被存放在硬盘上.</td></tr><tr><td align="left">MEMORY_ONLY_SER  (Java and Scala)</td><td align="left">将RDD序列化为Java对象进行持久化，每个分区对应一个字节数组。此方式比反序列化要节省空间，但是会占用更多cpu资源</td></tr><tr><td align="left">MEMORY_AND_DISK_SER  (Java and Scala)</td><td align="left">与 MEMORY_ONLY_SER, 如果内存放不下，则溢写到磁盘。</td></tr><tr><td align="left">DISK_ONLY</td><td align="left">将RDD的分区数据存储到磁盘</td></tr><tr><td align="left">MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.</td><td align="left">与上面的方式类似,但是会将分区数据复制到两个集群</td></tr><tr><td align="left">OFF_HEAP (experimental)</td><td align="left">与MEMORY_ONLY_SER类似,将数据存储到堆外内存 off-heap，需要将off-heap 开启</td></tr></tbody></table><h3 id="持久化级别的选择"><a href="#持久化级别的选择" class="headerlink" title="持久化级别的选择"></a>持久化级别的选择</h3><p>Spark提供的持久化存储级别是在<strong>内存使用</strong>与<strong>CPU效率</strong>之间做权衡，通常推荐下面的选择方式：</p><ul><li><p>如果内存可以容纳RDD，可以使用默认的持久化级别，即MEMORY_ONLY。这是CPU最有效率的选择，可以使作用在RDD上的算子尽可能第快速执行。</p></li><li><p>如果内存不够用，可以尝试使用MEMORY_ONLY_SER，使用一个快速的序列化库可以节省很多空间，比如 Kryo 。</p><blockquote><p>tips：在一些shuffle算子中，比如reduceByKey，即便没有显性调用persist方法，Spark也会自动将中间结果进行持久化，这样做的目的是避免在shuffle期间发生故障而造成重新计算整个输入。即便如此，还是推荐对需要被重复使用的RDD进行持久化处理。</p></blockquote></li></ul><h2 id="综合案例"><a href="#综合案例" class="headerlink" title="综合案例"></a>综合案例</h2><ul><li><strong>case 1</strong></li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *  1.数据集</span></span><br><span class="line"><span class="comment">  *          [orderId,userId,payment,productId]</span></span><br><span class="line"><span class="comment">  *          1,108,280,1002</span></span><br><span class="line"><span class="comment">  *          2,202,300,2004</span></span><br><span class="line"><span class="comment">  *          3,210,588,3241</span></span><br><span class="line"><span class="comment">  *          4,198,5000,3567</span></span><br><span class="line"><span class="comment">  *          5,200,590,2973</span></span><br><span class="line"><span class="comment">  *          6,678,8000,18378</span></span><br><span class="line"><span class="comment">  *          7,243,200,2819</span></span><br><span class="line"><span class="comment">  *          8,236,7890,2819</span></span><br><span class="line"><span class="comment">  *  2.需求描述</span></span><br><span class="line"><span class="comment">  *           计算Top3订单金额</span></span><br><span class="line"><span class="comment">  *           </span></span><br><span class="line"><span class="comment">  *  3.结果输出</span></span><br><span class="line"><span class="comment">  *     18000</span></span><br><span class="line"><span class="comment">  *     27890</span></span><br><span class="line"><span class="comment">  *          35000       </span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TopOrder</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"TopN"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    sc.setLogLevel(<span class="string">"ERROR"</span>)</span><br><span class="line">    <span class="keyword">val</span> lines = sc.textFile(<span class="string">"E://order.txt"</span>)</span><br><span class="line">    <span class="keyword">var</span> num = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">val</span> result = lines.filter(line =&gt; (line.trim().length &gt; <span class="number">0</span>) &amp;&amp; (line.split(<span class="string">","</span>).length == <span class="number">4</span>))</span><br><span class="line">      .map(_.split(<span class="string">","</span>)(<span class="number">2</span>))     <span class="comment">// 取出支付金额</span></span><br><span class="line">      .map(x =&gt; (x.toInt,<span class="string">""</span>))   </span><br><span class="line">      .sortByKey(<span class="literal">false</span>)         <span class="comment">// 按照支付金额降序排列    </span></span><br><span class="line">      .map(x =&gt; x._1).take(<span class="number">3</span>)   <span class="comment">// 取出前3个</span></span><br><span class="line">      .foreach(x =&gt; &#123;</span><br><span class="line">        num = num + <span class="number">1</span></span><br><span class="line">        println(num + <span class="string">"\t"</span> + x)</span><br><span class="line">      &#125;)</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>case 2</strong></li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 1.数据集(movielensSet)</span></span><br><span class="line"><span class="comment">  *        用户电影评分数据[UserID::MovieID::Rating::Timestamp]</span></span><br><span class="line"><span class="comment">  *        电影名称数据[MovieId::MovieName::MovieType]</span></span><br><span class="line"><span class="comment">  * 2.需求描述</span></span><br><span class="line"><span class="comment">  *        求平均评分大于5的电影名称</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MovieRating</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"MovieRating"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    sc.setLogLevel(<span class="string">"ERROR"</span>)</span><br><span class="line">    <span class="comment">// 用户电影评分数据[UserID::MovieID::Rating::Timestamp]</span></span><br><span class="line">    <span class="keyword">val</span> userRating = sc.textFile(<span class="string">"E://ml-1m/ratings.dat"</span>)</span><br><span class="line">    <span class="comment">// 电影名称数据[MovieId::MovieName::MovieType]</span></span><br><span class="line">    <span class="keyword">val</span> movies = sc.textFile(<span class="string">"E://ml-1m/movies.dat"</span>)</span><br><span class="line">    <span class="comment">//提取电影id和评分,(MovieID, Rating)</span></span><br><span class="line">    <span class="keyword">val</span> movieRating = userRating.map &#123; line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> rating = line.split(<span class="string">"::"</span>)</span><br><span class="line">      (rating(<span class="number">1</span>).toInt, rating(<span class="number">2</span>).toDouble)</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 计算电影id及其平均评分,(MovieId,AvgRating)</span></span><br><span class="line">    <span class="keyword">val</span> movieAvgRating = movieRating</span><br><span class="line">      .groupByKey()</span><br><span class="line">      .map &#123; rating =&gt;</span><br><span class="line">          <span class="keyword">val</span> avgRating = rating._2.sum / rating._2.size</span><br><span class="line">          (rating._1, avgRating)</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="comment">//提取电影id和电影名称,(MovieId,MovieName)</span></span><br><span class="line">   <span class="keyword">val</span> movieName =  movies.map &#123; movie =&gt;</span><br><span class="line">        <span class="keyword">val</span> fields = movie.split(<span class="string">"::"</span>)</span><br><span class="line">        (fields(<span class="number">0</span>).toInt, fields(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    &#125;.keyBy(_._1)</span><br><span class="line"></span><br><span class="line">    movieAvgRating</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line">      .join(movieName) <span class="comment">// Join的结果(MovieID,((MovieID,AvgRating),(MovieID,MovieName)))</span></span><br><span class="line">      .filter(joinData =&gt; joinData._2._1._2 &gt; <span class="number">5.0</span>)</span><br><span class="line">      .map(rs =&gt; (rs._1,rs._2._1._2,rs._2._2._2))</span><br><span class="line">      .saveAsTextFile(<span class="string">"E:/MovieRating/"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文对Spark Core进行了详细讲解，主要包括RDD的基本概念、RDD的操作算子、共享变量以及持计划，最后给出两个完整的Spark Core编程案例。下一篇将分享Spark SQL编程指南。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一篇|Spark概览</title>
      <link href="/2020/07/14/%E7%AC%AC%E4%B8%80%E7%AF%87-Spark%E6%A6%82%E8%A7%88/"/>
      <url>/2020/07/14/%E7%AC%AC%E4%B8%80%E7%AF%87-Spark%E6%A6%82%E8%A7%88/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>Apache Spark最初在2009年诞生于美国加州大学伯克利分校的APM实验室，并于2010年开源，如今是Apache软件基金会下的顶级开源项目之一。Spark的目标是设计一种编程模型，能够快速地进行数据分析。Spark提供了内存计算，减少了IO开销。另外Spark是基于Scala编写的，提供了交互式的编程体验。经过10年的发展，Spark成为了炙手可热的大数据处理平台，目前最新的版本是Spark3.0。本文主要是对Spark进行一个总体概览式的介绍,后续内容会对具体的细节进行展开讨论。本文的主要内容包括：</p><ul><li><input checked disabled type="checkbox"> Spark的关注度分析</li><li><input checked disabled type="checkbox"> Spark的特点</li><li><input checked disabled type="checkbox"> Spark的一些重要概念</li><li><input checked disabled type="checkbox"> Spark组件概览</li><li><input checked disabled type="checkbox"> Spark运行架构概览</li><li><input checked disabled type="checkbox"> Spark编程初体验</li></ul><h2 id="Spark的关注热度分析"><a href="#Spark的关注热度分析" class="headerlink" title="Spark的关注热度分析"></a>Spark的关注热度分析</h2><h3 id="概况"><a href="#概况" class="headerlink" title="概况"></a>概况</h3><p>下图展示了近1年内在国内关于Spark、Hadoop及Flink的搜索趋势</p><p><img src="//jiamaoxiang.top/2020/07/14/第一篇-Spark概览/%E8%B6%8B%E5%8A%BF.png" alt></p><p>近1年内全球关于Spark、Hadoop及Flink的搜索趋势，如下：</p><p><img src="//jiamaoxiang.top/2020/07/14/第一篇-Spark概览/%E5%85%A8%E7%90%83%E8%B6%8B%E5%8A%BF.png" alt></p><p>近1年国内关于Spark、Hadoop及Flink的搜索热度区域分布情况(按Flink搜索热度降序排列)：</p><p><img src="//jiamaoxiang.top/2020/07/14/第一篇-Spark概览/%E4%B8%AD%E5%9B%BD%E5%85%B3%E6%B3%A8%E6%83%85%E5%86%B5.png" alt></p><p>近1年全球关于Spark、Hadoop及Flink的搜索热度区域分布情况(按Flink搜索热度降序排列)：</p><p><img src="//jiamaoxiang.top/2020/07/14/第一篇-Spark概览/%E5%85%A8%E7%90%83%E5%85%B3%E6%B3%A8%E6%83%85%E5%86%B5.png" alt></p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>从上面的4幅图可以看出，近一年无论是在国内还是全球，关于Spark的搜索热度始终是比Hadoop和Flink要高。近年来Flink发展迅猛，其在国内有阿里的背书，Flink天然的流处理特点使其成为了开发流式应用的首选框架。可以看出，虽然Flink在国内很火，但是放眼全球，热度仍然不及Spark。所以学习并掌握Spark技术仍然是一个不错的选择，技术有很多的相似性，如果你已经掌握了Spark，再去学习Flink的话，相信你会有种似曾相识的感觉。</p><h2 id="Spark的特点"><a href="#Spark的特点" class="headerlink" title="Spark的特点"></a>Spark的特点</h2><ul><li><p>速度快</p><p>Apache Spark使用DAG调度程序、查询优化器和物理执行引擎，为批处理和流处理提供了高性能。</p></li><li><p>易于使用</p><p>支持使用Java，Scala，Python，R和SQL快速编写应用程序。Spark提供了80多个高级操作算子，可轻松构建并行应用程序。</p></li><li><p>通用性</p><p>Spark提供了非常丰富的生态栈，包括SQL查询、流式计算、机器学习和图计算等组件，这些组件可以无缝整合在一个应用中，通过一站部署，可以应对多种复杂的计算场景</p></li><li><p>运行模式多样</p><p>Spark可以使用Standalone模式运行，也可以运行在Hadoop，Apache Mesos，Kubernetes等环境中运行。并且可以访问HDFS、Alluxio、Apache Cassandra、Apache HBase、Apache Hive等多种数据源中的数据。</p></li></ul><h2 id="Spark的一些重要概念"><a href="#Spark的一些重要概念" class="headerlink" title="Spark的一些重要概念"></a>Spark的一些重要概念</h2><ul><li><p><strong>RDD</strong></p><p>弹性分布式数据集(Resilient Distributed Dataset)，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型</p></li><li><p><strong>DAG</strong></p><p>有向无环图(Directed Acyclic Graph),反映RDD之间的依赖关系</p></li><li><p><strong>Application</strong></p><p>  用户编写的Spark程序，由 driver program 和 <em>executors</em> 组成</p></li><li><p><strong>Application jar</strong><br>用户编写的应用程序JAR包</p></li><li><p><strong>Driver program</strong><br>用程序main()函数的进程，可以创建SparkContext</p></li><li><p><strong>Cluster manager</strong><br>集群管理器，属于一个外部服务，用于资源请求分配(如：standalone manager, Mesos, YARN)</p></li><li><p><strong>Deploy mode</strong></p><p>  部署模式，决定Driver进程在哪里运行。如果是<strong>cluster</strong>模式，会由框架本身在集群内部某台机器上启动Driver进程。如果是<strong>client</strong>模式，会在提交程序的机器上启动Driver进程</p></li><li><p><strong>Worker node</strong></p><p>  集群中运行应用程序的节点Executor运行在Worknode节点上的一个进程，负责运行具体的任务，并为应用程序存储数据</p></li><li><p><strong>Task</strong><br>运行在executor中的工作单元</p></li><li><p><strong>Job</strong><br>一个job包含多个RDD及一些列的运行在RDD之上的算子操作，job需要通过<strong>action</strong>操作进行触发(比如save、collect等)</p></li><li><p><strong>Stage</strong><br>每一个作业会被分成由一些列task组成的stage，stage之间会相互依赖</p></li></ul><h2 id="Spark组件概览"><a href="#Spark组件概览" class="headerlink" title="Spark组件概览"></a>Spark组件概览</h2><p>Spark生态系统主要包括Spark Core、SparkSQL、SparkStreaming、MLlib和GraphX等组件，具体如下图所示：</p><p><img src="//jiamaoxiang.top/2020/07/14/第一篇-Spark概览/spark%E7%BB%84%E4%BB%B6.png" alt></p><ul><li><p><strong>Spark Core</strong></p><p>Spark core是Spark的核心，包含了Spark的基本功能，如内存计算、任务调度、部署模式、存储管理等。SparkCore提供了基于RDD的API是其他高级API的基础，主要功能是实现批处理。</p></li><li><p><strong>Spark SQL</strong></p><p>Spark SQL主要是为了处理结构化和半结构化数据而设计的，SparkSQL允许用户在Spark程序中使用SQL、DataFrame和DataSetAPI查询结构化数据，支持Java、Scala、Python和R语言。由于DataFrame API提供了统一的访问各种数据源的方式(包括Hive、Avro、Parquet、ORC和JDBC)，用户可以通过相同的方式连接任何数据源。另外，Spark SQL可以使用hive的元数据，从而实现了与Hive的完美集成，用户可以将Hive的作业直接运行在Spark上。Spark SQL可以通过<strong>spark-sql</strong>的shell命令访问。</p></li><li><p><strong>SparkStreaming</strong></p><p>SparkStreaming是Spark很重要的一个模块，可实现实时数据流的可伸缩，高吞吐量，容错流处理。在内部，其工作方式是将实时输入的数据流拆分为一系列的micro batch，然后由Spark引擎进行处理。SparkStreaming支持多种数据源，如kafka、Flume和TCP套接字等</p></li><li><p><strong>MLlib</strong></p><p>MLlib是Spark提供的一个机器学习库，用户可以使用Spark API构建一个机器学习应用，Spark尤其擅长迭代计算，性能是Hadoop的100倍。该lib包含了常见机器学习算法，比如逻辑回归、支持向量机、分类、聚类、回归、随机森林、协同过滤、主成分分析等。</p></li><li><p><strong>GraphX</strong></p><p>GraphX是Spark中用于图计算的API，可认为是Pregel在Spark上的重写及优化，GraphX性能良好，拥有丰富的功能和运算符，能在海量数据上自如地运行复杂的图算法。GraphX内置了许多图算法，比如著名的PageRank算法。</p></li></ul><h2 id="Spark运行架构概览"><a href="#Spark运行架构概览" class="headerlink" title="Spark运行架构概览"></a>Spark运行架构概览</h2><p>从整体来看，Spark应用架构包括以下几个主要部分：</p><ul><li>Driver program</li><li>Master node</li><li>Work node</li><li>Executor</li><li>Tasks</li><li>SparkContext</li></ul><p>在<strong>Standalone</strong>模式下，运行架构如下图所示：</p><p><img src="//jiamaoxiang.top/2020/07/14/第一篇-Spark概览/%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84.png" alt></p><h3 id="Driver-program"><a href="#Driver-program" class="headerlink" title="Driver program"></a>Driver program</h3><p>Driver program是Spark应用程序的main()函数(创建SparkContext和Spark会话)。运行Driver进程的节点称之为Driver node，Driver进程与集群管理器(Cluster Manager)进行通信，向Executor发送调度的task。</p><h3 id="Cluster-Manager"><a href="#Cluster-Manager" class="headerlink" title="Cluster Manager"></a>Cluster Manager</h3><p>称之为集群管理器，主要用于管理集群。常见的集群管理器包括YARN、Mesos和Standalone，Standalone集群管理器包括两个长期运行的后台进程，其中一个是在Master节点，另外一个是在Work节点。在后续集群部署模式篇，将详细探讨这一部分的内容，此处先有有一个大致印象即可。</p><h3 id="Worker-node"><a href="#Worker-node" class="headerlink" title="Worker node"></a>Worker node</h3><p>熟悉Hadoop的朋友应该知道，Hadoop包括namenode和datanode节点。Spark也类似，Spark将运行具体任务的节点称之为Worker node。该节点会向Master节点汇报当前节点的可用资源，通常在每一台Worker node上启动一个work后台进程，用于启动和监控Executor。</p><h3 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h3><p>Master节点分配资源，使用集群中的Work node创建Executor，Driver使用这些Executor分配运行具体的Task。每一个应用程序都有自己的Executor进程，使用多个线程执行具体的Task。Executor主要负责运行任务和保存数据。</p><h3 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h3><p>Task是发送到Executor中的工作单元</p><h3 id="SparkContext"><a href="#SparkContext" class="headerlink" title="SparkContext"></a>SparkContext</h3><p>SparkContext是Spark会话的入口，用于连接Spark集群。在提交应用程序之前，首先需要初始化SparkContext，SparkContext隐含了网络通信、存储体系、计算引擎、WebUI等内容。值得注意的是，一个JVM进程中只能有一个SparkContext，如果想创建新的SparkContext，需要在原来的SparkContext上调用stop()方法。</p><h2 id="Spark编程小试牛刀"><a href="#Spark编程小试牛刀" class="headerlink" title="Spark编程小试牛刀"></a>Spark编程小试牛刀</h2><h3 id="Spark实现分组取topN案例"><a href="#Spark实现分组取topN案例" class="headerlink" title="Spark实现分组取topN案例"></a>Spark实现分组取topN案例</h3><p><strong>描述</strong>：在HDFS上有订单数据order.txt文件，文件字段的分割符号”,”，其中字段依次表示订单id，商品id，交易额。样本数据如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Order_00001,Pdt_01,222.8</span><br><span class="line">Order_00001,Pdt_05,25.8</span><br><span class="line">Order_00002,Pdt_03,522.8</span><br><span class="line">Order_00002,Pdt_04,122.4</span><br><span class="line">Order_00002,Pdt_05,722.4</span><br><span class="line">Order_00003,Pdt_01,222.8</span><br></pre></td></tr></table></figure><p><strong>问题</strong>：使用sparkcore，求每个订单中成交额最大的商品id</p><h3 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.hive.<span class="type">HiveContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TopOrderItemCluster</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"top n order and item"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> hctx = <span class="keyword">new</span> <span class="type">HiveContext</span>(sc)</span><br><span class="line">    <span class="keyword">val</span> orderData = sc.textFile(<span class="string">"data.txt"</span>)</span><br><span class="line">    <span class="keyword">val</span> splitOrderData = orderData.map(_.split(<span class="string">","</span>))</span><br><span class="line">    <span class="keyword">val</span> mapOrderData = splitOrderData.map &#123; arrValue =&gt;</span><br><span class="line">      <span class="keyword">val</span> orderID = arrValue(<span class="number">0</span>)</span><br><span class="line">      <span class="keyword">val</span> itemID = arrValue(<span class="number">1</span>)</span><br><span class="line">      <span class="keyword">val</span> total = arrValue(<span class="number">2</span>).toDouble</span><br><span class="line">      (orderID, (itemID, total))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> groupOrderData = mapOrderData.groupByKey()</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      ***groupOrderData.foreach(x =&gt; println(x))</span></span><br><span class="line"><span class="comment">      ***(Order_00003,CompactBuffer((Pdt_01,222.8)))</span></span><br><span class="line"><span class="comment">      ***(Order_00002,CompactBuffer((Pdt_03,522.8), (Pdt_04,122.4), (Pdt_05,722.4)))</span></span><br><span class="line"><span class="comment">      ***(Order_00001,CompactBuffer((Pdt_01,222.8), (Pdt_05,25.8)))</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">val</span> topOrderData = groupOrderData.map(tupleData =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> orderid = tupleData._1</span><br><span class="line">      <span class="keyword">val</span> maxTotal = tupleData._2.toArray.sortWith(_._2 &gt; _._2).take(<span class="number">1</span>)</span><br><span class="line">      (orderid, maxTotal)</span><br><span class="line">    &#125;</span><br><span class="line">    )</span><br><span class="line">    topOrderData.foreach(value =&gt;</span><br><span class="line">      println(<span class="string">"最大成交额的订单ID为："</span> + value._1 + <span class="string">" ,对应的商品ID为："</span> + value._2(<span class="number">0</span>)._1)</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">        ***最大成交额的订单ID为：Order_00003 ,对应的商品ID为：Pdt_01</span></span><br><span class="line"><span class="comment">        ***最大成交额的订单ID为：Order_00002 ,对应的商品ID为：Pdt_05</span></span><br><span class="line"><span class="comment">        ***最大成交额的订单ID为：Order_00001 ,对应的商品ID为：Pdt_01</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">      </span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//构造出元数据为Row的RDD</span></span><br><span class="line">    <span class="keyword">val</span> <span class="type">RowOrderData</span> = topOrderData.map(value =&gt; <span class="type">Row</span>(value._1, value._2(<span class="number">0</span>)._1))</span><br><span class="line">    <span class="comment">//构建元数据</span></span><br><span class="line">    <span class="keyword">val</span> structType = <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"orderid"</span>, <span class="type">StringType</span>, <span class="literal">false</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"itemid"</span>, <span class="type">StringType</span>, <span class="literal">false</span>))</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//转换成DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> orderDataDF = hctx.createDataFrame(<span class="type">RowOrderData</span>, structType)</span><br><span class="line">   <span class="comment">// 将数据写入Hive</span></span><br><span class="line">    orderDataDF.registerTempTable(<span class="string">"tmptable"</span>)</span><br><span class="line">    hctx.sql(<span class="string">"CREATE TABLE IF NOT EXISTS orderid_itemid(orderid STRING,itemid STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'"</span>)</span><br><span class="line">      hctx.sql(<span class="string">"INSERT INTO orderid_itemid SELECT * FROM tmptable"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将上述代码打包，提交到集群运行，可以进入hive cli或者spark-sql的shell查看Hive中的数据。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要从整体上对Spark进行了介绍，主要包括Spark的搜索热度分析、Spark的主要特点、Spark的一些重要概念以及Spark的运行架构，最后给出了一个Spark编程案例。本文是Spark系列分享的第一篇，可以先感受一下Spark的全局面貌，下一篇将分享Spark Core编程指南。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数仓|大数据时代,维度建模过时了吗?</title>
      <link href="/2020/07/11/%E6%95%B0%E4%BB%93-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E4%BB%A3-%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1%E8%BF%87%E6%97%B6%E4%BA%86%E5%90%97/"/>
      <url>/2020/07/11/%E6%95%B0%E4%BB%93-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E4%BB%A3-%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1%E8%BF%87%E6%97%B6%E4%BA%86%E5%90%97/</url>
      
        <content type="html"><![CDATA[<p>20世纪80年代末期，数据仓库技术兴起。自Ralph Kimball 于1996 年首次出版The Data Warehouse Toolkit(Wiley)一书以来，数据仓库和商业智能(Data Warehousing and Business Intelligence, DW/BI)行业渐趋成熟。Kimball提出了数据仓库的建模技术–<strong>维度建模(dimensional modelling)</strong>,该方法是在实践观察的基础上开发的。虽然它不基于任何理论，但是在实践中却非常成功。维度建模被视为设计数据仓库和数据集市的主要方法，对数据建模和数据库设计学科有着重要的影响。时至今日，维度建模依然是构建数仓首选的数据建模方法，但是随着技术的发展，获取超强的存储与计算能力的成本会变得很廉价。这在无形之中对传统的维度建模方法产生了一定的影响。本文将讨论以下内容：</p><ul><li>维度建模的概念</li><li>维度建模的优缺点</li><li>为什么星型模型依然有用</li><li>数据建模发生了哪些变化</li></ul><blockquote><p>规则是用来被打破的</p><p><em>First learn the rules, then break them</em></p></blockquote><h2 id="维度建模的概念"><a href="#维度建模的概念" class="headerlink" title="维度建模的概念"></a>维度建模的概念</h2><h3 id="事实表"><a href="#事实表" class="headerlink" title="事实表"></a>事实表</h3><p>事实表作为数据仓库维度建模的核心，紧紧围绕着业务过程来设计，通过获取描述业务过程的度量来表达业务过程，包含了引用的维度和与业务过程有关的度量。</p><p>事实表中一条记录所表达的业务细节程度被称为<strong>粒度</strong>。通常粒度可以通过两种方式来表述：一种是维度属性组合所表示的细节程度；一种是所表示的具体业务含义。</p><p>作为度量业务过程的事实，一般为整型或浮点型的十进制数值，有可加性、半可加性和不可加性三种类型。可加性事实是指可以按照与事实表关联的任意维度进行汇总。半可加性事实只能按照特定维度汇总，不能对所有维度汇总，比如库存可以按照地点和商品进行汇总，而按时间维度把一年中每个月的库存累加起来则毫无意义。还有一种度量完全不具备可加性，比如比率型事实。对于不可加性事实可分解为可加的组件来实现聚集。</p><p>事实表通常只有很少的列和很多行，是一种<strong>“瘦高”</strong>型的表。事实表定义为以下三种类型之一：</p><ul><li>事务事实表:记录有关特定事件的事实（例如，销售事件，保存在原子的粒度，也称为原子事实表）</li><li>周期快照事实表记录给定时间点的事实（例如，月末的帐户详细信息）</li><li>累积快照事实表记录了给定时间点的汇总事实（例如，某产品的当月迄今总销售额）</li></ul><h3 id="维表"><a href="#维表" class="headerlink" title="维表"></a>维表</h3><p>维度是维度建模的基础和灵魂。在维度建模中，将度量称为<strong>事实</strong>，将环境描述为<strong>维度</strong>，维度是用于分析事实所需要的多样环境。例如，在分析交易过程时，可以通过买家、卖家、商品和时间等维度描述交易发生的环境。维度所包含的表示维度的列，称为维度属性。维度属性是查询约束条件、分组和报表标签生成的基本来源，是数据易用性的关键。</p><p>维度通常是限定事实的描述性信息。例如，产品维度中的每个记录代表一个特定的产品。与事实表相比，维表通常具有相对较少的记录，但是每个记录可能具有大量的属性来描述事实数据。维度可以定义各种各样的特征，一些常见的维表：</p><ul><li>时间维度表：以最低时间粒度级别描述时间</li><li>地理维度表：描述了位置数据，例如国家/地区/城市</li><li>产品维度表：表描述了产品的详细信息</li><li>员工维度表：描述了员工，例如销售人员</li></ul><h3 id="星型模型"><a href="#星型模型" class="headerlink" title="星型模型"></a>星型模型</h3><p>大多数的数据仓库都采用星型模型。星型模型是由事实表和多个维表组成。事实表中存放大量关于企业的事实数据，元祖个数通常很大，而且非规范化程度很高。例如，多个时期的数据可能会出现在同一个表中。维表中通常存放描述性数据，维表是围绕事实表建立的，通常来说具有较少的行。如下图所示：</p><p><img src="//jiamaoxiang.top/2020/07/11/数仓-大数据时代-维度建模过时了吗/start_exam.png" alt></p><p>星型模型存取数据速度快，主要是针对各个维做了大量预处理，如按照维度进行预先的统计、分组合排序等。与规范化的关系型数据库设计相比，星型模型是非规范化的，通过数据冗余提升多维数据的查询速度，增加了存储空间的代价。当业务问题发生变化、原来的维度不能满足需求时，需要增加新的维度。由于事实表的主键由所有维表的主键组成，这种维的变化带来的数据变化将是非常复杂和耗时的。一个星型模型的示例：</p><p><img src="//jiamaoxiang.top/2020/07/11/数仓-大数据时代-维度建模过时了吗/%E6%98%9F%E5%9E%8B%E6%A8%A1%E5%9E%8B%E4%BE%8B%E5%AD%90.png" alt></p><h3 id="雪花模型"><a href="#雪花模型" class="headerlink" title="雪花模型"></a>雪花模型</h3><p>雪花模型是对星型模型的扩展，它将星型模型的维表进一步层次化，原来的各个维表可能被扩展为小的事实表，形成一些局部的层次区域。在雪花模型中能够定义多重父类维来描述某些特殊的维表，如在时间维上增加月维表和年维表，通过查看与时间有关的父类维，能够定义特殊的时间统计信息，如月统计和年统计等。</p><p><strong>雪花模式通过更多的连接引入了更多的复杂性。随着存储变得越来越廉价，大多数情况，一般不采用<code>雪花模型</code>方法。</strong></p><p>雪花模型的有点是最大限度地减少数据存储量，以及把较小的维表联合在一起来改善查询性能。但是它增加了用户必须处理的表的数量，增加了某些查询的复杂性。如下所示：</p><p><img src="//jiamaoxiang.top/2020/07/11/数仓-大数据时代-维度建模过时了吗/%E9%9B%AA%E8%8A%B1.png" alt></p><h2 id="维度建模的优缺点"><a href="#维度建模的优缺点" class="headerlink" title="维度建模的优缺点"></a>维度建模的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>每次需要从数据库中获取一些信息时，可以不用编写冗长的查询</li><li>针对读取进行了优化，可以写更少的JOIN，更快地返回结果</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>对数据进行非规范化意味着一次性插入或更新会导致数据异常。在实践中，星型模型是通过批处理实来弥补这一问题</li><li>分析灵活性有限。星型模型通常是为特定目的而设计的。在分析需求方面，它不像规范化数据模型那样灵活</li></ul><h2 id="为什么星型模型依然有用"><a href="#为什么星型模型依然有用" class="headerlink" title="为什么星型模型依然有用"></a>为什么星型模型依然有用</h2><h3 id="多种数据源"><a href="#多种数据源" class="headerlink" title="多种数据源"></a>多种数据源</h3><p>公司从各种数据源中收集越来越多的数据，因此需要对结果数据集进行整理以进行分析，从而减少异构数据源带来的分析复杂性。</p><h3 id="标准"><a href="#标准" class="headerlink" title="标准"></a>标准</h3><p>由Ralph Kimball编写的Data Warehouse Toolkit定义了业界广泛理解的概念。新员工可以快速掌握数据仓库的结构，而无需熟悉具体的业务系统数据。数据工程师和分析师通常对事实、维度、粒度这些概念比较了解，从而可以促进协作。</p><h3 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h3><p>新添加的事实表可以重用现有的维度。通过向事实表添加更多外键，实现向事实表添加新维度。另外，对于集成新的数据集，无需对模型进行重大调整。</p><h2 id="数据建模发生了哪些变化"><a href="#数据建模发生了哪些变化" class="headerlink" title="数据建模发生了哪些变化"></a>数据建模发生了哪些变化</h2><p>大数据时代，日新月异的技术发展促使存储和计算发生了天翻地覆的变化(存储和计算比以往任何时候都便宜)，因此数据模型也相应地发生了一些变化。</p><h4 id="缓慢变化维-SCD"><a href="#缓慢变化维-SCD" class="headerlink" title="缓慢变化维(SCD)"></a>缓慢变化维(SCD)</h4><p>对于随时间而变化的维度，比如：用户可以更改其家庭住址，产品可以更改名称。所以需要一种策略保存历史某个时间点对应的维度信息。</p><p>Kimball书中介绍了许多<strong>类型</strong>的SCD策略，大多数使用<em>UPDATE</em>就地添加或修改信息。在保留历史记录的维度中，当记录中的任何属性发生更改时，都需要复制整行数据,当属性经常更改时，同样会使用更多存储空间。值得注意的是，这些技术很复杂，因为它们是在严格的存储约束下设计的。</p><p>其实，可以使用维度快照来解决SCD的问题，虽然需要更多的存储空间，但创建和查询更简单。</p><h4 id="维度快照"><a href="#维度快照" class="headerlink" title="维度快照"></a>维度快照</h4><p>维度应比事实<strong>小得多</strong>。电子商务的交易，订单可能数以百万/千万计，但是客户（维度）的数量会少得多。</p><p>每天我们都会在版本快照中重新写入整个维度表</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/data_warehouse/dim_users/ds = 2020-01-01</span><br><span class="line">/data_warehouse/dim_users/ds = 2020-01-02 </span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>由于每天有一个快照数据，因此不管发生多少变化都没有影响。这种方式非常简单粗暴，但与复杂的不同类型的缓慢变化维策略相比，不失为一种可选的方案。</p><p>使用此种方式，可以通过JOIN特定日期的维度快照来获取历史某个时间点的维度信息。另外，这种方式不会对查询速度产生影响，因为通过分区日期可以直接定位选择的日期，而不是加载所有的数据。</p><p>系统地对维度进行快照（为每个ETL计划周期存储维度的完整副本，通常在不同的表分区中），作为处理缓慢变化的维度（SCD）的通用方法，是一种简单的通用方法，不需要太多的工程工作，并且与经典方法相比，在编写ETL和查询时很容易掌握。 复杂的SCD建模技术并不直观，并且会降低可访问性。</p><h3 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h3><p>为了避免上游数据处理错误导致事实表装载错误，需要从数据源系统中提取日期作为分区字段，这样可以实现数据装载的幂等性。此外，建议按 <strong>事件日期</strong> 进行分区，因为查询通常会将其用作过滤器（WHERE子句）。</p><h3 id="代理键与冗余维度"><a href="#代理键与冗余维度" class="headerlink" title="代理键与冗余维度"></a>代理键与冗余维度</h3><p>在维表中维护代理键是非常复杂的，除此之外，还会使事实表的可读性变差。在事实表中使用自然键和维度冗余的方式越来越普遍，这样可以减少JOIN带来的性能开销。值得注意的是，支持编码和压缩的序列化格式(如Parquet、ORC)解决了非规范化相关的大多数性能损失。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要介绍了维度建模的基本概念，包括维表、事实表、星型模型和雪花模型。其次对星型模型的优缺点进行了阐述。最后指出了维度建模正在发生的一些变化。</p>]]></content>
      
      
      <categories>
          
          <category> 数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数仓 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使SQL更易于阅读的几个小技巧</title>
      <link href="/2020/07/09/%E4%BD%BFSQL%E6%9B%B4%E6%98%93%E4%BA%8E%E9%98%85%E8%AF%BB%E7%9A%84%E5%87%A0%E4%B8%AA%E5%B0%8F%E6%8A%80%E5%B7%A7/"/>
      <url>/2020/07/09/%E4%BD%BFSQL%E6%9B%B4%E6%98%93%E4%BA%8E%E9%98%85%E8%AF%BB%E7%9A%84%E5%87%A0%E4%B8%AA%E5%B0%8F%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<p>无论是数仓开发还是数据分析，写一手好的SQL是一项基本的技能。毋庸置疑，编写性能较好的SQL是非常重要的，但是，SQL的可读性同样是不容小觑的。一个有着混乱格式的SQL脚本，往往需要花费较长的时间去弄清楚脚本的具体逻辑。如果你曾经被祖传的毫无章法的SQL脚本狂虐过，你一定心有感触。本文将分享几个SQL格式的规范，当然仁者见仁智者见智，其实没有严格的标准，如果有，那就是保证易于阅读和易于维护。</p><blockquote><p><strong>秦人不暇自哀，而后人哀之；后人哀之而不鉴之，亦使后人而复哀后人也</strong></p></blockquote><h2 id="大小写保持一致"><a href="#大小写保持一致" class="headerlink" title="大小写保持一致"></a>大小写保持一致</h2><p>可以对SQL关键字使用不同的大小写，但是要保持一致。看看这个：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> customer_city,<span class="keyword">count</span>(*) <span class="keyword">from</span> dim_customer <span class="keyword">WHERE</span> customerProvince = <span class="string">'上海'</span> <span class="keyword">Group</span> <span class="keyword">by</span> customer_city</span><br></pre></td></tr></table></figure><p>上面的SQL语句是不是很让人抓狂，大小写混用，看起来很不规范。总结起来，要注意下面几点：</p><ul><li><p>SQL的关键字可以大写，也可以小写，但是不要大小写混用。上面的SQL查询既有完全大写，也有首字母大写，更有小写。看似是不拘小节，但是万万使不得。</p></li><li><p>由于大小写是混合的，因此很难区分小写的关键字实际上是关键字还是列。此外，阅读也很烦人。</p></li><li><p>字段命名要保持一致的风格，上面的SQL与中<code>customer_city</code>是小写加下划线，而<code>customerProvince</code>字段是驼峰命名法，这种不一致性显然是不可取的。</p></li></ul><p>进行一些规范之后后，查询应如下所示：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> customer_city,</span><br><span class="line">       <span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> dim_customer</span><br><span class="line"><span class="keyword">WHERE</span> customer_province = <span class="string">'上海'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> customer_city</span><br></pre></td></tr></table></figure><h2 id="使用缩进"><a href="#使用缩进" class="headerlink" title="使用缩进"></a>使用缩进</h2><p>再来看看下面的一条查询语句：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> dp.region_name,<span class="keyword">count</span>(*) <span class="keyword">FROM</span> user_behavior_log ubl <span class="keyword">JOIN</span> dim_province dp <span class="keyword">ON</span> ubl.province = dp.province_name <span class="keyword">WHERE</span> ubl.province = <span class="string">'上海市'</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> dp.region_name</span><br></pre></td></tr></table></figure><p>将上面的SQL语句格式化下面的形式：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> dp.region_name, <span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> user_behavior_log ubl</span><br><span class="line"><span class="keyword">JOIN</span> dim_province dp <span class="keyword">ON</span> ubl.province = dp.province_name</span><br><span class="line"><span class="keyword">WHERE</span> ubl.province = <span class="string">'上海市'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> dp.region_name</span><br></pre></td></tr></table></figure><p>上面的格式化形式似乎清晰了很多，但是如果语句中包含了子查询、多个JOIN以及窗口函数时，同样会显得对阅读不是很友好。</p><p>再换一种格式化方式，如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    dp.region_name, </span><br><span class="line">    <span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> user_behavior_log ubl</span><br><span class="line">    <span class="keyword">JOIN</span> dim_province dp <span class="keyword">ON</span> ubl.province = dp.province_name</span><br><span class="line"><span class="keyword">WHERE</span> ubl.province = <span class="string">'上海市'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">    dp.region_name</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 或者下面的形式</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    dp.region_name </span><br><span class="line">    ,<span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> user_behavior_log ubl</span><br><span class="line">    <span class="keyword">JOIN</span> dim_province dp <span class="keyword">ON</span> ubl.province = dp.province_name</span><br><span class="line"><span class="keyword">WHERE</span> ubl.province = <span class="string">'上海市'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">    dp.region_name</span><br></pre></td></tr></table></figure><blockquote><p>尖叫提示：对于第二种形式，在SELECT字段中，从第二个字段开始，每个字段前面添加一个逗号，而不是每个字段后面使用逗号结尾。这种方式可以很方便地识别FROM前面是否存在逗号，从而造成语法错误。当然，这个只是个人习惯问题，并不是硬性的规定。</p><p>另外上面的SQL语句使用了4个字符缩进，当然也可以选择2个字符缩进，这个也是个人习惯问题。</p></blockquote><h2 id="在group-by-和order-by之后使用字段的排列序号"><a href="#在group-by-和order-by之后使用字段的排列序号" class="headerlink" title="在group by 和order by之后使用字段的排列序号"></a>在group by 和order by之后使用字段的排列序号</h2><p>同样，这种书写风格也是个人的一种偏好，并不是一条硬性规定。应该有很多的初学者对此种写法并不是很清楚。</p><p>看下面的这条SQL：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    dp.region_name, </span><br><span class="line">    dp.province_name,</span><br><span class="line">    <span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> user_behavior_log ubl</span><br><span class="line">    <span class="keyword">JOIN</span> dim_province dp <span class="keyword">ON</span> ubl.province = dp.province_name</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">    dp.region_name,</span><br><span class="line">    dp.province_name</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span></span><br><span class="line">    <span class="keyword">count</span>(*) <span class="keyword">desc</span> <span class="comment">-- Hive不支持</span></span><br></pre></td></tr></table></figure><p>可以写成下面的形式：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 注意：MySQL、Impala支持这种写法，Hive不支持</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    dp.region_name, </span><br><span class="line">    dp.province_name,</span><br><span class="line">    <span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> user_behavior_log ubl</span><br><span class="line">    <span class="keyword">JOIN</span> dim_province dp <span class="keyword">ON</span> ubl.province = dp.province_name</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="number">1</span>,<span class="number">2</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="number">3</span></span><br></pre></td></tr></table></figure><p>这样写有如下的好处：</p><ul><li><strong>可以节省行</strong>：通过许多字段进行分组不仅会在SELECT子句中添加更多行，还会在GROUP BY和ORDER BY子句中添加更多行，甚至可能使查询中的行数增加一倍。</li><li><strong>可维护性</strong>：如果想改变分组字段，只需在SELECT子句中进行操作，在GROUP BY语句中不需要修改。</li><li><strong>方便：</strong>只需要GROUP BY 1，2，3，…，n，其中n为分组列的字段序号。</li></ul><h2 id="使用Common-Table表达式-with语句"><a href="#使用Common-Table表达式-with语句" class="headerlink" title="使用Common Table表达式(with语句)"></a>使用Common Table表达式(with语句)</h2><p>该方式称之为<strong>Common Table Expressions(CTE)</strong>,用来简化复杂查询。它们可以定义为临时视图，因为它们仅在整个查询执行期间存在。</p><p>看一个简单的例子：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 注意Hive、Impala支持这种语法，低版本的MySQL不支持(高版本支持)</span></span><br><span class="line"><span class="keyword">WITH</span> employee_by_title_count <span class="keyword">AS</span> (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">        t.name <span class="keyword">as</span> job_title</span><br><span class="line">        , <span class="keyword">COUNT</span>(e.id) <span class="keyword">as</span> amount_of_employees</span><br><span class="line">    <span class="keyword">FROM</span> employees e</span><br><span class="line">        <span class="keyword">JOIN</span> job_titles t <span class="keyword">on</span> e.job_title_id = t.id</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="number">1</span></span><br><span class="line">),</span><br><span class="line">salaries_by_title <span class="keyword">AS</span> (</span><br><span class="line">     <span class="keyword">SELECT</span></span><br><span class="line">         <span class="keyword">name</span> <span class="keyword">as</span> job_title</span><br><span class="line">         , salary</span><br><span class="line">     <span class="keyword">FROM</span> job_titles</span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span> employee_by_title_count e</span><br><span class="line">    <span class="keyword">JOIN</span> salaries_by_title s <span class="keyword">ON</span> s.job_title = e.job_title</span><br></pre></td></tr></table></figure><p>上面的语句中，最终的查询使用<code>employee_by_title</code>和<code>salaries_by_title</code>的两个结果集进行JOIN产生最终结果。这比在SELECT子句中或直接在FROM子句中进行子查询更具可读性和可维护性。</p><h2 id="使用具有描述性的别名"><a href="#使用具有描述性的别名" class="headerlink" title="使用具有描述性的别名"></a>使用具有描述性的别名</h2><p>这一点非常重要，如果查询的列字段很多，肯能会存在一些id，count(*)等，很难辨识代表什么含义，所以需要为每个查询列加上可读的、易于理解的别名，能够让其他人一眼就能看出代表什么含义，这样可以增加脚本的可维护性。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>文中提到的一些规范有些是必须要遵守的，有些是个人的编码习惯，无论你是开发人员、数据分析师、数仓开发，遵循一些规范可以避免不必要的麻烦。值得注意的是，关于SQL的格式，没有一个标准的约定，需要与团队的其他成员达成共识，一起按照相同的约定进行开发，从而可以大大提高脚本的可读性和可维护性。</p>]]></content>
      
      
      <categories>
          
          <category> 数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数仓 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka的Controller Broker是什么</title>
      <link href="/2020/07/06/Kafka%E7%9A%84Controller-Broker%E6%98%AF%E4%BB%80%E4%B9%88/"/>
      <url>/2020/07/06/Kafka%E7%9A%84Controller-Broker%E6%98%AF%E4%BB%80%E4%B9%88/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p><strong>控制器组件（Controller），是 Apache Kafka 的核心组件。它的主要作用是在 Apache ZooKeeper 的帮助下管理和协调整个 Kafka 集群</strong>。集群中任意一台 Broker 都能充当控制器的角色，但是，在运行过程中，只能有一个 Broker 成为控制器，行使其管理和协调的职责。接下来，我们将讨论Controller原理和内部运行机制。通过本文你可以了解到：</p><ul><li>什么是Controller Broker</li><li>Controller Broker是怎么被选举的</li><li>Controller Broker主要作用是什么</li><li>Kafka是如何处理脑裂的</li></ul><h2 id="什么是Controller-Broker"><a href="#什么是Controller-Broker" class="headerlink" title="什么是Controller Broker"></a>什么是Controller Broker</h2><p>在分布式系统中，通常需要有一个协调者，该协调者会在分布式系统发生异常时发挥特殊的作用。在Kafka中该协调者称之为控制器(Controller),其实该控制器并没有什么特殊之处，它本身也是一个普通的Broker，只不过需要负责一些额外的工作(追踪集群中的其他Broker，并在合适的时候处理新加入的和失败的Broker节点、Rebalance分区、分配新的leader分区等)。值得注意的是：<strong>Kafka集群中始终只有一个Controller Broker。</strong></p><h2 id="Controller-Broker是如何被选出来的"><a href="#Controller-Broker是如何被选出来的" class="headerlink" title="Controller Broker是如何被选出来的"></a>Controller Broker是如何被选出来的</h2><p>上一小节解释了什么是Controller Broker，并且每台 Broker 都有充当控制器的可能性。那么，控制器是如何被选出来的呢？当集群启动后，Kafka 怎么确认控制器位于哪台 Broker 呢？</p><p>实际上，Broker 在启动时，会尝试去 ZooKeeper 中创建 /controller 节点。Kafka 当前选举控制器的规则是：<strong>第一个成功创建 /controller 节点的 Broker 会被指定为控制器</strong>。</p><h2 id="Controller-Broker的具体作用是什么"><a href="#Controller-Broker的具体作用是什么" class="headerlink" title="Controller Broker的具体作用是什么"></a>Controller Broker的具体作用是什么</h2><p>Controller Broker的主要职责有很多，主要是一些管理行为，主要包括以下几个方面：</p><ul><li>创建、删除主题，增加分区并分配leader分区</li><li>集群Broker管理（新增 Broker、Broker 主动关闭、Broker 故障)</li><li><strong>preferred leader</strong>选举</li><li>分区重分配</li></ul><h3 id="处理集群中下线的Broker"><a href="#处理集群中下线的Broker" class="headerlink" title="处理集群中下线的Broker"></a>处理集群中下线的Broker</h3><p>当某个Broker节点由于故障离开Kafka群集时，则存在于该Broker的leader分区将不可用(由于客户端仅对leader分区进行读写操作)。为了最大程度地减少停机时间，需要快速找到替代的leader分区。</p><p>Controller Broker可以对失败的Broker做出响应，Controller Broker可以从Zookeeper监听(zookeeper watch)中获取通知信息，ZooKeeper 赋予客户端监控 znode 变更的能力，即所谓的 Watch 通知功能。一旦 znode 节点被创建、删除，子节点数量发生变化，抑或是 znode 所存的数据本身变更，ZooKeeper 会通过节点变更监听器 (ChangeHandler) 的方式显式通知客户端。</p><p>每个 Broker 启动后，会在zookeeper的 /Brokers/ids 下创建一个临时 znode。当 Broker 宕机或主动关闭后，该 Broker 与 ZooKeeper 的会话结束，这个 znode 会被自动删除。同理，ZooKeeper 的 Watch 机制将这一变更推送给控制器，这样控制器就能知道有 Broker 关闭或宕机了，从而进行后续的协调操作。</p><p>Controller将收到通知并对此采取行动，决定哪些Broker上的分区成为leader分区，然后，它会通知每个相关的Broker，要么将Broker上的主题分区变成leader，要么通过<code>LeaderAndIsr</code>请求从新的leader分区中复制数据。</p><h3 id="处理新加入到集群中的Broker"><a href="#处理新加入到集群中的Broker" class="headerlink" title="处理新加入到集群中的Broker"></a>处理新加入到集群中的Broker</h3><p>通过将Leader分区副本均匀地分布在集群的不同Broker上，可以保障集群的负载均衡。在Broker发生故障时，某些Broker上的分区副本会被选举为leader，会造成一个Broker上存在多个leader分区副本的情况，由于客户端只与leader分区副本交互，所以这会给Broker增加额外的负担，并损害集群的性能和运行状况。因此，尽快恢复平衡对集群的健康运行是有益的。</p><p>Kafka认为leader分区副本最初的分配（每个节点都处于活跃状态）是均衡的。这些被最初选中的分区副本就是所谓的<strong>首选领导者(preferred leaders)</strong>。由于Kafka还支持<strong>机架感知的leader选举(rack-aware leader election</strong>) ,即尝试将leader分区和follower分区放置在不同的机架上，以增加对机架故障的容错能力。因此，leader分区副本的存在位置会对集群的可靠性产生影响。</p><p>默认情况下<strong>auto.leader.rebalance.enabled</strong>为true，表示允许 Kafka 定期地对一些 Topic 分区进行<br>Leader 重选举。大部分情况下，Broker的失败很短暂，这意味着Broker通常会在短时间内恢复。所以当节点离开群集时，与其相关联的元数据并不会被立即删除。</p><p>当Controller注意到Broker已加入集群时，它将使用Broker ID来检查该Broker上是否存在分区，如果存在，则Controller通知新加入的Broker和现有的Broker，新的Broker上面的follower分区再次开始复制现有leader分区的消息。为了保证负载均衡，Controller会将新加入的Broker上的follower分区选举为leader分区。</p><p><strong>注意</strong>：上面提到的选Leader分区，严格意义上是换Leader分区，为了达到负载均衡，可能会造成原来正常的Leader分区被强行变为follower分区。换一次 Leader 代价是很高的，原本向 Leader分区A(原Leader分区) 发送请求的所有客户端都要切换成向 B (新的Leader分区)发送请求，建议你在生产环境中把这个参数设置成 false。</p><h3 id="同步副本-in-sync-replica-ISR-列表"><a href="#同步副本-in-sync-replica-ISR-列表" class="headerlink" title="同步副本(in-sync replica ,ISR)列表"></a>同步副本(<strong>in-sync replica</strong> ,ISR)列表</h3><p>ISR中的副本都是与Leader进行同步的副本，所以不在该列表的follower会被认为与Leader是不同步的. 那么，ISR中存在是什么副本呢？首先可以明确的是：Leader副本总是存在于ISR中。 而follower副本是否在ISR中，取决于该follower副本是否与Leader副本保持了“同步”。</p><p>始终保证拥有足够数量的同步副本是非常重要的。要将follower提升为Leader，它必须存在于<strong>同步副本列表中</strong>。每个分区都有一个同步副本列表，该列表由Leader分区和Controller进行更新。</p><p>选择一个同步副本列表中的分区作为leader 分区的过程称为<strong>clean leader election</strong>。注意，这里要与在非同步副本中选一个分区作为leader分区的过程区分开，在非同步副本中选一个分区作为leader的过程称之为<strong>unclean leader election</strong>。由于ISR是动态调整的，所以会存在ISR列表为空的情况，通常来说，非同步副本落后 Leader 太多，因此，如果选择这些副本作为新 Leader，就可能出现数据的丢失。毕竟，这些副本中保存的消息远远落后于老 Leader 中的消息。在 Kafka 中，选举这种副本的过程可以通过Broker 端参数 *<em>unclean.leader.election.enable *</em>控制是否允许 Unclean 领导者选举。开启 Unclean 领导者选举可能会造成数据丢失，但好处是，它使得分区 Leader 副本一直存在，不至于停止对外提供服务，因此提升了高可用性。反之，禁止 Unclean Leader 选举的好处在于维护了数据的一致性，避免了消息丢失，但牺牲了高可用性。分布式系统的CAP理论说的就是这种情况。</p><p>不幸的是，<strong>unclean leader election</strong>的选举过程仍可能会造成数据的不一致，因为同步副本并不是<strong>完全</strong>同步的。由于复制是<strong>异步</strong>完成的，因此无法保证follower可以获取最新消息。比如Leader分区的最后一条消息的offset是100，此时副本的offset可能不是100，这受到两个参数的影响：</p><ul><li><strong>replica.lag.time.max.ms</strong>：同步副本滞后与leader副本的时间</li><li><strong>zookeeper.session.timeout.ms</strong>：与zookeeper会话超时时间</li></ul><h2 id="脑裂"><a href="#脑裂" class="headerlink" title="脑裂"></a>脑裂</h2><p>如果controller Broker 挂掉了，Kafka集群必须找到可以替代的controller，集群将不能正常运转。这里面存在一个问题，很难确定Broker是挂掉了，还是仅仅只是短暂性的故障。但是，集群为了正常运转，必须选出新的controller。如果之前被取代的controller又正常了，他并不知道自己已经被取代了，那么此时集群中会出现两台controller。</p><p>其实这种情况是很容易发生。比如，某个controller由于GC而被认为已经挂掉，并选择了一个新的controller。在GC的情况下，在最初的controller眼中，并没有改变任何东西，该Broker甚至不知道它已经暂停了。因此，它将继续充当当前controller，这是分布式系统中的常见情况，称为脑裂。</p><p><img src="//jiamaoxiang.top/2020/07/06/Kafka的Controller-Broker是什么/example.jpg" alt></p><p>假如，处于活跃状态的controller进入了长时间的GC暂停。它的ZooKeeper会话过期了，之前注册的<code>/controller</code>节点被删除。集群中其他Broker会收到zookeeper的这一通知。</p><p><img src="//jiamaoxiang.top/2020/07/06/Kafka的Controller-Broker是什么/%E8%84%91%E8%A3%821.png" alt></p><p>由于集群中必须存在一个controller Broker，所以现在每个Broker都试图尝试成为新的controller。假设Broker 2速度比较快，成为了最新的controller Broker。此时，每个Broker会收到Broker2成为新的controller的通知，由于Broker3正在进行”stop the world”的GC，可能不会收到Broker2成为最新的controller的通知。</p><p><img src="//jiamaoxiang.top/2020/07/06/Kafka的Controller-Broker是什么/%E8%84%91%E8%A3%822.png" alt></p><p>等到Broker3的GC完成之后，仍会认为自己是集群的controller，在Broker3的眼中好像什么都没有发生一样。</p><p><img src="//jiamaoxiang.top/2020/07/06/Kafka的Controller-Broker是什么/%E8%84%91%E8%A3%823.png" alt></p><p>现在，集群中出现了两个controller，它们可能一起发出具有冲突的命令，就会出现脑裂的现象。如果对这种情况不加以处理，可能会导致严重的不一致。所以需要一种方法来区分谁是集群当前最新的Controller。</p><p>Kafka是通过使用<strong>epoch number</strong>（纪元编号，也称为隔离令牌）来完成的。epoch number只是单调递增的数字，第一次选出Controller时，epoch number值为1，如果再次选出新的Controller，则epoch number将为2，依次单调递增。</p><p>每个新选出的controller通过Zookeeper 的条件递增操作获得一个全新的、数值更大的epoch number 。其他Broker 在知道当前epoch number 后，如果收到由controller发出的包含较旧(较小)epoch number的消息，就会忽略它们，即Broker根据最大的epoch number来区分当前最新的controller。</p><p><img src="//jiamaoxiang.top/2020/07/06/Kafka的Controller-Broker是什么/%E8%84%91%E8%A3%824.png" alt></p><p>上图，Broker3向Broker1发出命令:让Broker1上的某个分区副本成为leader，该消息的epoch number值为1。于此同时，Broker2也向Broker1发送了相同的命令，不同的是，该消息的epoch number值为2，此时Broker1只听从Broker2的命令(由于其epoch number较大)，会忽略Broker3的命令，从而避免脑裂的发生。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要讲解了什么是Kafka Controller，它其实就是一个普通的Broker，除了需要负责一些额外的工作之外，其角色与其他的Broker基本一样。另外还介绍了Kafka Controller的主要职责，并对其中的一些职责进行了详细解释，最后还说明了kafka是如何避免脑裂的。</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka生产者ack机制剖析</title>
      <link href="/2020/07/05/Kafka%E7%94%9F%E4%BA%A7%E8%80%85ack%E6%9C%BA%E5%88%B6%E5%89%96%E6%9E%90/"/>
      <url>/2020/07/05/Kafka%E7%94%9F%E4%BA%A7%E8%80%85ack%E6%9C%BA%E5%88%B6%E5%89%96%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>Kafka有两个很重要的配置参数，<code>acks</code>与<code>min.insync.replicas</code>.其中<code>acks</code>是producer的配置参数，<code>min.insync.replicas</code>是Broker端的配置参数，这两个参数对于生产者不丢失数据起到了很大的作用.接下来，本文会以图示的方式讲解这两个参数的含义和使用方式。通过本文，你可以了解到：</p><ul><li>Kafka的分区副本</li><li>什么是同步副本(In-sync replicas)</li><li>什么是acks确认机制</li><li>什么是最小同步副本</li><li>ack=all与最小同步副本是如何发挥作用的</li></ul><h2 id="分区副本"><a href="#分区副本" class="headerlink" title="分区副本"></a>分区副本</h2><p>Kafka的topic是可以分区的，并且可以为分区配置多个副本，改配置可以通过<code>replication.factor</code>参数实现. Kafka中的分区副本包括两种类型：领导者副本（Leader Replica）和追随者副本（Follower Replica)，每个分区在创建时都要选举一个副本作为领导者副本，其余的副本自动变为追随者副本. 在 Kafka 中，追随者副本是不对外提供服务的，也就是说，任何一个追随者副本都不能响应消费者和生产者的读写请求. 所有的请求都必须由领导者副本来处理. 换句话说，所有的读写请求都必须发往领导者副本所在的 Broker，由该 Broker 负责处理. 追随者副本不处理客户端请求，它唯一的任务就是从领导者副本<strong>异步拉取</strong>消息，并写入到自己的提交日志中，从而实现与领导者副本的同步.</p><p>Kafka默认的副本因子是3，即每个分区只有1个leader副本和2个follower副本.具体如下图所示：</p><p><img src="//jiamaoxiang.top/2020/07/05/Kafka生产者ack机制剖析/kafka%E5%89%AF%E6%9C%AC.png" alt></p><p>上面提到生产者客户端仅写入Leader broker，跟随者异步复制数据。由于Kafka是一个分布式系统，必然会存在与 Leader 不能实时同步的风险，所以需要一种方法来判断这些追随者是否跟上了领导者的步伐, 即追随者是否同步了最新的数据.换句话说，Kafka 要明确地告诉我们，追随者副本到底在什么条件下才算与 Leader 同步？这就是下面所要说的ISR同步副本机制.</p><h2 id="同步副本-In-sync-replicas"><a href="#同步副本-In-sync-replicas" class="headerlink" title="同步副本(In-sync replicas)"></a>同步副本(In-sync replicas)</h2><p>In-sync replica(ISR)称之为同步副本，ISR中的副本都是与Leader进行同步的副本，所以不在该列表的follower会被认为与Leader是不同步的. 那么，ISR中存在是什么副本呢？首先可以明确的是：Leader副本总是存在于ISR中. 而follower副本是否在ISR中，取决于该follower副本是否与Leader副本保持了“同步”.</p><blockquote><p>尖叫提示：对于”follower副本是否与Leader副本保持了同步”的理解如下：</p><p>(1)上面所说的同步不是指完全的同步，即并不是说一旦follower副本同步滞后与Leader副本，就会被踢出ISR列表.</p><p>(2)Kafka的broker端有一个参数<strong><code>replica.lag.time.max.ms</code></strong>, 该参数表示follower副本滞后与Leader副本的最长时间间隔，默认是10秒.  这就意味着，只要follower副本落后于leader副本的时间间隔不超过10秒，就可以认为该follower副本与leader副本是同步的，所以哪怕当前follower副本落后于Leader副本几条消息，只要在10秒之内赶上Leader副本，就不会被踢出出局.</p><p>（3）如果follower副本被踢出ISR列表，等到该副本追上了Leader副本的进度，该副本会被再次加入到ISR列表中，所以ISR是一个动态列表，并不是静态不变的。</p></blockquote><p><img src="//jiamaoxiang.top/2020/07/05/Kafka生产者ack机制剖析/ISR.png" alt></p><p>如上图所示：Broker3上的partition1副本超过了规定时间，未与Leader副本同步，所以被踢出ISR列表，此时的ISR为[1,3].</p><h2 id="acks确认机制"><a href="#acks确认机制" class="headerlink" title="acks确认机制"></a>acks确认机制</h2><p>acks参数指定了必须要有多少个分区副本收到消息，生产者才认为该消息是写入成功的，这个参数对于消息是否丢失起着重要作用，该参数的配置具体如下：</p><ul><li>acks=0，表示生产者在成功写入消息之前不会等待任何来自服务器的响应.  换句话说，一旦出现了问题导致服务器没有收到消息，那么生产者就无从得知，消息也就丢失了. 改配置由于不需要等到服务器的响应，所以可以以网络支持的最大速度发送消息，从而达到很高的吞吐量。</li></ul><p><img src="//jiamaoxiang.top/2020/07/05/Kafka生产者ack机制剖析/ack0.png" alt></p><ul><li><p>acks=1，表示只要集群的leader分区副本接收到了消息，就会向生产者发送一个成功响应的ack，此时生产者接收到ack之后就可以认为该消息是写入成功的. 一旦消息无法写入leader分区副本(比如网络原因、leader节点崩溃),生产者会收到一个错误响应，当生产者接收到该错误响应之后，为了避免数据丢失，会重新发送数据.这种方式的吞吐量取决于使用的是异步发送还是同步发送.</p><blockquote><p>尖叫提示：如果生产者收到了错误响应，即便是重新发消息，还是会有可能出现丢数据的现象. 比如，如果一个没有收到消息的节点成为了新的Leader，消息就会丢失.</p></blockquote></li></ul><p><img src="//jiamaoxiang.top/2020/07/05/Kafka生产者ack机制剖析/ack1.png" alt></p><ul><li>acks =all,表示只有所有参与复制的节点(ISR列表的副本)全部收到消息时，生产者才会接收到来自服务器的响应. 这种模式是最高级别的，也是最安全的，可以确保不止一个Broker接收到了消息. 该模式的延迟会很高.</li></ul><p><img src="//jiamaoxiang.top/2020/07/05/Kafka生产者ack机制剖析/ackall.png" alt></p><h2 id="最小同步副本"><a href="#最小同步副本" class="headerlink" title="最小同步副本"></a>最小同步副本</h2><p>上面提到，当acks=all时，需要所有的副本都同步了才会发送成功响应到生产者. 其实这里面存在一个问题：如果Leader副本是唯一的同步副本时会发生什么呢？此时相当于acks=1.所以是不安全的.</p><p>Kafka的Broker端提供了一个参数<strong><code>min.insync.replicas</code></strong>,该参数控制的是消息至少被写入到多少个副本才算是”真正写入”,该值默认值为1，生产环境设定为一个大于1的值可以提升消息的持久性. 因为如果同步副本的数量低于该配置值，则生产者会收到错误响应，从而确保消息不丢失.</p><h3 id="Case-1"><a href="#Case-1" class="headerlink" title="Case 1"></a>Case 1</h3><p>如下图，当min.insync.replicas=2且acks=all时，如果此时ISR列表只有[1,2],3被踢出ISR列表，只需要保证两个副本同步了，生产者就会收到成功响应.</p><p><img src="//jiamaoxiang.top/2020/07/05/Kafka生产者ack机制剖析/min2.png" alt></p><h3 id="Case-2"><a href="#Case-2" class="headerlink" title="Case 2"></a>Case 2</h3><p>如下图，当min.insync.replicas=2，如果此时ISR列表只有[1],2和3被踢出ISR列表，那么当acks=all时，则不能成功写入数；当acks=0或者acks=1可以成功写入数据.</p><p><img src="//jiamaoxiang.top/2020/07/05/Kafka生产者ack机制剖析/min1.png" alt></p><h3 id="Case-3"><a href="#Case-3" class="headerlink" title="Case 3"></a>Case 3</h3><p>这种情况是很容易引起误解的，如果acks=all且min.insync.replicas=2，此时ISR列表为[1,2,3],那么还是会等到所有的同步副本都同步了消息，才会向生产者发送成功响应的ack.因为min.insync.replicas=2只是一个最低限制，即同步副本少于该配置值，则会抛异常，而acks=all，是需要保证所有的ISR列表的副本都同步了才可以发送成功响应. 如下图所示：</p><p><img src="//jiamaoxiang.top/2020/07/05/Kafka生产者ack机制剖析/min0.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>acks=0，生产者在成功写入消息之前不会等待任何来自服务器的响应.</p><p>acks=1,只要集群的leader分区副本接收到了消息，就会向生产者发送一个成功响应的ack.</p><p>acks=all,表示只有所有参与复制的节点(ISR列表的副本)全部收到消息时，生产者才会接收到来自服务器的响应，此时如果ISR同步副本的个数小于<strong><code>min.insync.replicas</code></strong>的值，消息不会被写入.</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数仓开发应避免的10个陷阱</title>
      <link href="/2020/06/30/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91%E5%BA%94%E9%81%BF%E5%85%8D%E7%9A%8410%E4%B8%AA%E9%99%B7%E9%98%B1/"/>
      <url>/2020/06/30/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91%E5%BA%94%E9%81%BF%E5%85%8D%E7%9A%8410%E4%B8%AA%E9%99%B7%E9%98%B1/</url>
      
        <content type="html"><![CDATA[<p>在Ralph Kimball和Margy Ross 的《<em>数据仓库工具包</em>》一书中，提到了数据仓库设计中的10个常见陷阱，本文针对每个陷阱添加了一条与数据仓库设计经验有关的附加解释。在着手进行数据仓库项目之前，可以了解一下数这10个常见陷阱。这样才可以不被数据仓库设计的陷阱所困扰，避免这10个常见的陷阱可以在构建数仓的过程少走些弯路。</p><h2 id="陷阱10"><a href="#陷阱10" class="headerlink" title="陷阱10:"></a>陷阱10:</h2><p><strong>过于迷恋技术和数据，而没有将重点放在业务需求和目标上。</strong></p><blockquote><p>数仓归根结底是要解决业务问题的，狂拽酷炫的数据架构和层出不穷的新技术通常会比去了解用户需求更具有吸引力。其实，也没有完美的技术架构，只要是能够满足当下及未来可见的业务需求即可，合适就好。应当把时间投入在理解和梳理业务上，这样才能够构建出相对合理的数据模型，从而提高模型的复用性，及时响应业务需求。</p></blockquote><h2 id="陷阱9"><a href="#陷阱9" class="headerlink" title="陷阱9:"></a>陷阱9:</h2><p><strong>没有或无法找到一个有影响的、平易近人的、明白事理的高级管理人员作为数仓建设的发起人。</strong></p><blockquote><p>数仓建设是多部门合作的结果，只有这样才能够真正的实现数据赋能业务。所以没有高层的支持和重视，数仓的建设将会很难推进。缺乏远见，热情，支持，领导力以及影响企业投资于任何产品(不管是不是数仓)的能力，注定会走向失败。</p></blockquote><h2 id="陷阱8"><a href="#陷阱8" class="headerlink" title="陷阱8:"></a>陷阱8:</h2><p><strong>将项目处理为一个巨大的持续多年的项目，而不是追求更容易管理的、虽然仍然具有挑战性的迭代开发工作</strong></p><blockquote><p>这是一个经常出现的陷阱，试图建设一个庞大的，无所不包的系统，通常是不可取的。似乎只要建设一个“巨型无比”的系统就可以完成任何工作，解决任何问题一样，其实结果往往会适得其反。更糟的是，管理这些项目的人往往没有与业务进行足够详细的协商，从而开发有用的产品。一言以蔽之，银样镴枪头，中看不中用。                                        </p></blockquote><h2 id="陷阱7"><a href="#陷阱7" class="headerlink" title="陷阱7:"></a>陷阱7:</h2><p><strong>分配大量的精力去构建规范化数据结构， 在最终呈现数据之前，用尽所有的预算。</strong></p><blockquote><p>这个陷阱不像其他陷阱一样重要，在Kimball的方法论中，对维度模型进行更改所带来的业务风险要比更改源事务数据库小。所以应该留出足够的资源来构建它们，但是很少有中小型企业在资源上进行投资以创建完全一致的事实和维度表，更不用说OLAP数据立方体了，所以再多的理论也解决不了实际的问题，先跑起来才重要，不管姿势是否完美。</p></blockquote><h2 id="陷阱6"><a href="#陷阱6" class="headerlink" title="陷阱6 :"></a>陷阱6 :</h2><p><strong>将主要精力投入到后端操作型性能和易开发性，而没有重点考虑前端查询的性能和易用性。</strong></p><blockquote><p>为用户提供易于阅读的数据展示形式并具有良好的查询性能会很重要。</p></blockquote><h2 id="陷阱5"><a href="#陷阱5" class="headerlink" title="陷阱5:"></a>陷阱5:</h2><p><strong>使存在于应用层的可查询数据设计的过于复杂，应该通过简化解决方案开发出更适合需要的产品。</strong></p><blockquote><p>通常，大多数业务用户都希望简化数据表示方式。此外，对这些数据的访问应限于尽可能少入口。提高获取数据的易用性，会大大提升数仓的价值。</p></blockquote><h2 id="陷阱4"><a href="#陷阱4" class="headerlink" title="陷阱4:"></a>陷阱4:</h2><p><strong>烟囱式开发，不考虑使用可共享的、一致性维度将数据模型联系在一起。</strong></p><blockquote><p>当维度在整个数据仓库中不一致时，就是典型的烟囱式开发。其实，我们使用的维度在本质上是相同的，但是由于数据来自于不同的业务源，并会被随意更新。典型的例子是“时间”维度，在维模型不一致的情况下，最终用户通常完全不知道为什么一个报表中的数据可能与其他地方生成的报表有显着差异。一种好的做法是将数据模型与主数据管理（MDM）解决方案联系在一起，该解决方案包含可以在整个数据仓库中普遍使用的参考数据。</p></blockquote><h2 id="陷阱3"><a href="#陷阱3" class="headerlink" title="陷阱3:"></a>陷阱3:</h2><p><strong>只将汇总数据加载到展示区的维度结构中</strong></p><blockquote><p>在事务数据库和数据仓库之间创建的每个ETL（提取，转换和加载）过程中，不能只将汇总的数据装载到数仓中，要确保有一份原子数据存储在数仓中，即将数据同步一份放在准备区(ODS层)。</p></blockquote><h2 id="陷阱2"><a href="#陷阱2" class="headerlink" title="陷阱2 :"></a>陷阱2 :</h2><p><strong>臆想业务、业务需求及分析，其涉及的数据及支持技术都是静态的。</strong></p><blockquote><p>尽量不要开发仅限于某个特定业务需求和分析的数据模型，因为业务在不断地发生变化。一个差劲的模型设计通常是开发重复的数据模型及不一致的命名约定。在设计一个“完美”的事实表、维表与规范化程度之间取得平衡并不是一件容易的事情，但是开发出可伸缩的以适应业务发展的数据模型是非常重要的。</p></blockquote><h2 id="陷阱1"><a href="#陷阱1" class="headerlink" title="陷阱1 :"></a>陷阱1 :</h2><p><strong>忽略数据仓库的成功直接来源于业务的认可。如果用户未将数据仓库系统当成他们制定决策的基础，那么所有的工作都是徒劳。</strong></p><blockquote><p>这个是很致命的陷阱，如果从一开始都没有得到业务和高层的重视和认可，那么数仓项目多半是会夭折。从用户的角度出发，如果用户对建立的数仓不买账，根本就不会去使用它，结局只会game over。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数仓 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的架构剖析</title>
      <link href="/2020/06/27/Hive%E7%9A%84%E6%9E%B6%E6%9E%84%E5%89%96%E6%9E%90/"/>
      <url>/2020/06/27/Hive%E7%9A%84%E6%9E%B6%E6%9E%84%E5%89%96%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>本文主要介绍Hive的架构和以及HQL的查询阶段，主要内容包括：</p><ul><li>Hive的架构</li><li>架构中的相关组件介绍</li><li>HQL的查询阶段</li></ul><h2 id="Hive的架构"><a href="#Hive的架构" class="headerlink" title="Hive的架构"></a>Hive的架构</h2><p>hive的基本架构图如下图所示：</p><p><img src="//jiamaoxiang.top/2020/06/27/Hive的架构剖析/hive%E6%9E%B6%E6%9E%84.png" alt></p><h2 id="相关组件介绍"><a href="#相关组件介绍" class="headerlink" title="相关组件介绍"></a>相关组件介绍</h2><ul><li>数据存储</li></ul><p>Hive中的数据可以存储在任意与Hadoop兼容的文件系统，其最常见的存储文件格式主要有ORC和Parquet。除了HDFS之外，也支持一些商用的云对象存储，比如AWS S3等。另外，Hive可以读入并写入数据到其他的独立处理系统，比如Druid、HBase等。</p><ul><li>Data catalog</li></ul><p>Hive使用Hive Metastore(HMS)存储元数据信息，使用关系型数据库来持久化存储这些信息，其依赖于DataNucleus(提供了标准的接口（JDO, JPA）来访问各种类型的数据库资源 ),用于简化操作各种关系型数据库。为了请求低延迟，HMS会直接通过DataNucleus直接查询关系型数据库。HMS的API支持多种编程语言。</p><ul><li>执行引擎</li></ul><p>最初版本的Hive支持MapReduce作为执行引擎，后来又支持</p><p> Tez和Spark作为执行引擎，这些执行引擎都可以运行在YARN上。</p><ul><li>查询服务</li></ul><p>Hiveserver2(HS2)允许用户执行SQL查询，Hiveserver2允许多个客户端提交请求到Hive并返回执行结果，HS2支持本地和远程JDBC和ODBC连接，另外Hive的发布版中包括一个JDBC的客户端，称之为Beeline。</p><ul><li>Hive客户端</li></ul><p>Hive支持多种客户端，比如Python, Java, C++, Ruby等，可以使用JDBC、ODBC和Thrift drivers连接Hive，Hive的客户端主要归为3类：</p><p>（1）<strong>Thrift Clients</strong></p><p>Hive的Server是基于Apache Thrift的，所以支持thrift客户端的查询请求</p><p>（2）<strong>JDBC Client</strong></p><p>允许使用Java通过JDBC driver连接Hive，JDBC driver使用Thrift与Hive进行通信的</p><p>（3）<strong>ODBC Client</strong></p><p>Hive的ODBC driver允许使用基于ODBC协议的应用来连接Hive，与JDBC driver类似，ODBC driver也是通过Thrift与Hive server进行通信的</p><ul><li>Hive Driver</li></ul><p>Hive Driver接收来自客户端提交的HQL语句，创建session handles，并将查询发送到Compiler(编译器)。</p><ul><li>Hive Compiler</li></ul><p>Hive的Compiler解析查询语句，编译器会借助Hive的metastore存储的元数据信息，对不同的查询块和查询表达式执行语义分析和类型检查，然后生成执行计划。</p><p>编译器生成的执行计划就是DAG，每个Stage可能代表一个MR作业。</p><ul><li>Optimizer(优化器)</li></ul><p>比如列裁剪、谓词下推等优化，提升查询效率</p><h2 id="执行过程"><a href="#执行过程" class="headerlink" title="执行过程"></a>执行过程</h2><ul><li><p><strong>Step1：执行查询</strong></p><p>通过客户端提交查询</p></li><li><p><strong>Step2：获取执行计划</strong></p><p>dirver接收到查询，会创建session handle，并将该查询传递给编译器，生成执行计划</p></li><li><p><strong>Step3：获取元数据</strong></p><p>编译器会向metastore发送获取元数据的请求</p></li><li><p><strong>Step4：发送元数据</strong></p><p>metastore向编译器发送元数据，编译器使用元数据执行类型检查和语义分析。编译器会生成执行计划(DAG),对于MapReduce作业而言，执行计划包括<strong>map operator trees</strong></p><p>和<strong>reduce operator tree</strong></p></li><li><p><strong>Step5：发送执行计划</strong></p><p>编译器向Driver发送生成的执行计划</p></li><li><p><strong>Step6：执行查询计划</strong></p><p>从编译器那里获取执行计划之后，Driver会向执行引擎发送执行计划</p></li><li><p><strong>Step7：提交MR作业</strong></p></li><li><p><strong>Step8：返回查询结果</strong></p></li></ul><p>将查询结果通过Driver返回个查询客户端</p><h2 id="HQL的查询阶段"><a href="#HQL的查询阶段" class="headerlink" title="HQL的查询阶段"></a>HQL的查询阶段</h2><p>Hive的查询阶段如下图所示，具体分析如下：</p><p><img src="//jiamaoxiang.top/2020/06/27/Hive的架构剖析/%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B.png" alt></p><p>如上图所示，</p><ul><li>1.用户提交查询到HS2</li><li>2.该查询被Driver处理，由编译器会解析该查询语句并从AST中生成一个Calcite逻辑计划</li><li>3.优化逻辑计划，HS2会访问关于HMS的元数据信息，用来达到验证和优化的目的</li><li>4.优化的逻辑计划被转换为物理执行计划</li><li>5.向量化的执行计划</li><li>6.生成具体的task，可以是mr或者spark、Tez，并通过Driver提交任务到YARN</li><li>7.执行结束后将结果返回给用户</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先介绍了Hive的架构，并对每个组件进行了描述。然后阐述了Hive的具体执行过程，最后对HQL的执行阶段进行了说明。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据分析|使用多元线性回归构建销售额预测模型</title>
      <link href="/2020/06/26/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E4%BD%BF%E7%94%A8%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%9E%84%E5%BB%BA%E9%94%80%E5%94%AE%E9%A2%9D%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/"/>
      <url>/2020/06/26/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E4%BD%BF%E7%94%A8%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%9E%84%E5%BB%BA%E9%94%80%E5%94%AE%E9%A2%9D%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>回归是确定因变量和一组自变量之间的关系的过程。线性模型形式简单、易于建模，但却蕴含着机器学习中一些重要的基本思想。本文会通过EXCEL构建一个多元线性回归模型，来预测广告投入对销售的影响。本文的主要内容包括：</p><ul><li>线性回归的基本概念</li><li>回归模型的重要参数</li><li>多元线性回归模型案例</li><li>谁发明了最小二乘法</li></ul><h2 id="线性回归的基本概念"><a href="#线性回归的基本概念" class="headerlink" title="线性回归的基本概念"></a>线性回归的基本概念</h2><p>给定由d个属性描述示例x=(x1;x2;…;xd),其中xi是x在第i个属性上的取值，线性模型试图学得一个通过属性的线性组合来进行预测的函数，即</p><blockquote><p> f(x)=w1x1+w2x2+…+wdxd+b</p></blockquote><p>其中，b为常数项，w1，w2..wd为偏回归系数</p><p>上述的公式中<strong>w</strong>与b是未知的，那么该如何求解<strong>w</strong>与b呢？通过均方误差来进行求解，即最小二乘法，在线性模型中最小二乘法就是试图找到一条直线，使所有样本到直线的欧式距离之和最小。</p><h2 id="回归模型的重要参数"><a href="#回归模型的重要参数" class="headerlink" title="回归模型的重要参数"></a>回归模型的重要参数</h2><ul><li>R^2判定系数</li></ul><p>R平方即R的平方，又可以叫判定系数、拟合优度，取值范围是[0,1],R平方值越大，表示模型拟合的越好。一般大于70%表示拟合较好，60%以下的就需要修正模型了</p><ul><li>调整的R^2判定系数</li></ul><p>这个值是用来修正因自变量个数增加而导致模型拟合效果过高的情况，多用于衡量多重线性回归。</p><ul><li>F值</li></ul><p>Significance F是回归方程总体的显著性检验，F检验主要是检验因变量与自变量之间的线性关系是否显著，用线性模型来描述他们之间的关系是否恰当，越小越显著。</p><ul><li>残差</li></ul><p>残差是实际值与预测值之间的差，残差图用于回归诊断，回归模型在理想条件下的残差图是服从正态分布的。</p><ul><li>P值</li></ul><p>用来检验回归方程系数的显著性，一般以此来衡量检验结果是否具有显著性，如果P值&gt;0.05，则结果不具有显著的统计学意义，如果0.01&lt;P值&lt;0.05，则结果具有显著的统计学意义，如果P&lt;=0.01，则结果具有极其显著的统计学意义。</p><h2 id="多元线性回归模型案例"><a href="#多元线性回归模型案例" class="headerlink" title="多元线性回归模型案例"></a>多元线性回归模型案例</h2><h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><p>假设有如下的广告投入与销售收入的数据，[电视渠道,地铁渠道,搜索渠道,  销售额]。该表显示了在不同渠道广告投入与销售额之间的关系。具体如下表：</p><table><thead><tr><th>电视渠道</th><th>地铁渠道</th><th>搜索渠道</th><th>销售额</th></tr></thead><tbody><tr><td>230.1</td><td>37.8</td><td>69.2</td><td>22.1</td></tr><tr><td>44.5</td><td>39.3</td><td>45.1</td><td>10.4</td></tr><tr><td>17.2</td><td>45.9</td><td>69.3</td><td>12</td></tr><tr><td>151.5</td><td>41.3</td><td>58.5</td><td>16.5</td></tr><tr><td>180.8</td><td>10.8</td><td>58.4</td><td>17.9</td></tr><tr><td>8.7</td><td>48.9</td><td>75</td><td>7.2</td></tr><tr><td>57.5</td><td>32.8</td><td>23.5</td><td>11.8</td></tr><tr><td>120.2</td><td>19.6</td><td>11.6</td><td>13.2</td></tr><tr><td>8.6</td><td>2.1</td><td>1</td><td>4.8</td></tr></tbody></table><p>上述的数据有多个变量，我们先简化一下，假设只有两个变量，比如查找电视渠道广告与销售额之间的关系。最直接的办法就是绘制一个散点图，通过散点图观察两个变量之间是否有相关关系。</p><p>假设具有相关关系，那么该如何量化呢，同样很简单，只需要绘制一条最适合散点图中显示的所有点的直线，该直线方程式就是两个变量之间的关系，这就是回归的基本思想，即通过使用函数拟合所有点来量化变量之间的关系。</p><p>上面描述的示例成为<strong>简单线性回归</strong>，它涉及一个自变量和一个因变量。我们可以将相同的回归概念扩展到多个自变量，称之为<strong>多元线性回归</strong>。如上表的数据，存在3个因变量，虽然无法在2维平面上可视化他们之间的关系，但是线性回归的概念仍然可以用于确定这些点的最佳拟合函数</p><p>下面将使用EXCEL进行多元线性回归分析</p><ul><li>首先添加数据分析工具库</li></ul><p>默认情况下，Data Analysis ToolPak不可用，您需要单独激活该加载项。要激活，请转到<em>文件-&gt;选项-&gt;加载项</em>，然后激活<em>分析工具库</em>。激活加载项后，该加载项应显示在工具栏的“ <em>数据”</em>选项卡下。</p><ul><li>执行回归分析</li></ul><p><img src="//jiamaoxiang.top/2020/06/26/数据分析-使用多元线性回归构建销售额预测模型/%E5%9B%9E%E5%BD%921.png" alt></p><p>选择回归之后，确定之后，再选择数据集，点击确定</p><p><img src="//jiamaoxiang.top/2020/06/26/数据分析-使用多元线性回归构建销售额预测模型/%E5%9B%9E%E5%BD%922.png" alt></p><h3 id="模型分析"><a href="#模型分析" class="headerlink" title="模型分析"></a>模型分析</h3><p>上述会构建一个多元线性回归模型，具体分析如下：</p><p><img src="//jiamaoxiang.top/2020/06/26/数据分析-使用多元线性回归构建销售额预测模型/%E7%BB%93%E6%9E%9C.png" alt></p><p>上面第三张表，第一列就是系数。此列提供了多元线性回归方程式中每个变量的系数值。</p><blockquote><p>销售额= 0.0544 *（电视渠道）+ 0.1070 *（地铁渠道）+ 0.0003 *（搜索渠道）+ 4.6251</p></blockquote><h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>预测模型是通过为自变量提供值来预测因变量值的过程。上述的多元线性回归方程是预测模型函数，如果我们输入自变量的值，则可以得到销售额的预测值。</p><p>例如，如果要预测以下广告支出组合的销售收入，</p><p>电视渠道= 100</p><p>地铁渠道= 200</p><p>搜索渠道= 500</p><p>将值输入多元线性回归方程式。这将为您带来31.6377的销售收入，这是预期的收入。</p><h2 id="谁发明了最小二乘法"><a href="#谁发明了最小二乘法" class="headerlink" title="谁发明了最小二乘法"></a>谁发明了最小二乘法</h2><p>1801年，意大利天文学家皮亚齐发现了1号小行星“谷神星”，但在跟踪观测了40天后，因谷神星转至太阳背后，皮亚齐失去了谷神星的位置。许多天文学家试图重新找到谷神星，但都徒劳无获。这引起了德国数学家高斯的注意，他发明了一种方法，根据皮亚齐的观测数据计算出了谷神星的轨道，后来德国天文学家奥博斯在高斯语言的时间和星空领域重新找到了谷神星。1809年，高斯在他的著作《天体运动论》中发表了这种方法，即最小二乘法。</p><p>1805年，在椭圆积分、数论和几何方面都有重大贡献的法国数学家勒让德发表了《计算彗星轨道的新方法》，其附录中描述了最小二乘法，勒让德是法国18-19世纪数学界的三驾马车之一，早已是法国科学院院士。但勒让德的数中没有涉及最小二乘法的误差分析，高斯在1809年的著作中包括了这方面的内容，这对最小二乘法用于数理统计、乃至今天的机器学习有着极为重要的意义。由于高斯的这一重大发现，以及他声称自己在1799年就已经开始用这个方法，因此很多人将最小二乘法的发明优先权归之为高斯。当时这两位大数学家发生了著名的优先权之争，此后有许多数学史家专门进行研究，但至今也没弄清楚到底是谁最先发明了最小二乘法。</p>]]></content>
      
      
      <categories>
          
          <category> 回归分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 回归分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>‘Hive on MR执行计划与执行日志解析</title>
      <link href="/2020/06/10/%E2%80%98Hive-on-MR%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%E8%A7%A3%E6%9E%90/"/>
      <url>/2020/06/10/%E2%80%98Hive-on-MR%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>MR是Hive的默认执行引擎，在该引擎下，HQL会被转换为MR作业。通过Hive的执行计划可以看出一个SQL语句的执行阶段，通过Hive的执行日志可以看出转换之后的MR作业信息。本文会通过一个具体HQL，解读一下Hive执行计划与执行日志。通过本文，你可以了解到：</p><ul><li>如何查看一个HQL会被转换为几个MR Job</li><li>执行计划的Stage与转换后的MR之间有什么关系</li><li>如何调优一个HQL查询</li></ul><h2 id="查询SQL"><a href="#查询SQL" class="headerlink" title="查询SQL"></a>查询SQL</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> dp.region_name,</span><br><span class="line">       <span class="keyword">count</span>(*) <span class="keyword">AS</span> cnt</span><br><span class="line"><span class="keyword">FROM</span> user_behavior_log ubl</span><br><span class="line"><span class="keyword">JOIN</span> dim_province dp <span class="keyword">ON</span> ubl.province = dp.province_name</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">action</span> = <span class="string">'buy'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> dp.region_name</span><br></pre></td></tr></table></figure><h3 id="SQL介绍"><a href="#SQL介绍" class="headerlink" title="SQL介绍"></a>SQL介绍</h3><p>上面的SQL是统计每个区域购买行为的数量，其中user_behavior_log为用户的行为数据，大概有3G左右大小，dim_province是地区的维表，主要包括省份以及省份对应的区域。</p><h3 id="查询结果"><a href="#查询结果" class="headerlink" title="查询结果"></a>查询结果</h3><table><thead><tr><th align="left">dp.region_name</th><th align="left">cnt</th></tr></thead><tbody><tr><td align="left">东北</td><td align="left">290875</td></tr><tr><td align="left">华东</td><td align="left">870994</td></tr><tr><td align="left">华中</td><td align="left">291048</td></tr><tr><td align="left">华北</td><td align="left">484199</td></tr><tr><td align="left">华南</td><td align="left">484032</td></tr><tr><td align="left">西北</td><td align="left">483784</td></tr><tr><td align="left">西南</td><td align="left">483891</td></tr></tbody></table><h2 id="应用程序信息"><a href="#应用程序信息" class="headerlink" title="应用程序信息"></a>应用程序信息</h2><p><img src="//jiamaoxiang.top/2020/06/10/‘Hive-on-MR执行计划解析/job%E8%AF%A6%E7%BB%86%E6%8C%87%E6%A0%87%E4%BF%A1%E6%81%AF.png" alt></p><p>上图是该查询SQL转为MR的应用程序的执行信息，总共执行了4个MR Job。其中第一个Job包括1个Map任务，无Reduce任务。第二个Job包括11个Map任务，无reduce任务。第三个Job包括2个Map任务和2个Reduce任务。第四个Job包括1个Map任务和1个Reduce任务。</p><h2 id="执行计划分析"><a href="#执行计划分析" class="headerlink" title="执行计划分析"></a>执行计划分析</h2><h3 id="完整的执行计划"><a href="#完整的执行计划" class="headerlink" title="完整的执行计划"></a>完整的执行计划</h3><p>为了方便查看，下面给出完整的执行计划。可以先跳过该部分，直接看下面的详细解释：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage-13 is a root stage</span><br><span class="line">  Stage-10 depends on stages: Stage-13</span><br><span class="line">  Stage-9 depends on stages: Stage-10 , consists of Stage-11, Stage-12, Stage-2</span><br><span class="line">  Stage-11 has a <span class="keyword">backup</span> stage: Stage<span class="number">-2</span></span><br><span class="line">  Stage<span class="number">-7</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-11</span></span><br><span class="line">  Stage<span class="number">-3</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-2</span>, Stage<span class="number">-7</span>, Stage<span class="number">-8</span></span><br><span class="line">  Stage<span class="number">-4</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-3</span></span><br><span class="line">  Stage<span class="number">-12</span> has a <span class="keyword">backup</span> stage: Stage<span class="number">-2</span></span><br><span class="line">  Stage<span class="number">-8</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-12</span></span><br><span class="line">  Stage<span class="number">-2</span></span><br><span class="line">  Stage<span class="number">-0</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-4</span></span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage<span class="number">-13</span></span><br><span class="line">    <span class="keyword">Map</span> Reduce <span class="keyword">Local</span> <span class="keyword">Work</span></span><br><span class="line">      <span class="keyword">Alias</span> -&gt; <span class="keyword">Map</span> <span class="keyword">Local</span> <span class="keyword">Tables</span>:</span><br><span class="line">        dp:br </span><br><span class="line">          <span class="keyword">Fetch</span> <span class="keyword">Operator</span></span><br><span class="line">            <span class="keyword">limit</span>: <span class="number">-1</span></span><br><span class="line">      <span class="keyword">Alias</span> -&gt; <span class="keyword">Map</span> <span class="keyword">Local</span> <span class="keyword">Operator</span> Tree:</span><br><span class="line">        dp:br </span><br><span class="line">          TableScan</span><br><span class="line">            <span class="keyword">alias</span>: br</span><br><span class="line">            filterExpr: <span class="keyword">id</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span> (<span class="keyword">type</span>: <span class="built_in">boolean</span>)</span><br><span class="line">            <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">7</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">56</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            Filter <span class="keyword">Operator</span></span><br><span class="line">              predicate: <span class="keyword">id</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span> (<span class="keyword">type</span>: <span class="built_in">boolean</span>)</span><br><span class="line">              <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">4</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">32</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">              HashTable Sink <span class="keyword">Operator</span></span><br><span class="line">                <span class="keyword">keys</span>:</span><br><span class="line">                  <span class="number">0</span> <span class="keyword">id</span> (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">                  <span class="number">1</span> region_id (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line"></span><br><span class="line">  Stage: Stage<span class="number">-10</span></span><br><span class="line">    <span class="keyword">Map</span> Reduce</span><br><span class="line">      <span class="keyword">Map</span> <span class="keyword">Operator</span> Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            <span class="keyword">alias</span>: bp</span><br><span class="line">            filterExpr: (region_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">and</span> <span class="keyword">name</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>) (<span class="keyword">type</span>: <span class="built_in">boolean</span>)</span><br><span class="line">            <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">35</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">394</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            Filter <span class="keyword">Operator</span></span><br><span class="line">              predicate: (region_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">and</span> <span class="keyword">name</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>) (<span class="keyword">type</span>: <span class="built_in">boolean</span>)</span><br><span class="line">              <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">9</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">101</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">              <span class="keyword">Map</span> <span class="keyword">Join</span> <span class="keyword">Operator</span></span><br><span class="line">                condition <span class="keyword">map</span>:</span><br><span class="line">                     <span class="keyword">Inner</span> <span class="keyword">Join</span> <span class="number">0</span> <span class="keyword">to</span> <span class="number">1</span></span><br><span class="line">                <span class="keyword">keys</span>:</span><br><span class="line">                  <span class="number">0</span> <span class="keyword">id</span> (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">                  <span class="number">1</span> region_id (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">                outputColumnNames: _col1, _col6</span><br><span class="line">                <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">9</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">111</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                <span class="keyword">Select</span> <span class="keyword">Operator</span></span><br><span class="line">                  expressions: _col6 (<span class="keyword">type</span>: <span class="keyword">string</span>), _col1 (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">                  outputColumnNames: _col1, _col3</span><br><span class="line">                  <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">9</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">111</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                  <span class="keyword">File</span> <span class="keyword">Output</span> <span class="keyword">Operator</span></span><br><span class="line">                    compressed: <span class="literal">false</span></span><br><span class="line">                    <span class="keyword">table</span>:</span><br><span class="line">                        <span class="keyword">input</span> <span class="keyword">format</span>: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                        <span class="keyword">output</span> <span class="keyword">format</span>: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">                        serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe</span><br><span class="line">      <span class="keyword">Local</span> <span class="keyword">Work</span>:</span><br><span class="line">        <span class="keyword">Map</span> Reduce <span class="keyword">Local</span> <span class="keyword">Work</span></span><br><span class="line"></span><br><span class="line">  Stage: Stage<span class="number">-9</span></span><br><span class="line">    Conditional <span class="keyword">Operator</span></span><br><span class="line"></span><br><span class="line">  Stage: Stage<span class="number">-11</span></span><br><span class="line">    <span class="keyword">Map</span> Reduce <span class="keyword">Local</span> <span class="keyword">Work</span></span><br><span class="line">      <span class="keyword">Alias</span> -&gt; <span class="keyword">Map</span> <span class="keyword">Local</span> <span class="keyword">Tables</span>:</span><br><span class="line">        $INTNAME </span><br><span class="line">          <span class="keyword">Fetch</span> <span class="keyword">Operator</span></span><br><span class="line">            <span class="keyword">limit</span>: <span class="number">-1</span></span><br><span class="line">      <span class="keyword">Alias</span> -&gt; <span class="keyword">Map</span> <span class="keyword">Local</span> <span class="keyword">Operator</span> Tree:</span><br><span class="line">        $INTNAME </span><br><span class="line">          TableScan</span><br><span class="line">            HashTable Sink <span class="keyword">Operator</span></span><br><span class="line">              <span class="keyword">keys</span>:</span><br><span class="line">                <span class="number">0</span> province (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">                <span class="number">1</span> _col1 (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line"></span><br><span class="line">  Stage: Stage<span class="number">-7</span></span><br><span class="line">    <span class="keyword">Map</span> Reduce</span><br><span class="line">      <span class="keyword">Map</span> <span class="keyword">Operator</span> Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            <span class="keyword">alias</span>: ubl</span><br><span class="line">            filterExpr: (province <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">and</span> (<span class="keyword">action</span> = <span class="string">'buy'</span>)) (<span class="keyword">type</span>: <span class="built_in">boolean</span>)</span><br><span class="line">            <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">54925330</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">3300747847</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            Filter <span class="keyword">Operator</span></span><br><span class="line">              predicate: (province <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">and</span> (<span class="keyword">action</span> = <span class="string">'buy'</span>)) (<span class="keyword">type</span>: <span class="built_in">boolean</span>)</span><br><span class="line">              <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">13731332</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">825186931</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">              <span class="keyword">Map</span> <span class="keyword">Join</span> <span class="keyword">Operator</span></span><br><span class="line">                condition <span class="keyword">map</span>:</span><br><span class="line">                     <span class="keyword">Inner</span> <span class="keyword">Join</span> <span class="number">0</span> <span class="keyword">to</span> <span class="number">1</span></span><br><span class="line">                <span class="keyword">keys</span>:</span><br><span class="line">                  <span class="number">0</span> province (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">                  <span class="number">1</span> _col1 (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">                outputColumnNames: _col18</span><br><span class="line">                <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">15104465</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">907705643</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                <span class="keyword">File</span> <span class="keyword">Output</span> <span class="keyword">Operator</span></span><br><span class="line">                  compressed: <span class="literal">false</span></span><br><span class="line">                  <span class="keyword">table</span>:</span><br><span class="line">                      <span class="keyword">input</span> <span class="keyword">format</span>: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                      <span class="keyword">output</span> <span class="keyword">format</span>: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">                      serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe</span><br><span class="line">      <span class="keyword">Local</span> <span class="keyword">Work</span>:</span><br><span class="line">        <span class="keyword">Map</span> Reduce <span class="keyword">Local</span> <span class="keyword">Work</span></span><br><span class="line"></span><br><span class="line">  Stage: Stage<span class="number">-3</span></span><br><span class="line">    <span class="keyword">Map</span> Reduce</span><br><span class="line">      <span class="keyword">Map</span> <span class="keyword">Operator</span> Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            Reduce <span class="keyword">Output</span> <span class="keyword">Operator</span></span><br><span class="line">              <span class="keyword">key</span> expressions: _col18 (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">              <span class="keyword">sort</span> <span class="keyword">order</span>: +</span><br><span class="line">              <span class="keyword">Map</span>-reduce <span class="keyword">partition</span> <span class="keyword">columns</span>: <span class="keyword">rand</span>() (<span class="keyword">type</span>: <span class="keyword">double</span>)</span><br><span class="line">              <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">15104465</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">907705643</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">      Reduce <span class="keyword">Operator</span> Tree:</span><br><span class="line">        <span class="keyword">Group</span> <span class="keyword">By</span> <span class="keyword">Operator</span></span><br><span class="line">          aggregations: <span class="keyword">count</span>()</span><br><span class="line">          <span class="keyword">keys</span>: KEY._col0 (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">          <span class="keyword">mode</span>: partial1</span><br><span class="line">          outputColumnNames: _col0, _col1</span><br><span class="line">          <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">15104465</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">907705643</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">          <span class="keyword">File</span> <span class="keyword">Output</span> <span class="keyword">Operator</span></span><br><span class="line">            compressed: <span class="literal">false</span></span><br><span class="line">            <span class="keyword">table</span>:</span><br><span class="line">                <span class="keyword">input</span> <span class="keyword">format</span>: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                <span class="keyword">output</span> <span class="keyword">format</span>: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe</span><br><span class="line"></span><br><span class="line">  Stage: Stage<span class="number">-4</span></span><br><span class="line">    <span class="keyword">Map</span> Reduce</span><br><span class="line">      <span class="keyword">Map</span> <span class="keyword">Operator</span> Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            Reduce <span class="keyword">Output</span> <span class="keyword">Operator</span></span><br><span class="line">              <span class="keyword">key</span> expressions: _col0 (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">              <span class="keyword">sort</span> <span class="keyword">order</span>: +</span><br><span class="line">              <span class="keyword">Map</span>-reduce <span class="keyword">partition</span> <span class="keyword">columns</span>: _col0 (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">              <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">15104465</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">907705643</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">              <span class="keyword">value</span> expressions: _col1 (<span class="keyword">type</span>: <span class="built_in">bigint</span>)</span><br><span class="line">      Reduce <span class="keyword">Operator</span> Tree:</span><br><span class="line">        <span class="keyword">Group</span> <span class="keyword">By</span> <span class="keyword">Operator</span></span><br><span class="line">          aggregations: <span class="keyword">count</span>(VALUE._col0)</span><br><span class="line">          <span class="keyword">keys</span>: KEY._col0 (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">          <span class="keyword">mode</span>: <span class="keyword">final</span></span><br><span class="line">          outputColumnNames: _col0, _col1</span><br><span class="line">          <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">7552232</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">453852791</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">          <span class="keyword">File</span> <span class="keyword">Output</span> <span class="keyword">Operator</span></span><br><span class="line">            compressed: <span class="literal">false</span></span><br><span class="line">            <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">7552232</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">453852791</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            <span class="keyword">table</span>:</span><br><span class="line">                <span class="keyword">input</span> <span class="keyword">format</span>: org.apache.hadoop.mapred.TextInputFormat</span><br><span class="line">                <span class="keyword">output</span> <span class="keyword">format</span>: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</span><br><span class="line">                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><br><span class="line"></span><br><span class="line">  Stage: Stage<span class="number">-12</span></span><br><span class="line">    <span class="keyword">Map</span> Reduce <span class="keyword">Local</span> <span class="keyword">Work</span></span><br><span class="line">      <span class="keyword">Alias</span> -&gt; <span class="keyword">Map</span> <span class="keyword">Local</span> <span class="keyword">Tables</span>:</span><br><span class="line">        ubl </span><br><span class="line">          <span class="keyword">Fetch</span> <span class="keyword">Operator</span></span><br><span class="line">            <span class="keyword">limit</span>: <span class="number">-1</span></span><br><span class="line">      <span class="keyword">Alias</span> -&gt; <span class="keyword">Map</span> <span class="keyword">Local</span> <span class="keyword">Operator</span> Tree:</span><br><span class="line">        ubl </span><br><span class="line">          TableScan</span><br><span class="line">            <span class="keyword">alias</span>: ubl</span><br><span class="line">            filterExpr: (province <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">and</span> (<span class="keyword">action</span> = <span class="string">'buy'</span>)) (<span class="keyword">type</span>: <span class="built_in">boolean</span>)</span><br><span class="line">            <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">54925330</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">3300747847</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            Filter <span class="keyword">Operator</span></span><br><span class="line">              predicate: (province <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">and</span> (<span class="keyword">action</span> = <span class="string">'buy'</span>)) (<span class="keyword">type</span>: <span class="built_in">boolean</span>)</span><br><span class="line">              <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">13731332</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">825186931</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">              HashTable Sink <span class="keyword">Operator</span></span><br><span class="line">                <span class="keyword">keys</span>:</span><br><span class="line">                  <span class="number">0</span> province (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">                  <span class="number">1</span> _col1 (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line"></span><br><span class="line">  Stage: Stage<span class="number">-8</span></span><br><span class="line">    <span class="keyword">Map</span> Reduce</span><br><span class="line">      <span class="keyword">Map</span> <span class="keyword">Operator</span> Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            <span class="keyword">Map</span> <span class="keyword">Join</span> <span class="keyword">Operator</span></span><br><span class="line">              condition <span class="keyword">map</span>:</span><br><span class="line">                   <span class="keyword">Inner</span> <span class="keyword">Join</span> <span class="number">0</span> <span class="keyword">to</span> <span class="number">1</span></span><br><span class="line">              <span class="keyword">keys</span>:</span><br><span class="line">                <span class="number">0</span> province (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">                <span class="number">1</span> _col1 (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">              outputColumnNames: _col18</span><br><span class="line">              <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">15104465</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">907705643</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">              <span class="keyword">File</span> <span class="keyword">Output</span> <span class="keyword">Operator</span></span><br><span class="line">                compressed: <span class="literal">false</span></span><br><span class="line">                <span class="keyword">table</span>:</span><br><span class="line">                    <span class="keyword">input</span> <span class="keyword">format</span>: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                    <span class="keyword">output</span> <span class="keyword">format</span>: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">                    serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe</span><br><span class="line">      <span class="keyword">Local</span> <span class="keyword">Work</span>:</span><br><span class="line">        <span class="keyword">Map</span> Reduce <span class="keyword">Local</span> <span class="keyword">Work</span></span><br><span class="line"></span><br><span class="line">  Stage: Stage<span class="number">-2</span></span><br><span class="line">    <span class="keyword">Map</span> Reduce</span><br><span class="line">      <span class="keyword">Map</span> <span class="keyword">Operator</span> Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            Reduce <span class="keyword">Output</span> <span class="keyword">Operator</span></span><br><span class="line">              <span class="keyword">key</span> expressions: _col1 (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">              <span class="keyword">sort</span> <span class="keyword">order</span>: +</span><br><span class="line">              <span class="keyword">Map</span>-reduce <span class="keyword">partition</span> <span class="keyword">columns</span>: _col1 (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">              <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">9</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">111</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">              <span class="keyword">value</span> expressions: _col3 (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">          TableScan</span><br><span class="line">            <span class="keyword">alias</span>: ubl</span><br><span class="line">            filterExpr: (province <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">and</span> (<span class="keyword">action</span> = <span class="string">'buy'</span>)) (<span class="keyword">type</span>: <span class="built_in">boolean</span>)</span><br><span class="line">            <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">54925330</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">3300747847</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            Filter <span class="keyword">Operator</span></span><br><span class="line">              predicate: (province <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">and</span> (<span class="keyword">action</span> = <span class="string">'buy'</span>)) (<span class="keyword">type</span>: <span class="built_in">boolean</span>)</span><br><span class="line">              <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">13731332</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">825186931</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">              Reduce <span class="keyword">Output</span> <span class="keyword">Operator</span></span><br><span class="line">                <span class="keyword">key</span> expressions: province (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">                <span class="keyword">sort</span> <span class="keyword">order</span>: +</span><br><span class="line">                <span class="keyword">Map</span>-reduce <span class="keyword">partition</span> <span class="keyword">columns</span>: province (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">                <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">13731332</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">825186931</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">      Reduce <span class="keyword">Operator</span> Tree:</span><br><span class="line">        <span class="keyword">Join</span> <span class="keyword">Operator</span></span><br><span class="line">          condition <span class="keyword">map</span>:</span><br><span class="line">               <span class="keyword">Inner</span> <span class="keyword">Join</span> <span class="number">0</span> <span class="keyword">to</span> <span class="number">1</span></span><br><span class="line">          <span class="keyword">keys</span>:</span><br><span class="line">            <span class="number">0</span> province (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">            <span class="number">1</span> _col1 (<span class="keyword">type</span>: <span class="keyword">string</span>)</span><br><span class="line">          outputColumnNames: _col18</span><br><span class="line">          <span class="keyword">Statistics</span>: <span class="keyword">Num</span> <span class="keyword">rows</span>: <span class="number">15104465</span> <span class="keyword">Data</span> <span class="keyword">size</span>: <span class="number">907705643</span> Basic stats: <span class="keyword">COMPLETE</span> <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">          <span class="keyword">File</span> <span class="keyword">Output</span> <span class="keyword">Operator</span></span><br><span class="line">            compressed: <span class="literal">false</span></span><br><span class="line">            <span class="keyword">table</span>:</span><br><span class="line">                <span class="keyword">input</span> <span class="keyword">format</span>: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                <span class="keyword">output</span> <span class="keyword">format</span>: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe</span><br><span class="line"></span><br><span class="line">  Stage: Stage<span class="number">-0</span></span><br><span class="line">    <span class="keyword">Fetch</span> <span class="keyword">Operator</span></span><br><span class="line">      <span class="keyword">limit</span>: <span class="number">-1</span></span><br><span class="line">      Processor Tree:</span><br><span class="line">        ListSink</span><br></pre></td></tr></table></figure><h3 id="执行计划解析"><a href="#执行计划解析" class="headerlink" title="执行计划解析"></a>执行计划解析</h3><h4 id="stage-dependencies"><a href="#stage-dependencies" class="headerlink" title="stage dependencies"></a>stage dependencies</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Stage-13 is a root stage</span><br><span class="line">  Stage-10 depends on stages: Stage-13</span><br><span class="line">  Stage-9 depends on stages: Stage-10 , consists of Stage-11, Stage-12, Stage-2</span><br><span class="line">  Stage-11 has a backup stage: Stage-2</span><br><span class="line">  Stage-7 depends on stages: Stage-11</span><br><span class="line">  Stage-3 depends on stages: Stage-2, Stage-7, Stage-8</span><br><span class="line">  Stage-4 depends on stages: Stage-3</span><br><span class="line">  Stage-12 has a backup stage: Stage-2</span><br><span class="line">  Stage-8 depends on stages: Stage-12</span><br><span class="line">  Stage-2</span><br><span class="line">  Stage-0 depends on stages: Stage-4</span><br></pre></td></tr></table></figure><h4 id="stage-plans"><a href="#stage-plans" class="headerlink" title="stage plans"></a>stage plans</h4><h2 id="执行日志分析"><a href="#执行日志分析" class="headerlink" title="执行日志分析"></a>执行日志分析</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">INFO  : Compiling command(queryId=hive_20200618145454_29fb89e3-b5e6-41ef-8e8a-e2635722656f): <span class="keyword">SELECT</span> dp.region_name,</span><br><span class="line">       <span class="keyword">count</span>(*) <span class="keyword">AS</span> cnt</span><br><span class="line"><span class="keyword">FROM</span> user_behavior_log ubl</span><br><span class="line"><span class="keyword">JOIN</span> dim_province dp <span class="keyword">ON</span> ubl.province = dp.province_name</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">action</span> = <span class="string">'buy'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> dp.region_name</span><br><span class="line">INFO  : Semantic Analysis Completed</span><br><span class="line">INFO  : <span class="keyword">Returning</span> Hive <span class="keyword">schema</span>: <span class="keyword">Schema</span>(fieldSchemas:[FieldSchema(<span class="keyword">name</span>:dp.region_name, <span class="keyword">type</span>:<span class="keyword">string</span>, <span class="keyword">comment</span>:<span class="literal">null</span>), FieldSchema(<span class="keyword">name</span>:cnt, <span class="keyword">type</span>:<span class="built_in">bigint</span>, <span class="keyword">comment</span>:<span class="literal">null</span>)], properties:<span class="literal">null</span>)</span><br><span class="line">INFO  : Completed compiling command(queryId=hive_20200618145454_29fb89e3-b5e6<span class="number">-41</span>ef<span class="number">-8e8</span>a-e2635722656f); Time taken: 0.458 seconds</span><br><span class="line">INFO  : Executing command(queryId=hive_20200618145454_29fb89e3-b5e6-41ef-8e8a-e2635722656f): <span class="keyword">SELECT</span> dp.region_name,</span><br><span class="line">       <span class="keyword">count</span>(*) <span class="keyword">AS</span> cnt</span><br><span class="line"><span class="keyword">FROM</span> user_behavior_log ubl</span><br><span class="line"><span class="keyword">JOIN</span> dim_province dp <span class="keyword">ON</span> ubl.province = dp.province_name</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">action</span> = <span class="string">'buy'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> dp.region_name</span><br><span class="line">INFO  : <span class="keyword">Query</span> <span class="keyword">ID</span> = hive_20200618145454_29fb89e3-b5e6<span class="number">-41</span>ef<span class="number">-8e8</span>a-e2635722656f</span><br><span class="line">INFO  : Total jobs = <span class="number">6</span></span><br><span class="line">INFO  : <span class="keyword">Starting</span> task [Stage<span class="number">-13</span>:MAPREDLOCAL] <span class="keyword">in</span> <span class="built_in">serial</span> <span class="keyword">mode</span></span><br><span class="line"><span class="number">20</span>/<span class="number">06</span>/<span class="number">18</span> <span class="number">14</span>:<span class="number">54</span>:<span class="number">56</span> WARN conf.HiveConf: HiveConf <span class="keyword">of</span> <span class="keyword">name</span> hive.server2.idle.session.timeout_check_operation does <span class="keyword">not</span> exist</span><br><span class="line"><span class="number">20</span>/<span class="number">06</span>/<span class="number">18</span> <span class="number">14</span>:<span class="number">54</span>:<span class="number">56</span> WARN conf.HiveConf: HiveConf <span class="keyword">of</span> <span class="keyword">name</span> hive.entity.capture.input.URI does <span class="keyword">not</span> exist</span><br><span class="line"></span><br><span class="line"><span class="number">2020</span><span class="number">-06</span><span class="number">-18</span> <span class="number">02</span>:<span class="number">54</span>:<span class="number">58</span>Dump the side-<span class="keyword">table</span> <span class="keyword">for</span> tag: <span class="number">0</span> <span class="keyword">with</span> <span class="keyword">group</span> <span class="keyword">count</span>: <span class="number">7</span> <span class="keyword">into</span> <span class="keyword">file</span>: <span class="keyword">file</span>:/tmp/hive/dd28ddf1<span class="number">-03</span>db<span class="number">-472</span>a<span class="number">-8e34</span><span class="number">-98</span>ee658d82a1/hive_2020<span class="number">-06</span><span class="number">-18</span>_14<span class="number">-54</span><span class="number">-52</span>_573_920702636242824197<span class="number">-27</span>/-<span class="keyword">local</span><span class="number">-10010</span>/HashTable-Stage<span class="number">-10</span>/MapJoin-mapfile370<span class="comment">--.hashtable</span></span><br><span class="line"><span class="number">2020</span><span class="number">-06</span><span class="number">-18</span> <span class="number">02</span>:<span class="number">54</span>:<span class="number">58</span><span class="keyword">End</span> <span class="keyword">of</span> <span class="keyword">local</span> task; Time Taken: 2.035 sec.</span><br><span class="line">INFO  : Execution completed successfully</span><br><span class="line">INFO  : MapredLocal task succeeded</span><br><span class="line">INFO  : Launching Job 1 out of 6</span><br><span class="line">INFO  : Starting task [Stage-10:MAPRED] in parallel</span><br><span class="line">INFO  : Number of reduce tasks is <span class="keyword">set</span> <span class="keyword">to</span> <span class="number">0</span> since there<span class="string">'s no reduce operator</span></span><br><span class="line"><span class="string">INFO  : Selecting local mode for task: Stage-10</span></span><br><span class="line"><span class="string">INFO  : number of splits:1</span></span><br><span class="line"><span class="string">INFO  : Submitting tokens for job: job_1582600603106_1942</span></span><br><span class="line"><span class="string">INFO  : The url to track the job: http://cdh03:8088/proxy/application_1582600603106_1942/</span></span><br><span class="line"><span class="string">INFO  : Starting Job = job_1582600603106_1942, Tracking URL = http://cdh03:8088/proxy/application_1582600603106_1942/</span></span><br><span class="line"><span class="string">INFO  : Kill Command = /opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/hadoop/bin/hadoop job  -kill job_1582600603106_1942</span></span><br><span class="line"><span class="string">INFO  : Hadoop job information for Stage-10: number of mappers: 1; number of reducers: 0</span></span><br><span class="line"><span class="string">INFO  : 2020-06-18 14:55:05,948 Stage-10 map = 0%,  reduce = 0%</span></span><br><span class="line"><span class="string">INFO  : 2020-06-18 14:55:11,196 Stage-10 map = 100%,  reduce = 0%, Cumulative CPU 3.2 sec</span></span><br><span class="line"><span class="string">INFO  : MapReduce Total cumulative CPU time: 3 seconds 200 msec</span></span><br><span class="line"><span class="string">INFO  : Ended Job = job_1582600603106_1942</span></span><br><span class="line"><span class="string">INFO  : Starting task [Stage-9:CONDITIONAL] in parallel</span></span><br><span class="line"><span class="string">INFO  : Stage-11 is selected by condition resolver.</span></span><br><span class="line"><span class="string">INFO  : Stage-12 is filtered out by condition resolver.</span></span><br><span class="line"><span class="string">INFO  : Stage-2 is filtered out by condition resolver.</span></span><br><span class="line"><span class="string">INFO  : Starting task [Stage-11:MAPREDLOCAL] in serial mode</span></span><br><span class="line"><span class="string">20/06/18 14:55:17 WARN conf.HiveConf: HiveConf of name hive.server2.idle.session.timeout_check_operation does not exist</span></span><br><span class="line"><span class="string">INFO  : Execution completed successfully</span></span><br><span class="line"><span class="string">INFO  : MapredLocal task succeeded</span></span><br><span class="line"><span class="string">INFO  : Launching Job 3 out of 6</span></span><br><span class="line"><span class="string">INFO  : Starting task [Stage-7:MAPRED] in parallel</span></span><br><span class="line"><span class="string">INFO  : Number of reduce tasks is set to 0 since there'</span>s <span class="keyword">no</span> reduce <span class="keyword">operator</span></span><br><span class="line">INFO  : Cannot run job locally: <span class="keyword">Input</span> <span class="keyword">Size</span> (= <span class="number">3355673177</span>) <span class="keyword">is</span> larger <span class="keyword">than</span> hive.exec.mode.local.auto.inputbytes.max (= <span class="number">50000000</span>)</span><br><span class="line">INFO  : <span class="built_in">number</span> <span class="keyword">of</span> splits:<span class="number">11</span></span><br><span class="line">INFO  : Submitting tokens <span class="keyword">for</span> job: job_1582600603106_1943</span><br><span class="line">INFO  : The <span class="keyword">url</span> <span class="keyword">to</span> track the job: <span class="keyword">http</span>://cdh03:<span class="number">8088</span>/proxy/application_1582600603106_1943/</span><br><span class="line">INFO  : <span class="keyword">Starting</span> Job = job_1582600603106_1943, <span class="keyword">Tracking</span> <span class="keyword">URL</span> = <span class="keyword">http</span>://cdh03:<span class="number">8088</span>/proxy/application_1582600603106_1943/</span><br><span class="line">INFO  : <span class="keyword">Kill</span> Command = /opt/cloudera/parcels/CDH<span class="number">-5.16</span><span class="number">.1</span><span class="number">-1.</span>cdh5<span class="number">.16</span><span class="number">.1</span>.p0<span class="number">.3</span>/lib/hadoop/<span class="keyword">bin</span>/hadoop job  -<span class="keyword">kill</span> job_1582600603106_1943</span><br><span class="line">INFO  : Hadoop job information <span class="keyword">for</span> Stage<span class="number">-7</span>: <span class="built_in">number</span> <span class="keyword">of</span> mappers: <span class="number">11</span>; number of reducers: 0</span><br><span class="line">INFO  : 2020-06-18 14:55:27,886 Stage-7 map = 0%,  reduce = 0%</span><br><span class="line">INFO  : 2020-06-18 14:55:37,354 Stage-7 map = 9%,  reduce = 0%, Cumulative CPU 8.64 sec</span><br><span class="line">INFO  : 2020-06-18 14:55:38,419 Stage-7 map = 36%,  reduce = 0%, Cumulative CPU 38.01 sec</span><br><span class="line">INFO  : 2020-06-18 14:55:40,511 Stage-7 map = 55%,  reduce = 0%, Cumulative CPU 60.16 sec</span><br><span class="line">INFO  : 2020-06-18 14:55:41,569 Stage-7 map = 64%,  reduce = 0%, Cumulative CPU 71.54 sec</span><br><span class="line">INFO  : 2020-06-18 14:55:45,756 Stage-7 map = 73%,  reduce = 0%, Cumulative CPU 79.75 sec</span><br><span class="line">INFO  : 2020-06-18 14:55:46,813 Stage-7 map = 91%,  reduce = 0%, Cumulative CPU 96.4 sec</span><br><span class="line">INFO  : 2020-06-18 14:55:47,866 Stage-7 map = 100%,  reduce = 0%, Cumulative CPU 105.15 sec</span><br><span class="line">INFO  : MapReduce Total cumulative CPU time: 1 minutes 45 seconds 150 msec</span><br><span class="line">INFO  : Ended Job = job_1582600603106_1943</span><br><span class="line">INFO  : Launching Job 4 out of 6</span><br><span class="line">INFO  : Starting task [Stage-3:MAPRED] in parallel</span><br><span class="line">INFO  : Number of reduce tasks not specified. Estimated from input data size: 2</span><br><span class="line">INFO  : In order to <span class="keyword">change</span> the average <span class="keyword">load</span> <span class="keyword">for</span> a reducer (<span class="keyword">in</span> <span class="keyword">bytes</span>):</span><br><span class="line">INFO  :   <span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer=&lt;<span class="built_in">number</span>&gt;</span><br><span class="line">INFO  : <span class="keyword">In</span> <span class="keyword">order</span> <span class="keyword">to</span> <span class="keyword">limit</span> the maximum <span class="built_in">number</span> <span class="keyword">of</span> reducers:</span><br><span class="line">INFO  :   <span class="keyword">set</span> hive.exec.reducers.max=&lt;<span class="built_in">number</span>&gt;</span><br><span class="line">INFO  : <span class="keyword">In</span> <span class="keyword">order</span> <span class="keyword">to</span> <span class="keyword">set</span> a <span class="keyword">constant</span> <span class="built_in">number</span> <span class="keyword">of</span> reducers:</span><br><span class="line">INFO  :   <span class="keyword">set</span> mapreduce.job.reduces=&lt;<span class="built_in">number</span>&gt;</span><br><span class="line">INFO  : Cannot run job locally: <span class="keyword">Input</span> <span class="keyword">Size</span> (= <span class="number">82139608</span>) <span class="keyword">is</span> larger <span class="keyword">than</span> hive.exec.mode.local.auto.inputbytes.max (= <span class="number">50000000</span>)</span><br><span class="line">INFO  : <span class="built_in">number</span> <span class="keyword">of</span> splits:<span class="number">2</span></span><br><span class="line">INFO  : Submitting tokens <span class="keyword">for</span> job: job_1582600603106_1944</span><br><span class="line">INFO  : The <span class="keyword">url</span> <span class="keyword">to</span> track the job: <span class="keyword">http</span>://cdh03:<span class="number">8088</span>/proxy/application_1582600603106_1944/</span><br><span class="line">INFO  : <span class="keyword">Starting</span> Job = job_1582600603106_1944, <span class="keyword">Tracking</span> <span class="keyword">URL</span> = <span class="keyword">http</span>://cdh03:<span class="number">8088</span>/proxy/application_1582600603106_1944/</span><br><span class="line">INFO  : <span class="keyword">Kill</span> Command = /opt/cloudera/parcels/CDH<span class="number">-5.16</span><span class="number">.1</span><span class="number">-1.</span>cdh5<span class="number">.16</span><span class="number">.1</span>.p0<span class="number">.3</span>/lib/hadoop/<span class="keyword">bin</span>/hadoop job  -<span class="keyword">kill</span> job_1582600603106_1944</span><br><span class="line">INFO  : Hadoop job information <span class="keyword">for</span> Stage<span class="number">-3</span>: <span class="built_in">number</span> <span class="keyword">of</span> mappers: <span class="number">2</span>; number of reducers: 2</span><br><span class="line">INFO  : 2020-06-18 14:55:58,247 Stage-3 map = 0%,  reduce = 0%</span><br><span class="line">INFO  : 2020-06-18 14:56:07,695 Stage-3 map = 50%,  reduce = 0%, Cumulative CPU 8.96 sec</span><br><span class="line">INFO  : 2020-06-18 14:56:12,969 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 23.17 sec</span><br><span class="line">INFO  : 2020-06-18 14:56:19,310 Stage-3 map = 100%,  reduce = 50%, Cumulative CPU 28.33 sec</span><br><span class="line">INFO  : 2020-06-18 14:56:20,348 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 33.89 sec</span><br><span class="line">INFO  : MapReduce Total cumulative CPU time: 33 seconds 890 msec</span><br><span class="line">INFO  : Ended Job = job_1582600603106_1944</span><br><span class="line">INFO  : Launching Job 5 out of 6</span><br><span class="line">INFO  : Starting task [Stage-4:MAPRED] in parallel</span><br><span class="line">INFO  : Number of reduce tasks not specified. Estimated from input data size: 1</span><br><span class="line">INFO  : In order to <span class="keyword">change</span> the average <span class="keyword">load</span> <span class="keyword">for</span> a reducer (<span class="keyword">in</span> <span class="keyword">bytes</span>):</span><br><span class="line">INFO  :   <span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer=&lt;<span class="built_in">number</span>&gt;</span><br><span class="line">INFO  : <span class="keyword">In</span> <span class="keyword">order</span> <span class="keyword">to</span> <span class="keyword">limit</span> the maximum <span class="built_in">number</span> <span class="keyword">of</span> reducers:</span><br><span class="line">INFO  :   <span class="keyword">set</span> hive.exec.reducers.max=&lt;<span class="built_in">number</span>&gt;</span><br><span class="line">INFO  : <span class="keyword">In</span> <span class="keyword">order</span> <span class="keyword">to</span> <span class="keyword">set</span> a <span class="keyword">constant</span> <span class="built_in">number</span> <span class="keyword">of</span> reducers:</span><br><span class="line">INFO  :   <span class="keyword">set</span> mapreduce.job.reduces=&lt;<span class="built_in">number</span>&gt;</span><br><span class="line">INFO  : Selecting <span class="keyword">local</span> <span class="keyword">mode</span> <span class="keyword">for</span> task: Stage<span class="number">-4</span></span><br><span class="line">INFO  : <span class="built_in">number</span> <span class="keyword">of</span> splits:<span class="number">1</span></span><br><span class="line">INFO  : Submitting tokens <span class="keyword">for</span> job: job_1582600603106_1945</span><br><span class="line">INFO  : The <span class="keyword">url</span> <span class="keyword">to</span> track the job: <span class="keyword">http</span>://cdh03:<span class="number">8088</span>/proxy/application_1582600603106_1945/</span><br><span class="line">INFO  : <span class="keyword">Starting</span> Job = job_1582600603106_1945, <span class="keyword">Tracking</span> <span class="keyword">URL</span> = <span class="keyword">http</span>://cdh03:<span class="number">8088</span>/proxy/application_1582600603106_1945/</span><br><span class="line">INFO  : <span class="keyword">Kill</span> Command = /opt/cloudera/parcels/CDH<span class="number">-5.16</span><span class="number">.1</span><span class="number">-1.</span>cdh5<span class="number">.16</span><span class="number">.1</span>.p0<span class="number">.3</span>/lib/hadoop/<span class="keyword">bin</span>/hadoop job  -<span class="keyword">kill</span> job_1582600603106_1945</span><br><span class="line">INFO  : Hadoop job information <span class="keyword">for</span> Stage<span class="number">-4</span>: <span class="built_in">number</span> <span class="keyword">of</span> mappers: <span class="number">1</span>; number of reducers: 1</span><br><span class="line">INFO  : 2020-06-18 14:56:28,550 Stage-4 map = 0%,  reduce = 0%</span><br><span class="line">INFO  : 2020-06-18 14:56:33,832 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.51 sec</span><br><span class="line">INFO  : 2020-06-18 14:56:40,167 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.94 sec</span><br><span class="line">INFO  : MapReduce Total cumulative CPU time: 4 seconds 940 msec</span><br><span class="line">INFO  : Ended Job = job_1582600603106_1945</span><br><span class="line">INFO  : MapReduce Jobs Launched: </span><br><span class="line">INFO  : Stage-Stage-10: Map: 1   Cumulative CPU: 3.2 sec   HDFS Read: 6389 HDFS Write: 1199 SUCCESS</span><br><span class="line">INFO  : Stage-Stage-7: Map: 11   Cumulative CPU: 105.15 sec   HDFS Read: 3356995378 HDFS Write: 82139608 SUCCESS</span><br><span class="line">INFO  : Stage-Stage-3: Map: 2  Reduce: 2   Cumulative CPU: 33.89 sec   HDFS Read: 82150071 HDFS Write: 584 SUCCESS</span><br><span class="line">INFO  : Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.94 sec   HDFS Read: 5695 HDFS Write: 140 SUCCESS</span><br><span class="line">INFO  : Total MapReduce CPU Time Spent: 2 minutes 27 seconds 180 msec</span><br><span class="line">INFO  : Completed executing command(queryId=hive_20200618145454_29fb89e3-b5e6-41ef-8e8a-e2635722656f); Time taken: 108.915 seconds</span><br><span class="line">INFO  : OK</span><br></pre></td></tr></table></figure><p>三个参数：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 每个reduce处理的数据量字节数，默认64M</span></span><br><span class="line"><span class="comment">-- 例如：如果输入大小为 10GiB并且该项设置为1GiB，Hive 将使用10个reducer</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer = <span class="number">67108864</span> </span><br><span class="line"><span class="comment">-- 每个job的最大reduce数量，默认1099</span></span><br><span class="line"><span class="comment">-- 如果配置参数mapreduce.job.reduces为负，则Hive会将reducer数量限制为此参数的值。</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.max=<span class="number">1099</span></span><br><span class="line"><span class="comment">-- 设定每个job的reduce数量，默认-1，表示不限制</span></span><br><span class="line"><span class="comment">-- 每项作业中reduce任务的默认数量。通常设置为接近可用主机数量的素数。Hadoop 默认将该项设置为 1，而 Hive -- 默认使用 -1。当设置为 -1 时，Hive 将自动决定用于每项作业的 reducer 的适用数量</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces = <span class="number">-1</span></span><br></pre></td></tr></table></figure><p>mapreduce.input.fileinputformat.split.minsize = 1默认</p><p>reduce数量计算：3357127507/67108864≈51</p><p>数据总行数：54925330  </p><p>数据总大小：3300747847</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数仓|Hive性能调优指北</title>
      <link href="/2020/06/06/%E6%95%B0%E4%BB%93-Hive%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E6%8C%87%E5%8C%97/"/>
      <url>/2020/06/06/%E6%95%B0%E4%BB%93-Hive%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E6%8C%87%E5%8C%97/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>在企业中使用Hive构建离线数仓是一种十分普遍的方案。尽管Hive的使用场景是通过批处理的方式处理大数据，通常对处理时间不敏感。但是在资源有限的情况下，我们需要关注Hive的性能调优，从而方便数据的快速产出。同时，关于Hive的性能调优，也是面试中比较常见的问题，因此掌握Hive性能调优的一些方法，不仅能够在工作中提升效率而且还可以在面试中脱颖而出。本文会通过四个方面介绍Hive性能调优，主要包括：</p><ul><li>性能调优的工具</li><li>设计优化</li><li>数据存储优化</li><li>作业优化</li></ul><h2 id="性能调优的工具"><a href="#性能调优的工具" class="headerlink" title="性能调优的工具"></a>性能调优的工具</h2><p>HQL提供了两个查看查询性能的工具：<strong>explain</strong>与<strong>analyze</strong>，除此之外Hive的日志也提供了非常详细的信息，方便查看执行性能和报错排查。</p><h3 id="善用explain语句"><a href="#善用explain语句" class="headerlink" title="善用explain语句"></a>善用explain语句</h3><p>explain语句是查看执行计划经常使用的一个工具，可以使用该语句分析查询执行计划，具体使用语法如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span> [FORMATTED|<span class="keyword">EXTENDED</span>|DEPENDENCY|AUTHORIZATION] hql_query</span><br></pre></td></tr></table></figure><p>上面的执行语句中，有4个可选的关键字，其具体含义如下：</p><ul><li>FORMATTED：对执行计划进行格式化，返回JSON格式的执行计划</li><li>EXTENDED：提供一些额外的信息，比如文件的路径信息</li><li>DEPENDENCY：以JSON格式返回查询所依赖的表和分区的列表，从Hive0.10开始使用，如下图</li></ul><p><img src="//jiamaoxiang.top/2020/06/06/数仓-Hive性能调优指北/%E4%BE%9D%E8%B5%96.png" alt></p><ul><li>AUTHORIZATION：列出需要被授权的条目，包括输入与输出，从Hive0.14开始使用,如下图</li></ul><p><img src="//jiamaoxiang.top/2020/06/06/数仓-Hive性能调优指北/explain%E6%8E%88%E6%9D%83.png" alt></p><p>一个典型的查询执行计划主要包括三部分，具体如下：</p><ul><li><strong>Abstract Syntax Tree (AST)</strong>：抽象语法树，Hive使用一个称之为antlr的解析生成器，可以自动地将HQL生成为抽象语法树</li><li><strong>Stage Dependencies</strong>：会列出运行查询所有的依赖以及stage的数量</li><li><strong>Stage Plans</strong>：包含了非常重要的信息，比如运行作业时的operator 和sort orders</li></ul><h4 id="举个栗子"><a href="#举个栗子" class="headerlink" title="举个栗子"></a>举个栗子</h4><p>假设有一张表：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> employee_partitioned</span><br><span class="line">(</span><br><span class="line">  <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">  work_place <span class="built_in">ARRAY</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">  gender_age <span class="keyword">STRUCT</span>&lt;gender:<span class="keyword">string</span>,age:<span class="built_in">int</span>&gt;,</span><br><span class="line">  skills_score <span class="keyword">MAP</span>&lt;<span class="keyword">string</span>,<span class="built_in">int</span>&gt;,</span><br><span class="line">  depart_title <span class="keyword">MAP</span>&lt;<span class="keyword">STRING</span>,<span class="built_in">ARRAY</span>&lt;<span class="keyword">STRING</span>&gt;&gt;</span><br><span class="line">)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (<span class="keyword">Year</span> <span class="built_in">INT</span>, <span class="keyword">Month</span> <span class="built_in">INT</span>)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line"><span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'|'</span></span><br><span class="line">COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span></span><br><span class="line"><span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">':'</span>;</span><br></pre></td></tr></table></figure><p>查看执行计划：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span></span><br><span class="line"><span class="keyword">SELECT</span> gender_age.gender,</span><br><span class="line">       <span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> employee_partitioned</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">YEAR</span>=<span class="number">2020</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> gender_age.gender</span><br><span class="line"><span class="keyword">LIMIT</span> <span class="number">2</span>;</span><br></pre></td></tr></table></figure><p>执行计划概览：</p><p><img src="//jiamaoxiang.top/2020/06/06/数仓-Hive性能调优指北/%E6%9F%A5%E8%AF%A2%E8%AE%A1%E5%88%921.png" alt></p><p>如上图：<em>Map/Reduce operator tree</em>是抽象语法树<strong>AST</strong>部分；<strong>STAGE<br>DEPENDENCIES</strong>包括三个阶段：Stage-0 、Stage-1及Stage-2，其中Stage-0 是root stage，即Stage-1与Stage-2依赖于Stage-0；<strong>STAGE PLANS</strong>部分，Stage-1与Stage2都包含一个Map Operator Tree和一个Reduce Operator Tree，Stage-0不包含map和reduce，仅仅是一个fetch数据的操作。</p><p>执行计划详细信息：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage-1 is a root stage</span><br><span class="line">  Stage-2 depends on stages: Stage-1</span><br><span class="line">  Stage-0 depends on stages: Stage-2</span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage-1</span><br><span class="line">    Map Reduce</span><br><span class="line">      Map Operator Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            <span class="built_in">alias</span>: employee_partitioned</span><br><span class="line">            filterExpr: (year = 2020) (<span class="built_in">type</span>: boolean)</span><br><span class="line">            Statistics: Num rows: 1 Data size: 227 Basic stats: PARTIAL Column stats: NONE</span><br><span class="line">            Select Operator</span><br><span class="line">              expressions: gender_age (<span class="built_in">type</span>: struct&lt;gender:string,age:int&gt;)</span><br><span class="line">              outputColumnNames: gender_age</span><br><span class="line">              Statistics: Num rows: 1 Data size: 227 Basic stats: PARTIAL Column stats: NONE</span><br><span class="line">              Reduce Output Operator</span><br><span class="line">                key expressions: gender_age.gender (<span class="built_in">type</span>: string)</span><br><span class="line">                sort order: +</span><br><span class="line">                Map-reduce partition columns: rand() (<span class="built_in">type</span>: double)</span><br><span class="line">                Statistics: Num rows: 1 Data size: 227 Basic stats: PARTIAL Column stats: NONE</span><br><span class="line">      Reduce Operator Tree:</span><br><span class="line">        Group By Operator</span><br><span class="line">          aggregations: count()</span><br><span class="line">          keys: KEY._col0 (<span class="built_in">type</span>: string)</span><br><span class="line">          mode: partial1</span><br><span class="line">          outputColumnNames: _col0, _col1</span><br><span class="line">          Statistics: Num rows: 1 Data size: 227 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">          File Output Operator</span><br><span class="line">            compressed: <span class="literal">false</span></span><br><span class="line">            table:</span><br><span class="line">                input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe</span><br><span class="line"></span><br><span class="line">  Stage: Stage-2</span><br><span class="line">    Map Reduce</span><br><span class="line">      Map Operator Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            Reduce Output Operator</span><br><span class="line">              key expressions: _col0 (<span class="built_in">type</span>: string)</span><br><span class="line">              sort order: +</span><br><span class="line">              Map-reduce partition columns: _col0 (<span class="built_in">type</span>: string)</span><br><span class="line">              Statistics: Num rows: 1 Data size: 227 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">              value expressions: _col1 (<span class="built_in">type</span>: bigint)</span><br><span class="line">      Reduce Operator Tree:</span><br><span class="line">        Group By Operator</span><br><span class="line">          aggregations: count(VALUE._col0)</span><br><span class="line">          keys: KEY._col0 (<span class="built_in">type</span>: string)</span><br><span class="line">          mode: final</span><br><span class="line">          outputColumnNames: _col0, _col1</span><br><span class="line">          Statistics: Num rows: 1 Data size: 227 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">          Limit</span><br><span class="line">            Number of rows: 2</span><br><span class="line">            Statistics: Num rows: 1 Data size: 227 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">            File Output Operator</span><br><span class="line">              compressed: <span class="literal">false</span></span><br><span class="line">              Statistics: Num rows: 1 Data size: 227 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">              table:</span><br><span class="line">                  input format: org.apache.hadoop.mapred.TextInputFormat</span><br><span class="line">                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</span><br><span class="line">                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><br><span class="line"></span><br><span class="line">  Stage: Stage-0</span><br><span class="line">    Fetch Operator</span><br><span class="line">      <span class="built_in">limit</span>: 2</span><br><span class="line">      Processor Tree:</span><br><span class="line">        ListSink</span><br></pre></td></tr></table></figure><h3 id="巧用analyze语句"><a href="#巧用analyze语句" class="headerlink" title="巧用analyze语句"></a>巧用analyze语句</h3><p>analyze语句可以收集一些详细的统计信息，比如表的行数、文件数、数据的大小等信息。这些统计信息作为元数据存储在hive的元数据库中。Hive支持表、分区和列级别的统计(与Impala类似)，这些信息作为Hive基于成本优化策略(Cost-Based Optimizer (CBO))的输入,该优化器的主要作用是选择耗费最小系统资源的查询计划。其实，在Hive3.2.0版本中，可以自动收集这些统计信息，当然也可以通过analyze语句进行手动统计表、分区或者字段的信息。具体的使用方式如下：</p><ul><li>1.收集表的统计信息(非分区表)，当指定NOSCAN关键字时，会忽略扫描文件内容，仅仅统计文件的数量与大小，速度会比较快</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 不使用NOSCAN关键字</span></span><br><span class="line">hive&gt; ANALYZE TABLE user_behavior  COMPUTE STATISTICS;</span><br><span class="line">...</span><br><span class="line">Table default.user_behavior stats: [numFiles=1, numRows=10, totalSize=229, rawDataSize=219]</span><br><span class="line">Time taken: 23.504 seconds</span><br><span class="line"><span class="comment">-- 使用NOSCAN关键字</span></span><br><span class="line">hive&gt; ANALYZE TABLE user_behavior  COMPUTE STATISTICS NOSCAN;</span><br><span class="line">Table default.user_behavior stats: [numFiles=1, numRows=10, totalSize=229, rawDataSize=219]</span><br><span class="line">Time taken: 0.309 seconds</span><br></pre></td></tr></table></figure><ul><li>2.收集分区表的统计信息</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 收集具体分区的统计信息</span></span><br><span class="line">hive&gt; ANALYZE TABLE employee_partitioned PARTITION(year=2020, month=06) COMPUTE STATISTICS;</span><br><span class="line">...</span><br><span class="line">Partition default.employee_partitioned&#123;year=2020, month=06&#125; stats: [numFiles=1, numRows=0, totalSize=227, rawDataSize=0]</span><br><span class="line">Time taken: 19.283 seconds</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 收集所有分区的统计信息</span></span><br><span class="line">hive&gt; ANALYZE TABLE employee_partitioned PARTITION(year, month) COMPUTE STATISTICS;</span><br><span class="line">...</span><br><span class="line">Partition default.employee_partitioned&#123;year=2020, month=06&#125; stats: [numFiles=1, numRows=0, totalSize=227, rawDataSize=0]</span><br><span class="line">Time taken: 17.528 seconds</span><br></pre></td></tr></table></figure><ul><li>3.收集表的某个字段的统计信息</li></ul><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; ANALYZE TABLE user_behavior COMPUTE STATISTICS FOR COLUMNS user_id ;</span><br></pre></td></tr></table></figure><blockquote><p><strong>尖叫提示</strong>：</p><p>可以通过设置：<em>SET hive.stats.autogather=true</em>，进行自动收集统计信息，对于INSERT OVERWRITE/INTO操作的表或者分区，可以自动收集统计信息。值得注意的是，LOAD操作不能够自动收集统计信息</p></blockquote><p>一旦这些统计信息收集完毕，可以通过DESCRIBE EXTENDED/FORMATTED语句查询统计信息，具体使用如下：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查看一个分区的统计信息</span></span><br><span class="line">hive&gt; DESCRIBE FORMATTED employee_partitioned PARTITION(year=2020, month=06);</span><br><span class="line">...</span><br><span class="line">Partition Parameters:            </span><br><span class="line">        COLUMN_STATS_ACCURATE   true                </span><br><span class="line">        numFiles                1                   </span><br><span class="line">        numRows                 0                   </span><br><span class="line">        rawDataSize             0                   </span><br><span class="line">        totalSize               227                 </span><br><span class="line">        transient_lastDdlTime   1591437967 </span><br><span class="line">...</span><br><span class="line"><span class="comment">-- 查看一张表的统计信息</span></span><br><span class="line">hive&gt; DESCRIBE FORMATTED employee_partitioned;</span><br><span class="line">...</span><br><span class="line">Table Parameters:                </span><br><span class="line">        numPartitions           1                   </span><br><span class="line">        transient_lastDdlTime   1591431482 </span><br><span class="line">...</span><br><span class="line"><span class="comment">-- 查看某列的统计信息</span></span><br><span class="line">hive&gt; DESCRIBE FORMATTED  user_behavior.user_id;</span><br></pre></td></tr></table></figure><h3 id="常用日志分析"><a href="#常用日志分析" class="headerlink" title="常用日志分析"></a>常用日志分析</h3><p>日志提供了job运行的详细信息，通过查看日志信息，可以分析出导致作业执行瓶颈的问题，主要包括两种类型的日志：系统日志和作业日志。</p><p>系统日志包含了Hive运行时的状态等信息，可以通过{HIVE_HOME}/conf/hive-log4j.properties文件进行配置，主要的配置选项有：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive.root.logger=WARN,DRFA <span class="comment">## 日志级别</span></span><br><span class="line">hive.log.dir=/tmp/<span class="variable">$&#123;user.name&#125;</span> <span class="comment">## 日志路径</span></span><br><span class="line">hive.log.file=hive.log <span class="comment">## 日志名称</span></span><br></pre></td></tr></table></figure><p>也可以通过Hive cli命令行设置日志级别：<code>$hive --hiveconf hive.root.logger=DEBUG,console</code>这种方式只能在当前会话生效。</p><p>作业日志所包含的作业信息通常是由YARN管理的，可以通过<code>yarn logs -applicationId &lt;application_id&gt;</code>命令查看作业日志。</p><h2 id="设计优化"><a href="#设计优化" class="headerlink" title="设计优化"></a>设计优化</h2><h3 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h3><p>对于一张比较大的表，将其设计成分区表可以提升查询的性能，对于一个特定分区的查询，只会加载对应分区路径的文件数据，所以执行速度会比较快。值得注意的是，分区字段的选择是影响查询性能的重要因素，尽量避免层级较深的分区，这样会造成太多的子文件夹。一些常见的分区字段可以是：</p><ul><li>日期或者时间</li></ul><p>比如year、month、day或者hour，当表中存在时间或者日期字段时，可以使用些字段。</p><ul><li>地理位置</li></ul><p>比如国家、省份、城市等</p><ul><li>业务逻辑</li></ul><p>比如部门、销售区域、客户等等</p><h3 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h3><p>与分区表类似，分桶表的组织方式是将HDFS上的文件分割成多个文件。分桶可以加快数据采样，也可以提升join的性能(join的字段是分桶字段)，因为分桶可以确保某个key对应的数据在一个特定的桶内(文件)，所以巧妙地选择分桶字段可以大幅度提升join的性能。通常情况下，分桶字段可以选择经常用在过滤操作或者join操作的字段。</p><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><p>创建索引是关系型数据库性能调优的常见手段，在Hive中也不例外。Hive从0.7版本开始支持索引，使用索引相比全表扫描而言，是一种比较廉价的操作，Hive中创建索引的方式如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">INDEX</span> idx_user_id_user_behavior</span><br><span class="line"><span class="keyword">ON</span> <span class="keyword">TABLE</span> user_behavior (user_id)</span><br><span class="line"><span class="keyword">AS</span> <span class="string">'COMPACT'</span></span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">DEFERRED</span> <span class="keyword">REBUILD</span>;</span><br></pre></td></tr></table></figure><p>上面创建的是COMPACT索引，存储的是索引列与其对应的block id的pair对。除了此种索引外，Hive还支持位图索引(BITMAP),使用方式如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">INDEX</span> idx_behavior_user_behavior</span><br><span class="line"><span class="keyword">ON</span> <span class="keyword">TABLE</span> user_behavior (behavior)</span><br><span class="line"><span class="keyword">AS</span> <span class="string">'BITMAP'</span></span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">DEFERRED</span> <span class="keyword">REBUILD</span>;</span><br></pre></td></tr></table></figure><p>上面创建的索引时，使用了<code>WITH DEFERRED REBUILD</code>选项，该选项可以避免索引立即被创建，当建立索引时，可以使用<code>LTER...REBUILD</code>命令(见下面的示例)，值得注意的是：当基表(被创建索引的表)发生变化时，该命令需要被再次执行以便更新索引到最新的状态。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">INDEX</span> idx_user_id_user_behavior <span class="keyword">ON</span> user_behavior <span class="keyword">REBUILD</span>;</span><br></pre></td></tr></table></figure><p>一旦索引创建成功，会生成一张索引表，表的名称格式为：<code>数据库名__表名_索引名__</code>，可以使用下面的命令查看索引：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; SHOW TABLES '*idx*';</span><br><span class="line">OK</span><br><span class="line">default__user_behavior_idx_user_id_user_behavior__</span><br><span class="line">Time taken: 0.044 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p>索引表包含索引列、HDFS的文件URI以及每行的偏移量，可以通过下面命令查看：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查看索引表结构</span></span><br><span class="line">hive&gt; DESC default__user_behavior_idx_user_id_user_behavior__;</span><br><span class="line">OK</span><br><span class="line">user_id                 int                                         </span><br><span class="line">_bucketname             string                                      </span><br><span class="line">_offsets                array&lt;bigint&gt;                               </span><br><span class="line">Time taken: 0.109 seconds, Fetched: 3 row(s)</span><br><span class="line"><span class="comment">-- 查看索引表内容</span></span><br><span class="line">hive&gt; SELECT * FROM default__user_behavior_idx_user_id_user_behavior__;</span><br><span class="line">OK</span><br><span class="line">9       hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [181]</span><br><span class="line">7       hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [136]</span><br><span class="line">1       hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [0]</span><br><span class="line">6       hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [113]</span><br><span class="line">5       hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [90]</span><br><span class="line">10      hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [205]</span><br><span class="line">4       hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [66]</span><br><span class="line">8       hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [158]</span><br><span class="line">3       hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [44]</span><br><span class="line">2       hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [22]</span><br><span class="line">Time taken: 0.28 seconds, Fetched: 10 row(s)</span><br></pre></td></tr></table></figure><p>如果要删除索引，可以使用DROP INDEX命令，如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">INDEX</span> idx_user_id_user_behavior <span class="keyword">ON</span> user_behavior;</span><br></pre></td></tr></table></figure><h3 id="使用skewed-temporary表"><a href="#使用skewed-temporary表" class="headerlink" title="使用skewed/temporary表"></a>使用skewed/temporary表</h3><p>Hive除了可以使用内部表、外部表、分区表、分桶表之外，也可以使用skewed/temporary表，也可以在一定程度上提升性能。</p><p>Hive从0.10版本之后开始支持skewed表，该表可以缓解数据倾斜。这种表之所以能够提升性能，是因为可以自动将造成数据倾斜的数据分割成不同的文件或者路径。使用示例如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sample_skewed_table (</span><br><span class="line">dept_no <span class="built_in">int</span>, </span><br><span class="line">dept_name <span class="keyword">string</span></span><br><span class="line">) </span><br><span class="line">SKEWED <span class="keyword">BY</span> (dept_no) <span class="keyword">ON</span> (<span class="number">1000</span>, <span class="number">2000</span>);<span class="comment">-- 指定数据倾斜字段</span></span><br></pre></td></tr></table></figure><p>另外，还可以使用temporary临时表，将公共使用部分的数据集建成临时表，同时临时表支持SSD或memory的数据存储，从而可以提升性能。</p><h2 id="数据存储优化"><a href="#数据存储优化" class="headerlink" title="数据存储优化"></a>数据存储优化</h2><h3 id="文件格式"><a href="#文件格式" class="headerlink" title="文件格式"></a>文件格式</h3><p>Hive支持TEXTFILE, SEQUENCEFILE, AVRO, RCFILE, ORC,以及PARQUET文件格式，可以通过两种方式指定表的文件格式：</p><ul><li>CREATE TABLE … STORE AS <file_format>:即在建表时指定文件格式，默认是TEXTFILE</file_format></li><li>ALTER TABLE … [PARTITION partition_spec] SET FILEFORMAT <file_format>:修改具体表的文件格式</file_format></li></ul><p>一旦存储文件格式为TEXT的表被创建，可以直接通过load命令装载一个text类型的文件。我们可以先使用此命令将数据装载到一张TEXT格式的表中，然后在通过<code>INSERT OVERWRITE/INTO TABLE ... SELECT</code>命令将数据装载到其他文件格式的表中。</p><blockquote><p><strong>尖叫提示</strong>：</p><p>如果要改变创建表的默认文件格式，可以使用hive.default.fileformat=<file_format>进行配置，改配置可以针对所有表。同时也可以使用hive.default.fileformat.managed =<br><file_format>进行配置，改配置仅适用于内部表或外部表</file_format></file_format></p></blockquote><p>TEXT, SEQUENCE和 AVRO文件是面向行的文件存储格式，不是最佳的文件格式，因为即便是只查询一列数据，使用这些存储格式的表也需要读取完整的一行数据。另一方面，面向列的存储格式(RCFILE, ORC, PARQUET)可以很好地解决上面的问题。关于每种文件格式的说明，如下：</p><ul><li>TEXTFILE</li></ul><p>创建表时的默认文件格式，数据被存储成文本格式。文本文件可以被分割和并行处理，也可以使用压缩，比如GZip、LZO或者Snappy。然而大部分的压缩文件不支持分割和并行处理，会造成一个作业只有一个mapper去处理数据，使用压缩的文本文件要确保文件的不要过大，一般接近两个HDFS块的大小。</p><ul><li>SEQUENCEFILE</li></ul><p>key/value对的二进制存储格式，sequence文件的优势是比文本格式更好压缩，sequence文件可以被压缩成块级别的记录，块级别的压缩是一个很好的压缩比例。如果使用块压缩，需要使用下面的配置：set hive.exec.compress.output=true; set io.seqfile.compression.type=BLOCK</p><ul><li>AVRO</li></ul><p>二进制格式文件，除此之外，avro也是一个序列化和反序列化的框架。avro提供了具体的数据schema。</p><ul><li>RCFILE</li></ul><p>全称是Record Columnar File，首先将表分为几个行组，对每个行组内的数据进行按列存储，每一列的数据都是分开存储，即先水平划分，再垂直划分。</p><ul><li>ORC</li></ul><p>全称是Optimized Row Columnar，从hive0.11版本开始支持，ORC格式是RCFILE格式的一种优化的格式，提供了更大的默认块(256M)</p><ul><li>PARQUET</li></ul><p>另外一种列式存储的文件格式，与ORC非常类似，与ORC相比，Parquet格式支持的生态更广，比如低版本的impala不支持orc格式</p><h3 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h3><p>压缩技术可以减少map与reduce之间的数据传输，从而可以提升查询性能，关于压缩的配置可以在hive的命令行中或者hive-site.xml文件中进行配置</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.exec.compress.intermediate=<span class="literal">true</span></span><br></pre></td></tr></table></figure><p>开启压缩之后，可以选择下面的压缩格式：</p><table><thead><tr><th>压缩格式</th><th>codec</th><th>扩展名</th><th>支持分割</th></tr></thead><tbody><tr><td>Deflate</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>.deflate</td><td>N</td></tr><tr><td>Gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td><td>.gz</td><td>N</td></tr><tr><td>Bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td><td>.gz</td><td>Y</td></tr><tr><td>LZO</td><td>com.apache.compression.lzo.LzopCodec</td><td>.lzo</td><td>N</td></tr><tr><td>LZ4</td><td>org.apache.hadoop.io.compress.Lz4Codec</td><td>.lz4</td><td>N</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td><td>.snappy</td><td>N</td></tr></tbody></table><p>关于压缩的编码器可以通过mapred-site.xml, hive-site.xml进行配置，也可以通过命令行进行配置,比如：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-- 中间结果压缩</span><br><span class="line">SET hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec</span><br><span class="line">-- 输出结果压缩</span><br><span class="line">SET hive.exec.compress.output=true;</span><br><span class="line">SET mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodc</span><br></pre></td></tr></table></figure><h3 id="存储优化"><a href="#存储优化" class="headerlink" title="存储优化"></a>存储优化</h3><p>经常被访问的数据称之为热数据，可以针对热数据提升查询的性能。比如通过增加热数据的副本数，可以增加数据本地性命中的可能性，从而提升查询性能，当然这要与存储容量之间做出权衡。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ hdfs dfs -setrep -R -w 4 /user/hive/warehouse/employee</span><br></pre></td></tr></table></figure><p>注意，大量的小文件或者冗余副本会造成namenode节点内存耗费，尤其是大量小于HDFS块大小的文件。HDSF本身提供了应对小文件的解决方案：</p><ul><li>Hadoop Archive/HAR:将小文件打包成大文件</li><li>SEQUENCEFILE格式：将小文件压缩成大文件</li><li>CombineFileInputFormat:在map和reduce处理之前组合小文件</li><li>HDFS Federation:HDFS联盟，使用多个namenode节点管理文件</li></ul><p>对于Hive而言，可以使用下面的配置将查询结果的文件进行合并，从而避免产生小文件：</p><ul><li>hive.merge.mapfiles: 在一个仅有map的作业中，合并最后的结果文件，默认为true</li><li>hive.merge.mapredfiles:合并mapreduce作业的结果小文件 默认false，可以设置true</li><li>hive.merge.size.per.task:定义合并文件的大小，默认 256,000,000，即256MB</li><li>hive.merge.smallfiles.avgsize: T触发文件合并的文件大小阈值，默认值是16,000,000</li></ul><p>当一个作业的输出结果文件的大小小于hive.merge.smallfiles.avgsize设定的阈值，并且hive.merge.mapfiles与hive.merge.mapredfiles设置为true，Hive会额外启动一个mr作业将输出小文件合并成大文件。</p><h2 id="作业优化"><a href="#作业优化" class="headerlink" title="作业优化"></a>作业优化</h2><h3 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h3><p>当Hive处理的数据量较小时，启动分布式去处理数据会有点浪费，因为可能启动的时间比数据处理的时间还要长，从Hive0.7版本之后，Hive支持将作业动态地转为本地模式，需要使用下面的配置：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.exec.mode.local.auto=<span class="literal">true</span>; <span class="comment">-- 默认 false</span></span><br><span class="line"><span class="keyword">SET</span> hive.exec.mode.local.auto.inputbytes.max=<span class="number">50000000</span>;</span><br><span class="line"><span class="keyword">SET</span> hive.exec.mode.local.auto.input.files.max=<span class="number">5</span>; <span class="comment">-- 默认 4</span></span><br></pre></td></tr></table></figure><p>一个作业只要满足下面的条件，会启用本地模式</p><ul><li>输入文件的大小小于<code>hive.exec.mode.local.auto.inputbytes.max</code>配置的大小</li><li>map任务的数量小于<code>hive.exec.mode.local.auto.input.files.max</code>配置的大小</li><li>reduce任务的数量是1或者0</li></ul><h3 id="JVM重用"><a href="#JVM重用" class="headerlink" title="JVM重用"></a>JVM重用</h3><p>默认情况下，Hadoop会为为一个map或者reduce启动一个JVM，这样可以并行执行map和reduce。当map或者reduce是那种仅运行几秒钟的轻量级作业时，JVM启动进程所耗费的时间会比作业执行的时间还要长。Hadoop可以重用JVM，通过共享JVM以串行而非并行的方式运行map或者reduce。JVM的重用适用于同一个作业的map和reduce，对于不同作业的task不能够共享JVM。如果要开启JVM重用，需要配置一个作业最大task数量，默认值为1，如果设置为-1，则表示不限制：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SET mapreduce.job.jvm.numtasks=5;</span><br></pre></td></tr></table></figure><p>这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。</p><h3 id="并行执行"><a href="#并行执行" class="headerlink" title="并行执行"></a>并行执行</h3><p>Hive的查询通常会被转换成一系列的stage，这些stage之间并不是一直相互依赖的，所以可以并行执行这些stage，可以通过下面的方式进行配置：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SET hive.exec.parallel=<span class="literal">true</span>; -- 默认<span class="literal">false</span></span><br><span class="line">SET hive.exec.parallel.thread.number=16; -- 默认8</span><br></pre></td></tr></table></figure><p>并行执行可以增加集群资源的利用率，如果集群的资源使用率已经很高了，那么并行执行的效果不会很明显。</p><h3 id="Fetch模式"><a href="#Fetch模式" class="headerlink" title="Fetch模式"></a>Fetch模式</h3><p>Fetch模式是指Hive中对某些情况的查询可以不必使用MapReduce计算。可以简单地读取表对应的存储目录下的文件，然后输出查询结果到控制台。在开启fetch模式之后，在全局查找、字段查找、limit查找等都启动mapreduce，通过下面方式进行配置：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive.fetch.task.conversion=more</span><br></pre></td></tr></table></figure><h3 id="JOIN优化"><a href="#JOIN优化" class="headerlink" title="JOIN优化"></a>JOIN优化</h3><h4 id="普通join"><a href="#普通join" class="headerlink" title="普通join"></a>普通join</h4><p>普通join又称之为reduce端join，是一种最基本的join，并且耗时较长。对于大表join小表，需要将大表放在右侧，即小表join大表。新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。</p><h4 id="map端join"><a href="#map端join" class="headerlink" title="map端join"></a>map端join</h4><p>map端join适用于当一张表很小(可以存在内存中)的情况，即可以将小表加载至内存。Hive从0.7开始支持自动转为map端join，具体配置如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SET hive.auto.convert.join=<span class="literal">true</span>; --  hivev0.11.0之后默认<span class="literal">true</span></span><br><span class="line">SET hive.mapjoin.smalltable.filesize=600000000; -- 默认 25m</span><br><span class="line">SET hive.auto.convert.join.noconditionaltask=<span class="literal">true</span>; -- 默认<span class="literal">true</span>，所以不需要指定map join hint</span><br><span class="line">SET hive.auto.convert.join.noconditionaltask.size=10000000; -- 控制加载到内存的表的大小</span><br></pre></td></tr></table></figure><p>一旦开启map端join配置，Hive会自动检查小表是否大于<code>hive.mapjoin.smalltable.filesize</code>配置的大小，如果大于则转为普通的join，如果小于则转为map端join。</p><p>关于map端join的原理，如下图所示：</p><p><img src="//jiamaoxiang.top/2020/06/06/数仓-Hive性能调优指北/map%E7%AB%AFjoin.png" alt></p><p>首先，Task A(客户端本地执行的task)负责读取小表a，并将其转成一个HashTable的数据结构，写入到本地文件，之后将其加载至分布式缓存。</p><p>然后，Task B任务会启动map任务读取大表b，在Map阶段，根据每条记录与分布式缓存中的a表对应的hashtable关联，并输出结果</p><p>注意：map端join没有reduce任务，所以map直接输出结果，即有多少个map任务就会产生多少个结果文件。</p><h4 id="Bucket-map-join"><a href="#Bucket-map-join" class="headerlink" title="Bucket map join"></a>Bucket map join</h4><p>bucket map join是一种特殊的map端join，主要区别是其应用在分桶表上。如果要开启分桶的map端join，需要开启一下配置：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.auto.convert.join=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">SET</span> hive.optimize.bucketmapjoin=<span class="literal">true</span>; <span class="comment">-- 默认false</span></span><br></pre></td></tr></table></figure><p>在一个分桶的map端join中，所有参与join的表必须是分桶表，并且join的字段是分桶字段(通过CLUSTERED BY指定)，另外，对于大表的分桶数量必须是小表分桶数量的倍数。</p><p>与普通的join相比，分桶join仅仅只读取所需要的桶数据，不需要全表扫描。</p><h4 id="Sort-merge-bucket-SMB-join"><a href="#Sort-merge-bucket-SMB-join" class="headerlink" title="Sort merge bucket (SMB) join"></a>Sort merge bucket (SMB) join</h4><p>SMBjoin应用与分桶表，如果两张参与join的表是排序的，并且分桶字段相同，这样可以使用sort-merge join，其优势在于不用把小表完全加载至内存中，会读取两张分桶表对应的桶，执行普通join(包括map与reduce)配置如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.input.format=</span><br><span class="line">org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;</span><br><span class="line"><span class="keyword">SET</span> hive.auto.convert.sortmerge.join=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">SET</span> hive.optimize.bucketmapjoin=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">SET</span> hive.optimize.bucketmapjoin.sortedmerge=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">SET</span> hive.auto.convert.sortmerge.join.noconditionaltask=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure><h4 id="Sort-merge-bucket-map-SMBM-join"><a href="#Sort-merge-bucket-map-SMBM-join" class="headerlink" title="Sort merge bucket map (SMBM) join"></a>Sort merge bucket map (SMBM) join</h4><p>SMBM join是一种特殊的bucket map join，与map端join不同的是，不用将小表的所有数据行都加载至内存中。使用SMBM join，参与join的表必须是排序的，有着相同的分桶字段，并且join字段与分桶字段相同。配置如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SET hive.auto.convert.join=<span class="literal">true</span>;</span><br><span class="line">SET hive.auto.convert.sortmerge.join=<span class="literal">true</span></span><br><span class="line">SET hive.optimize.bucketmapjoin=<span class="literal">true</span>;</span><br><span class="line">SET hive.optimize.bucketmapjoin.sortedmerge=<span class="literal">true</span>;</span><br><span class="line">SET hive.auto.convert.sortmerge.join.noconditionaltask=<span class="literal">true</span>;</span><br><span class="line">SET hive.auto.convert.sortmerge.join.bigtable.selection.policy=</span><br><span class="line">org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ;</span><br></pre></td></tr></table></figure><h4 id="Skew-join"><a href="#Skew-join" class="headerlink" title="Skew join"></a>Skew join</h4><p>当被处理的数据分布极其不均匀时，会造成数据倾斜的现象。Hive可以通过如下的配置优化数据倾斜的情况：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 默认false，如果数据倾斜，可以将其设置为true</span></span><br><span class="line"><span class="keyword">SET</span> hive.optimize.skewjoin=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 默认为100000，如果key的数量大于配置的值，则超过的数量的key对应的数据会被发送到其他的reduce任务</span></span><br><span class="line"><span class="keyword">SET</span> hive.skewjoin.key=<span class="number">100000</span>;</span><br></pre></td></tr></table></figure><blockquote><p><strong>尖叫提示</strong>：</p><p>数据倾斜在group by的情况下也会发生，所以可以开启一个配置：set hive.groupby.skewindata=true，优化group by出现的数据倾斜，一旦开启之后，执行作业时会首先额外触发一个mr作业，该作业的map任务的输出会被随机地分配到reduce任务上，从而避免数据倾斜</p></blockquote><h3 id="执行引擎"><a href="#执行引擎" class="headerlink" title="执行引擎"></a>执行引擎</h3><p>Hive支持多种执行引擎，比如spark、tez。对于执行引擎的选择，会影响整体的查询性能。使用的配置如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SET hive.execution.engine=&lt;engine&gt;; -- &lt;engine&gt; = mr|tez|spark</span><br></pre></td></tr></table></figure><ul><li>mr:默认的执行引擎，在Hive2.0版本版本中被标记过时</li><li>tez:可以将多个有依赖的作业转换为一个作业，这样只需写一次HDFS，且中间节点较少，从而大大提升作业的计算性能。</li><li>spark:一个通用的大数据计算框架，基于内存计算，速度较快</li></ul><h3 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h3><p>与关系型数据库类似，Hive会在真正执行计算之前，生成和优化逻辑执行计划与物理执行计划。Hive有两种优化器：<strong>Vectorize(向量化优化器)</strong>与<strong>Cost-Based Optimization (CBO,成本优化器)</strong>。</p><h4 id="向量化优化器"><a href="#向量化优化器" class="headerlink" title="向量化优化器"></a>向量化优化器</h4><p>向量化优化器会同时处理大批量的数据，而不是一行一行地处理。要使用这种向量化的操作，要求表的文件格式为ORC，配置如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SET hive.vectorized.execution.enabled=<span class="literal">true</span>; -- 默认 <span class="literal">false</span></span><br></pre></td></tr></table></figure><h4 id="成本优化器"><a href="#成本优化器" class="headerlink" title="成本优化器"></a>成本优化器</h4><p>Hive的CBO是基于apache Calcite的，Hive的CBO通过查询成本(有analyze收集的统计信息)会生成有效率的执行计划，最终会减少执行的时间和资源的利用，使用CBO的配置如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SET hive.cbo.enable=<span class="literal">true</span>; --从 v0.14.0默认<span class="literal">true</span></span><br><span class="line">SET hive.compute.query.using.stats=<span class="literal">true</span>; -- 默认<span class="literal">false</span></span><br><span class="line">SET hive.stats.fetch.column.stats=<span class="literal">true</span>; -- 默认<span class="literal">false</span></span><br><span class="line">SET hive.stats.fetch.partition.stats=<span class="literal">true</span>; -- 默认<span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要介绍了Hive调优的基本思路。总共分为四部分，首先介绍了调优的基本工具使用(explain、analyze);接着从表设计层面介绍了一些优化策略(分区、分桶、索引)；然后介绍了数据存储方面的优化(文件格式、压缩、存储优化)；最后从作业层面介绍了优化的技巧(开启本地模式、JVM重用、并行执行、fetch模式、Join优化、执行引擎与优化器)。本文主要为Hive性能调优提供一些思路，在实际的操作过程中需要具体问题具体分析。总之一句话，重剑无锋，为作业分配合理的资源基本上可以满足大部分的情况，适合的就是最好的，没有必要追求狂拽酷炫的技巧，应该把更多的精力放在业务问题上，因为工具的存在的价值是为了解决业务问题的，切不可本末倒置。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实时数仓|Flink SQL之维表join</title>
      <link href="/2020/06/05/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-Flink-SQL%E4%B9%8B%E7%BB%B4%E8%A1%A8join/"/>
      <url>/2020/06/05/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-Flink-SQL%E4%B9%8B%E7%BB%B4%E8%A1%A8join/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>维表是数仓中的一个概念，维表中的维度属性是观察数据的角度，在建设离线数仓的时候，通常是将维表与事实表进行关联构建星型模型。在实时数仓中，同样也有维表与事实表的概念，其中事实表通常存储在kafka中，维表通常存储在外部设备中(比如MySQL，HBase)。对于每条流式数据，可以关联一个外部维表数据源，为实时计算提供数据关联查询。维表可能是会不断变化的，在维表JOIN时，需指明这条记录关联维表快照的时刻。需要注意是，目前Flink SQL的维表JOIN仅支持对当前时刻维表快照的关联(处理时间语义)，而不支持事实表rowtime所对应的的维表快照(事件时间语义)。通过本文你可以了解到：</p><ul><li>如何使用Flink SQL创建表</li><li>如何定义Kafka数据源表</li><li>如何定义MySQL数据源表</li><li>什么是Temporal Table Join</li><li>维表join的案例</li></ul><h2 id="Flink-SQL创建表"><a href="#Flink-SQL创建表" class="headerlink" title="Flink SQL创建表"></a>Flink SQL创建表</h2><p><strong>注意</strong>：本文的所有操作都是在Flink SQL cli中进行的</p><h3 id="创建表的语法"><a href="#创建表的语法" class="headerlink" title="创建表的语法"></a>创建表的语法</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> [catalog_name.][db_name.]table_name</span><br><span class="line">  (</span><br><span class="line">    &#123; &lt;column_definition&gt; | &lt;computed_column_definition&gt; &#125;[ , ...n]</span><br><span class="line">    [ &lt;watermark_definition&gt; ]</span><br><span class="line">  )</span><br><span class="line">  [<span class="keyword">COMMENT</span> table_comment]</span><br><span class="line">  [PARTITIONED <span class="keyword">BY</span> (partition_column_name1, partition_column_name2, ...)]</span><br><span class="line">  <span class="keyword">WITH</span> (key1=val1, key2=val2, ...)</span><br><span class="line"><span class="comment">-- 定义表字段</span></span><br><span class="line">&lt;column_definition&gt;:</span><br><span class="line">  column_name column_type [<span class="keyword">COMMENT</span> column_comment]</span><br><span class="line"><span class="comment">-- 定义计算列</span></span><br><span class="line">&lt;computed_column_definition&gt;:</span><br><span class="line">  column_name <span class="keyword">AS</span> computed_column_expression [<span class="keyword">COMMENT</span> column_comment]</span><br><span class="line"><span class="comment">-- 定义水位线</span></span><br><span class="line">&lt;watermark_definition&gt;:</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> rowtime_column_name <span class="keyword">AS</span> watermark_strategy_expression</span><br></pre></td></tr></table></figure><h3 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h3><h4 id="COMPUTED-COLUMN-计算列"><a href="#COMPUTED-COLUMN-计算列" class="headerlink" title="COMPUTED COLUMN(计算列)"></a>COMPUTED COLUMN(计算列)</h4><p>计算列是一个通过<code>column_name AS computed_column_expression</code>生成的虚拟列，产生的计算列不是物理存储在数据源表中。一个计算列可以通过原有数据源表中的某个字段、运算符及内置函数生成。比如，定义一个消费金额的计算列(cost)，可以使用表的价格(price)*数量(quantity)计算得到。</p><p>计算列常常被用在定义时间属性(见另一篇文章<a href="https://mp.weixin.qq.com/s/Qvi2AshGjkaES-Ce_dg6UA" target="_blank" rel="noopener">Flink Table API&amp;SQL编程指南之时间属性(3)</a>，可以通过PROCTIME()函数定义处理时间属性，语法为<code>proc AS PROCTIME()</code>。除此之外，计算列可以被用作提取事件时间列，因为原始的事件时间可能不是TIMESTAMP(3)类型或者是存在JSON串中。</p><blockquote><p><strong>尖叫提示</strong>：</p><p>1.在源表上定义计算列，是在读取数据源之后计算的，计算列需要跟在SELECT查询语句之后；</p><p>2.计算列不能被INSERT语句插入数据，在INSERT语句中，只能包括实际的目标表的schema，不能包括计算列</p></blockquote><h4 id="水位线"><a href="#水位线" class="headerlink" title="水位线"></a>水位线</h4><p>水位线定义了表的事件时间属性，其语法为:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">WATERMARK FOR rowtime_column_name AS watermark_strategy_expression</span><br></pre></td></tr></table></figure><p>其中<code>rowtime_column_name</code>表示表中已经存在的事件时间字段，值得注意的是，该事件时间字段必须是TIMESTAMP(3)类型，即形如<code>yyyy-MM-dd HH:mm:ss</code>,如果不是这种形式的数据类型，需要通过定义计算列进行转换。</p><p><code>watermark_strategy_expression</code>定义了水位线生成的策略，该表达式的返回数据类型必须是TIMESTAMP(3)类型。</p><p>Flink提供了许多常用的水位线生成策略：</p><ul><li><p>严格单调递增的水位线：语法为</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">WATERMARK FOR rowtime_column AS rowtime_column</span><br></pre></td></tr></table></figure></li></ul><p>即直接使用时间时间戳作为水位线</p><ul><li><p>递增水位线:语法为</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL '0.001' SECOND</span><br></pre></td></tr></table></figure></li><li><p>乱序水位线：语法为</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL 'string' timeUnit</span><br><span class="line"><span class="comment">-- 比如，允许5秒的乱序</span></span><br><span class="line">WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL '5' SECOND</span><br></pre></td></tr></table></figure></li></ul><h4 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h4><p>  根据具体的字段创建分区表，每一个分区会对应一个文件路径</p><h4 id="WITH-选项"><a href="#WITH-选项" class="headerlink" title="WITH 选项"></a>WITH 选项</h4><p>创建Table source或者Table sink需要指定表的属性，属性是以key/value的形式配置的，具体参考其相对应的connector</p><blockquote><p><strong>尖叫提示</strong>：</p><p>Note：创建表时指定的表名有三种形式：</p><p>（1）catalog_name.db_name.table_name</p><p>（2）db_name.table_name</p><p>（3）table_name</p><p>对于第一种形式：会将表注册到一个名为‘catalog_name’的catalog以及一个名为’db_name’d的数据库的元数据中；</p><p>对于第二种形式：会将表注册到当前执行环境的catalog以及名为‘db_name’的数据库的元数据中；</p><p>对于第三种形式：会将表注册到当前执行环境的catalog与数据库的元数据中</p></blockquote><h2 id="定义Kafka数据表"><a href="#定义Kafka数据表" class="headerlink" title="定义Kafka数据表"></a>定义Kafka数据表</h2><p>kafka是构建实时数仓常用的数据存储设备，使用Flink SQL创建kafka数据源表的语法如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyKafkaTable (</span><br><span class="line">  ...</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">'connector.type'</span> = <span class="string">'kafka'</span>, <span class="comment">-- 连接类型       </span></span><br><span class="line">  <span class="string">'connector.version'</span> = <span class="string">'0.11'</span>,<span class="comment">-- 必选: 可选的kafka版本有：0.8/0.9/0.10/0.11/universal</span></span><br><span class="line">  <span class="string">'connector.topic'</span> = <span class="string">'topic_name'</span>, <span class="comment">-- 必选: 主题名称</span></span><br><span class="line">  <span class="string">'connector.properties.zookeeper.connect'</span> = <span class="string">'localhost:2181'</span>, <span class="comment">-- 必选: zk连接地址</span></span><br><span class="line">  <span class="string">'connector.properties.bootstrap.servers'</span> = <span class="string">'localhost:9092'</span>, <span class="comment">-- 必选: Kafka连接地址</span></span><br><span class="line">  <span class="string">'connector.properties.group.id'</span> = <span class="string">'testGroup'</span>, <span class="comment">--可选: 消费者组</span></span><br><span class="line">   <span class="comment">-- 可选:偏移量, earliest-offset/latest-offset/group-offsets/specific-offsets</span></span><br><span class="line">  <span class="string">'connector.startup-mode'</span> = <span class="string">'earliest-offset'</span>,                                          </span><br><span class="line">  <span class="comment">-- 可选: 当偏移量指定为specific offsets，为指定每个分区指定具体位置</span></span><br><span class="line">  <span class="string">'connector.specific-offsets'</span> = <span class="string">'partition:0,offset:42;partition:1,offset:300'</span>,</span><br><span class="line">  <span class="string">'connector.sink-partitioner'</span> = <span class="string">'...'</span>,  <span class="comment">-- 可选: sink分区器，fixed/round-robin/custom</span></span><br><span class="line">  <span class="comment">-- 可选: 当自定义分区器时，指定分区器的类名</span></span><br><span class="line">  <span class="string">'connector.sink-partitioner-class'</span> = <span class="string">'org.mycompany.MyPartitioner'</span>,</span><br><span class="line">  <span class="string">'format.type'</span> = <span class="string">'...'</span>,                 <span class="comment">-- 必选: 指定格式，支持csv/json/avro</span></span><br><span class="line">   <span class="comment">-- 指定update-mode，支持append/retract/upsert</span></span><br><span class="line">  <span class="string">'update-mode'</span> = <span class="string">'append'</span>,</span><br><span class="line"></span><br><span class="line">)</span><br></pre></td></tr></table></figure><blockquote><p><strong>尖叫提示</strong>：</p><ul><li>指定具体的偏移量位置：默认是从当前消费者组提交的偏移量开始消费</li><li>sink分区：默认是尽可能向更多的分区写数据(每一个sink并行度实例只向一个分区写数据)，也可以自已分区策略。当使用 round-robin分区器时，可以避免分区不均衡，但是会造成Flink实例与kafka broker之间大量的网络连接</li><li>一致性保证：默认sink语义是at-least-once</li><li><strong>Kafka 0.10+</strong> 是时间戳：从kafka0.10开始，kafka消息附带一个时间戳作为消息的元数据，表示记录被写入kafka主题的时间，这个时间戳可以作为事件时间属性( rowtime attribute)</li><li><strong>Kafka 0.11+</strong>版本：Flink从1.7开始，支持使用universal版本作为kafka的连接器 ，可以兼容kafka0.11之后版本</li></ul></blockquote><h2 id="定义MySQL数据表"><a href="#定义MySQL数据表" class="headerlink" title="定义MySQL数据表"></a>定义MySQL数据表</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MySQLTable (</span><br><span class="line">  ...</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">'connector.type'</span> = <span class="string">'jdbc'</span>, <span class="comment">-- 必选: jdbc方式</span></span><br><span class="line">  <span class="string">'connector.url'</span> = <span class="string">'jdbc:mysql://localhost:3306/flink-test'</span>, <span class="comment">-- 必选: JDBC url</span></span><br><span class="line">  <span class="string">'connector.table'</span> = <span class="string">'jdbc_table_name'</span>,  <span class="comment">-- 必选: 表名</span></span><br><span class="line">   <span class="comment">-- 可选: JDBC driver，如果不配置，会自动通过url提取 </span></span><br><span class="line">  <span class="string">'connector.driver'</span> = <span class="string">'com.mysql.jdbc.Driver'</span>,                                           </span><br><span class="line">  <span class="string">'connector.username'</span> = <span class="string">'name'</span>, <span class="comment">-- 可选: 数据库用户名</span></span><br><span class="line">  <span class="string">'connector.password'</span> = <span class="string">'password'</span>,<span class="comment">-- 可选: 数据库密码</span></span><br><span class="line">    <span class="comment">-- 可选, 将输入进行分区的字段名.</span></span><br><span class="line">  <span class="string">'connector.read.partition.column'</span> = <span class="string">'column_name'</span>,</span><br><span class="line">    <span class="comment">-- 可选, 分区数量.</span></span><br><span class="line">  <span class="string">'connector.read.partition.num'</span> = <span class="string">'50'</span>, </span><br><span class="line">    <span class="comment">-- 可选, 第一个分区的最小值.</span></span><br><span class="line">  <span class="string">'connector.read.partition.lower-bound'</span> = <span class="string">'500'</span>,</span><br><span class="line">    <span class="comment">-- 可选, 最后一个分区的最大值</span></span><br><span class="line">  <span class="string">'connector.read.partition.upper-bound'</span> = <span class="string">'1000'</span>, </span><br><span class="line">    <span class="comment">-- 可选, 一次提取数据的行数，默认为0，表示忽略此配置</span></span><br><span class="line">  <span class="string">'connector.read.fetch-size'</span> = <span class="string">'100'</span>, </span><br><span class="line">   <span class="comment">-- 可选, lookup缓存数据的最大行数，如果超过改配置，老的数据会被清除</span></span><br><span class="line">  <span class="string">'connector.lookup.cache.max-rows'</span> = <span class="string">'5000'</span>, </span><br><span class="line">   <span class="comment">-- 可选，lookup缓存存活的最大时间，超过该时间旧数据会过时，注意cache.max-rows与cache.ttl必须同时配置</span></span><br><span class="line">  <span class="string">'connector.lookup.cache.ttl'</span> = <span class="string">'10s'</span>, </span><br><span class="line">   <span class="comment">-- 可选, 查询数据最大重试次数</span></span><br><span class="line">  <span class="string">'connector.lookup.max-retries'</span> = <span class="string">'3'</span>, </span><br><span class="line">   <span class="comment">-- 可选,写数据最大的flush行数，默认5000，超过改配置，会触发刷数据 </span></span><br><span class="line">  <span class="string">'connector.write.flush.max-rows'</span> = <span class="string">'5000'</span>, </span><br><span class="line">   <span class="comment">--可选，flush数据的间隔时间，超过该时间，会通过一个异步线程flush数据，默认是0s </span></span><br><span class="line">  <span class="string">'connector.write.flush.interval'</span> = <span class="string">'2s'</span>, </span><br><span class="line">  <span class="comment">-- 可选, 写数据失败最大重试次数</span></span><br><span class="line">  <span class="string">'connector.write.max-retries'</span> = <span class="string">'3'</span> </span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="Temporal-Table-Join"><a href="#Temporal-Table-Join" class="headerlink" title="Temporal Table Join"></a>Temporal Table Join</h2><h3 id="使用语法"><a href="#使用语法" class="headerlink" title="使用语法"></a>使用语法</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">column</span>-<span class="keyword">names</span></span><br><span class="line"><span class="keyword">FROM</span> table1  [<span class="keyword">AS</span> &lt;alias1&gt;]</span><br><span class="line">[<span class="keyword">LEFT</span>] <span class="keyword">JOIN</span> table2 <span class="keyword">FOR</span> SYSTEM_TIME <span class="keyword">AS</span> <span class="keyword">OF</span> table1.proctime [<span class="keyword">AS</span> &lt;alias2&gt;]</span><br><span class="line"><span class="keyword">ON</span> table1.column-name1 = table2.key-name1</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：目前，仅支持INNER JOIN与LEFT JOIN。在join的时候需要使用 <code>FOR SYSTEM_TIME AS OF</code> ，其中table1.proctime表示table1的proctime处理时间属性(计算列)。使用<code>FOR SYSTEM_TIME AS OF table1.proctime</code>表示当左边表的记录与右边的维表join时，只匹配当前处理时间维表所对应的的快照数据。</p><h3 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  o.amout, o.currency, r.rate, o.amount * r.rate</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  Orders <span class="keyword">AS</span> o</span><br><span class="line">  <span class="keyword">JOIN</span> LatestRates <span class="keyword">FOR</span> SYSTEM_TIME <span class="keyword">AS</span> <span class="keyword">OF</span> o.proctime <span class="keyword">AS</span> r</span><br><span class="line">  <span class="keyword">ON</span> r.currency = o.currency</span><br></pre></td></tr></table></figure><h3 id="使用说明"><a href="#使用说明" class="headerlink" title="使用说明"></a>使用说明</h3><ul><li>仅支持Blink planner</li><li>仅支持SQL，目前不支持Table API</li><li>目前不支持基于事件时间(event time)的temporal table join</li><li>维表可能会不断变化，JOIN行为发生后，维表中的数据发生了变化（新增、更新或删除），则已关联的维表数据不会被同步变化</li><li>维表和维表不能进行JOIN</li><li>维表必须指定主键。维表JOIN时，ON的条件必须包含所有主键的等值条件</li></ul><h2 id="维表Join案例"><a href="#维表Join案例" class="headerlink" title="维表Join案例"></a>维表Join案例</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>Kafka中有一份用户行为数据，包括pv，buy，cart，fav行为；MySQL中有一份省份区域的维表数据。现将两种表进行JOIN，统计每个区域的购买行为数量。</p><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p>维表存储在MySQL中，如下创建维表数据源：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> dim_province (</span><br><span class="line">    province_id <span class="built_in">BIGINT</span>,  <span class="comment">-- 省份id</span></span><br><span class="line">    province_name  <span class="built_in">VARCHAR</span>, <span class="comment">-- 省份名称</span></span><br><span class="line">region_name <span class="built_in">VARCHAR</span> <span class="comment">-- 区域名称</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector.type'</span> = <span class="string">'jdbc'</span>,</span><br><span class="line">    <span class="string">'connector.url'</span> = <span class="string">'jdbc:mysql://192.168.10.203:3306/mydw'</span>,</span><br><span class="line">    <span class="string">'connector.table'</span> = <span class="string">'dim_province'</span>,</span><br><span class="line">    <span class="string">'connector.driver'</span> = <span class="string">'com.mysql.jdbc.Driver'</span>,</span><br><span class="line">    <span class="string">'connector.username'</span> = <span class="string">'root'</span>,</span><br><span class="line">    <span class="string">'connector.password'</span> = <span class="string">'123qwe'</span>,</span><br><span class="line">    <span class="string">'connector.lookup.cache.max-rows'</span> = <span class="string">'5000'</span>,</span><br><span class="line">    <span class="string">'connector.lookup.cache.ttl'</span> = <span class="string">'10min'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>事实表存储在kafka中，数据为用户点击行为，格式为JSON，具体数据样例如下：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&#123;"user_id":63401,"item_id":6244,"cat_id":143,"action":"pv","province":3,"ts":1573445919&#125;</span><br><span class="line">&#123;"user_id":9164,"item_id":2817,"cat_id":611,"action":"fav","province":28,"ts":1573420486&#125;</span><br></pre></td></tr></table></figure><p>创建kafka数据源表，如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_behavior (</span><br><span class="line">    user_id <span class="built_in">BIGINT</span>, <span class="comment">-- 用户id</span></span><br><span class="line">    item_id <span class="built_in">BIGINT</span>, <span class="comment">-- 商品id</span></span><br><span class="line">    cat_id <span class="built_in">BIGINT</span>,  <span class="comment">-- 品类id</span></span><br><span class="line">    <span class="keyword">action</span> <span class="keyword">STRING</span>,  <span class="comment">-- 用户行为</span></span><br><span class="line">province <span class="built_in">INT</span>,   <span class="comment">-- 用户所在的省份</span></span><br><span class="line">ts     <span class="built_in">BIGINT</span>,  <span class="comment">-- 用户行为发生的时间戳</span></span><br><span class="line">    proctime <span class="keyword">as</span> PROCTIME(),   <span class="comment">-- 通过计算列产生一个处理时间列</span></span><br><span class="line">eventTime <span class="keyword">AS</span> TO_TIMESTAMP(FROM_UNIXTIME(ts, <span class="string">'yyyy-MM-dd HH:mm:ss'</span>)), <span class="comment">-- 事件时间</span></span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> eventTime <span class="keyword">as</span> eventTime - <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span>  <span class="comment">-- 在eventTime上定义watermark</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector.type'</span> = <span class="string">'kafka'</span>,  <span class="comment">-- 使用 kafka connector</span></span><br><span class="line">    <span class="string">'connector.version'</span> = <span class="string">'universal'</span>,  <span class="comment">-- kafka 版本，universal 支持 0.11 以上的版本</span></span><br><span class="line">    <span class="string">'connector.topic'</span> = <span class="string">'user_behavior'</span>,  <span class="comment">-- kafka主题</span></span><br><span class="line">    <span class="string">'connector.startup-mode'</span> = <span class="string">'earliest-offset'</span>,  <span class="comment">-- 偏移量，从起始 offset 开始读取</span></span><br><span class="line"><span class="string">'connector.properties.group.id'</span> = <span class="string">'group1'</span>, <span class="comment">-- 消费者组</span></span><br><span class="line">    <span class="string">'connector.properties.zookeeper.connect'</span> = <span class="string">'kms-2:2181,kms-3:2181,kms-4:2181'</span>,  <span class="comment">-- zookeeper 地址</span></span><br><span class="line">    <span class="string">'connector.properties.bootstrap.servers'</span> = <span class="string">'kms-2:9092,kms-3:9092,kms-4:9092'</span>,  <span class="comment">-- kafka broker 地址</span></span><br><span class="line">    <span class="string">'format.type'</span> = <span class="string">'json'</span>  <span class="comment">-- 数据源格式为 json</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>创建MySQL的结果表，表示区域销量</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> region_sales_sink (</span><br><span class="line">    region_name <span class="keyword">STRING</span>,  <span class="comment">-- 区域名称</span></span><br><span class="line">    buy_cnt <span class="built_in">BIGINT</span>  <span class="comment">-- 销量</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  </span><br><span class="line">    <span class="string">'connector.type'</span> = <span class="string">'jdbc'</span>,</span><br><span class="line">    <span class="string">'connector.url'</span> = <span class="string">'jdbc:mysql://192.168.10.203:3306/mydw'</span>,</span><br><span class="line">    <span class="string">'connector.table'</span> = <span class="string">'top_region'</span>, <span class="comment">-- MySQL中的待插入数据的表</span></span><br><span class="line">    <span class="string">'connector.driver'</span> = <span class="string">'com.mysql.jdbc.Driver'</span>,</span><br><span class="line">    <span class="string">'connector.username'</span> = <span class="string">'root'</span>,</span><br><span class="line">    <span class="string">'connector.password'</span> = <span class="string">'123qwe'</span>,</span><br><span class="line">    <span class="string">'connector.write.flush.interval'</span> = <span class="string">'1s'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>用户行为数据与省份维表数据join</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> user_behavior_detail <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  u.user_id, </span><br><span class="line">  u.item_id,</span><br><span class="line">  u.cat_id,</span><br><span class="line">  u.action,  </span><br><span class="line">  p.province_name,</span><br><span class="line">  p.region_name</span><br><span class="line"><span class="keyword">FROM</span> user_behavior <span class="keyword">AS</span> u <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> dim_province <span class="keyword">FOR</span> SYSTEM_TIME <span class="keyword">AS</span> <span class="keyword">OF</span> u.proctime <span class="keyword">AS</span> p</span><br><span class="line"><span class="keyword">ON</span> u.province = p.province_id;</span><br></pre></td></tr></table></figure><p>计算区域的销量，并将计算结果写入MySQL</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> region_sales_sink</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">  region_name,</span><br><span class="line">  <span class="keyword">COUNT</span>(*) buy_cnt</span><br><span class="line"><span class="keyword">FROM</span> user_behavior_detail</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">action</span> = <span class="string">'buy'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> region_name;</span><br></pre></td></tr></table></figure><p>结果查看：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">Flink SQL&gt; select * from  region_sales_sink; -- 在Flink SQL cli中查看</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2020/06/05/实时数仓-Flink-SQL之维表join/%E7%BB%93%E6%9E%9C.png" alt></p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from top_region; -- 查看MySQL的数据</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2020/06/05/实时数仓-Flink-SQL之维表join/%E7%BB%93%E6%9E%9C1.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要介绍了FlinkSQL的维表join，使用的方式为Temporal Table Join。首先介绍了Flink SQL创建表的基本语法，并对其中的细节进行了描述。接着介绍了如何创建Kafka以及MySQL的数据源表。然后介绍了Temporal Table Join的基本概念与使用语法。最后，给出了一个完整的维表JOIN案例。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink Table API&amp;SQL编程指南之时间属性(3)</title>
      <link href="/2020/06/02/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97%E4%B9%8B%E6%97%B6%E9%97%B4%E5%B1%9E%E6%80%A7-3/"/>
      <url>/2020/06/02/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97%E4%B9%8B%E6%97%B6%E9%97%B4%E5%B1%9E%E6%80%A7-3/</url>
      
        <content type="html"><![CDATA[<p>Flink总共有三种时间语义：<em>Processing time</em>(处理时间)、<em>Event time</em>(事件时间)以及<em>Ingestion time</em>(摄入时间)。关于这些时间语义的具体解释，可以参考另一篇文章<a href="https://mp.weixin.qq.com/s/ycz_N5m6RjsW9ZBhNMvACw" target="_blank" rel="noopener">Flink的时间与watermarks详解</a>。本文主要讲解Flink Table API &amp; SQL中基于时间的算子如何定义时间语义。通过本文你可以了解到：</p><ul><li>时间属性的简介</li><li>处理时间</li><li>事件时间</li></ul><h2 id="时间属性简介"><a href="#时间属性简介" class="headerlink" title="时间属性简介"></a>时间属性简介</h2><p>Flink TableAPI&amp;SQL中的基于时间的操作(如window)，需要指定时间语义，表可以根据指定的时间戳提供一个逻辑时间属性。</p><p>时间属性是表schama的一部分，当使用DDL创建表时、DataStream转为表时或者使用TableSource时，会定义时间属性。一旦时间属性被定义完成，该时间属性可以看做是一个字段的引用，从而在基于时间的操作中使用该字段。</p><p>时间属性像一个时间戳，可以被访问并参与计算，如果一个时间属性参与计算，那么该时间属性会被雾化成一个常规的时间戳，常规的时间戳不能与Flink的时间与水位线兼容，不能被基于时间的操作所使用。</p><p>Flink TableAPI &amp; SQL所需要的时间属性可以通过Datastream程序中指定，如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime); <span class="comment">// 默认</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 可以选择:</span></span><br><span class="line"><span class="comment">// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);</span></span><br><span class="line"><span class="comment">// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span></span><br></pre></td></tr></table></figure><h2 id="处理时间"><a href="#处理时间" class="headerlink" title="处理时间"></a>处理时间</h2><p>基于本地的机器时间，是一种最简单的时间语义，但是不能保证结果一致性，使用该时间语义不需要提取时间戳和生成水位线。总共有三种方式定义处理时间属性，具体如下</p><h3 id="DDL语句创建表时定义处理时间"><a href="#DDL语句创建表时定义处理时间" class="headerlink" title="DDL语句创建表时定义处理时间"></a>DDL语句创建表时定义处理时间</h3><p>处理时间的属性可以在DDL语句中被定义为一个计算列，需要使用PROCTIME()函数，如下所示：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_actions (</span><br><span class="line">  user_name <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="keyword">data</span> <span class="keyword">STRING</span>,</span><br><span class="line">  user_action_time <span class="keyword">AS</span> PROCTIME() <span class="comment">-- 声明一个额外字段，作为处理时间属性</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> TUMBLE_START(user_action_time, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>), <span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> user_name)</span><br><span class="line"><span class="keyword">FROM</span> user_actions</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> TUMBLE(user_action_time, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>); <span class="comment">-- 10分钟的滚动窗口</span></span><br></pre></td></tr></table></figure><h3 id="DataStream转为Table的过程中定义处理时间"><a href="#DataStream转为Table的过程中定义处理时间" class="headerlink" title="DataStream转为Table的过程中定义处理时间"></a>DataStream转为Table的过程中定义处理时间</h3><p>在将DataStream转为表时，在schema定义中可以通过.proctime属性指定时间属性，并将其放在其他schema字段的最后面，具体如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, String&gt;&gt; stream = ...;</span><br><span class="line"><span class="comment">// 声明一个额外逻辑字段作为处理时间属性</span></span><br><span class="line">Table table = tEnv.fromDataStream(stream, <span class="string">"user_name, data, user_action_time.proctime"</span>);</span><br><span class="line"></span><br><span class="line">WindowedTable windowedTable = table.window(Tumble.over(<span class="string">"10.minutes"</span>).on(<span class="string">"user_action_time"</span>).as(<span class="string">"userActionWindow"</span>));</span><br></pre></td></tr></table></figure><h3 id="使用TableSource"><a href="#使用TableSource" class="headerlink" title="使用TableSource"></a>使用TableSource</h3><p>自定义TableSource并实现<code>DefinedProctimeAttribute</code> 接口，如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义个带有处理时间属性的table source</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserActionSource</span> <span class="keyword">implements</span> <span class="title">StreamTableSource</span>&lt;<span class="title">Row</span>&gt;, <span class="title">DefinedProctimeAttribute</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> TypeInformation&lt;Row&gt; <span class="title">getReturnType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">String[] names = <span class="keyword">new</span> String[] &#123;<span class="string">"user_name"</span> , <span class="string">"data"</span>&#125;;</span><br><span class="line">TypeInformation[] types = <span class="keyword">new</span> TypeInformation[] &#123;Types.STRING(), Types.STRING()&#125;;</span><br><span class="line"><span class="keyword">return</span> Types.ROW(names, types);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> DataStream&lt;Row&gt; <span class="title">getDataStream</span><span class="params">(StreamExecutionEnvironment execEnv)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 创建stream</span></span><br><span class="line">DataStream&lt;Row&gt; stream = ...;</span><br><span class="line"><span class="keyword">return</span> stream;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getProctimeAttribute</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 该字段会追加到schema中，作为第三个字段</span></span><br><span class="line"><span class="keyword">return</span> <span class="string">"user_action_time"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册table source</span></span><br><span class="line">tEnv.registerTableSource(<span class="string">"user_actions"</span>, <span class="keyword">new</span> UserActionSource());</span><br><span class="line"></span><br><span class="line">WindowedTable windowedTable = tEnv</span><br><span class="line">.from(<span class="string">"user_actions"</span>)</span><br><span class="line">.window(Tumble.over(<span class="string">"10.minutes"</span>).on(<span class="string">"user_action_time"</span>).as(<span class="string">"userActionWindow"</span>));</span><br></pre></td></tr></table></figure><h2 id="事件时间"><a href="#事件时间" class="headerlink" title="事件时间"></a>事件时间</h2><p>基于记录的具体时间戳，即便是存在乱序或者迟到数据也会保证结果的一致性。总共有三种方式定义处理时间属性，具体如下</p><h3 id="DDL语句创建表时定事件时间"><a href="#DDL语句创建表时定事件时间" class="headerlink" title="DDL语句创建表时定事件时间"></a>DDL语句创建表时定事件时间</h3><p>事件时间属性可以通过 WATERMARK语句进行定义，如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_actions (</span><br><span class="line">  user_name <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="keyword">data</span> <span class="keyword">STRING</span>,</span><br><span class="line">  user_action_time <span class="built_in">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  <span class="comment">-- 声明user_action_time作为事件时间属性，并允许5S的延迟  </span></span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> user_action_time <span class="keyword">AS</span> user_action_time - <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> TUMBLE_START(user_action_time, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>), <span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> user_name)</span><br><span class="line"><span class="keyword">FROM</span> user_actions</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> TUMBLE(user_action_time, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>);</span><br></pre></td></tr></table></figure><h3 id="DataStream转为Table的过程中定义事件时间"><a href="#DataStream转为Table的过程中定义事件时间" class="headerlink" title="DataStream转为Table的过程中定义事件时间"></a>DataStream转为Table的过程中定义事件时间</h3><p>当定义Schema时通过.rowtime属性指定事件时间属性，必须在DataStream中指定时间戳与水位线。例如在数据集中，事件时间属性为event_time，此时Table中的事件时间字段中可以通过’event_time. rowtime‘来指定。</p><p>目前Flink支持两种方式定义EventTime字段，如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 方式1:</span></span><br><span class="line"><span class="comment">// 提取timestamp并分配watermarks</span></span><br><span class="line">DataStream&lt;Tuple2&lt;String, String&gt;&gt; stream = inputStream.assignTimestampsAndWatermarks(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 声明一个额外逻辑字段作为事件时间属性</span></span><br><span class="line"><span class="comment">// 在table schema的末尾使用user_action_time.rowtime定义事件时间属性</span></span><br><span class="line"><span class="comment">// 系统会在TableEnvironment中获取事件时间属性</span></span><br><span class="line">Table table = tEnv.fromDataStream(stream, <span class="string">"user_name, data, user_action_time.rowtime"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方式2:</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 从第一个字段提取timestamp并分配watermarks</span></span><br><span class="line">DataStream&lt;Tuple3&lt;Long, String, String&gt;&gt; stream = inputStream.assignTimestampsAndWatermarks(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第一个字段已经用来提取时间戳，可以直接使用对应的字段作为事件时间属性</span></span><br><span class="line">Table table = tEnv.fromDataStream(stream, <span class="string">"user_action_time.rowtime, user_name, data"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用:</span></span><br><span class="line"></span><br><span class="line">WindowedTable windowedTable = table.window(Tumble.over(<span class="string">"10.minutes"</span>).on(<span class="string">"user_action_time"</span>).as(<span class="string">"userActionWindow"</span>));</span><br></pre></td></tr></table></figure><h3 id="使用TableSource-1"><a href="#使用TableSource-1" class="headerlink" title="使用TableSource"></a>使用TableSource</h3><p>另外也可以在创建TableSource的时候，实现DefinedRowtimeAttributes接口来定义EventTime字段，在接口中需要实现getRowtimeAttributeDescriptors方法，创建基于EventTime的时间属性信息。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义带有rowtime属性的table source</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserActionSource</span> <span class="keyword">implements</span> <span class="title">StreamTableSource</span>&lt;<span class="title">Row</span>&gt;, <span class="title">DefinedRowtimeAttributes</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> TypeInformation&lt;Row&gt; <span class="title">getReturnType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">String[] names = <span class="keyword">new</span> String[] &#123;<span class="string">"user_name"</span>, <span class="string">"data"</span>, <span class="string">"user_action_time"</span>&#125;;</span><br><span class="line">TypeInformation[] types =</span><br><span class="line">    <span class="keyword">new</span> TypeInformation[] &#123;Types.STRING(), Types.STRING(), Types.LONG()&#125;;</span><br><span class="line"><span class="keyword">return</span> Types.ROW(names, types);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> DataStream&lt;Row&gt; <span class="title">getDataStream</span><span class="params">(StreamExecutionEnvironment execEnv)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建流，基于user_action_time属性分配水位线</span></span><br><span class="line">DataStream&lt;Row&gt; stream = inputStream.assignTimestampsAndWatermarks(...);</span><br><span class="line"><span class="keyword">return</span> stream;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;RowtimeAttributeDescriptor&gt; <span class="title">getRowtimeAttributeDescriptors</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 标记user_action_time字段作为事件时间属性</span></span><br><span class="line">        <span class="comment">// 创建user_action_time描述符，用来标识时间属性字段</span></span><br><span class="line">RowtimeAttributeDescriptor rowtimeAttrDescr = <span class="keyword">new</span> RowtimeAttributeDescriptor(</span><br><span class="line"><span class="string">"user_action_time"</span>,</span><br><span class="line"><span class="keyword">new</span> ExistingField(<span class="string">"user_action_time"</span>),</span><br><span class="line"><span class="keyword">new</span> AscendingTimestamps());</span><br><span class="line">List&lt;RowtimeAttributeDescriptor&gt; listRowtimeAttrDescr = Collections.singletonList(rowtimeAttrDescr);</span><br><span class="line"><span class="keyword">return</span> listRowtimeAttrDescr;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// register表</span></span><br><span class="line">tEnv.registerTableSource(<span class="string">"user_actions"</span>, <span class="keyword">new</span> UserActionSource());</span><br><span class="line"></span><br><span class="line">WindowedTable windowedTable = tEnv</span><br><span class="line">.from(<span class="string">"user_actions"</span>)</span><br><span class="line">.window(Tumble.over(<span class="string">"10.minutes"</span>).on(<span class="string">"user_action_time"</span>).as(<span class="string">"userActionWindow"</span>));</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了如何在Flink Table API和SQL中使用时间语义，可以使用两种时间语义：处理时间和事件时间。分别对每种的时间语义的使用方式进行了详细解释。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数仓开发需要了解的BI数据分析方法</title>
      <link href="/2020/05/28/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91%E9%9C%80%E8%A6%81%E4%BA%86%E8%A7%A3%E7%9A%84BI%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/"/>
      <url>/2020/05/28/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91%E9%9C%80%E8%A6%81%E4%BA%86%E8%A7%A3%E7%9A%84BI%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>数仓开发经常需要与数据表打交道，那么数仓表开发完成之后就万事大吉了吗？显然不是，还需要思考一下如何分析数据以及如何呈现数据，因为这是发挥数据价值很重要的一个方面。通过数据的分析与可视化呈现可以更加直观的提供数据背后的秘密，从而辅助业务决策，实现真正的数据赋能业务。通过本文你可以了解到：</p><ul><li><p>帕累托分析方法与数据可视化</p></li><li><p>RFM分析与数据可视化</p></li><li><p>波士顿矩阵与数据可视化</p></li></ul><h2 id="帕累托分析与数据可视化"><a href="#帕累托分析与数据可视化" class="headerlink" title="帕累托分析与数据可视化"></a>帕累托分析与数据可视化</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>帕累托(Pareto)分析法，又称ABC分析法，即我们平时所提到的80／20法则。关于帕累托(Pareto)分析法，在不同的行业都有不同的应用。</p><ul><li><strong>举个栗子</strong></li></ul><p>在企业的库存管理中，可以发现少数品种在总需用量(或是总供给额、库存总量、储备金总额)中，占了很大的比重，但在相应的量值中所占的比重很少。因此可以运用帕累托分析法，将企业所需的各种物品，按其需用量的大小、物品的重要程度、资源短缺和采购的难易程度、单价的高低、占用储备资金的多少等因素分为若干类，实施分类管理。</p><p>商品销售额分析中，某些商品的销售额占了总销售额的很大部分，某些商品的销售额仅占很小的比例，这样就可以将其分为A、B、C几大类，对销售额占比较多的分类进行投入，以获得更多的销售额。</p><p>在质量分析中，对某种原因导致产品质量不合格的产品数量进行分析，使用帕累托(Pareto)分析法，可以很直观的看出哪些原因造成了产品质量不合格以及哪些原因比较严重。这样就可以着重解决重要的问题，明确目标，更易于操作。</p><ul><li><strong>另一种表述方式</strong></li></ul><p>根据事物在技术或经济方面的主要特征，进行分类，分清重点与非重点。对每一种分类进行区别对待管理，把被分析的对象分成 A、B、C 三类，三类物品没有明确的划分数值界限。</p><table><thead><tr><th>分类与重要程度</th><th>描述</th></tr></thead><tbody><tr><td>A类(非常重要)</td><td>数量占比少，价值占比大</td></tr><tr><td>B类(比较重要)</td><td>没有A类那么重要，介于 A、C 之间</td></tr><tr><td>C类(一般重要)</td><td>数量占比大但价值占比很小</td></tr></tbody></table><p>分类的核心思想：少数贡献了大部分价值。以商品品类和销售额为例：A 品类数量占总体 10% ，却贡献了 80% 的销售额。</p><p><img src="//jiamaoxiang.top/2020/05/28/数仓开发需要了解的BI数据分析方法/%E5%B8%95%E7%B4%AF%E6%89%98%E4%B8%BE%E4%BE%8B.png" alt></p><h3 id="数据分析案例"><a href="#数据分析案例" class="headerlink" title="数据分析案例"></a>数据分析案例</h3><ul><li>效果图</li></ul><p><img src="//jiamaoxiang.top/2020/05/28/数仓开发需要了解的BI数据分析方法/%E5%B8%95%E7%B4%AF%E6%89%98%E5%9B%BE.png" alt></p><ul><li>实现步骤</li></ul><p>假设有如下数据集格式：</p><table><thead><tr><th>品牌</th><th>销售额</th></tr></thead><tbody><tr><td>NEW BALANCE(新百伦)</td><td>8750</td></tr><tr><td>ZIPPO(之宝)</td><td>9760</td></tr><tr><td>OCTMAMI(十月妈咪)</td><td>5800</td></tr></tbody></table><p>需要将数据加工成下面的格式：</p><table><thead><tr><th>品牌</th><th>销售额</th><th>销售总额</th><th>累计销售额</th><th>累计销售额占比</th></tr></thead><tbody><tr><td></td><td></td><td>=∑所有品牌销售额</td><td>=当前品牌销售额 +上一个品牌销售额</td><td>累计销售额/销售总额</td></tr></tbody></table><p>具体的SQL实现如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">     brand, <span class="comment">-- 品牌</span></span><br><span class="line">     total_money, <span class="comment">-- 销售额</span></span><br><span class="line">     <span class="keyword">sum</span>(total_money) <span class="keyword">over</span>() <span class="keyword">AS</span> sum_total_money,<span class="comment">-- 销售总额</span></span><br><span class="line">     <span class="keyword">sum</span>(total_money) <span class="keyword">over</span>(<span class="keyword">ORDER</span> <span class="keyword">BY</span> total_money <span class="keyword">DESC</span> <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) <span class="keyword">AS</span> acc_sum_total_money <span class="comment">-- 累计销售额</span></span><br><span class="line"><span class="keyword">FROM</span> sales_money</span><br></pre></td></tr></table></figure><p>上面给出了具体的SQL实现，其实BI工具已经内置了许多的处理函数和拖拽式的数据处理，不需要写SQL也可以将一份明细数据加工成上面的形式。</p><ul><li>结论分析</li></ul><p>从上面的帕累托图中可以看出：A类的(绿色部分)占了总销售额的80%左右，B类(黄色部分)占总销售额的10%,C类(红色部分)占总销售额的10%。接下来可以进行长尾分析，制定营销策略等等。</p><h2 id="RFM分析与数据可视化"><a href="#RFM分析与数据可视化" class="headerlink" title="RFM分析与数据可视化"></a>RFM分析与数据可视化</h2><h3 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h3><p>RFM模型是在客户关系管理(CRM)中常用到的一个模型,RFM模型是衡量客户价值和客户创利能力的重要工具和手段。该模型通过一个客户的近期购买行为、购买的总体频率以及花了多少钱三项指标来描述该客户的价值状况。</p><p>RFM模型较为动态地层示了一个客户的全部轮廓，这对个性化的沟通和服务提供了依据，同时，如果与该客户打交道的时间足够长，也能够较为精确地判断该客户的长期价值(甚至是终身价值)，通过改善三项指标的状况，从而为更多的营销决策提供支持。</p><p>　　在RFM模式中，包括三个关键的因素，分别为：</p><ul><li>R(Recency)：表示客户最近一次购买的时间有多远，即最近的一次消费，消费时间越近的客户价值越大</li><li>F(Frequency)：表示客户在最近一段时间内购买的次数，即消费频率，经常购买的用户也就是熟客，价值肯定比偶尔来一次的客户价值大</li><li>M (Monetary)：表示客户在最近一段时间内购买的金额，即客户的消费能力，通常以客户单次的平均消费金额作为衡量指标，消费越多的用户价值越大。</li></ul><p>最近一次消费、消费频率、消费金额是测算消费者价值最重要也是最容易的方法，这充分的表现了这三个指标对营销活动的指导意义。而其中，最近一次消费是最有力的预测指标。</p><p>通过上面分析可以对客户群体进行分类：</p><table><thead><tr><th>客户类型与等级</th><th>R</th><th>F</th><th>M</th><th>客户特征</th></tr></thead><tbody><tr><td>重要价值客户(A级/111)</td><td>高(1)</td><td>高(1)</td><td>高(1)</td><td>最近消费时间近、消费频次和消费金额都很高</td></tr><tr><td>重要发展客户(A级/101)</td><td>高(1)</td><td>低(0)</td><td>高(1)</td><td>最近消费时间较近、消费金额高，但频次不高，忠诚度不高，很有潜力的用户，必须重点发展</td></tr><tr><td>重要保持客户(B级/011)</td><td>低(0)</td><td>高(1)</td><td>高(1)</td><td>最近消费时间交远，消费金额和频次都很高。</td></tr><tr><td>重要挽留客户(B级/001)</td><td>低(0)</td><td>低(0)</td><td>高(1)</td><td>最近消费时间较远、消费频次不高，但消费金额高的用户，可能是将要流失或者已经要流失的用户，应当基于挽留措施。</td></tr><tr><td>一般价值客户(B级/110)</td><td>高(1)</td><td>高(1)</td><td>低(0)</td><td>最近消费时间近，频率高，但消费金额低，需要提高其客单价。</td></tr><tr><td>一般发展客户(B级/100)</td><td>高(1)</td><td>低(0)</td><td>低(0)</td><td>最近消费时间较近、消费金额，频次都不高。</td></tr><tr><td>一般保持客户(C级/010)</td><td>低(0)</td><td>高(1)</td><td>低(0)</td><td>最近消费时间较远、消费频次高，但金额不高。</td></tr><tr><td>一般挽留客户(C级/000)</td><td>低(0)</td><td>低(0)</td><td>低(0)</td><td>都很低</td></tr></tbody></table><h3 id="数据分析案例-1"><a href="#数据分析案例-1" class="headerlink" title="数据分析案例"></a>数据分析案例</h3><ul><li>效果图</li></ul><p><img src="//jiamaoxiang.top/2020/05/28/数仓开发需要了解的BI数据分析方法/RFM%E6%95%88%E6%9E%9C%E5%9B%BE.png" alt></p><ul><li>实现步骤</li></ul><p>假设有如下的样例数据：</p><table><thead><tr><th>客户名称</th><th>日期</th><th>消费金额</th><th>消费数量</th></tr></thead><tbody><tr><td>上海****有限公司</td><td>2020-05-20</td><td>76802</td><td>2630</td></tr></tbody></table><p>需要将数据集加工成如下格式：</p><p><img src="//jiamaoxiang.top/2020/05/28/数仓开发需要了解的BI数据分析方法/RFM%E8%A1%A8%E7%BB%93%E6%9E%84.png" alt></p><p>具体SQL实现</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> customer_name,<span class="comment">-- 客户名称</span></span><br><span class="line">customer_avg_money,<span class="comment">-- 当前客户的平均消费金额</span></span><br><span class="line">customer_frequency, <span class="comment">-- 当前客户的消费频次</span></span><br><span class="line">total_frequency,<span class="comment">-- 所有客户的总消费频次</span></span><br><span class="line">total_avg_frequency, <span class="comment">-- 所有客户平均消费频次</span></span><br><span class="line">customer_recency_diff, <span class="comment">-- 当前客户最近一次消费日期与当前日期差值</span></span><br><span class="line">total_recency, <span class="comment">-- 所有客户最近一次消费日期与当前日期差值的平均值</span></span><br><span class="line">monetary,<span class="comment">-- 消费金额向量化</span></span><br><span class="line">frequency, <span class="comment">-- 消费频次向量化</span></span><br><span class="line">recency, <span class="comment">-- 最近消费向量化</span></span><br><span class="line">rfm, <span class="comment">-- rfm</span></span><br><span class="line"><span class="keyword">CASE</span></span><br><span class="line">    <span class="keyword">WHEN</span> rfm = <span class="string">"111"</span> <span class="keyword">THEN</span> <span class="string">"重要价值客户"</span></span><br><span class="line">    <span class="keyword">WHEN</span> rfm = <span class="string">"101"</span> <span class="keyword">THEN</span> <span class="string">"重要发展客户"</span></span><br><span class="line">    <span class="keyword">WHEN</span> rfm = <span class="string">"011"</span> <span class="keyword">THEN</span> <span class="string">"重要保持客户"</span></span><br><span class="line">    <span class="keyword">WHEN</span> rfm = <span class="string">"001"</span> <span class="keyword">THEN</span> <span class="string">"重要挽留客户"</span></span><br><span class="line">    <span class="keyword">WHEN</span> rfm = <span class="string">"110"</span> <span class="keyword">THEN</span> <span class="string">"一般价值客户"</span></span><br><span class="line">    <span class="keyword">WHEN</span> rfm = <span class="string">"100"</span> <span class="keyword">THEN</span> <span class="string">"一般发展客户"</span></span><br><span class="line">    <span class="keyword">WHEN</span> rfm = <span class="string">"010"</span> <span class="keyword">THEN</span> <span class="string">"一般保持客户"</span></span><br><span class="line">    <span class="keyword">WHEN</span> rfm = <span class="string">"000"</span> <span class="keyword">THEN</span> <span class="string">"一般挽留客户"</span></span><br><span class="line">           <span class="keyword">END</span> <span class="keyword">AS</span> rfm_text</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  (<span class="keyword">SELECT</span> customer_name,<span class="comment">-- 客户名称</span></span><br><span class="line">customer_avg_money,<span class="comment">-- 当前客户的平均消费金额</span></span><br><span class="line">customer_frequency, <span class="comment">-- 当前客户的消费频次</span></span><br><span class="line">total_avg_money ,<span class="comment">-- 所有客户的平均消费总额</span></span><br><span class="line">total_frequency,<span class="comment">-- 所有客户的总消费频次</span></span><br><span class="line">total_frequency / <span class="keyword">count</span>(*) <span class="keyword">over</span>() <span class="keyword">AS</span> total_avg_frequency, <span class="comment">-- 所有客户平均消费频次</span></span><br><span class="line">customer_recency_diff, <span class="comment">-- 当前客户最近一次消费日期与当前日期差值</span></span><br><span class="line"><span class="keyword">avg</span>(customer_recency_diff) <span class="keyword">over</span>() <span class="keyword">AS</span> total_recency, <span class="comment">-- 所有客户最近一次消费日期与当前日期差值的平均值</span></span><br><span class="line"><span class="keyword">if</span>(customer_avg_money &gt; total_avg_money,<span class="number">1</span>,<span class="number">0</span>) <span class="keyword">AS</span> monetary, <span class="comment">-- 消费金额向量化</span></span><br><span class="line"><span class="keyword">if</span>(customer_frequency &gt; total_frequency / <span class="keyword">count</span>(*) <span class="keyword">over</span>(),<span class="number">1</span>,<span class="number">0</span>) <span class="keyword">AS</span> frequency, <span class="comment">-- 消费频次向量化</span></span><br><span class="line"><span class="keyword">if</span>(customer_recency_diff &gt; <span class="keyword">avg</span>(customer_recency_diff) <span class="keyword">over</span>(),<span class="number">0</span>,<span class="number">1</span>) <span class="keyword">AS</span> recency, <span class="comment">-- 最近消费向量化</span></span><br><span class="line"><span class="keyword">concat</span>(<span class="keyword">if</span>(customer_recency_diff &gt; <span class="keyword">avg</span>(customer_recency_diff) <span class="keyword">over</span>(),<span class="number">0</span>,<span class="number">1</span>),<span class="keyword">if</span>(customer_frequency &gt; total_frequency / <span class="keyword">count</span>(*) <span class="keyword">over</span>(),<span class="number">1</span>,<span class="number">0</span>),<span class="keyword">if</span>(customer_avg_money &gt; total_avg_money,<span class="number">1</span>,<span class="number">0</span>)) <span class="keyword">AS</span> rfm</span><br><span class="line">   <span class="keyword">FROM</span></span><br><span class="line">     (<span class="keyword">SELECT</span> customer_name, <span class="comment">-- 客户名称</span></span><br><span class="line"><span class="keyword">max</span>(customer_avg_money) <span class="keyword">AS</span> customer_avg_money , <span class="comment">-- 当前客户的平均消费金额</span></span><br><span class="line"><span class="keyword">max</span>(customer_frequency) <span class="keyword">AS</span> customer_frequency, <span class="comment">-- 当前客户的消费频次</span></span><br><span class="line"><span class="keyword">max</span>(total_avg_money) <span class="keyword">AS</span> total_avg_money ,<span class="comment">-- 所有客户的平均消费总额</span></span><br><span class="line"><span class="keyword">max</span>(total_frequency) <span class="keyword">AS</span> total_frequency,<span class="comment">-- 所有客户的总消费频次</span></span><br><span class="line"><span class="keyword">datediff</span>(<span class="keyword">CURRENT_DATE</span>,<span class="keyword">max</span>(customer_recency)) <span class="keyword">AS</span> customer_recency_diff <span class="comment">-- 当前客户最近一次消费日期与当前日期差值</span></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">        (<span class="keyword">SELECT</span> customer_name, <span class="comment">-- 客户名称</span></span><br><span class="line"><span class="keyword">avg</span>(money) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">BY</span> customer_name) <span class="keyword">AS</span> customer_avg_money, <span class="comment">-- 当前客户的平均消费金额</span></span><br><span class="line"><span class="keyword">count</span>(amount) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">BY</span> customer_name) <span class="keyword">AS</span> customer_frequency, <span class="comment">-- 当前客户的消费频次</span></span><br><span class="line"><span class="keyword">avg</span>(money) <span class="keyword">over</span>() <span class="keyword">AS</span> total_avg_money,<span class="comment">-- 所有客户的平均消费总额</span></span><br><span class="line"><span class="keyword">count</span>(amount) <span class="keyword">over</span>() <span class="keyword">AS</span> total_frequency, <span class="comment">--所有客户的总消费频次</span></span><br><span class="line"><span class="keyword">max</span>(sale_date) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">BY</span> customer_name) <span class="keyword">AS</span> customer_recency <span class="comment">-- 当前客户最近一次消费日期</span></span><br><span class="line"></span><br><span class="line">         <span class="keyword">FROM</span> customer_sales) t1</span><br><span class="line">      <span class="keyword">GROUP</span> <span class="keyword">BY</span> customer_name)t2) t3</span><br></pre></td></tr></table></figure><p>通过上面的分析，可以为相对应的客户打上客户特征标签，这样就可以针对某类客户指定不同的营销策略。</p><h2 id="波士顿矩阵与数据可视化"><a href="#波士顿矩阵与数据可视化" class="headerlink" title="波士顿矩阵与数据可视化"></a>波士顿矩阵与数据可视化</h2><h3 id="基本概念-2"><a href="#基本概念-2" class="headerlink" title="基本概念"></a>基本概念</h3><p>波士顿矩阵<strong>(BCG Matrix)</strong>又称市场增长率-相对市场份额矩阵、波士顿咨询集团法、四象限分析法、产品系列结构管理法等。</p><p>　BCG矩阵区分出4种业务组合:</p><ul><li>1.明星型业务（Stars，指高增长、高市场份额）</li><li>2.问题型业务（Question Marks，指高增长、低市场份额）</li><li>3.现金牛业务（Cash cows，指低增长、高市场份额）</li><li>4.瘦狗型业务（Dogs，指低增长、低市场份额）</li></ul><p>波士顿矩阵通过销售增长率（反映市场引力的指标）和市场占有率（反映企业实力的指标）来分析决定企业的产品结构。</p><p><img src="//jiamaoxiang.top/2020/05/28/数仓开发需要了解的BI数据分析方法/%E6%B3%A2%E5%A3%AB%E9%A1%BF%E7%9F%A9%E9%98%B5.png" alt></p><h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><ul><li>效果图</li></ul><p><img src="//jiamaoxiang.top/2020/05/28/数仓开发需要了解的BI数据分析方法/%E6%B3%A2%E5%A3%AB%E9%A1%BF%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90.png" alt></p><ul><li>实现步骤</li></ul><p>本案例以分析客户为背景，将客户分类，找到明星客户、现金牛客户、问题客户以及瘦狗客户。</p><p>假设数据集的样式如下：</p><table><thead><tr><th>客户类型</th><th>客户名称</th><th>消费金额</th><th></th></tr></thead><tbody><tr><td>A类</td><td>上海****公司</td><td>20000</td><td>2020-05-30</td></tr></tbody></table><p>首先需要计算<strong>客单价</strong>：每个客户的平均消费金额，即客单价=某客户总消费金额)/某客户消费次数</p><p>其次需要计算<strong>记录数</strong>：每个客户的消费次数，即某个客户总共消费的次数</p><p>接着需要计算<strong>平均消费金额</strong>：所有客户的平均消费金额，即所有客户的总消费金额/所有客户消费次数</p><p>最后计算<strong>平均消费次数</strong>：所有客户的平均消费次数，即所有客户的总消费次数/总客户数</p><p>具体SQL实现：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    customer_name, <span class="comment">-- 客户名称</span></span><br><span class="line">    customer_avg_money, <span class="comment">-- 客单价</span></span><br><span class="line">    customer_frequency , <span class="comment">-- 当前客户的消费次数</span></span><br><span class="line">    total_avg_money,<span class="comment">-- 所有客户的平均消费金额</span></span><br><span class="line">    total_frequency / <span class="keyword">count</span>(*) <span class="keyword">over</span>() <span class="keyword">AS</span> total_avg_frequency <span class="comment">-- 平均消费次数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  (<span class="keyword">SELECT</span> </span><br><span class="line">        customer_name, <span class="comment">-- 客户名称</span></span><br><span class="line">        <span class="keyword">max</span>(customer_avg_money) <span class="keyword">AS</span> customer_avg_money, <span class="comment">-- 客单价</span></span><br><span class="line">        <span class="keyword">max</span>(customer_frequency) <span class="keyword">AS</span> customer_frequency , <span class="comment">-- 当前客户的消费次数</span></span><br><span class="line">        <span class="keyword">max</span>(total_avg_money) <span class="keyword">AS</span> total_avg_money,<span class="comment">-- 所有客户的平均消费金额</span></span><br><span class="line">        <span class="keyword">max</span>(total_frequency) <span class="keyword">AS</span> total_frequency <span class="comment">--所有客户的总消费频次</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">FROM</span></span><br><span class="line">     (</span><br><span class="line">        <span class="keyword">SELECT</span></span><br><span class="line">             customer_name, <span class="comment">-- 客户名称</span></span><br><span class="line">             <span class="keyword">avg</span>(money) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">BY</span> customer_name) <span class="keyword">AS</span> customer_avg_money, <span class="comment">-- 客单价</span></span><br><span class="line">             <span class="keyword">count</span>(*) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">BY</span> customer_name) <span class="keyword">AS</span> customer_frequency, <span class="comment">-- 当前客户的消费次数</span></span><br><span class="line">            <span class="keyword">avg</span>(money) <span class="keyword">over</span>() <span class="keyword">AS</span> total_avg_money,<span class="comment">-- 所有客户的平均消费金额</span></span><br><span class="line">            <span class="keyword">count</span>(*) <span class="keyword">over</span>() <span class="keyword">AS</span> total_frequency <span class="comment">--所有客户的总消费频次</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">FROM</span> customer_sales ) t1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> customer_name) t2</span><br></pre></td></tr></table></figure><p>经过上面的分析，大致可以看出客户画像：</p><ul><li>某客户的消费次数超过平均值，并且每次消费力度(客单价)也超过平均水平的客户：判定为明星客户，这类客户需要重点关注；</li><li>某客户的消费次数超过平均值，但每次消费力度未达到平均水平的客户：被判定为现金牛客户，这类客户通常消费频次比较频繁，能给企业带来较为稳定的现金流，这类客户是企业利润基石；</li><li>某客户的消费次数未达到平均值，但每次消费力度超过平均水平的客户：是问题客户，这类客户最有希望转化为明星客户，但是因为客户存在一定的潜在问题，导致消费频次不高，这类客户需要进行重点跟进和长期沟通；</li><li>消费次数未达到平均值，消费力度也未达到平均水平的客户：属于瘦狗客户，这类客户通常占企业客户的大多数，只需要一般性维护，如果企业资源有限，则可以不用投入太多的精力。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要介绍了数仓开发应该要了解的常见的数据分析方法，主要有三种：帕累托分析、RFM分析以及波士顿矩阵分析。本文分别介绍了三种分析方法的基本概念、操作步骤以及SQL实现，并给出了相应的可视化分析图表，每个案例都是企业的真实应用场景。希望给数仓开发的同学提供一些观察数据的分析角度，从而在实际的开发过程中能够多思考一下数据的应用价值以及数据如何赋能业务，进一步提升自己的综合能力。</p><p>​    </p>]]></content>
      
      
      <categories>
          
          <category> 数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数仓 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink Table API &amp; SQL编程指南之动态表(2)</title>
      <link href="/2020/05/28/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97-2/"/>
      <url>/2020/05/28/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97-2/</url>
      
        <content type="html"><![CDATA[<p>在<a href="https://mp.weixin.qq.com/s/NUuwqzsJmUAn8QEWrjjlyQ" target="_blank" rel="noopener">Flink Table API &amp; SQL编程指南(1)</a>一文中介绍了Flink Table API &amp;SQL的一些基本的概念和通用的API，在本文将会更加深入地讲解Flink Table API &amp;SQL的流处理的基本概念。Flink Table API &amp;SQL是实现了批流处理的统一，这也意味着无论是有界的批处理输入还是无界的流处理输入，使用Flink Table API &amp;SQL进行查询操作，都具有相同的语义。此外，由于SQL最初是为批处理而设计的，所有在无界流上使用关系查询与在有界流上使用关系查询是有所不同的，本文将着重介绍一下动态表。</p><h2 id="动态表"><a href="#动态表" class="headerlink" title="动态表"></a>动态表</h2><p>SQL与关系代数最初的设计不是为了流处理，所以SQL与流处理之间存在一定的差异，Flink实现了在无界的数据流上使用SQL操作。</p><h3 id="数据流上的关系查询"><a href="#数据流上的关系查询" class="headerlink" title="数据流上的关系查询"></a>数据流上的关系查询</h3><p>传统的关系代数(SQL)与流处理在数据的输入、执行以及结果的输出都有所差异，具体如下：</p><table><thead><tr><th>区别</th><th>关系代数/SQL</th><th>流处理</th></tr></thead><tbody><tr><td>数据输入</td><td>表、有界的元祖的集合</td><td>无界的数据流</td></tr><tr><td>执行</td><td>批处理，在整个输入数据上执行查询等操作</td><td>不能在所有的数据上执行查询，需要等待数据流的到来</td></tr><tr><td>结果输出</td><td>查询处理结束之后，输出固定大小的结果</td><td>需要连续不断地更新之前的结果，永远不会结束</td></tr></tbody></table><p>尽管存在这些差异，但并不意味着SQL与流处理不能融合。一些高级的关系型数据库都提供了物化视图的功能，一个物化视图由一个查询语句进行定义，与普通的视图相比，物化视图缓存了查询的结果，所以当访问物化视图时，不需要重复执行SQL查询操作。缓存的一个常见挑战是如何防止提供过时的结果，当定义物化视图的查询基表发生变化时，物化视图的结果就会过时。*<em>Eager View Maintenance *</em>是一种更新物化视图的技术，只要物化视图的查询基表被更新，那么物化视图就会被更新。</p><p>如果考虑以下因素，那么*<em>Eager View Maintenance *</em>与对流进行SQL查询之间的联系将变得显而易见：</p><ul><li>数据库表是在流上执行<code>INSERT</code>，<code>UPDATE</code>和<code>DELETE</code>DML操作语句的结果，通常被称为changelog stream(更新日志流)。</li><li>一个物化视图由一个查询语句进行定义。为了更新雾化视图，查询会连续处理雾化视图的变更日志流。</li><li>雾化视图是流式SQL查询的结果。</li></ul><h3 id="动态表与持续的查询"><a href="#动态表与持续的查询" class="headerlink" title="动态表与持续的查询"></a>动态表与持续的查询</h3><p>动态表是Flink TableAPI &amp;SQL支持流处理的核心概念，与批处理的静态表相比，动态表会随着时间的变化而变化。动态表查询会产生一个<em>Continuous Query</em>，Continuous Query不会终止并且会产生动态表 的结果。</p><p>关于流、动态表与Continuous Query之间的关系，如下图所示：</p><p><img src="//jiamaoxiang.top/2020/05/28/Flink-Table-API-SQL编程指南-2/%E5%8A%A8%E6%80%81%E8%A1%A8%E6%95%B0%E6%8D%AE%E6%B5%81.png" alt></p><ul><li>1.流被转换为动态表</li><li>2.Continuous Query在动态表上不停的执行，产生一个新的动态表</li><li>3.动态表被转换成流</li></ul><p><strong>尖叫提示：</strong>动态表是一个逻辑概念，在执行查询期间动态表不会被雾化。</p><h3 id="在流上定义表"><a href="#在流上定义表" class="headerlink" title="在流上定义表"></a>在流上定义表</h3><p>在数据流上使用SQL查询，需要将流转换成表。流中的记录会被解析并插入到表中(对于一个只有插入操作的流)，如下图所示：</p><p><img src="//jiamaoxiang.top/2020/05/28/Flink-Table-API-SQL编程指南-2/%E8%A1%A81.png" alt></p><h4 id="持续查询"><a href="#持续查询" class="headerlink" title="持续查询"></a>持续查询</h4><ul><li>分组聚合(group aggregation)</li></ul><p>在一个动态表上的持续查询，会产生一个新的动态表结果。与批处理的查询相比，持续的查询从不会结束并且会根据输入的数据更新之前的结果。下面的示例中，展示了点击事件流，并使用分组聚合计算，如下图：</p><p><img src="//jiamaoxiang.top/2020/05/28/Flink-Table-API-SQL编程指南-2/%E8%A1%A82.png" alt></p><p>上图中展示了一个用户点击行为的数据，计算操作使用的是分组聚合(group aggregation)。当第一条数据[Mary,./home]进来时，会立即进行计算操作，并输出计算结果：[Mary，1]。当[Bob,./cart]进来时，也会立即进行计算，并输出计算结果：[Mary，1],[Bob，1]。当[Mary,./prod?id=1]进来时，会立即进行计算，并输出结果：[Mary，2],[Bob，1]。由此可以看出：分组聚合会作用在所有的数据上，并且会更新之前输出的结果。</p><ul><li>窗口聚合(window aggregation)</li></ul><p>上面演示的是一个分组聚合的案例，下面再来看一个窗口聚合的案例。按照一个小时的滚动窗口(Tumble Window)计算该一小时内的用户点击情况，具体如下图所示：</p><p><img src="//jiamaoxiang.top/2020/05/28/Flink-Table-API-SQL编程指南-2/F:%5Cnpm%5Cmywebsite%5Csource_posts%5CFlink-Table-API-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97-2%5C%E5%9B%BE3.png" alt></p><p>如上图所示：Ctime表示事件发生的时间，可以看出在[12:00:00,12:59:59]的一小时内总共有四行数据，在[13:00:00,13:59:59]的一小时内有三行数据，在[14:00:00,14:59:59]一小时内总共有四行数据。</p><p>可以看出：在[12:00:00,12:59:59]时间段内，计算的结果为[Marry,13:00:00,3],[Bob,13:00:00,1],该结果会追加(Append)到该结果表中，在[13:00:00,13:59:59]时间段内，计算结果为[Bob,14:00:00,1],[Liz,14:00:00,2]，该结果同样是追加到结果表中，之前窗口的数据并不会更新。所以窗口聚合的特点就是只计算属于该窗口的数据，并以追加的方式将结果插入结果表中。</p><ul><li>分组聚合与窗口聚合的异同</li></ul><table><thead><tr><th>比较</th><th>分组聚合</th><th>窗口聚合</th></tr></thead><tbody><tr><td>输出模式</td><td>提前输出，每来一条数据计算一次</td><td>按窗口触发时间输出</td></tr><tr><td>输出量</td><td>一个窗口输出一次结果</td><td>每个key输出N个结果</td></tr><tr><td>输出流</td><td>追加流(Append Stream)</td><td>更新流(Update Stream)</td></tr><tr><td>状态清理</td><td>及时清理掉过时的数据</td><td>状态会无限增长</td></tr><tr><td></td><td>不要求输出端支持update操作</td><td>支持更新操作(kudu,HBase,MySQL等)</td></tr></tbody></table><h4 id="更新与追加查询"><a href="#更新与追加查询" class="headerlink" title="更新与追加查询"></a>更新与追加查询</h4><p>上面的两个例子分别演示了更新的查询与追加查询，第一个分组聚合的案例输出的结果会更新之前的结果，即结果表包含<strong>INSERT</strong>与<strong>UPDATE</strong>操作。</p><p>第二个窗口聚合的案例仅仅是追加计算结果的结果表中，即结果表仅包含<strong>INSERT</strong>操作。</p><p>当一个查询产生一个仅追加(append-only)的表或者更新表(updated table)时，区别如下：</p><ul><li>当查询产生的是一个更新表时(即会更新之前输出的结果)，需要维护一个更大的状态</li><li>append-only表与更新表(updated table)转为流(Stream)的方式有所不同</li></ul><h3 id="表到流的转换"><a href="#表到流的转换" class="headerlink" title="表到流的转换"></a>表到流的转换</h3><p>动态表会被INSER、UPDATE、DELETE操作持续地被更改。当将一个动态表转为流或者写出到外部的存储系统时，这些改变的值需要被编码，Flink Table API和SQL支持三种方式对这些改变的数据进行编码：</p><ul><li><strong>Append-only stream</strong></li></ul><p>动态表只会被INSERT操作进行修改，改变的数据(新增的数据)会被插入到动态表的行中</p><ul><li><strong>Retract stream</strong></li></ul><p>一个retract stream包含两种类型的消息，分别为添加消息(add messages)与撤回消息(retract message)。动态表被转为retract stream(撤回流)时，将INSERT操作的变化数据编码为添加消息(add messages)，将DELETE操作引起的变化数据编码为撤回消息(retract message)，将UPDATE操作引起的变化数据而言，会先将旧数据(需要被更新的)编码为retract message，将新的更新的数据编码为添加消息(add messages)，具体如下图所示：</p><p><img src="//jiamaoxiang.top/2020/05/28/Flink-Table-API-SQL编程指南-2/%E5%9B%BE4.png" alt></p><ul><li><strong>Upsert stream</strong></li></ul><p>upsert 流有两种类型的消息：<em>upsert messages</em>与<em>delete messages</em>。动态表被转换为upsert流需要一个唯一主键(可能是复合)key，附带唯一主键key的动态表在被转化为流的时候，会把INSERT与UPDATE操作引起的变化数据编码为upsert messages，把DELETE操作引起的变化数据编码为delete message。与retract 流相比，upsert 流对于UPADTE操作引起的变化数据的编码，使用的是单个消息，即upsert message。对于retract 流，需要先将旧数据编码为retract message，然后再将新数据编码为add message，即需要编码Delete与Insert两条消息，因此使用upsert流效率更高。具体如下图所示：</p><p><img src="//jiamaoxiang.top/2020/05/28/Flink-Table-API-SQL编程指南-2/%E5%9B%BE5.png" alt></p><p><strong>尖叫提示：</strong>将动态表转为datastream时，仅支持append 流与retract流。将动态表输出到外部系统时，支持Append、Retract以及Upsert模式。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了Flink TableAPI&amp;SQL中动态表的概念。首先介绍了动态表的基本概念，然后介绍了在流上定义表的方式，并指出了分组聚合与窗口聚合的异同，最后介绍了表到流的转换并输出到外部系统的三种模式。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink Table API &amp; SQL编程指南(1)</title>
      <link href="/2020/05/25/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/"/>
      <url>/2020/05/25/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink提供了两种顶层的关系型API，分别为Table API和SQL，Flink通过Table API&amp;SQL实现了批流统一。其中Table API是用于Scala和Java的语言集成查询API，它允许以非常直观的方式组合关系运算符（例如select，where和join）的查询。Flink SQL基于<a href="https://calcite.apache.org/" target="_blank" rel="noopener">Apache Calcite</a> 实现了标准的SQL，用户可以使用标准的SQL处理数据集。Table API和SQL与Flink的DataStream和DataSet API紧密集成在一起，用户可以实现相互转化，比如可以将DataStream或者DataSet注册为table进行操作数据。值得注意的是，<strong>Table API and SQL</strong>目前尚未完全完善，还在积极的开发中，所以并不是所有的算子操作都可以通过其实现。</p><a id="more"></a><h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><p>从Flink1.9开始，Flink为Table &amp; SQL API提供了两种planner,分别为Blink planner和old planner，其中old planner是在Flink1.9之前的版本使用。主要区别如下：</p><p><strong>尖叫提示</strong>：对于生产环境，目前推荐使用old planner.</p><ul><li><code>flink-table-common</code>: 通用模块，包含 Flink Planner 和 Blink Planner 一些共用的代码</li><li><code>flink-table-api-java</code>: java语言的Table &amp; SQL API，仅针对table(处于早期的开发阶段，不推荐使用) </li><li><code>flink-table-api-scala</code>: scala语言的Table &amp; SQL API，仅针对table(处于早期的开发阶段，不推荐使用) </li><li><code>flink-table-api-java-bridge</code>: java语言的Table &amp; SQL API，支持DataStream/DataSet API(推荐使用) </li><li><code>flink-table-api-scala-bridge</code>: scala语言的Table &amp; SQL API，支持DataStream/DataSet API(推荐使用) </li><li><code>flink-table-planner</code>:planner 和runtime. planner为Flink1,9之前的old planner(推荐使用) </li><li><code>flink-table-planner-blink</code>: 新的Blink planner.</li><li><code>flink-table-runtime-blink</code>: 新的Blink runtime.</li><li><code>flink-table-uber</code>: 将上述的API模块及old planner打成一个jar包，形如flink-table-*.jar，位与/lib目录下</li><li><code>flink-table-uber-blink</code>:将上述的API模块及Blink 模块打成一个jar包，形如fflink-table-blink-*.jar，位与/lib目录下</li></ul><h2 id="Blink-planner-amp-old-planner"><a href="#Blink-planner-amp-old-planner" class="headerlink" title="Blink planner &amp; old planner"></a>Blink planner &amp; old planner</h2><p>Blink planner和old planner有许多不同的特点，具体列举如下：</p><ul><li>Blink planner将批处理作业看做是流处理作业的特例。所以，不支持Table 与DataSet之间的转换，批处理的作业也不会被转成DataSet程序，而是被转为DataStream程序。</li><li>Blink planner不支持 <code>BatchTableSource</code>，使用的是有界的StreamTableSource。</li><li>Blink planner仅支持新的 <code>Catalog</code>，不支持<code>ExternalCatalog</code> (已过时)。</li><li>对于FilterableTableSource的实现，两种Planner是不同的。old planner会谓词下推到<code>PlannerExpression</code>(未来会被移除)，而Blink planner 会谓词下推到 <code>Expression</code>(表示一个产生计算结果的逻辑树)。</li><li>仅仅Blink planner支持key-value形式的配置，即通过Configuration进行参数设置。</li><li>关于PlannerConfig的实现，两种planner有所不同。</li><li>Blink planner 会将多个sink优化成一个DAG(仅支持TableEnvironment，StreamTableEnvironment不支持)，old planner总是将每一个sink优化成一个新的DAG，每一个DAG都是相互独立的。</li><li>old planner不支持catalog统计，Blink planner支持catalog统计。</li></ul><h2 id="Flink-Table-amp-SQL程序的pom依赖"><a href="#Flink-Table-amp-SQL程序的pom依赖" class="headerlink" title="Flink Table &amp; SQL程序的pom依赖"></a>Flink Table &amp; SQL程序的pom依赖</h2><p>根据使用的语言不同，可以选择下面的依赖，包括scala版和java版，如下：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;!-- java版 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-table-api-java-bridge_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;!-- scala版 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-table-api-scala-bridge_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>除此之外，如果需要在本地的IDE中运行Table API &amp; SQL的程序，则需要添加下面的pom依赖：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;!-- Flink <span class="number">1.9</span>之前的old planner --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-table-planner_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;!-- 新的Blink planner --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-table-planner-blink_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>另外，如果需要实现自定义的格式(比如和kafka交互)或者用户自定义函数，需要添加如下依赖：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h2 id="Table-API-amp-SQL的编程模板"><a href="#Table-API-amp-SQL的编程模板" class="headerlink" title="Table API &amp; SQL的编程模板"></a>Table API &amp; SQL的编程模板</h2><p>所有的Table API&amp;SQL的程序(无论是批处理还是流处理)都有着相同的形式，下面将给出通用的编程结构形式：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建一个TableEnvironment对象，指定planner、处理模式(batch、streaming)</span></span><br><span class="line">TableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// 创建一个表</span></span><br><span class="line">tableEnv.connect(...).createTemporaryTable(<span class="string">"table1"</span>);</span><br><span class="line"><span class="comment">// 注册一个外部的表</span></span><br><span class="line">tableEnv.connect(...).createTemporaryTable(<span class="string">"outputTable"</span>);</span><br><span class="line"><span class="comment">// 通过Table API的查询创建一个Table 对象</span></span><br><span class="line">Table tapiResult = tableEnv.from(<span class="string">"table1"</span>).select(...);</span><br><span class="line"><span class="comment">// 通过SQL查询的查询创建一个Table 对象</span></span><br><span class="line">Table sqlResult  = tableEnv.sqlQuery(<span class="string">"SELECT ... FROM table1 ... "</span>);</span><br><span class="line"><span class="comment">// 将结果写入TableSink</span></span><br><span class="line">tapiResult.insertInto(<span class="string">"outputTable"</span>);</span><br><span class="line"><span class="comment">// 执行</span></span><br><span class="line">tableEnv.execute(<span class="string">"java_job"</span>);</span><br></pre></td></tr></table></figure><p>注意：Table API &amp; SQL的查询可以相互集成，另外还可以在DataStream或者DataSet中使用Table API &amp; SQL的API，实现DataStreams、 DataSet与Table之间的相互转换。</p><h2 id="创建TableEnvironment"><a href="#创建TableEnvironment" class="headerlink" title="创建TableEnvironment"></a>创建TableEnvironment</h2><p>TableEnvironment是Table API &amp; SQL程序的一个入口，主要包括如下的功能：</p><ul><li>在内部的catalog中注册Table</li><li>注册catalog</li><li>加载可插拔模块</li><li>执行SQL查询</li><li>注册用户定义函数</li><li><code>DataStream</code> 、<code>DataSet</code>与Table之间的相互转换</li><li>持有对<code>ExecutionEnvironment</code> 、<code>StreamExecutionEnvironment</code>的引用</li></ul><p>一个Table必定属于一个具体的TableEnvironment，不可以将不同TableEnvironment的表放在一起使用(比如join，union等操作)。</p><p>TableEnvironment是通过调用 <code>BatchTableEnvironment.create()</code> 或者StreamTableEnvironment.create()的静态方法进行创建的。另外，默认两个planner的jar包都存在与classpath下，所有需要明确指定使用的planner。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// **********************</span></span><br><span class="line"><span class="comment">// FLINK 流处理查询</span></span><br><span class="line"><span class="comment">// **********************</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line"></span><br><span class="line">EnvironmentSettings fsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build();</span><br><span class="line">StreamExecutionEnvironment fsEnv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment fsTableEnv = StreamTableEnvironment.create(fsEnv, fsSettings);</span><br><span class="line"><span class="comment">//或者TableEnvironment fsTableEnv = TableEnvironment.create(fsSettings);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line"><span class="comment">// FLINK 批处理查询</span></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.BatchTableEnvironment;</span><br><span class="line"></span><br><span class="line">ExecutionEnvironment fbEnv = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">BatchTableEnvironment fbTableEnv = BatchTableEnvironment.create(fbEnv);</span><br><span class="line"></span><br><span class="line"><span class="comment">// **********************</span></span><br><span class="line"><span class="comment">// BLINK 流处理查询</span></span><br><span class="line"><span class="comment">// **********************</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line"></span><br><span class="line">StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();</span><br><span class="line">StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);</span><br><span class="line"><span class="comment">// 或者 TableEnvironment bsTableEnv = TableEnvironment.create(bsSettings);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line"><span class="comment">// BLINK 批处理查询</span></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"></span><br><span class="line">EnvironmentSettings bbSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build();</span><br><span class="line">TableEnvironment bbTableEnv = TableEnvironment.create(bbSettings);</span><br></pre></td></tr></table></figure><h2 id="在catalog中创建表"><a href="#在catalog中创建表" class="headerlink" title="在catalog中创建表"></a>在catalog中创建表</h2><h3 id="临时表与永久表"><a href="#临时表与永久表" class="headerlink" title="临时表与永久表"></a>临时表与永久表</h3><p>表可以分为临时表和永久表两种，其中永久表需要一个catalog(比如Hive的Metastore)俩维护表的元数据信息，一旦永久表被创建，只要连接到该catalog就可以访问该表，只有显示删除永久表，该表才可以被删除。临时表的生命周期是Flink Session，这些表不能够被其他的Flink Session访问，这些表不属于任何的catalog或者数据库，如果与临时表相对应的数据库被删除了，该临时表也不会被删除。</p><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><h4 id="虚表-Virtual-Tables"><a href="#虚表-Virtual-Tables" class="headerlink" title="虚表(Virtual Tables)"></a>虚表(Virtual Tables)</h4><p>一个Table对象相当于SQL中的视图(虚表)，它封装了一个逻辑执行计划，可以通过一个catalog创建，具体如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取一个TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// table对象，查询的结果集</span></span><br><span class="line">Table projTable = tableEnv.from(<span class="string">"X"</span>).select(...);</span><br><span class="line"><span class="comment">// 注册一个表，名称为 "projectedTable"</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"projectedTable"</span>, projTable);</span><br></pre></td></tr></table></figure><h4 id="外部数据源表-Connector-Tables"><a href="#外部数据源表-Connector-Tables" class="headerlink" title="外部数据源表(Connector Tables)"></a>外部数据源表(Connector Tables)</h4><p>可以把外部的数据源注册成表，比如可以读取MySQL数据库数据、Kafka数据等</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">tableEnvironment</span><br><span class="line">  .connect(...)</span><br><span class="line">  .withFormat(...)</span><br><span class="line">  .withSchema(...)</span><br><span class="line">  .inAppendMode()</span><br><span class="line">  .createTemporaryTable(<span class="string">"MyTable"</span>)</span><br></pre></td></tr></table></figure><h3 id="扩展创建表的标识属性"><a href="#扩展创建表的标识属性" class="headerlink" title="扩展创建表的标识属性"></a>扩展创建表的标识属性</h3><p>表的注册总是包含三部分标识属性：catalog、数据库、表名。用户可以在内部设置一个catalog和一个数据库作为当前的catalog和数据库，所以对于catalog和数据库这两个标识属性是可选的，即如果不指定，默认使用的是“current catalog”和 “current database”。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">TableEnvironment tEnv = ...;</span><br><span class="line">tEnv.useCatalog(<span class="string">"custom_catalog"</span>);<span class="comment">//设置catalog</span></span><br><span class="line">tEnv.useDatabase(<span class="string">"custom_database"</span>);<span class="comment">//设置数据库</span></span><br><span class="line">Table table = ...;</span><br><span class="line"><span class="comment">// 注册一个名为exampleView的视图，catalog名为custom_catalog</span></span><br><span class="line"><span class="comment">// 数据库的名为custom_database</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"exampleView"</span>, table);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个名为exampleView的视图，catalog的名为custom_catalog</span></span><br><span class="line"><span class="comment">// 数据库的名为other_database</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"other_database.exampleView"</span>, table);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 注册一个名为'View'的视图，catalog的名称为custom_catalog</span></span><br><span class="line"><span class="comment">// 数据库的名为custom_database，'View'是保留关键字，需要使用``(反引号)</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"`View`"</span>, table);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个名为example.View的视图，catalog的名为custom_catalog，</span></span><br><span class="line"><span class="comment">// 数据库名为custom_database</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"`example.View`"</span>, table);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个名为'exampleView'的视图， catalog的名为'other_catalog'</span></span><br><span class="line"><span class="comment">// 数据库名为other_database' </span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"other_catalog.other_database.exampleView"</span>, table);</span><br></pre></td></tr></table></figure><h2 id="查询表"><a href="#查询表" class="headerlink" title="查询表"></a>查询表</h2><h3 id="Table-API"><a href="#Table-API" class="headerlink" title="Table API"></a>Table API</h3><p>Table API是一个集成Scala与Java语言的查询API，与SQL相比，它的查询不是一个标准的SQL语句，而是由一步一步的操作组成的。如下展示了一个使用Table API实现一个简单的聚合查询。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"><span class="comment">//注册Orders表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 查询注册的表</span></span><br><span class="line">Table orders = tableEnv.from(<span class="string">"Orders"</span>);</span><br><span class="line"><span class="comment">// 计算操作</span></span><br><span class="line">Table revenue = orders</span><br><span class="line">  .filter(<span class="string">"cCountry === 'FRANCE'"</span>)</span><br><span class="line">  .groupBy(<span class="string">"cID, cName"</span>)</span><br><span class="line">  .select(<span class="string">"cID, cName, revenue.sum AS revSum"</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h3><p>Flink SQL依赖于<a href="https://calcite.apache.org/" target="_blank" rel="noopener">Apache Calcite</a>，其实现了标准的SQL语法，如下案例：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//注册Orders表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算逻辑同上面的Table API</span></span><br><span class="line">Table revenue = tableEnv.sqlQuery(</span><br><span class="line">    <span class="string">"SELECT cID, cName, SUM(revenue) AS revSum "</span> +</span><br><span class="line">    <span class="string">"FROM Orders "</span> +</span><br><span class="line">    <span class="string">"WHERE cCountry = 'FRANCE' "</span> +</span><br><span class="line">    <span class="string">"GROUP BY cID, cName"</span></span><br><span class="line">  );</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册"RevenueFrance"外部输出表</span></span><br><span class="line"><span class="comment">// 计算结果插入"RevenueFrance"表</span></span><br><span class="line">tableEnv.sqlUpdate(</span><br><span class="line">    <span class="string">"INSERT INTO RevenueFrance "</span> +</span><br><span class="line">    <span class="string">"SELECT cID, cName, SUM(revenue) AS revSum "</span> +</span><br><span class="line">    <span class="string">"FROM Orders "</span> +</span><br><span class="line">    <span class="string">"WHERE cCountry = 'FRANCE' "</span> +</span><br><span class="line">    <span class="string">"GROUP BY cID, cName"</span></span><br><span class="line">  );</span><br></pre></td></tr></table></figure><h2 id="输出表"><a href="#输出表" class="headerlink" title="输出表"></a>输出表</h2><p>一个表通过将其写入到TableSink，然后进行输出。TableSink是一个通用的支持多种文件格式(CSV、Parquet, Avro)和多种外部存储系统(JDBC, Apache HBase, Apache Cassandra, Elasticsearch)以及多种消息对列(Apache Kafka, RabbitMQ)的接口。</p><p>批处理的表只能被写入到 <code>BatchTableSink</code>,流处理的表需要指明AppendStreamTableSink、RetractStreamTableSink或者 <code>UpsertStreamTableSink</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建输出表</span></span><br><span class="line"><span class="keyword">final</span> Schema schema = <span class="keyword">new</span> Schema()</span><br><span class="line">    .field(<span class="string">"a"</span>, DataTypes.INT())</span><br><span class="line">    .field(<span class="string">"b"</span>, DataTypes.STRING())</span><br><span class="line">    .field(<span class="string">"c"</span>, DataTypes.LONG());</span><br><span class="line"></span><br><span class="line">tableEnv.connect(<span class="keyword">new</span> FileSystem(<span class="string">"/path/to/file"</span>))</span><br><span class="line">    .withFormat(<span class="keyword">new</span> Csv().fieldDelimiter(<span class="string">'|'</span>).deriveSchema())</span><br><span class="line">    .withSchema(schema)</span><br><span class="line">    .createTemporaryTable(<span class="string">"CsvSinkTable"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算结果表</span></span><br><span class="line">Table result = ...</span><br><span class="line"><span class="comment">// 输出结果表到注册的TableSink</span></span><br><span class="line">result.insertInto(<span class="string">"CsvSinkTable"</span>);</span><br></pre></td></tr></table></figure><h2 id="Table-API-amp-SQL底层的转换与执行"><a href="#Table-API-amp-SQL底层的转换与执行" class="headerlink" title="Table API &amp; SQL底层的转换与执行"></a>Table API &amp; SQL底层的转换与执行</h2><p>上文提到了Flink提供了两种planner，分别为old planner和Blink planner，对于不同的planner而言，Table API &amp; SQL底层的执行与转换是有所不同的。</p><h4 id="Old-planner"><a href="#Old-planner" class="headerlink" title="Old planner"></a>Old planner</h4><p>根据是流处理作业还是批处理作业，Table API &amp;SQL会被转换成DataStream或者DataSet程序。一个查询在内部表示为一个逻辑查询计划，会被转换为两个阶段:</p><ul><li>1.逻辑查询计划优化</li><li>2.转换成DataStream或者DataSet程序</li></ul><p>上面的两个阶段只有下面的操作被执行时才会被执行：</p><ul><li>当一个表被输出到TableSink时，比如调用了Table.insertInto()方法</li><li>当执行更新查询时，比如调用TableEnvironment.sqlUpdate()方法</li><li>当一个表被转换为DataStream或者DataSet时</li></ul><p>一旦执行上述两个阶段，Table API &amp; SQL的操作会被看做是普通的DataStream或者DataSet程序，所以当<code>StreamExecutionEnvironment.execute()</code>或者<code>ExecutionEnvironment.execute()</code> 被调用时，会执行转换后的程序。</p><h4 id="Blink-planner"><a href="#Blink-planner" class="headerlink" title="Blink planner"></a>Blink planner</h4><p>无论是批处理作业还是流处理作业，如果使用的是Blink planner，底层都会被转换为DataStream程序。在一个查询在内部表示为一个逻辑查询计划，会被转换成两个阶段：</p><ul><li>1.逻辑查询计划优化</li><li>2.转换成DataStream程序</li></ul><p>对于<code>TableEnvironment</code> and <code>StreamTableEnvironment</code>而言，一个查询的转换是不同的</p><p>首先对于TableEnvironment，当TableEnvironment.execute()方法执行时，Table API &amp; SQL的查询才会被转换，因为TableEnvironment会将多个sink优化为一个DAG。</p><p>对于StreamTableEnvironment，转换发生的时间与old planner相同。</p><h2 id="与DataStream-amp-DataSet-API集成"><a href="#与DataStream-amp-DataSet-API集成" class="headerlink" title="与DataStream &amp; DataSet API集成"></a>与DataStream &amp; DataSet API集成</h2><p>对于Old planner与Blink planner而言，只要是流处理的操作，都可以与DataStream API集成，<strong>仅仅只有Old planner才可以与DataSet API集成</strong>，由于Blink planner的批处理作业会被转换成DataStream程序，所以不能够与DataSet API集成。值得注意的是，下面提到的table与DataSet之间的转换仅适用于Old planner。</p><p>Table API &amp; SQL的查询很容易与DataStream或者DataSet程序集成，并可以将Table API &amp; SQL的查询嵌入DataStream或者DataSet程序中。DataStream或者DataSet可以转换成表，反之，表也可以被转换成DataStream或者DataSet。</p><h3 id="从DataStream或者DataSet中注册临时表-视图"><a href="#从DataStream或者DataSet中注册临时表-视图" class="headerlink" title="从DataStream或者DataSet中注册临时表(视图)"></a>从DataStream或者DataSet中注册临时表(视图)</h3><p><strong>尖叫提示：</strong>只能将DataStream或者DataSet转换为临时表(视图)</p><p>下面演示DataStream的转换，对于DataSet的转换类似。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream注册为一个名为myTable的视图，其中字段分别为"f0", "f1"</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"myTable"</span>, stream);</span><br><span class="line"><span class="comment">// 将DataStream注册为一个名为myTable2的视图,其中字段分别为"myLong", "myString"</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"myTable2"</span>, stream, <span class="string">"myLong, myString"</span>);</span><br></pre></td></tr></table></figure><h3 id="将DataStream或者DataSet转化为Table对象"><a href="#将DataStream或者DataSet转化为Table对象" class="headerlink" title="将DataStream或者DataSet转化为Table对象"></a>将DataStream或者DataSet转化为Table对象</h3><p>可以直接将DataStream或者DataSet转换为Table对象，之后可以使用Table API进行查询操作。下面演示DataStream的转换，对于DataSet的转换类似。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转换为Table对象，默认的字段为"f0", "f1"</span></span><br><span class="line">Table table1 = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">// 将DataStream转换为Table对象，默认的字段为"myLong", "myString"</span></span><br><span class="line">Table table2 = tableEnv.fromDataStream(stream, <span class="string">"myLong, myString"</span>);</span><br></pre></td></tr></table></figure><h3 id="将表转换为DataStream或者DataSet"><a href="#将表转换为DataStream或者DataSet" class="headerlink" title="将表转换为DataStream或者DataSet"></a>将表转换为DataStream或者DataSet</h3><p>当将Table转为DataStream或者DataSet时，需要指定DataStream或者DataSet的数据类型。通常最方便的数据类型是row类型，Flink提供了很多的数据类型供用户选择，具体包括Row、POJO、样例类、Tuple和原子类型。</p><h4 id="将表转换为DataStream"><a href="#将表转换为DataStream" class="headerlink" title="将表转换为DataStream"></a>将表转换为DataStream</h4><p>一个流处理查询的结果是动态变化的，所以将表转为DataStream时需要指定一个更新模式，共有两种模式：<strong>Append Mode</strong>和<strong>Retract Mode</strong>。</p><ul><li><strong>Append Mode</strong></li></ul><p>如果动态表仅只有Insert操作，即之前输出的结果不会被更新，则使用该模式。如果更新或删除操作使用追加模式会失败报错</p><ul><li><strong>Retract Mode</strong></li></ul><p>始终可以使用此模式。返回值是boolean类型。它用true或false来标记数据的插入和撤回，返回true代表数据插入，false代表数据的撤回。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment. </span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// 包含两个字段的表(String name, Integer age)</span></span><br><span class="line">Table table = ...</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用Append Mode追加模式，数据类型为Row</span></span><br><span class="line">DataStream&lt;Row&gt; dsRow = tableEnv.toAppendStream(table, Row.class);</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用Append Mode追加模式，数据类型为定义好的TypeInformation</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(</span><br><span class="line">  Types.STRING(),</span><br><span class="line">  Types.INT());</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = </span><br><span class="line">  tableEnv.toAppendStream(table, tupleType);</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用的模式为Retract Mode撤回模式，类型为Row</span></span><br><span class="line"><span class="comment">// 对于转换后的DataStream&lt;Tuple2&lt;Boolean, X&gt;&gt;，X表示流的数据类型，</span></span><br><span class="line"><span class="comment">// boolean值表示数据改变的类型，其中INSERT返回true，DELETE返回的是false</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retractStream = </span><br><span class="line">  tableEnv.toRetractStream(table, Row.class);</span><br></pre></td></tr></table></figure><h4 id="将表转换为DataSet"><a href="#将表转换为DataSet" class="headerlink" title="将表转换为DataSet"></a>将表转换为DataSet</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取BatchTableEnvironment</span></span><br><span class="line">BatchTableEnvironment tableEnv = BatchTableEnvironment.create(env);</span><br><span class="line"><span class="comment">// 包含两个字段的表(String name, Integer age)</span></span><br><span class="line">Table table = ...</span><br><span class="line"><span class="comment">// 将表转为DataSet数据类型为Row</span></span><br><span class="line">DataSet&lt;Row&gt; dsRow = tableEnv.toDataSet(table, Row.class);</span><br><span class="line"><span class="comment">// 将表转为DataSet，通过TypeInformation定义Tuple2&lt;String, Integer&gt;数据类型</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(</span><br><span class="line">  Types.STRING(),</span><br><span class="line">  Types.INT());</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = </span><br><span class="line">  tableEnv.toDataSet(table, tupleType);</span><br></pre></td></tr></table></figure><h3 id="表的Schema与数据类型之间的映射"><a href="#表的Schema与数据类型之间的映射" class="headerlink" title="表的Schema与数据类型之间的映射"></a>表的Schema与数据类型之间的映射</h3><p>表的Schema与数据类型之间的映射有两种方式：分别是基于字段下标位置的映射和基于字段名称的映射。</p><h4 id="基于字段下标位置的映射"><a href="#基于字段下标位置的映射" class="headerlink" title="基于字段下标位置的映射"></a>基于字段下标位置的映射</h4><p>该方式是按照字段的顺序进行一一映射，使用方式如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Integer&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为"f0"和"f1"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">// 将DataStream转为表，选取tuple的第一个元素，指定一个名为"myLong"的字段名</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"myLong"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，为tuple的第一个元素指定名为"myLong"，为第二个元素指定myInt的字段名</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"myLong, myInt"</span>);</span><br></pre></td></tr></table></figure><h4 id="基于字段名称的映射"><a href="#基于字段名称的映射" class="headerlink" title="基于字段名称的映射"></a>基于字段名称的映射</h4><p>基于字段名称的映射方式支持任意的数据类型包括POJO类型，可以很灵活地定义表Schema映射，所有的字段被映射成一个具体的字段名称，同时也可以使用”as”为字段起一个别名。其中Tuple元素的第一个元素为f0,第二个元素为f1，以此类推。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Integer&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为"f0"和"f1"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">// 将DataStream转为表，选择tuple的第二个元素，指定一个名为"f1"的字段名</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"f1"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，交换字段的顺序</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"f1, f0"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，交换字段的顺序，并为f1起别名为"myInt"，为f0起别名为"myLong</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"f1 as myInt, f0 as myLong"</span>);</span><br></pre></td></tr></table></figure><h4 id="原子类型"><a href="#原子类型" class="headerlink" title="原子类型"></a>原子类型</h4><p>Flink将<code>Integer</code>, <code>Double</code>, <code>String</code>或者普通的类型称之为原子类型，一个数据类型为原子类型的DataStream或者DataSet可以被转成单个字段属性的表，这个字段的类型与DataStream或者DataSet的数据类型一致，这个字段的名称可以进行指定。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// 数据类型为原子类型Long</span></span><br><span class="line">DataStream&lt;Long&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为"f0"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">// 将DataStream转为表，指定字段名为myLong"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"myLong"</span>);</span><br></pre></td></tr></table></figure><h4 id="Tuple类型"><a href="#Tuple类型" class="headerlink" title="Tuple类型"></a>Tuple类型</h4><p>Tuple类型的DataStream或者DataSet都可以转为表，可以重新设定表的字段名(即根据tuple元素的位置进行一一映射，转为表之后，每个元素都有一个别名)，如果不为字段指定名称，则使用默认的名称(java语言默认的是f0,f1,scala默认的是_1),用户也可以重新排列字段的顺序，并为每个字段起一个别名。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">//Tuple2&lt;Long, String&gt;类型的DataStream</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为 "f0", "f1"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">// 将DataStream转为表，指定字段名为 "myLong", "myString"(按照Tuple元素的顺序位置)</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"myLong, myString"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，指定字段名为 "f0", "f1"，并且交换顺序</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"f1, f0"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，只选择Tuple的第二个元素，指定字段名为"f1"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"f1"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，为Tuple的第二个元素指定别名为myString，为第一个元素指定字段名为myLong</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"f1 as 'myString', f0 as 'myLong'"</span>);</span><br></pre></td></tr></table></figure><h4 id="POJO类型"><a href="#POJO类型" class="headerlink" title="POJO类型"></a>POJO类型</h4><p>当将POJO类型的DataStream或者DataSet转为表时，如果不指定表名，则默认使用的是POJO字段本身的名称，原始字段名称的映射需要指定原始字段的名称，可以为其起一个别名，也可以调换字段的顺序，也可以只选择部分的字段。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">//数据类型为Person的POJO类型，字段包括"name"和"age"</span></span><br><span class="line">DataStream&lt;Person&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名称为"age", "name"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">//  将DataStream转为表，为"age"字段指定别名myAge, 为"name"字段指定别名myName</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"age as myAge, name as myName"</span>);</span><br><span class="line"><span class="comment">//  将DataStream转为表，只选择一个name字段</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"name"</span>);</span><br><span class="line"><span class="comment">//  将DataStream转为表，只选择一个name字段，并起一个别名myName</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"name as myName"</span>);</span><br></pre></td></tr></table></figure><h4 id="Row类型"><a href="#Row类型" class="headerlink" title="Row类型"></a>Row类型</h4><p>Row类型的DataStream或者DataSet转为表的过程中，可以根据字段的位置或者字段名称进行映射，同时也可以为字段起一个别名，或者只选择部分字段。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// Row类型的DataStream，通过RowTypeInfo指定两个字段"name"和"age"</span></span><br><span class="line">DataStream&lt;Row&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为原始字段名"name"和"age"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据位置映射，为第一个字段指定myName别名，为第二个字段指定myAge别名</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"myName, myAge"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据字段名映射，为name字段起别名myName，为age字段起别名myAge</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"name as myName, age as myAge"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据字段名映射，只选择name字段</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"name"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据字段名映射，只选择name字段，并起一个别名"myName"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"name as myName"</span>);</span><br></pre></td></tr></table></figure><h2 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a>查询优化</h2><h3 id="Old-planner-1"><a href="#Old-planner-1" class="headerlink" title="Old planner"></a>Old planner</h3><p>Apache Flink利用Apache Calcite来优化和转换查询。当前执行的优化包括投影和过滤器下推，去相关子查询以及其他类型的查询重写。Old Planner目前不支持优化JOIN的顺序，而是按照查询中定义的顺序执行它们。</p><p>通过提供一个<code>CalciteConfig</code>对象，可以调整在不同阶段应用的优化规则集。这可通过调用<code>CalciteConfig.createBuilder()</code>方法来进行创建，并通过调用<code>tableEnv.getConfig.setPlannerConfig(calciteConfig)</code>方法将该对象传递给TableEnvironment。</p><h3 id="Blink-planner-1"><a href="#Blink-planner-1" class="headerlink" title="Blink planner"></a>Blink planner</h3><p>Apache Flink利用并扩展了Apache Calcite来执行复杂的查询优化。这包括一系列基于规则和基于成本的优化(cost_based)，例如：</p><ul><li>基于Apache Calcite的去相关子查询</li><li>投影裁剪</li><li>分区裁剪</li><li>过滤器谓词下推</li><li>过滤器下推</li><li>子计划重复数据删除以避免重复计算</li><li>特殊的子查询重写，包括两个部分：<ul><li>将IN和EXISTS转换为左半联接( left semi-join)</li><li>将NOT IN和NOT EXISTS转换为left anti-join</li></ul></li><li>调整join的顺序，需要启用 <code>table.optimizer.join-reorder-enabled</code></li></ul><p><strong>注意：</strong> IN / EXISTS / NOT IN / NOT EXISTS当前仅在子查询重写的结合条件下受支持。</p><p>查询优化器不仅基于计划，而且还可以基于数据源的统计信息以及每个操作的细粒度开销(例如io，cpu，网络和内存）,从而做出更加明智且合理的优化决策。</p><p>高级用户可以通过<code>CalciteConfig</code>对象提供自定义优化规则，通过调用tableEnv.getConfig.setPlannerConfig(calciteConfig)，将参数传递给TableEnvironment。</p><h3 id="查看执行计划"><a href="#查看执行计划" class="headerlink" title="查看执行计划"></a>查看执行计划</h3><p>SQL语言支持通过explain来查看某条SQL的执行计划，Flink Table API也可以通过调用explain()方法来查看具体的执行计划。该方法返回一个字符串用来描述三个部分计划，分别为：</p><ol><li>关系查询的抽象语法树，即未优化的逻辑查询计划，</li><li>优化的逻辑查询计划</li><li>实际执行计划</li></ol><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);</span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream1 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>, <span class="string">"hello"</span>));</span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream2 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>, <span class="string">"hello"</span>));</span><br><span class="line">Table table1 = tEnv.fromDataStream(stream1, <span class="string">"count, word"</span>);</span><br><span class="line">Table table2 = tEnv.fromDataStream(stream2, <span class="string">"count, word"</span>);</span><br><span class="line">Table table = table1</span><br><span class="line">  .where(<span class="string">"LIKE(word, 'F%')"</span>)</span><br><span class="line">  .unionAll(table2);</span><br><span class="line"><span class="comment">// 查看执行计划</span></span><br><span class="line">String explanation = tEnv.explain(table);</span><br><span class="line">System.out.println(explanation);</span><br></pre></td></tr></table></figure><p>执行计划的结果为：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">== 抽象语法树 ==</span><br><span class="line">LogicalUnion(all=[<span class="keyword">true</span>])</span><br><span class="line">  LogicalFilter(condition=[LIKE($<span class="number">1</span>, _UTF-<span class="number">16L</span>E<span class="string">'F%'</span>)])</span><br><span class="line">    FlinkLogicalDataStreamScan(id=[<span class="number">1</span>], fields=[count, word])</span><br><span class="line">  FlinkLogicalDataStreamScan(id=[<span class="number">2</span>], fields=[count, word])</span><br><span class="line"></span><br><span class="line">== 优化的逻辑执行计划 ==</span><br><span class="line">DataStreamUnion(all=[<span class="keyword">true</span>], union all=[count, word])</span><br><span class="line">  DataStreamCalc(select=[count, word], where=[LIKE(word, _UTF-<span class="number">16L</span>E<span class="string">'F%'</span>)])</span><br><span class="line">    DataStreamScan(id=[<span class="number">1</span>], fields=[count, word])</span><br><span class="line">  DataStreamScan(id=[<span class="number">2</span>], fields=[count, word])</span><br><span class="line"></span><br><span class="line">== 物理执行计划 ==</span><br><span class="line">Stage <span class="number">1</span> : Data Source</span><br><span class="line">content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">Stage <span class="number">2</span> : Data Source</span><br><span class="line">content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">Stage <span class="number">3</span> : Operator</span><br><span class="line">content : from: (count, word)</span><br><span class="line">ship_strategy : REBALANCE</span><br><span class="line"></span><br><span class="line">Stage <span class="number">4</span> : Operator</span><br><span class="line">content : where: (LIKE(word, _UTF-<span class="number">16L</span>E<span class="string">'F%'</span>)), select: (count, word)</span><br><span class="line">ship_strategy : FORWARD</span><br><span class="line"></span><br><span class="line">Stage <span class="number">5</span> : Operator</span><br><span class="line">content : from: (count, word)</span><br><span class="line">ship_strategy : REBALANCE</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了Flink TableAPI &amp;SQL，首先介绍了Flink Table API &amp;SQL的基本概念 ，然后介绍了构建Flink Table API &amp; SQL程序所需要的依赖，接着介绍了Flink的两种planner，还介绍了如何注册表以及DataStream、DataSet与表的相互转换，最后介绍了Flink的两种planner对应的查询优化并给出了一个查看执行计划的案例。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一统江湖的数仓开发辅助神器--DBeaver</title>
      <link href="/2020/05/21/%E4%B8%80%E7%BB%9F%E6%B1%9F%E6%B9%96%E7%9A%84%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E7%A5%9E%E5%99%A8-DBeaver/"/>
      <url>/2020/05/21/%E4%B8%80%E7%BB%9F%E6%B1%9F%E6%B9%96%E7%9A%84%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E7%A5%9E%E5%99%A8-DBeaver/</url>
      
        <content type="html"><![CDATA[<p><strong>DBeaver</strong>是一个SQL客户端和数据库管理工具。对于关系数据库，它使用JDBC API通过JDBC驱动程序与数据库交互。对于其他数据库NoSQL，它使用专有数据库驱动程序。<strong>DBeaver</strong>支持非常丰富的数据库，可以说只有你想不到的，没有它做不到的，开箱即用的<strong>DBeaver</strong>支持80多种数据库产品，主要包括：</p><table><thead><tr><th>种类</th><th>名称</th></tr></thead><tbody><tr><td>关系型</td><td><a href="https://mysql.com/" target="_blank" rel="noopener">MySQL</a>、<a href="https://mariadb.org/" target="_blank" rel="noopener">MariaDB</a>、<a href="https://postgresql.org/" target="_blank" rel="noopener">PostgreSQL</a>、<a href="https://www.microsoft.com/en-us/sql-server/sql-server-2017" target="_blank" rel="noopener">Microsoft SQL Server</a>、<a href="https://dbeaver.com/databases/#" target="_blank" rel="noopener">Oracle</a>、<a href="https://www.ibm.com/analytics/us/en/db2/" target="_blank" rel="noopener">DB2</a>、<a href="https://www.ibm.com/analytics/informix" target="_blank" rel="noopener">Informix</a>等等</td></tr><tr><td>分析型</td><td><a href="https://greenplum.org/" target="_blank" rel="noopener">Greenplum</a>、<a href="http://www.teradata.com/products-and-services/teradata-database" target="_blank" rel="noopener">Teradata</a>、<a href="https://prestosql.io/" target="_blank" rel="noopener">PrestoDB</a>、<a href="https://clickhouse.yandex/" target="_blank" rel="noopener">ClickHouse</a>、<a href="https://www.vertica.com/" target="_blank" rel="noopener">Vertica</a>等等</td></tr><tr><td>文档型</td><td><a href="https://dbeaver.com/databases/mongo/" target="_blank" rel="noopener">MongoDB</a>、<a href="https://dbeaver.com/databases/#" target="_blank" rel="noopener">Couchbase</a></td></tr><tr><td>云数据库</td><td>AWS Athena、AWS Redshift、<a href="https://aws.amazon.com/dynamodb/" target="_blank" rel="noopener">Amazon DynamoDB</a>、<a href="https://azure.microsoft.com/services/sql-database/" target="_blank" rel="noopener">SQL Azure</a>、<a href="https://dbeaver.com/databases/#" target="_blank" rel="noopener">Snowflake</a>、<a href="https://cloud.google.com/bigtable/" target="_blank" rel="noopener">Google Bigtable</a>等等</td></tr><tr><td>大数据</td><td><a href="https://hive.apache.org/" target="_blank" rel="noopener">Apache Hive</a>、<a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-sql-thrift-server.html" target="_blank" rel="noopener">Spark Hive</a>、<a href="https://drill.apache.org/" target="_blank" rel="noopener">Apache Drill</a>、<a href="http://phoenix.apache.org/" target="_blank" rel="noopener">Apache Phoenix</a>、<a href="http://impala.apache.org/" target="_blank" rel="noopener">Apache Impala</a>、<a href="https://content.pivotal.io/pivotal-gemfire" target="_blank" rel="noopener">Gemfire XD</a>、<a href="https://snappydatainc.github.io/snappydata/" target="_blank" rel="noopener">SnappyData</a></td></tr><tr><td>键值型</td><td><a href="https://dbeaver.com/databases/cassandra/" target="_blank" rel="noopener">Apache Cassandra</a>、<a href="https://dbeaver.com/databases/redis/" target="_blank" rel="noopener">Redis</a></td></tr><tr><td>时间序列</td><td><a href="https://dbeaver.com/databases/#" target="_blank" rel="noopener">TimescaleDB</a>、<a href="https://dbeaver.com/databases/influxdb/" target="_blank" rel="noopener">InfluxDB</a></td></tr><tr><td>图数据库</td><td><a href="https://neo4j.com/" target="_blank" rel="noopener">Neo4j</a>、<a href="http://orientdb.com/orientdb/" target="_blank" rel="noopener">OrientDB</a></td></tr><tr><td>搜索引擎</td><td><a href="https://dbeaver.com/databases/#" target="_blank" rel="noopener">Elasticsearch</a>、<a href="http://lucene.apache.org/solr/" target="_blank" rel="noopener">Solr</a></td></tr><tr><td>内嵌型</td><td>SQLite、<a href="http://ucanaccess.sourceforge.net/site.html" target="_blank" rel="noopener">Microsoft Access</a>、<a href="https://db.apache.org/derby/" target="_blank" rel="noopener">Apache Derby</a>等等</td></tr></tbody></table><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>DBeaver支持在Window、MacOS和Linux上安装，本文主要演示在Window上安装，其他的操作系统可以参考官网。DBeaver有企业版和社区版两种，其中企业版支持所有的功能(两周的试用时间)，开源版仅支持部分功能，具体差异请参考[<a href="https://dbeaver.com/edition/]。" target="_blank" rel="noopener">https://dbeaver.com/edition/]。</a></p><p>商用版的收费是按时长计费的，具体可以参考官网，列举如下：</p><table><thead><tr><th>时长</th><th>服务</th><th>收费标准</th></tr></thead><tbody><tr><td>一个月</td><td>无</td><td>19美元</td></tr><tr><td>一年</td><td>升级和客户支持</td><td>199美元</td></tr><tr><td>两年</td><td>升级和客户支持</td><td>333美元</td></tr></tbody></table><p>在Window、MacOS上安装DBeaver的方式有两种，官方推荐的安装方式是使用installer安装(也可以使用 ZIP archive)，<a href="https://dbeaver.io/download/" target="_blank" rel="noopener">下载地址</a>。安装非常方便，下载dbeaver-ce-7.0.5-x86_64-setup.exe，直接双击安装即可。下面将会演示如何连接MySQL、Hive、Impala和 Phoenix。</p><h2 id="连接MySQL"><a href="#连接MySQL" class="headerlink" title="连接MySQL"></a>连接MySQL</h2><ul><li>第一步，新建连接，选择MySQL</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/mysql.png" alt></p><ul><li>第二步，下载驱动。点击之后，需要下载MySQL的驱动，可以点击驱动属性进行下载，填好服务器地址、用户名和密码之后测试连接：</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/%E6%B5%8B%E8%AF%95%E8%BF%9E%E6%8E%A5.png" alt></p><p>完成上面的步骤之后，就可以使用了，可以非常方便的查看表的元数据信息、数据以及ER图。连接之后的信息如下：</p><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/%E8%BF%9E%E6%8E%A5%E4%BF%A1%E6%81%AF.png" alt></p><h2 id="连接Hive"><a href="#连接Hive" class="headerlink" title="连接Hive"></a>连接Hive</h2><ul><li>第一步，新建连接，选择Apache Hive</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/Hive.png" alt></p><ul><li>第二步，点击驱动属性、下载对应的驱动，驱动下载完成后，填写连接的url信息。必须开启HiveServer2服务，HiveServer2的默认端口是10000</li></ul><p><strong>尖叫提示</strong>：如果选择自动下载驱动，会出现版本不兼容或者下载失败的情况，所以不建议使用这种方式。最简单的方式是将hive JDBC的jar包直接加载进去即可，本文使用的Hive是CDH5.16的hive1.1.0版本，在<code>/opt/cloudera/parcels/CDH/lib/hive/lib</code>目录下找到<code>hive-jdbc-1.1.0-cdh5.16.1-standalone.jar</code>文件，将其放在本地的一个文件夹下(可以放置在DBeaver的安装目录下)，然后选择编辑驱动设置：如下图</p><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/hive%E9%A9%B1%E5%8A%A8%E9%85%8D%E7%BD%AE.png" alt></p><p>在点击编辑驱动设置之后，会弹出一个窗口，让你选择驱动的位置，点击添加文件，选择相应的hive驱动即可。然后点击确定。</p><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/%E9%80%89%E6%8B%A9hive%E9%A9%B1%E5%8A%A8.png" alt></p><p>然后填写好url，点击测试链接进行测试，如下图：</p><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/hive%E6%B5%8B%E8%AF%95%E8%BF%9E%E6%8E%A5.png" alt></p><p>成功链接之后，就可以像Hue一样操作Hive了，如下:</p><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/hive%E8%BF%9E%E6%8E%A5%E6%88%90%E5%8A%9F.png" alt></p><h2 id="连接Impala"><a href="#连接Impala" class="headerlink" title="连接Impala"></a>连接Impala</h2><ul><li>第一步，点击工具栏的数据库，新建连接</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/impala%E6%96%B0%E5%BB%BA%E8%BF%9E%E6%8E%A51.png" alt></p><ul><li>第二步，选择Hadoop/Bigdata,选择Cloudera Impala，然后点击下一步，如下：</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/impala%E6%96%B0%E5%BB%BA%E8%BF%9E%E6%8E%A52.png" alt></p><ul><li>第三步，填写好url，端口号默认是21050，该端口被使用 JDBC 或 Cloudera ODBC 2.0 及以上驱动的诸如 BI 工具之类的应用用来传递命令和接收结果，关于Impala的各端口的解释说明，可以参考我的另一篇文章：<a href="https://mp.weixin.qq.com/s/7cIlpcXnm_j8LPYUtPgYiw" target="_blank" rel="noopener">Impala使用端口号汇总</a>。如下图：</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/impala%E6%96%B0%E5%BB%BA%E8%BF%9E%E6%8E%A53.png" alt></p><ul><li>第四步，编辑驱动配置，与Hive的配置一样，选择相对应的驱动jar包，并添加。关于jar包的下载，可以在Cloudera官网进行下载[<a href="https://www.cloudera.com/downloads/connectors/impala/jdbc/2-5-41.html],本文使用的是`ImpalaJDBC41.jar`,后台回复:impala驱动，即可获取。" target="_blank" rel="noopener">https://www.cloudera.com/downloads/connectors/impala/jdbc/2-5-41.html],本文使用的是`ImpalaJDBC41.jar`,后台回复:impala驱动，即可获取。</a></li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/impala%E6%96%B0%E5%BB%BA%E8%BF%9E%E6%8E%A54.png" alt></p><ul><li>第五步，测试连接成功，接下来就可以访问Hive中的表了</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/%E6%B5%8B%E8%AF%95%E8%BF%9E%E6%8E%A5%E6%88%90%E5%8A%9F.png" alt></p><h2 id="连接Phoenix"><a href="#连接Phoenix" class="headerlink" title="连接Phoenix"></a>连接Phoenix</h2><ul><li>第一步，选择Apache Phoenix连接</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/phoenix%E8%BF%9E%E6%8E%A5.png" alt></p><ul><li>第二步，填写连接的url，主机名为zookeeper的地址，端口号为zookeeper的端口号2181,填写完成之后，点击编辑驱动设置</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/Phoenix%E8%BF%9E%E6%8E%A5url.png" alt></p><ul><li>第三步，编辑驱动设置，把Phoenix安装目录下的<code>phoenix-4.14.3-HBase-1.3-client.jar</code>文件复制到本地的一个文件下，并且把hbase-site.xml文件添加到该jar包中，然后选择添加文件，选择该jar包。</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/phoenix%E6%B7%BB%E5%8A%A0%E9%A9%B1%E5%8A%A8.png" alt></p><ul><li>第四步，phoenix测试连接</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/phoenix%E6%B5%8B%E8%AF%95%E9%93%BE%E6%8E%A5.png" alt></p><p><strong>尖叫提示:</strong>我使用的版本是DBeaverEE6.0，需要在快捷方式的属性中配置重新配置java路径，否则会报错。具体配置为:在属性后面添加java的目录，<code>-vm C:\mysoftwares\Java\jdk1.8.0_151\bin\javaw</code></p><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/java%E9%85%8D%E7%BD%AE.png" alt></p><p>连接完成之后就可以通过SQL的语法访问HBase了。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了<strong>DBeaver</strong>数据库管理工具，该工具提供了非常丰富的数据库支持，在工作中只需要一个工具就可以花式连接各种各样的数据库。另外本文主要演示了如何连接MySQL、Hive、Impala以及Phoenix，对于其他的数据库而言，用户可以自行测试连接。</p>]]></content>
      
      
      <categories>
          
          <category> 数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DBeaver </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的条件函数与日期函数全面汇总解析</title>
      <link href="/2020/05/20/Hive%E7%9A%84%E6%9D%A1%E4%BB%B6%E5%87%BD%E6%95%B0%E4%B8%8E%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0%E5%85%A8%E9%9D%A2%E6%B1%87%E6%80%BB%E8%A7%A3%E6%9E%90/"/>
      <url>/2020/05/20/Hive%E7%9A%84%E6%9D%A1%E4%BB%B6%E5%87%BD%E6%95%B0%E4%B8%8E%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0%E5%85%A8%E9%9D%A2%E6%B1%87%E6%80%BB%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>在<a href="https://mp.weixin.qq.com/s/K2TA_PhNzGEkucYxBXqhLw" target="_blank" rel="noopener">Hive的开窗函数实战</a>的文章中，主要介绍了Hive的分析函数的基本使用。本文是这篇文章的延续，涵盖了Hive所有的条件函数和日期函数，对于每个函数，本文都给出了具体的解释和使用案例，方便在工作中查阅。</p><a id="more"></a><h2 id="条件函数"><a href="#条件函数" class="headerlink" title="条件函数"></a>条件函数</h2><h3 id="assert-true-BOOLEAN-condition"><a href="#assert-true-BOOLEAN-condition" class="headerlink" title="assert_true(BOOLEAN condition)"></a>assert_true(BOOLEAN condition)</h3><ul><li>解释</li></ul><p>如果condition不为true，则抛出异常，否则返回null</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> assert_true(<span class="number">1</span>&lt;<span class="number">2</span>) <span class="comment">-- 返回null</span></span><br><span class="line"><span class="keyword">select</span> assert_true(<span class="number">1</span>&gt;<span class="number">2</span>) <span class="comment">-- 抛出异常</span></span><br></pre></td></tr></table></figure><h3 id="coalesce-T-v1-T-v2-…"><a href="#coalesce-T-v1-T-v2-…" class="headerlink" title="coalesce(T v1, T v2, …)"></a>coalesce(T v1, T v2, …)</h3><ul><li>解释</li></ul><p>返回第一个不为null的值，如果都为null，则返回null</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">coalesce</span>(<span class="literal">null</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="literal">null</span>)  <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">coalesce</span>(<span class="number">1</span>,<span class="literal">null</span>) <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">coalesce</span>(<span class="literal">null</span>,<span class="literal">null</span>) <span class="comment">-- 返回null</span></span><br></pre></td></tr></table></figure><h3 id="if-BOOLEAN-testCondition-T-valueTrue-T-valueFalseOrNull"><a href="#if-BOOLEAN-testCondition-T-valueTrue-T-valueFalseOrNull" class="headerlink" title="if(BOOLEAN testCondition, T valueTrue, T valueFalseOrNull)"></a>if(BOOLEAN testCondition, T valueTrue, T valueFalseOrNull)</h3><ul><li>解释</li></ul><p>如果testCondition条件为true，则返回第一个值，否则返回第二个值</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">if</span>(<span class="number">1</span> <span class="keyword">is</span> <span class="literal">null</span>,<span class="number">0</span>,<span class="number">1</span>)  <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">if</span>(<span class="literal">null</span> <span class="keyword">is</span> <span class="literal">null</span>,<span class="number">0</span>,<span class="number">1</span>) <span class="comment">-- 返回0</span></span><br></pre></td></tr></table></figure><h3 id="isnotnull-a"><a href="#isnotnull-a" class="headerlink" title="isnotnull(a)"></a>isnotnull(a)</h3><ul><li>解释</li></ul><p>如果参数a不为null，则返回true，否则返回false</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> isnotnull(<span class="number">1</span>) <span class="comment">-- 返回true</span></span><br><span class="line"><span class="keyword">select</span> isnotnull(<span class="literal">null</span>) <span class="comment">-- 返回false</span></span><br></pre></td></tr></table></figure><h3 id="isnull-a"><a href="#isnull-a" class="headerlink" title="isnull(a)"></a>isnull(a)</h3><ul><li>解释</li></ul><p>与isnotnull相反，如果参数a为null，则返回true，否则返回false</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">isnull</span>(<span class="literal">null</span>) <span class="comment">-- 返回true</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">isnull</span>(<span class="number">1</span>) <span class="comment">-- 返回false</span></span><br></pre></td></tr></table></figure><h3 id="nullif-a-b"><a href="#nullif-a-b" class="headerlink" title="nullif(a, b)"></a>nullif(a, b)</h3><ul><li>解释</li></ul><p>如果参数a=b，返回null，否则返回a值(Hive2.2.0版本)</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">nullif</span>(<span class="number">1</span>,<span class="number">2</span>) <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">nullif</span>(<span class="number">1</span>,<span class="number">1</span>) <span class="comment">-- 返回null</span></span><br></pre></td></tr></table></figure><h3 id="nvl-T-value-T-default-value"><a href="#nvl-T-value-T-default-value" class="headerlink" title="nvl(T value, T default_value)"></a>nvl(T value, T default_value)</h3><ul><li>解释</li></ul><p>如果value的值为null，则返回default_value默认值，否则返回value的值。在null值判断时，可以使用if函数给定默认值，也可以使用此函数给定默认值，使用该函数sql特别简洁。</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> nvl(<span class="number">1</span>,<span class="number">0</span>) <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> nvl(<span class="literal">null</span>,<span class="number">0</span>) <span class="comment">-- 返回0</span></span><br></pre></td></tr></table></figure><h2 id="日期函数"><a href="#日期函数" class="headerlink" title="日期函数"></a>日期函数</h2><h3 id="add-months-DATE-STRING-TIMESTAMP-start-date-INT-num-months"><a href="#add-months-DATE-STRING-TIMESTAMP-start-date-INT-num-months" class="headerlink" title="add_months(DATE|STRING|TIMESTAMP start_date, INT num_months)"></a>add_months(DATE|STRING|TIMESTAMP start_date, INT num_months)</h3><ul><li>解释</li></ul><p>start_date参数可以是string, date 或者timestamp类型，num_months参数时int类型。返回一个日期，该日期是在start_date基础之上加上num_months个月，即start_date之后null_months个月的一个日期。如果start_date的时间部分的数据会被忽略。注意：如果start_date所在月份的天数大于结果日期月的天数，则返回结果月的最后一天的日期。</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> add_months(<span class="string">"2020-05-20"</span>,<span class="number">2</span>); <span class="comment">-- 返回2020-07-20</span></span><br><span class="line"><span class="keyword">select</span> add_months(<span class="string">"2020-05-20"</span>,<span class="number">8</span>); <span class="comment">-- 返回2021-01-20</span></span><br><span class="line"><span class="keyword">select</span> add_months(<span class="string">"2020-05-31"</span>,<span class="number">1</span>); <span class="comment">-- 返回2020-06-30,5月有31天，6月只有30天，所以返回下一个月的最后一天</span></span><br></pre></td></tr></table></figure><h3 id="current-date"><a href="#current-date" class="headerlink" title="current_date"></a>current_date</h3><ul><li>解释</li></ul><p>返回查询时刻的当前日期</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">current_date</span>() <span class="comment">-- 返回当前查询日期2020-05-20</span></span><br></pre></td></tr></table></figure><h3 id="current-timestamp"><a href="#current-timestamp" class="headerlink" title="current_timestamp()"></a>current_timestamp()</h3><ul><li>解释</li></ul><p>返回查询时刻的当前时间</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">current_timestamp</span>() <span class="comment">-- 2020-05-20 14:40:47.273</span></span><br></pre></td></tr></table></figure><h3 id="datediff-STRING-enddate-STRING-startdate"><a href="#datediff-STRING-enddate-STRING-startdate" class="headerlink" title="datediff(STRING enddate, STRING startdate)"></a>datediff(STRING enddate, STRING startdate)</h3><ul><li>解释</li></ul><p>返回开始日期startdate与结束日期enddate之前相差的天数</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">datediff</span>(<span class="string">"2020-05-20"</span>,<span class="string">"2020-05-21"</span>); <span class="comment">-- 返回-1</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">datediff</span>(<span class="string">"2020-05-21"</span>,<span class="string">"2020-05-20"</span>); <span class="comment">-- 返回1</span></span><br></pre></td></tr></table></figure><h3 id="date-add-DATE-startdate-INT-days"><a href="#date-add-DATE-startdate-INT-days" class="headerlink" title="date_add(DATE startdate, INT days)"></a>date_add(DATE startdate, INT days)</h3><ul><li>解释</li></ul><p>在startdate基础上加上几天，然后返回加上几天之后的一个日期</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_add</span>(<span class="string">"2020-05-20"</span>,<span class="number">1</span>); <span class="comment">-- 返回2020-05-21,1表示加1天</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">date_add</span>(<span class="string">"2020-05-20"</span>,<span class="number">-1</span>); <span class="comment">-- 返回2020-05-19，-1表示减一天</span></span><br></pre></td></tr></table></figure><h3 id="date-sub-DATE-startdate-INT-days"><a href="#date-sub-DATE-startdate-INT-days" class="headerlink" title="date_sub(DATE startdate, INT days)"></a>date_sub(DATE startdate, INT days)</h3><ul><li>解释</li></ul><p>在startdate基础上减去几天，然后返回减去几天之后的一个日期,功能与date_add很类似</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_sub</span>(<span class="string">"2020-05-20"</span>,<span class="number">1</span>); <span class="comment">-- 返回2020-05-19,1表示减1天</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">date_sub</span>(<span class="string">"2020-05-20"</span>,<span class="number">-1</span>); <span class="comment">-- 返回2020-05-21，-1表示加1天</span></span><br></pre></td></tr></table></figure><h3 id="date-format-DATE-TIMESTAMP-STRING-ts-STRING-fmt"><a href="#date-format-DATE-TIMESTAMP-STRING-ts-STRING-fmt" class="headerlink" title="date_format(DATE|TIMESTAMP|STRING ts, STRING fmt)"></a>date_format(DATE|TIMESTAMP|STRING ts, STRING fmt)</h3><ul><li>解释</li></ul><p>将date/timestamp/string类型的值转换为一个具体格式化的字符串。支持java的SimpleDateFormat格式，第二个参数fmt必须是一个常量</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">'2020-05-20'</span>, <span class="string">'yyyy'</span>); <span class="comment">-- 返回2020</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">'2020-05-20'</span>, <span class="string">'MM'</span>); <span class="comment">-- 返回05</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">'2020-05-20'</span>, <span class="string">'dd'</span>); <span class="comment">-- 返回20</span></span><br><span class="line"><span class="comment">-- 返回2020年05月20日 00时00分00秒</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">'2020-05-20'</span>, <span class="string">'yyyy年MM月dd日 HH时mm分ss秒'</span>) ;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">'2020-05-20'</span>, <span class="string">'yy/MM/dd'</span>) <span class="comment">-- 返回 20/05/20</span></span><br></pre></td></tr></table></figure><h3 id="dayofmonth-STRING-date"><a href="#dayofmonth-STRING-date" class="headerlink" title="dayofmonth(STRING date)"></a>dayofmonth(STRING date)</h3><ul><li>解释</li></ul><p>返回一个日期或时间的天,与day()函数功能相同</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">dayofmonth</span>(<span class="string">'2020-05-20'</span>) <span class="comment">-- 返回20</span></span><br></pre></td></tr></table></figure><h3 id="extract-field-FROM-source"><a href="#extract-field-FROM-source" class="headerlink" title="extract(field FROM source)"></a>extract(field FROM source)</h3><ul><li>解释</li></ul><p>提取 day, dayofweek, hour, minute, month, quarter, second, week 或者year的值，field可以选择day, dayofweek, hour, minute, month, quarter, second, week 或者year，source必须是一个date、timestamp或者可以转为 date 、timestamp的字符串。注意：Hive 2.2.0版本之后支持该函数</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">extract</span>(<span class="keyword">year</span> <span class="keyword">from</span> <span class="string">'2020-05-20 15:21:34.467'</span>); <span class="comment">-- 返回2020，年</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">extract</span>(<span class="keyword">quarter</span> <span class="keyword">from</span> <span class="string">'2020-05-20 15:21:34.467'</span>); <span class="comment">-- 返回2，季度</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">extract</span>(<span class="keyword">month</span> <span class="keyword">from</span> <span class="string">'2020-05-20 15:21:34.467'</span>); <span class="comment">-- 返回05，月份</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">extract</span>(<span class="keyword">week</span> <span class="keyword">from</span> <span class="string">'2020-05-20 15:21:34.467'</span>); <span class="comment">-- 返回21，同weekofyear，一年中的第几周</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">extract</span>(<span class="keyword">dayofweek</span> <span class="keyword">from</span> <span class="string">'2020-05-20 15:21:34.467'</span>); <span class="comment">-- 返回4,代表星期三</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">extract</span>(<span class="keyword">day</span> <span class="keyword">from</span> <span class="string">'2020-05-20 15:21:34.467'</span>); <span class="comment">-- 返回20，天</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">extract</span>(<span class="keyword">hour</span> <span class="keyword">from</span> <span class="string">'2020-05-20 15:21:34.467'</span>); <span class="comment">-- 返回15，小时</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">extract</span>(<span class="keyword">minute</span> <span class="keyword">from</span> <span class="string">'2020-05-20 15:21:34.467'</span>); <span class="comment">-- 返回21，分钟</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">extract</span>(<span class="keyword">second</span> <span class="keyword">from</span> <span class="string">'2020-05-20 15:21:34.467'</span>); <span class="comment">-- 返回34，秒</span></span><br></pre></td></tr></table></figure><h3 id="year-STRING-date"><a href="#year-STRING-date" class="headerlink" title="year(STRING date)"></a>year(STRING date)</h3><ul><li>解释</li></ul><p>返回时间的年份,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">year</span>(<span class="string">'2020-05-20 15:21:34'</span>); <span class="comment">-- 返回2020</span></span><br></pre></td></tr></table></figure><h3 id="quarter-DATE-TIMESTAMP-STRING-a"><a href="#quarter-DATE-TIMESTAMP-STRING-a" class="headerlink" title="quarter(DATE|TIMESTAMP|STRING a)"></a>quarter(DATE|TIMESTAMP|STRING a)</h3><ul><li>解释</li></ul><p>返回给定时间或日期的季度，1至4个季度,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">quarter</span>(<span class="string">'2020-05-20 15:21:34'</span>); <span class="comment">-- 返回2，第2季度</span></span><br></pre></td></tr></table></figure><h3 id="month-STRING-date"><a href="#month-STRING-date" class="headerlink" title="month(STRING date)"></a>month(STRING date)</h3><ul><li>解释</li></ul><p>返回时间的月份,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">month</span>(<span class="string">'2020-05-20 15:21:34'</span>) <span class="comment">-- 返回5</span></span><br></pre></td></tr></table></figure><h3 id="day-STRING-date"><a href="#day-STRING-date" class="headerlink" title="day(STRING date),"></a>day(STRING date),</h3><ul><li>解释</li></ul><p>返回一个日期或者时间的天,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">day</span>(<span class="string">"2020-05-20"</span>); <span class="comment">-- 返回20</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">day</span>(<span class="string">"2020-05-20 15:05:27.5"</span>); <span class="comment">-- 返回20</span></span><br></pre></td></tr></table></figure><h3 id="hour-STRING-date"><a href="#hour-STRING-date" class="headerlink" title="hour(STRING date)"></a>hour(STRING date)</h3><ul><li>解释</li></ul><p>返回一个时间的小时,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">hour</span>(<span class="string">'2020-05-20 15:21:34'</span>);<span class="comment">-- 返回15</span></span><br></pre></td></tr></table></figure><h3 id="minute-STRING-date"><a href="#minute-STRING-date" class="headerlink" title="minute(STRING date)"></a>minute(STRING date)</h3><ul><li>解释</li></ul><p>返回一个时间的分钟值,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">minute</span>(<span class="string">'2020-05-20 15:21:34'</span>); <span class="comment">-- 返回21</span></span><br></pre></td></tr></table></figure><h3 id="second-STRING-date"><a href="#second-STRING-date" class="headerlink" title="second(STRING date)"></a>second(STRING date)</h3><ul><li>解释</li></ul><p>返回一个时间的秒,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">second</span>(<span class="string">'2020-05-20 15:21:34'</span>); <span class="comment">--返回34</span></span><br></pre></td></tr></table></figure><h3 id="from-unixtime-BIGINT-unixtime-STRING-format"><a href="#from-unixtime-BIGINT-unixtime-STRING-format" class="headerlink" title="from_unixtime(BIGINT unixtime [, STRING format])"></a>from_unixtime(BIGINT unixtime [, STRING format])</h3><ul><li>解释</li></ul><p>将将Unix时间戳转换为字符串格式的时间(比如yyyy-MM-dd HH:mm:ss格式)</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1589960708</span>); <span class="comment">-- 返回2020-05-20 15:45:08</span></span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1589960708</span>, <span class="string">'yyyy-MM-dd hh:mm:ss'</span>); <span class="comment">-- -- 返回2020-05-20 15:45:08</span></span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1589960708</span>, <span class="string">'yyyy-MM-dd'</span>); <span class="comment">-- 返回2020-05-20</span></span><br></pre></td></tr></table></figure><h3 id="from-utc-timestamp-T-a-STRING-timezone"><a href="#from-utc-timestamp-T-a-STRING-timezone" class="headerlink" title="from_utc_timestamp(T a, STRING timezone)"></a>from_utc_timestamp(T a, STRING timezone)</h3><ul><li>解释</li></ul><p>转换为特定时区的时间</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">'2020-05-20 15:21:34'</span>,<span class="string">'PST'</span>); <span class="comment">-- 返回2020-05-20 08:21:34.0</span></span><br><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">'2020-05-20 15:21:34'</span>,<span class="string">'GMT'</span>); <span class="comment">-- 返回2020-05-20 15:21:34.0</span></span><br><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">'2020-05-20 15:21:34'</span>,<span class="string">'UTC'</span>); <span class="comment">-- 返回2020-05-20 15:21:34.0</span></span><br><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">'2020-05-20 15:21:34'</span>,<span class="string">'DST'</span>); <span class="comment">-- 返回2020-05-20 15:21:34.0</span></span><br><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">'2020-05-20 15:21:34'</span>,<span class="string">'CST'</span>); <span class="comment">-- 返回2020-05-20 10:21:34.0</span></span><br></pre></td></tr></table></figure><h3 id="last-day-STRING-date"><a href="#last-day-STRING-date" class="headerlink" title="last_day(STRING date)"></a>last_day(STRING date)</h3><ul><li>解释</li></ul><p>返回给定时间或日期所在月的最后一天，参数可以是’yyyy-MM-dd HH:mm:ss’ 或者 ‘yyyy-MM-dd’类型，时间部分会被忽略</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">last_day</span>(<span class="string">'2020-05-20 15:21:34'</span>); <span class="comment">-- 返回2020-05-31</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">last_day</span>(<span class="string">'2020-05-20'</span>); <span class="comment">-- 返回2020-05-31</span></span><br></pre></td></tr></table></figure><h3 id="to-date-STRING-timestamp"><a href="#to-date-STRING-timestamp" class="headerlink" title="to_date(STRING timestamp)"></a>to_date(STRING timestamp)</h3><ul><li>解释</li></ul><p>返回一个字符串时间的日期部分，去掉时间部分，2.1.0之前版本返回的是string，2.1.0版本及之后返回的是date</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">to_date</span>(<span class="string">'2020-05-20 15:21:34'</span>); <span class="comment">-- 返回2020-05-20</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">to_date</span>(<span class="string">'2020-05-20'</span>); <span class="comment">-- 返回2020-05-20</span></span><br></pre></td></tr></table></figure><h3 id="to-utc-timestamp-T-a-STRING-timezone"><a href="#to-utc-timestamp-T-a-STRING-timezone" class="headerlink" title="to_utc_timestamp(T a, STRING timezone)"></a>to_utc_timestamp(T a, STRING timezone)</h3><ul><li>解释</li></ul><p>转换为世界标准时间UTC的时间戳,与from_utc_timestamp类似</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> to_utc_timestamp(<span class="string">'2020-05-20 15:21:34'</span>, <span class="string">'GMT'</span>); <span class="comment">-- 返回2020-05-20 15:21:34.0</span></span><br></pre></td></tr></table></figure><h3 id="trunc-STRING-date-STRING-format"><a href="#trunc-STRING-date-STRING-format" class="headerlink" title="trunc(STRING date, STRING format)"></a>trunc(STRING date, STRING format)</h3><ul><li>解释</li></ul><p>截断日期到指定的日期精度，仅支持月（MONTH/MON/MM）或者年（YEAR/YYYY/YY）</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> trunc(<span class="string">'2020-05-20'</span>, <span class="string">'YY'</span>);   <span class="comment">-- 返回2020-01-01，返回年的1月1日</span></span><br><span class="line"><span class="keyword">select</span> trunc(<span class="string">'2020-05-20'</span>, <span class="string">'MM'</span>);   <span class="comment">-- 返回2020-05-01，返回月的第一天</span></span><br><span class="line"><span class="keyword">select</span> trunc(<span class="string">'2020-05-20 15:21:34'</span>, <span class="string">'MM'</span>);   <span class="comment">-- 返回2020-05-01</span></span><br></pre></td></tr></table></figure><h3 id="unix-timestamp-STRING-date-STRING-pattern"><a href="#unix-timestamp-STRING-date-STRING-pattern" class="headerlink" title="unix_timestamp([STRING date [, STRING pattern]])"></a>unix_timestamp([STRING date [, STRING pattern]])</h3><ul><li>解释</li></ul><p>参数时可选的，当参数为空时，返回当前Unix是时间戳，精确到秒。可以指定一个具体的日期，转换为Unix时间戳格式</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 返回1589959294</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">unix_timestamp</span>(<span class="string">'2020-05-20 15:21:34'</span>,<span class="string">'yyyy-MM-dd hh:mm:ss'</span>);</span><br><span class="line"><span class="comment">-- 返回1589904000</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">unix_timestamp</span>(<span class="string">'2020-05-20'</span>,<span class="string">'yyyy-MM-dd'</span>);</span><br></pre></td></tr></table></figure><h3 id="weekofyear-STRING-date"><a href="#weekofyear-STRING-date" class="headerlink" title="weekofyear(STRING date)"></a>weekofyear(STRING date)</h3><ul><li>解释</li></ul><p>返回一个日期或时间在一年中的第几周，可以用extract替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">weekofyear</span>(<span class="string">'2020-05-20 15:21:34'</span>); <span class="comment">-- 返回21，第21周</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">weekofyear</span>(<span class="string">'2020-05-20'</span>); <span class="comment">-- 返回21，第21周</span></span><br></pre></td></tr></table></figure><h3 id="next-day-STRING-start-date-STRING-day-of-week"><a href="#next-day-STRING-start-date-STRING-day-of-week" class="headerlink" title="next_day(STRING start_date, STRING day_of_week)"></a>next_day(STRING start_date, STRING day_of_week)</h3><ul><li>解释</li></ul><p>参数start_date可以是一个时间或日期，day_of_week表示星期几，比如Mo表示星期一，Tu表示星期二，Wed表示星期三，Thur表示星期四，Fri表示星期五，Sat表示星期六，Sun表示星期日。如果指定的星期几在该日期所在的周且在该日期之后，则返回当周的星期几日期，如果指定的星期几不在该日期所在的周，则返回下一个星期几对应的日期</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> next_day(<span class="string">'2020-05-20'</span>,<span class="string">'Mon'</span>);<span class="comment">-- 返回当前日期的下一个周一日期:2020-05-25</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">'2020-05-20'</span>,<span class="string">'Tu'</span>);<span class="comment">-- 返回当前日期的下一个周二日期:2020-05-26</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">'2020-05-20'</span>,<span class="string">'Wed'</span>);<span class="comment">-- 返回当前日期的下一个周三日期:2020-05-27</span></span><br><span class="line"><span class="comment">-- 2020-05-20为周三，指定的参数为周四，所以返回当周的周四就是2020-05-21</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">'2020-05-20'</span>,<span class="string">'Th'</span>);</span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">'2020-05-20'</span>,<span class="string">'Fri'</span>);<span class="comment">-- 返回周五日期2020-05-22</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">'2020-05-20'</span>,<span class="string">'Sat'</span>); <span class="comment">-- 返回周六日期2020-05-23</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">'2020-05-20'</span>,<span class="string">'Sun'</span>); <span class="comment">-- 返回周六日期2020-05-24</span></span><br></pre></td></tr></table></figure><p>该函数比较重要：比如取当前日期所在的周一和周日，通过长用在按周进行汇总数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_add</span>(next_day(<span class="string">'2020-05-20'</span>,<span class="string">'MO'</span>),<span class="number">-7</span>); <span class="comment">-- 返回当前日期的周一日期2020-05-18</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">date_add</span>(next_day(<span class="string">'2020-05-20'</span>,<span class="string">'MO'</span>),<span class="number">-1</span>); <span class="comment">-- 返回当前日期的周日日期2020-05-24</span></span><br></pre></td></tr></table></figure><h3 id="months-between-DATE-TIMESTAMP-STRING-date1-DATE-TIMESTAMP-STRING-date2"><a href="#months-between-DATE-TIMESTAMP-STRING-date1-DATE-TIMESTAMP-STRING-date2" class="headerlink" title="months_between(DATE|TIMESTAMP|STRING date1, DATE|TIMESTAMP|STRING date2)"></a>months_between(DATE|TIMESTAMP|STRING date1, DATE|TIMESTAMP|STRING date2)</h3><ul><li>解释</li></ul><p>返回 date1 和 date2 的月份差。如果date1大于date2，返回正值，否则返回负值，如果是相减是整数月，则返回一个整数，否则会返回小数</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> months_between(<span class="string">'2020-05-20'</span>,<span class="string">'2020-05-20'</span>); <span class="comment">-- 返回0</span></span><br><span class="line"><span class="keyword">select</span> months_between(<span class="string">'2020-05-20'</span>,<span class="string">'2020-06-20'</span>); <span class="comment">-- 返回-1</span></span><br><span class="line"><span class="comment">-- 相差的整数月</span></span><br><span class="line"><span class="keyword">select</span> months_between(<span class="string">'2020-06-30'</span>,<span class="string">'2020-05-31'</span>); <span class="comment">-- 返回1</span></span><br><span class="line"><span class="comment">-- 非整数月，一个月差一天</span></span><br><span class="line"><span class="keyword">select</span> months_between(<span class="string">'2020-06-29'</span>,<span class="string">'2020-05-31'</span>); <span class="comment">-- 返回0.93548387</span></span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了Hive的条件函数和日期函数，并给出了每个函数的解释说明和使用案例，本文覆盖了所有Hive内置的条件函数和日期函数，可以作为一个函数字典，方便工作中使用。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Greenplum集群Master与Segment节点故障检测与恢复</title>
      <link href="/2020/05/18/Greenplum%E9%9B%86%E7%BE%A4Master%E4%B8%8ESegment%E8%8A%82%E7%82%B9%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B%E4%B8%8E%E6%81%A2%E5%A4%8D/"/>
      <url>/2020/05/18/Greenplum%E9%9B%86%E7%BE%A4Master%E4%B8%8ESegment%E8%8A%82%E7%82%B9%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B%E4%B8%8E%E6%81%A2%E5%A4%8D/</url>
      
        <content type="html"><![CDATA[<p>Greenplum集群主要包括Master节点和Segment节点，Master节点称之为主节点，Segment节点称之为数据节点。Master节点与Segment节点都是有备份的，其中Master节点的备节点为Standby Master(不能够自动故障转移)，Segment是通过Primary Segment与Mirror Segment进行容错的。通过本文你可以了解：</p><ul><li>Greenplum数据库的高可用(HA)原理</li><li>Greenplum生产集群中master节点故障恢复</li><li>greenplum生产集群中segment故障检测与恢复</li><li>Segment节点故障恢复原理与实践</li></ul><h2 id="Greenplum数据库的HA"><a href="#Greenplum数据库的HA" class="headerlink" title="Greenplum数据库的HA"></a>Greenplum数据库的HA</h2><h3 id="master-mirroring概述"><a href="#master-mirroring概述" class="headerlink" title="master mirroring概述"></a>master mirroring概述</h3><p>可以在单独的主机或同一主机上部署master实例的备份或镜像。如果primary master服务器宕机，则standby master服务器将用作热备用服务器。在primary master服务器在线时，可以从primary master服务器创建备用master服务器。</p><p>Primary master服务器持续为用户提供服务，同时获取Primary master实例的事务快照。在standby master服务器上部署事务快照时，还会记录对primary master服务器的更改。在standby master服务器上部署快照后，更新也会被部署，用于使standby master服务器与primary master服务器同步。</p><p>Primary master服务器和备用master服务器同步后，standbymaster服务器将通过walsender 和 walreceiver 的复制进程保持最新状态。该walreceiver是standby master上的进程， walsender流程是primary master上的流程。这两个进程使用基于预读日志（WAL）的流复制来保持primary master和standby master服务器同步。在WAL日志记录中，所有修改都会在应用生效之前写入日志，以确保任何进程内操作的数据完整性。</p><p>由于primary master不包含任何用户数据，因此只需要在主master和备份master之间同步系统目录表(catalog tables)。当这些表发生更新时，更改的结果会自动复制到备用master上，以确保与主master同步。</p><p>如果primary master发生故障，管理员需要使用gpactivatestandby工具激活standby master。可以为primary master和standby master配置一个虚拟IP地址，这样，在primary master出现故障时，客户端程序就不用切换到其他的网络地址，因为在master出现故障时，虚拟IP地址可以交换到充当primary master的主机上。</p><h3 id="mirror-segment概述"><a href="#mirror-segment概述" class="headerlink" title="mirror segment概述"></a>mirror segment概述</h3><p>当启用Greenplum数据库高可用性时，有两种类型的segment：primary和mirror。每个主segment具有一个对应的mirror segment。主segment接收来自master的请求以更改主segment的数据库，然后将这些更改复制到相应的mirror segment上。如果主segment不可用，则数据库查询将故障转移到mirror segment上。</p><p>Mirror segment采用物理文件复制的方案—primary segment中数据文件I / O被复制到mirror segment上，因此mirror segment的文件与primary segment上的文件相同。Greenplum数据库中的数据用元组(tuple)表示，元组被打包成块（block）。数据库的表存储在由一个或多个块组成的磁盘文件中。对元组进行更改操作，同时会更改保存的块，然后将其写入primary segment上的磁盘并通过网络复制到mirror segment。Mirror segment只更新其文件副本中的相应块。</p><p>对于堆表(heap)而言，块被保存在内存缓存区中，直到为新更改的块腾出空间时，才会将它们清除，这允许系统可以多次读取或更新内存中的块，而无需执行昂贵的磁盘I / O。 当块从缓存中清除时，它将被写入磁盘并复制到mirror segment主机的磁盘。当块保存在缓存中时，primary segment和mirror segment具有不同的块镜像，但是，数据库仍然是一致的，因为事务日志已经被复制了。</p><p>AO表(Append-optimized)不使用内存缓存机制。对AO表的块所做的更改会立即复制到mirror segment上。通常，文件写入操作是异步的，但是打开、创建和同步文件是“同步复制”的，这意味着primary segment的块需要从mirror segment上接收确认。</p><p>如果primary segment失败，则文件复制进程将会停止，mirror segment会自动作为活动segment实例，活动mirror segment的系统状态会变为“ 更改跟踪”(<em>Change Tracking</em>)，这意味着在primary segment不可用时，mirror segment维护着一个系统表和记录所有块的更改日志。当故障的primary segment被修复好，并准备重新上线时，管理员启动恢复过程，系统进入重新同步状态。恢复过程将记录的更改日志应用于已修复的primary segment上。恢复过程完成后，mirror segment的系统状态将更改为“已同步 ”。</p><p>如果mirror segment在primary segment处于活动状态时失败或无法访问，则primary segment的系统状态将更改为“ 更改跟踪”，并且它会记录更改的状态，用于mirror segment的恢复。</p><ul><li><strong>group mirroring方式</strong></li></ul><p>只要primary segment实例和mirror segment实例位于不同的主机上，mirror segment就可以以不同的配置方式放置在群集中的主机上。每个主机必须具有相同数量的mirror segment和primary segment。默认mirror segment配置方式是group mirroring，其中每个主机的primary segment的mirror segment位于另一个主机上。如果单个主机发生故障，则部署该主机的mirror segment主机上的活动primary segment数量会翻倍，从而会加大该主机的负载。下图说明了group mirroring配置。</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/grouping.png" alt></p><ul><li><strong>Spread mirroring方式</strong></li></ul><p><em>Spread mirroring</em>方式是指将每个主机的mirror segment分布在多个主机上，这样，如果任何单个主机发生故障，该主机的mirror segment会分散到其他多个主机上运行，从而达到负载均衡的效果。仅当主机数量多于每个主机的segment数时，才可以使用<em>Spread</em>方式。下图说明了<em>Spread mirroring</em>方式。</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/spread.png" alt></p><h2 id="Master节点故障恢复"><a href="#Master节点故障恢复" class="headerlink" title="Master节点故障恢复"></a>Master节点故障恢复</h2><p>如果primary master节点失败，日志复制进程就会停止。可以使用<code>gpstate -f</code>命令查看standby master的状态，使用<code>gpactivatestandby</code>命令激活standby master。</p><h3 id="激活Standby-master"><a href="#激活Standby-master" class="headerlink" title="激活Standby master"></a>激活Standby master</h3><ul><li><p>(1)确保原来的集群中配置了standby master</p></li><li><p>(2)在standby master主机上运行gpactivatestandby命令</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpactivatestandby -d /data/master/gpseg-1</span><br></pre></td></tr></table></figure><p><code>-d</code>参数是指standby master的数据目录，一旦激活成功，原来的standby master就成为了primary master。</p></li><li><p>(3)执行激活命令后，运行gpstate命令检查状态</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -f</span><br></pre></td></tr></table></figure></li></ul><p>  新激活的master的状态是active，如果已经为集群配置一个新的standby master节点，则其状态会是passive。如果还没有为集群配置一个新的standby master，则会看到下面的信息：No entries found，该信息表明尚未配置standby master。</p><ul><li><p>(4)在成功切换到了standbymaster之后，运行ANALYZE命令，收集该数据库的统计信息</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ psql postgres -c <span class="string">'ANALYZE;'</span></span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(5)可选：如果在成功激活standby master之后，尚未指定新的standby master，可以在active master上运行gpinitstandby命令，配置一个新的standby master</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpinitstandby -s new_standby_master_hostname</span><br></pre></td></tr></table></figure></li></ul><h3 id="恢复到原来的设置-可选的"><a href="#恢复到原来的设置-可选的" class="headerlink" title="恢复到原来的设置(可选的)"></a>恢复到原来的设置(可选的)</h3><ul><li><p>(1)确保之前的master节点能够正常使用</p></li><li><p>(2)在原来的master主机上，移除(备份)原来的数据目录gpseg-1,比如：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mv /data/master/gpseg-1  /data/master/backup_gpseg-1</span><br></pre></td></tr></table></figure></li><li><p>(3)在原来的master节点上，初始化standby master，在active master上运行如下命令</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpinitstandby -s mdw</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(4)初始化完成之后，检查standby master的状态</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -f</span><br></pre></td></tr></table></figure></li></ul><p>   显示的状态应该是–Sync state: sync</p><ul><li><p>(5)在active master节点上运行下面的命令，用于停止master</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstop -m</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(6)在原来的master节点(mdw)上运行gpactivatestandby命令</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpactivatestandby -d /data/master/gpseg-1</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(7)在上述命名运行结束之后，再运行gpstate命令查看状态</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -f</span><br></pre></td></tr></table></figure></li></ul><p>  确认原始的primary master状态是active。</p><ul><li><p>(8)在原来的standby master节点(smdw)上，移除(备份)数据目录gpseg-1</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mv /data/master/gpseg-1  /data/master/backup_gpseg-1</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(9)原来的master节点正常运行之后，在该节点上执行如下命令，用于激活standby master</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpinitstandby -s smdw</span><br></pre></td></tr></table></figure></li></ul><h3 id="检查standby-master的状态"><a href="#检查standby-master的状态" class="headerlink" title="检查standby master的状态"></a>检查standby master的状态</h3><p>可以通过查看视图pg_stat_replication，来获取更多的信息。该视图可以列出walsender进程的信息，下面的命令是查看walsender进程的进程id和状态信息。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ psql postgres -c <span class="string">'SELECT procpid, state FROM pg_stat_replication;'</span></span><br></pre></td></tr></table></figure><h2 id="segment节点故障检测与恢复"><a href="#segment节点故障检测与恢复" class="headerlink" title="segment节点故障检测与恢复"></a>segment节点故障检测与恢复</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>Greenplum数据库服务器（Postgres）有一个子进程，该子进程为ftsprobe，主要作用是处理故障检测。 ftsprobe 监视Greenplum数据库阵列，它以可以配置的间隔连接并扫描所有segment和数据库进程。<br>如果 ftsprobe无法连接到segment，它会在Greenplum数据库系统目录中将segment标记为”down”。在管理员启动恢复进程之前，该segment是不可以被操作的。</p><p>启用mirror备份后，如果primary segment不可用，Greenplum数据库会自动故障转移到mirror segment。如果segment实例或主机发生故障，系统仍可以运行，前提是所有在剩余的活动segment上数据都可用。</p><p>要恢复失败的segment，管理员需要执行 gprecoverseg 恢复工具。此工具可以找到失败的segment，验证它们是否有效，并将事务状态与当前活动的segment进行比较，以确定在segment脱机时所做的更改。gprecoverseg将更改的数据库文件与活动segment同步，并使该segment重新上线。管理员需要在在Greenplum数据库启动并运行时执行恢复操作。</p><p>禁用mirror备份时，如果segment实例失败，系统将会自动关闭。管理员需要手动恢复所有失败的segment。</p><h3 id="检测和管理失败的segment"><a href="#检测和管理失败的segment" class="headerlink" title="检测和管理失败的segment"></a>检测和管理失败的segment</h3><h4 id="使用工具命令查看"><a href="#使用工具命令查看" class="headerlink" title="使用工具命令查看"></a>使用工具命令查看</h4><p>启用mirror备份后，当primary segment发生故障时，Greenplum会自动故障转移到mirror segment。如果每个数据部分所在的segment实例都是在线的，则用户可能无法意识到segment已经出现故障。如果在发生故障时正在进行事务，则正在进行的事务将回滚并在重新配置的segment集上自动重新启动。</p><p>如果整个Greenplum数据库系统由于segment故障而变得不可访问（例如，如果未启用mirror备份或没有足够的segment在线），则用户在尝试连接数据库时将看到错误。返回到客户端程序的错误可能表示失败。例如：<br><code>ERROR: All segment databases are unavailable</code></p><ul><li><p>(1)在master节点上，运行gpstate命令，使用-e参数显示错误的segment</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -e</span><br></pre></td></tr></table></figure><p>   标记为<code>Change Tracking</code>的segment节点表明对应的mirror segment已经宕机。</p></li><li><p>(2)要获取有关故障segment的详细信息，可以查看 gp_segment_configuration目录表</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ psql -c <span class="string">"SELECT * FROM gp_segment_configuration WHERE status='d';"</span></span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(3) 对于失败的segment实例，记下主机，端口，初始化时的角色和数据目录。此信息将帮助确定要进行故障排除的主机和segment实例。</p></li><li><p>(4) 显示mirror segment详细信息，运行下面命名：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -m</span><br></pre></td></tr></table></figure></li></ul><h4 id="检查日志文件"><a href="#检查日志文件" class="headerlink" title="检查日志文件"></a>检查日志文件</h4><p>  日志文件可以提供信息以帮助确定错误的原因。Master实例和segment实例都有自己的日志文件，这些日志文件位于pg_log的目录下。Master的日志文件包含最多信息，应该首先检查它。</p><p>  使用 gplogfilter工具检查Greenplum数据库日志文件，可以获取额外信息。要检查segment日志文件，可以在master主机上使用gpssh命令运行 gplogfilter。</p><ul><li><p>(1)使用 gplogfilter 检查master日志文件的WARNING, ERROR, FATAL 或者 PANIC日志级别消息</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gplogfilter -t</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(2)使用 gpssh 检查每个segment实例上的日志级别为WARNING, ERROR, FATAL 或者 PANIC的消息。例如：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpssh -f seg_hosts_file -e <span class="string">'source</span></span><br><span class="line"><span class="string">/usr/local/greenplum-db/greenplum_path.sh ; gplogfilter -t</span></span><br><span class="line"><span class="string">/data1/primary/*/pg_log/gpdb*.log'</span> &gt; seglog.out</span><br></pre></td></tr></table></figure></li></ul><h3 id="恢复失败的segment"><a href="#恢复失败的segment" class="headerlink" title="恢复失败的segment"></a>恢复失败的segment</h3><p>  如果master服务器无法连接到segment实例，则会在Greenplum数据库系统目录中将该segment标记为“down”状态。在管理员采取措施使segment实例重新上线之前，segment实例将保持脱机离线状态。segment实例可能由于多种原因而不可用：</p><ul><li><p>(1)segment主机不可用; 例如，由于网络或硬件故障。</p></li><li><p>(2)segment实例未运行; 例如，没Postgres的数据库监听进程。</p></li><li><p>(3)segment实例的数据目录损坏或丢失; 例如，无法访问数据，文件系统已损坏或磁盘发生故障。</p><h4 id="在启用mirror-segment的情况下进行恢复"><a href="#在启用mirror-segment的情况下进行恢复" class="headerlink" title="在启用mirror segment的情况下进行恢复"></a>在启用mirror segment的情况下进行恢复</h4></li><li><p>(1)确保master主机能够ping通失败的segment主机</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ping failed_seg_host_address</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(2)如果是阻止master主机连接segment主机，则可以重启该segment主机。</p></li><li><p>(3)如果该segment主机上线之后，可以通过master连接，则在master主机上运行下面命令，重新激活失败的segment</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gprecoverseg</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(4)恢复进程会显示故障segment并标识需要同步的已更改文件。这个过程可能需要一些时间， 等待该过程完成。在此过程中，数据库不允许写入操作。</p></li><li><p>(5)在 gprecoverseg完成后，系统进入重新同步模式并开始复制已更改的文件。当系统处于联机状态并接受数据库请求时，此进程在后台运行。</p></li><li><p>(6)重新同步过程完成后，系统状态为“已同步”（ Synchronized）。运行gpstate 命令用于验证重新同步过程状态</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -m</span><br></pre></td></tr></table></figure></li></ul><h4 id="将所有的segment恢复到原来的角色设置"><a href="#将所有的segment恢复到原来的角色设置" class="headerlink" title="将所有的segment恢复到原来的角色设置"></a>将所有的segment恢复到原来的角色设置</h4><p>  当primary segment发生故障时，mirror segment会被激活为primary segment。运行gprecoverseg命令之后，当前活动的segment是primary segment，失败的primary segment成为了mirror segment。segment实例不会返回到在系统初始化时配置的首选角色。这意味着某些segment主机上可能运行多个primary segment实例，而某些segment主机上运行较少的segment，即系统可能处于潜在的不平衡状态。要检查不平衡的segment并重新平衡系统，可以使用如下命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -e</span><br></pre></td></tr></table></figure><p>  所有segment必须在线并完全同步以重新平衡系统，数据库会话在重新平衡期间保持连接，但正在进行的查询将被取消并回滚。</p><ul><li><p>(1)运行下面命令，查看mirror segment的角色和同步状态</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -m</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(2)如果有mirror segment处于非同步状态，等待他们同步完成</p></li><li><p>(3)运行gprecoverseg命令，使用-r参数将segment恢复到原来初始化时的角色设置</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gprecoverseg -r</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(4)运行gpstate -e命令，确认所有的segment是否恢复到初始化时的角色设置</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -e</span><br></pre></td></tr></table></figure></li></ul><h4 id="从双重故障中恢复"><a href="#从双重故障中恢复" class="headerlink" title="从双重故障中恢复"></a>从双重故障中恢复</h4><p>  在双重故障情况下，即primary segment和mirror segment都处于失败状态。如果不同segment的主机同时发生硬件故障，则会导致primary segment和mirror segment都处于失败状态，如果发生双重故障，Greenplum数据库将不可用。要从双重故障中恢复，执行如下步骤：</p><ul><li><p>(1)重启greenplum数据库</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstop -r</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(2)再重启系统之后，运行gprecoverseg命令</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gprecoverseg</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(3)在gprecoverseg执行结束后，运行gpstate命令查看mirror状态信息</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="variable">$gpstate</span> -m</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(4)如果segment仍是“Change Tracking”状态，则运行下面命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gprecoverseg -F</span><br></pre></td></tr></table></figure></li></ul><h4 id="从segment主机故障中恢复"><a href="#从segment主机故障中恢复" class="headerlink" title="从segment主机故障中恢复"></a>从segment主机故障中恢复</h4><p>  如果主机处于不可操作状态（例如，由于硬件故障），可以将segment恢复到备用主机上。如果启用了mirror segment，则可以使用gprecoverseg命令将mirror segment恢复到备用主机。例如：</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gprecoverseg -i recover_config_file</span><br></pre></td></tr></table></figure><p>生成的recover_config_file文件的格式为：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">filespaceOrder=[filespace1_name[:filespace2_name:...]failed_host_address:</span><br><span class="line">port:fselocation [recovery_host_address:port:replication_port:fselocation</span><br><span class="line">[:fselocation:...]]</span><br></pre></td></tr></table></figure><p>​     例如，要在没有配置其他文件空间的情况下恢复到与故障主机不同的另一台主机（除了默认的pg_system文件空间）：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">filespaceOrder=sdw5-2:50002:/gpdata/gpseg2 sdw9-2:50002:53002:/gpdata/gpseg2</span><br></pre></td></tr></table></figure><p> 该gp_segment_configuration和pg_filespace_entry系统目录表可以帮助确定当前的段配置，这样可以计划mirror的恢复配置。例如，运行以下查询：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">=<span class="comment"># SELECT dbid, content, hostname, address, port,</span></span><br><span class="line">replication_port, fselocation as datadir</span><br><span class="line">FROM gp_segment_configuration, pg_filespace_entry</span><br><span class="line">WHERE dbid=fsedbid</span><br><span class="line">ORDER BY dbid;</span><br></pre></td></tr></table></figure><p>上述命令会输出:</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/%E7%BB%93%E6%9E%9C%E8%BE%93%E5%87%BA.png" alt></p><p>新恢复的segment主机必须预先安装Greenplum数据库软件，并且其配置要与现有的segment主机一致。</p><h2 id="Segment故障恢复原理与实践"><a href="#Segment故障恢复原理与实践" class="headerlink" title="Segment故障恢复原理与实践"></a>Segment故障恢复原理与实践</h2><h3 id="greenplum集群环境介绍"><a href="#greenplum集群环境介绍" class="headerlink" title="greenplum集群环境介绍"></a>greenplum集群环境介绍</h3><p>该生产环境集群由四台服务器构成，其中一台为primary master节点，一台为standby master节点，两外两台为segment节点，每个segment节点有四个segment(两个primary segment，两个mirror segment)，segment采用group方式进行备份(sdw1的备份都在sdw2上，sdw2的备份都在sdw1上)，其角色分配如下图所示：</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83.png" alt></p><h3 id="segment故障检查"><a href="#segment故障检查" class="headerlink" title="segment故障检查"></a>segment故障检查</h3><ul><li>gpstate -m日志信息</li></ul><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstatem.png" alt></p><ul><li>gpstate -c 日志信息</li></ul><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstatec.png" alt></p><ul><li>gpstate -e 日志信息</li></ul><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstatee.png" alt></p><ul><li><p>gpstate -s 日志信息</p><p>(1)sdw1节点的日志信息</p></li></ul><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstates1.png" alt></p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstates2.png" alt></p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstates3.png" alt></p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstates4.png" alt></p><p>  (1)sdw2节点的日志信息</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstates5.png" alt></p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstates6.png" alt></p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstates7.png" alt></p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstates8.png" alt></p><h3 id="故障说明"><a href="#故障说明" class="headerlink" title="故障说明"></a>故障说明</h3><p>Sdw1节点primary segment正常，mirror segment被激活，其mirror segment为sdw2节点上的primary segment备份。Sdw2节点primary segment失败，mirror segment失败。此时集群环境能够正常提供服务，全部负载到sdw1节点上。</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/%E6%95%85%E9%9A%9C%E8%AF%B4%E6%98%8E1.png" alt></p><p>使用<code>select * from gp_segment_configuration</code>查看segment角色信息，如下图所示：</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/%E6%95%85%E9%9A%9C%E8%AF%B4%E6%98%8E2.png" alt></p><h3 id="segment故障恢复"><a href="#segment故障恢复" class="headerlink" title="segment故障恢复"></a>segment故障恢复</h3><ul><li>在master主机上运行下面命令，重新激活失败的segment</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gprecoverseg</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/%E6%81%A2%E5%A4%8D1.png" alt></p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/%E6%81%A2%E5%A4%8D2.png" alt></p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/%E6%81%A2%E5%A4%8D3.png" alt></p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/%E6%81%A2%E5%A4%8D4.png" alt></p><ul><li>运行gpstate 命令用于验证重新同步过程状态</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -m</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstatem%E6%81%A2%E5%A4%8D.png" alt></p><p>当primary segment发生故障时，mirror segment会被激活为primary segment。运行gprecoverseg命令之后，失败的primary segment成为了mirror segment，而被激活的mirror segment成为了primary segment，segment实例不会返回到在系统初始化时配置的首选角色。这意味着某些segment主机上可能运行多个primary segment实例，而某些segment主机上运行较少的segment，即系统可能处于潜在的不平衡状态。如下图所示，sdw1上的mirror segment变为了primary segment，sdw2上的primary segment变为了mirror segment。即sdw2的primary segment运行在sdw1节点上，系统处于不平衡状态。</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/%E4%B8%8D%E5%B9%B3%E8%A1%A1.png" alt></p><p>此时GPCC的状态为：</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpcc1.png" alt></p><ul><li><p>运行gprecoverseg命令，使用-r参数将segment恢复到原来初始化时的角色设置</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gprecoverseg -r</span><br></pre></td></tr></table></figure><p>查看gpcc状态:</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpcc2.png" alt></p></li></ul><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了GP的高可用原理及实践。首先介绍了Master与Segment的容错策略，然后介绍了Master节点与Segment节点故障恢复的步骤，最后给出了一个完整的实践过程。</p>]]></content>
      
      
      <categories>
          
          <category> greenplum </category>
          
      </categories>
      
      
        <tags>
            
            <tag> greenplum </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink DataSet API编程指南</title>
      <link href="/2020/05/09/Flink-DataSet-API%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/"/>
      <url>/2020/05/09/Flink-DataSet-API%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>Flink最大的亮点是实时处理部分，Flink认为批处理是流处理的特殊情况，可以通过一套引擎处理批量和流式数据，而Flink在未来也会重点投入更多的资源到批流融合中。我在<a href="https://mp.weixin.qq.com/s/rllW7XS9m-BH-2lxp_N8QA" target="_blank" rel="noopener">Flink DataStream API编程指南</a>中介绍了DataStream API的使用，在本文中将介绍Flink批处理计算的DataSet API的使用。通过本文你可以了解：</p><ul><li>DataSet转换操作(Transformation)</li><li>Source与Sink的使用</li><li>广播变量的基本概念与使用Demo</li><li>分布式缓存的概念及使用Demo</li><li>DataSet API的Transformation使用Demo案例</li></ul><h2 id="WordCount示例"><a href="#WordCount示例" class="headerlink" title="WordCount示例"></a>WordCount示例</h2><p>在开始讲解DataSet API之前，先看一个Word Count的简单示例，来直观感受一下DataSet API的编程模型，具体代码如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 用于批处理的执行环境</span></span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 数据源</span></span><br><span class="line">        DataSource&lt;String&gt; stringDataSource = env.fromElements(<span class="string">"hello Flink What is Apache Flink"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 转换</span></span><br><span class="line">        AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; wordCnt = stringDataSource</span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        String[] split = value.split(<span class="string">" "</span>);</span><br><span class="line">                        <span class="keyword">for</span> (String word : split) &#123;</span><br><span class="line">                            out.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .groupBy(<span class="number">0</span>)</span><br><span class="line">                .sum(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 输出</span></span><br><span class="line">        wordCnt.print();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上面的示例中可以看出，基本的编程模型是：</p><ul><li>获取批处理的执行环境ExecutionEnvironment</li><li>加载数据源</li><li>转换操作</li><li>数据输出</li></ul><p>下面会对数据源、转换操作、数据输出进行一一解读。</p><h2 id="Data-Source"><a href="#Data-Source" class="headerlink" title="Data Source"></a>Data Source</h2><p>DataSet API支持从多种数据源中将批量数据集读到Flink系统中，并转换成DataSet数据集。主要包括三种类型：分别是基于文件的、基于集合的及通用类数据源。同时在DataSet API中可以自定义实现InputFormat/RichInputFormat接口，以接入不同数据格式类型的数据源，比如CsvInputFormat、TextInputFormat等。从ExecutionEnvironment类提供的方法中可以看出支持的数据源方法，如下图所示：</p><p><img src="//jiamaoxiang.top/2020/05/09/Flink-DataSet-API编程指南/dataset%E6%95%B0%E6%8D%AE%E6%BA%90.png" alt></p><h3 id="基于文件的数据源"><a href="#基于文件的数据源" class="headerlink" title="基于文件的数据源"></a>基于文件的数据源</h3><h4 id="readTextFile-path-TextInputFormat"><a href="#readTextFile-path-TextInputFormat" class="headerlink" title="readTextFile(path) / TextInputFormat"></a>readTextFile(path) / TextInputFormat</h4><ul><li>解释</li></ul><p>读取文本文件，传递文件路径参数，并将文件内容转换成DataSet<string>类型数据集。</string></p><ul><li>使用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 读取本地文件</span></span><br><span class="line">DataSet&lt;String&gt; localLines = env.readTextFile(<span class="string">"file:///path/to/my/textfile"</span>);</span><br><span class="line"><span class="comment">// 读取HDSF文件</span></span><br><span class="line">DataSet&lt;String&gt; hdfsLines = env.readTextFile(<span class="string">"hdfs://nnHost:nnPort/path/to/my/textfile"</span>);</span><br></pre></td></tr></table></figure><h4 id="readTextFileWithValue-path-TextValueInputFormat"><a href="#readTextFileWithValue-path-TextValueInputFormat" class="headerlink" title="readTextFileWithValue(path)/ TextValueInputFormat"></a>readTextFileWithValue(path)/ TextValueInputFormat</h4><ul><li>解释</li></ul><p>读取文本文件内容，将文件内容转换成DataSet[StringValue]类型数据集。该方法与readTextFile(String)不同的是，其泛型是StringValue，是一种可变的String类型，通过StringValue存储文本数据可以有效降低String对象创建数量，减小垃圾回收的压力。</p><ul><li>使用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 读取本地文件</span></span><br><span class="line">DataSet&lt;StringValue&gt; localLines = env.readTextFileWithValue(<span class="string">"file:///some/local/file"</span>);</span><br><span class="line"><span class="comment">// 读取HDSF文件</span></span><br><span class="line">DataSet&lt;StringValue&gt; hdfsLines = env.readTextFileWithValue(<span class="string">"hdfs://host:port/file/path"</span>);</span><br></pre></td></tr></table></figure><h4 id="readCsvFile-path-CsvInputFormat"><a href="#readCsvFile-path-CsvInputFormat" class="headerlink" title="readCsvFile(path)/ CsvInputFormat"></a>readCsvFile(path)/ CsvInputFormat</h4><ul><li>解释</li></ul><p>创建一个CSV的reader，读取逗号分隔(或其他分隔符)的文件。可以直接转换成Tuple类型、POJOs类的DataSet。在方法中可以指定行切割符、列切割符、字段等信息。</p><ul><li>使用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// read a CSV file with five fields, taking only two of them</span></span><br><span class="line"><span class="comment">// 读取一个具有5个字段的CSV文件，只取第一个和第四个字段</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Double&gt;&gt; csvInput = env.readCsvFile(<span class="string">"hdfs:///the/CSV/file"</span>)</span><br><span class="line">                               .includeFields(<span class="string">"10010"</span>)  </span><br><span class="line">                          .types(String.class, Double.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取一个有三个字段的CSV文件，将其转为POJO类型</span></span><br><span class="line">DataSet&lt;Person&gt;&gt; csvInput = env.readCsvFile(<span class="string">"hdfs:///the/CSV/file"</span>)</span><br><span class="line">                         .pojoType(Person.class, <span class="string">"name"</span>, <span class="string">"age"</span>, <span class="string">"zipcode"</span>);</span><br></pre></td></tr></table></figure><h4 id="readFileOfPrimitives-path-Class-PrimitiveInputFormat"><a href="#readFileOfPrimitives-path-Class-PrimitiveInputFormat" class="headerlink" title="readFileOfPrimitives(path, Class) / PrimitiveInputFormat"></a>readFileOfPrimitives(path, Class) / PrimitiveInputFormat</h4><ul><li>解释</li></ul><p>读取一个原始数据类型(如String,Integer)的文件,返回一个对应的原始类型的DataSet集合</p><ul><li>使用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;String&gt; Data = env.readFileOfPrimitives(<span class="string">"file:///some/local/file"</span>, String.class);</span><br></pre></td></tr></table></figure><h3 id="基于集合的数据源"><a href="#基于集合的数据源" class="headerlink" title="基于集合的数据源"></a>基于集合的数据源</h3><h4 id="fromCollection-Collection"><a href="#fromCollection-Collection" class="headerlink" title="fromCollection(Collection)"></a>fromCollection(Collection)</h4><ul><li>解释</li></ul><p>从java的集合中创建DataSet数据集，集合中的元素数据类型相同</p><ul><li>使用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;String&gt; data= env.fromCollection(arrayList);</span><br></pre></td></tr></table></figure><h4 id="fromElements-T-…"><a href="#fromElements-T-…" class="headerlink" title="fromElements(T …)"></a>fromElements(T …)</h4><ul><li>解释</li></ul><p>从给定数据元素序列中创建DataSet数据集，且所有的数据对象类型必须一致</p><ul><li>使用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;String&gt; stringDataSource = env.fromElements(<span class="string">"hello Flink What is Apache Flink"</span>);</span><br></pre></td></tr></table></figure><h4 id="generateSequence-from-to"><a href="#generateSequence-from-to" class="headerlink" title="generateSequence(from, to)"></a>generateSequence(from, to)</h4><ul><li>解释</li></ul><p>指定from到to范围区间，然后在区间内部生成数字序列数据集,由于是并行处理的，所以最终的顺序不能保证一致。</p><ul><li>使用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Long&gt; longDataSource = env.generateSequence(<span class="number">1</span>, <span class="number">20</span>);</span><br></pre></td></tr></table></figure><h3 id="通用类型数据源"><a href="#通用类型数据源" class="headerlink" title="通用类型数据源"></a>通用类型数据源</h3><p>DataSet API中提供了Inputformat通用的数据接口，以接入不同数据源和格式类型的数据。InputFormat接口主要分为两种类型：一种是基于文件类型，在DataSet API对应readFile()方法；另外一种是基于通用数据类型的接口，例如读取RDBMS或NoSQL数据库中等，在DataSet API中对应createInput()方法。</p><h4 id="readFile-inputFormat-path-FileInputFormat"><a href="#readFile-inputFormat-path-FileInputFormat" class="headerlink" title="readFile(inputFormat, path) / FileInputFormat"></a>readFile(inputFormat, path) / FileInputFormat</h4><ul><li>解释</li></ul><p>自定义文件类型输入源，将指定格式文件读取并转成DataSet数据集</p><ul><li>使用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.readFile(<span class="keyword">new</span> MyInputFormat(), <span class="string">"file:///some/local/file"</span>);</span><br></pre></td></tr></table></figure><h4 id="createInput-inputFormat-InputFormat"><a href="#createInput-inputFormat-InputFormat" class="headerlink" title="createInput(inputFormat) / InputFormat"></a>createInput(inputFormat) / InputFormat</h4><ul><li>解释</li></ul><p>自定义通用型数据源，将读取的数据转换为DataSet数据集。如以下实例使用Flink内置的JDBCInputFormat，创建读取mysql数据源的JDBCInput Format，完成从mysql中读取Person表，并转换成DataSet [Row]数据集</p><ul><li>使用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt; dbData =</span><br><span class="line">    env.createInput(</span><br><span class="line">      JDBCInputFormat.buildJDBCInputFormat()</span><br><span class="line">                     .setDrivername(<span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">                     .setDBUrl(<span class="string">"jdbc:mysql://localhost/mydb"</span>)</span><br><span class="line">                     .setQuery(<span class="string">"select name, age from stu"</span>)</span><br><span class="line">                     .setRowTypeInfo(<span class="keyword">new</span> RowTypeInfo(BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.INT_TYPE_INFO))</span><br><span class="line">                     .finish()</span><br><span class="line">    );</span><br></pre></td></tr></table></figure><h2 id="Data-Sink"><a href="#Data-Sink" class="headerlink" title="Data Sink"></a>Data Sink</h2><p>Flink在DataSet API中的数据输出共分为三种类型。第一种是基于文件实现，对应DataSet的write()方法，实现将DataSet数据输出到文件系统中。第二种是基于通用存储介质实现，对应DataSet的output()方法，例如使用JDBCOutputFormat将数据输出到关系型数据库中。最后一种是客户端输出，直接将DataSet数据从不同的节点收集到Client，并在客户端中输出，例如DataSet的print()方法。</p><h3 id="标准的数据输出方法"><a href="#标准的数据输出方法" class="headerlink" title="标准的数据输出方法"></a>标准的数据输出方法</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 文本数据</span></span><br><span class="line">DataSet&lt;String&gt; textData = <span class="comment">// [...]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 将数据写入本地文件</span></span><br><span class="line">textData.writeAsText(<span class="string">"file:///my/result/on/localFS"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将数据写入HDFS文件</span></span><br><span class="line">textData.writeAsText(<span class="string">"hdfs://nnHost:nnPort/my/result/on/localFS"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 写数据到本地文件，如果文件存在则覆盖</span></span><br><span class="line">textData.writeAsText(<span class="string">"file:///my/result/on/localFS"</span>, WriteMode.OVERWRITE);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将数据输出到本地的CSV文件，指定分隔符为"|"</span></span><br><span class="line">DataSet&lt;Tuple3&lt;String, Integer, Double&gt;&gt; values = <span class="comment">// [...]</span></span><br><span class="line">values.writeAsCsv(<span class="string">"file:///path/to/the/result/file"</span>, <span class="string">"\n"</span>, <span class="string">"|"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用自定义的TextFormatter对象</span></span><br><span class="line">values.writeAsFormattedText(<span class="string">"file:///path/to/the/result/file"</span>,</span><br><span class="line">    <span class="keyword">new</span> TextFormatter&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">format</span> <span class="params">(Tuple2&lt;Integer, Integer&gt; value)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> value.f1 + <span class="string">" - "</span> + value.f0;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><h3 id="使用自定义的输出类型"><a href="#使用自定义的输出类型" class="headerlink" title="使用自定义的输出类型"></a>使用自定义的输出类型</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple3&lt;String, Integer, Double&gt;&gt; myResult = [...]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将tuple类型的数据写入关系型数据库</span></span><br><span class="line">myResult.output(</span><br><span class="line">    <span class="comment">// 创建并配置OutputFormat</span></span><br><span class="line">    JDBCOutputFormat.buildJDBCOutputFormat()</span><br><span class="line">                    .setDrivername(<span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">                    .setDBUrl(<span class="string">"jdbc:mysql://localhost/mydb"</span>)</span><br><span class="line">                    .setQuery(<span class="string">"insert into persons (name, age, height) values (?,?,?)"</span>)</span><br><span class="line">                    .finish()</span><br><span class="line">    );</span><br></pre></td></tr></table></figure><h2 id="DataSet转换"><a href="#DataSet转换" class="headerlink" title="DataSet转换"></a>DataSet转换</h2><p>转换(transformations)将一个DataSet转成另外一个DataSet，Flink提供了非常丰富的转换操作符。具体使用如下：</p><h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><p>一进一出</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;String&gt; source = env.fromElements(<span class="string">"I"</span>, <span class="string">"like"</span>, <span class="string">"flink"</span>);</span><br><span class="line">      source.map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">          <span class="meta">@Override</span></span><br><span class="line">          <span class="comment">// 将数据转为大写</span></span><br><span class="line">          <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">              <span class="keyword">return</span> value.toUpperCase();</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;).print();</span><br></pre></td></tr></table></figure><h3 id="FlatMap"><a href="#FlatMap" class="headerlink" title="FlatMap"></a>FlatMap</h3><p>输入一个元素，产生0个、1个或多个元素</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stringDataSource</span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        String[] split = value.split(<span class="string">" "</span>);</span><br><span class="line">                        <span class="keyword">for</span> (String word : split) &#123;</span><br><span class="line">                            out.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .groupBy(<span class="number">0</span>)</span><br><span class="line">                .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h3 id="MapPartition"><a href="#MapPartition" class="headerlink" title="MapPartition"></a>MapPartition</h3><p>功能和Map函数相似，只是MapPartition操作是在DataSet中基于分区对数据进行处理，函数调用中会按照分区将数据通过Iteator的形式传入，每个分区中的元素数与并行度有关，并返回任意数量的结果值。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">source.mapPartition(<span class="keyword">new</span> MapPartitionFunction&lt;String, Long&gt;() &#123;</span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">           <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mapPartition</span><span class="params">(Iterable&lt;String&gt; values, Collector&lt;Long&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">               <span class="keyword">long</span> c = <span class="number">0</span>;</span><br><span class="line">               <span class="keyword">for</span> (String value : values) &#123;</span><br><span class="line">                   c++;</span><br><span class="line">               &#125;</span><br><span class="line">               <span class="comment">//输出每个分区元素个数</span></span><br><span class="line">               out.collect(c);</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;).print();</span><br></pre></td></tr></table></figure><h3 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h3><p>过滤数据，如果返回true则保留数据，如果返回false则过滤掉</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Long&gt; source = env.fromElements(<span class="number">1L</span>, <span class="number">2L</span>, <span class="number">3L</span>,<span class="number">4L</span>,<span class="number">5L</span>);</span><br><span class="line">        source.filter(<span class="keyword">new</span> FilterFunction&lt;Long&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Long value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value % <span class="number">2</span> == <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();</span><br></pre></td></tr></table></figure><h3 id="Project"><a href="#Project" class="headerlink" title="Project"></a><strong>Project</strong></h3><p>仅能用在Tuple类型的数据集，投影操作，选取Tuple数据的字段的子集</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Tuple3&lt;Long, Integer, String&gt;&gt; source = env.fromElements(</span><br><span class="line">              Tuple3.of(<span class="number">1L</span>, <span class="number">20</span>, <span class="string">"tom"</span>), </span><br><span class="line">              Tuple3.of(<span class="number">2L</span>, <span class="number">25</span>, <span class="string">"jack"</span>), </span><br><span class="line">              Tuple3.of(<span class="number">3L</span>, <span class="number">22</span>, <span class="string">"bob"</span>));</span><br><span class="line">      <span class="comment">// 去第一个和第三个元素</span></span><br><span class="line">      source.project(<span class="number">0</span>, <span class="number">2</span>).print();</span><br></pre></td></tr></table></figure><h3 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a><strong>Reduce</strong></h3><p>通过两两合并，将数据集中的元素合并成一个元素，可以在整个数据集上使用，也可以在分组之后的数据集上使用。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Tuple2&lt;String, Integer&gt;&gt; source = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Hadoop"</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Spark"</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1</span>));</span><br><span class="line">        source</span><br><span class="line">                .groupBy(<span class="number">0</span>)</span><br><span class="line">                .reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Tuple2.of(value1.f0, value1.f1 + value2.f1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();</span><br></pre></td></tr></table></figure><h3 id="ReduceGroup"><a href="#ReduceGroup" class="headerlink" title="ReduceGroup"></a><strong>ReduceGroup</strong></h3><p>将数据集中的元素合并成一个元素，可以在整个数据集上使用，也可以在分组之后的数据集上使用。reduce函数的输入值是一个分组元素的Iterable。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Tuple2&lt;String, Long&gt;&gt; source = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Hadoop"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Spark"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1L</span>));</span><br><span class="line">        source</span><br><span class="line">                .groupBy(<span class="number">0</span>)</span><br><span class="line">                .reduceGroup(<span class="keyword">new</span> GroupReduceFunction&lt;Tuple2&lt;String,Long&gt;, Tuple2&lt;String,Long&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Iterable&lt;Tuple2&lt;String, Long&gt;&gt; values, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        Long sum = <span class="number">0L</span>;</span><br><span class="line">                        String word = <span class="string">""</span>;</span><br><span class="line">                        <span class="keyword">for</span>(Tuple2&lt;String, Long&gt; value:values)&#123;</span><br><span class="line">                            sum += value.f1;</span><br><span class="line">                            word = value.f0;</span><br><span class="line"></span><br><span class="line">                        &#125;</span><br><span class="line">                        out.collect(Tuple2.of(word,sum));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).print();</span><br></pre></td></tr></table></figure><h3 id="Aggregate"><a href="#Aggregate" class="headerlink" title="Aggregate"></a><strong>Aggregate</strong></h3><p>通过Aggregate Function将一组元素值合并成单个值，可以在整个DataSet数据集上使用，也可以在分组之后的数据集上使用。仅仅用在Tuple类型的数据集上，主要包括Sum,Min,Max函数</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Tuple2&lt;String, Long&gt;&gt; source = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Hadoop"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Spark"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1L</span>));</span><br><span class="line">        source</span><br><span class="line">                .groupBy(<span class="number">0</span>)</span><br><span class="line">                .aggregate(SUM,<span class="number">1</span>)<span class="comment">// 按第2个值求和</span></span><br><span class="line">                 .print();</span><br></pre></td></tr></table></figure><h3 id="Distinct"><a href="#Distinct" class="headerlink" title="Distinct"></a><strong>Distinct</strong></h3><p>DataSet数据集元素去重</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Tuple&gt; source = env.fromElements(Tuple1.of(<span class="string">"Flink"</span>),Tuple1.of(<span class="string">"Flink"</span>),Tuple1.of(<span class="string">"hadoop"</span>));</span><br><span class="line">        source.distinct(<span class="number">0</span>).print();<span class="comment">// 按照tuple的第一个字段去重</span></span><br><span class="line"><span class="comment">// 结果：</span></span><br><span class="line">(Flink)</span><br><span class="line">(hadoop)</span><br></pre></td></tr></table></figure><h3 id="Join"><a href="#Join" class="headerlink" title="Join"></a><strong>Join</strong></h3><p>默认的join是产生一个Tuple2数据类型的DataSet，关联的key可以通过key表达式、Key-selector函数、字段位置以及CaseClass字段指定。对于两个Tuple类型的数据集可以通过字段位置进行关联，左边数据集的字段通过where方法指定，右边数据集的字段通过equalTo()方法指定。比如：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Tuple2&lt;Integer,String&gt;&gt; source1 = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="number">1</span>,<span class="string">"jack"</span>),</span><br><span class="line">                Tuple2.of(<span class="number">2</span>,<span class="string">"tom"</span>),</span><br><span class="line">                Tuple2.of(<span class="number">3</span>,<span class="string">"Bob"</span>));</span><br><span class="line">        DataSource&lt;Tuple2&lt;String, Integer&gt;&gt; source2 = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"order1"</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"order2"</span>, <span class="number">2</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"order3"</span>, <span class="number">3</span>));</span><br><span class="line">        source1.join(source2).where(<span class="number">0</span>).equalTo(<span class="number">1</span>).print();</span><br></pre></td></tr></table></figure><p>可以在关联的过程中指定自定义Join Funciton, Funciton的入参为左边数据集中的数据元素和右边数据集的中的数据元素所组成的元祖，并返回一个经过计算处理后的数据。如：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 用户id，购买商品名称，购买商品数量</span></span><br><span class="line">        DataSource&lt;Tuple3&lt;Integer,String,Integer&gt;&gt; source1 = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1</span>,<span class="string">"item1"</span>,<span class="number">2</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2</span>,<span class="string">"item2"</span>,<span class="number">3</span>),</span><br><span class="line">                Tuple3.of(<span class="number">3</span>,<span class="string">"item3"</span>,<span class="number">4</span>));</span><br><span class="line">        <span class="comment">//商品名称与商品单价</span></span><br><span class="line">        DataSource&lt;Tuple2&lt;String, Integer&gt;&gt; source2 = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"item1"</span>, <span class="number">10</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"item2"</span>, <span class="number">20</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"item3"</span>, <span class="number">15</span>));</span><br><span class="line">        source1.join(source2)</span><br><span class="line">                .where(<span class="number">1</span>)</span><br><span class="line">                .equalTo(<span class="number">0</span>)</span><br><span class="line">                .with(<span class="keyword">new</span> JoinFunction&lt;Tuple3&lt;Integer,String,Integer&gt;, Tuple2&lt;String,Integer&gt;, Tuple3&lt;Integer,String,Double&gt;&gt;() &#123;</span><br><span class="line">                    <span class="comment">// 用户每种商品购物总金额</span></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, String, Double&gt; <span class="title">join</span><span class="params">(Tuple3&lt;Integer, String, Integer&gt; first, Tuple2&lt;String, Integer&gt; second)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> Tuple3.of(first.f0,first.f1,first.f2 * second.f1.doubleValue());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).print();</span><br></pre></td></tr></table></figure><p>为了能够更好地引导Flink底层去正确地处理数据集，可以在DataSet数据集关联中，通过Size Hint标记数据集的大小，Flink可以根据用户给定的hint(提示)调整计算策略，例如可以使用joinWithTiny或joinWithHuge提示第二个数据集的大小。示例如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;Integer, String&gt;&gt; input1 = <span class="comment">// [...]</span></span><br><span class="line">DataSet&lt;Tuple2&lt;Integer, String&gt;&gt; input2 = <span class="comment">// [...]</span></span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;Tuple2&lt;Integer, String&gt;, Tuple2&lt;Integer, String&gt;&gt;&gt;</span><br><span class="line">            result1 =</span><br><span class="line">            <span class="comment">// 提示第二个数据集为小数据集</span></span><br><span class="line">            input1.joinWithTiny(input2)</span><br><span class="line">                  .where(<span class="number">0</span>)</span><br><span class="line">                  .equalTo(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;Tuple2&lt;Integer, String&gt;, Tuple2&lt;Integer, String&gt;&gt;&gt;</span><br><span class="line">            result2 =</span><br><span class="line">            <span class="comment">// h提示第二个数据集为大数据集</span></span><br><span class="line">            input1.joinWithHuge(input2)</span><br><span class="line">                  .where(<span class="number">0</span>)</span><br><span class="line">                  .equalTo(<span class="number">0</span>);</span><br></pre></td></tr></table></figure><p>Flink的runtime可以使用多种方式执行join。在不同的情况下，每种可能的方式都会胜过其他方式。系统会尝试自动选择一种合理的方法，但是允许用户手动选择一种策略， 可以让Flink更加灵活且高效地执行Join操作。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;SomeType&gt; input1 = <span class="comment">// [...]</span></span><br><span class="line">DataSet&lt;AnotherType&gt; input2 = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// 广播第一个输入并从中构建一个哈希表，第二个输入将对其进行探测，适用于第一个数据集非常小的场景</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result =</span><br><span class="line">      input1.join(input2, JoinHint.BROADCAST_HASH_FIRST)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br><span class="line"><span class="comment">// 广播第二个输入并从中构建一个哈希表，第一个输入将对其进行探测，适用于第二个数据集非常小的场景</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result =</span><br><span class="line">      input1.join(input2, JoinHint.BROADCAST_HASH_SECOND)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br><span class="line"><span class="comment">// 将两个数据集重新分区，并将第一个数据集转换成哈希表，适用于第一个数据集比第二个数据集小，但两个数据集都比较大的场景</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result =</span><br><span class="line">      input1.join(input2, JoinHint.REPARTITION_HASH_FIRST)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br><span class="line"><span class="comment">// 将两个数据集重新分区，并将第二个数据集转换成哈希表，适用于第二个数据集比第一个数据集小，但两个数据集都比较大的场景</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result =</span><br><span class="line">      input1.join(input2, JoinHint.REPARTITION_HASH_SECOND)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br><span class="line"><span class="comment">// 将两个数据集重新分区，并将每个分区排序，适用于两个数据集都已经排好序的场景</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result =</span><br><span class="line">      input1.join(input2, JoinHint.REPARTITION_SORT_MERGE)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br><span class="line"><span class="comment">// 相当于不指定，有系统自行处理</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result =</span><br><span class="line">      input1.join(input2, JoinHint.OPTIMIZER_CHOOSES)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br></pre></td></tr></table></figure><h3 id="OuterJoin"><a href="#OuterJoin" class="headerlink" title="OuterJoin"></a>OuterJoin</h3><p>OuterJoin对两个数据集进行外关联，包含left、right、full outer join三种关联方式，分别对应DataSet API中的leftOuterJoin、rightOuterJoin以及fullOuterJoin方法。注意外连接仅适用于Java 和 Scala DataSet API.</p><p>使用方式几乎和join类似：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//左外连接</span></span><br><span class="line">source1.leftOuterJoin(source2).where(<span class="number">1</span>).equalTo(<span class="number">0</span>);</span><br><span class="line"><span class="comment">//右外链接</span></span><br><span class="line">source1.rightOuterJoin(source2).where(<span class="number">1</span>).equalTo(<span class="number">0</span>);</span><br></pre></td></tr></table></figure><p>此外，外连接也提供了相应的关联算法提示，可以跟据左右数据集的分布情况选择合适的优化策略，提升数据处理的效率。下面代码可以参考上面join的解释。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;SomeType&gt; input1 = <span class="comment">// [...]</span></span><br><span class="line">DataSet&lt;AnotherType&gt; input2 = <span class="comment">// [...]</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result1 =</span><br><span class="line">      input1.leftOuterJoin(input2, JoinHint.REPARTITION_SORT_MERGE)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result2 =</span><br><span class="line">      input1.rightOuterJoin(input2, JoinHint.BROADCAST_HASH_FIRST)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br></pre></td></tr></table></figure><p>对于外连接的关联算法，与join有所不同。每种外连接只支持部分算法。如下：</p><ul><li><p>LeftOuterJoin支持：</p><ul><li><p>OPTIMIZER_CHOOSES</p></li><li><p>BROADCAST_HASH_SECOND</p></li><li><p>REPARTITION_HASH_SECOND</p></li><li><p>REPARTITION_SORT_MERGE</p><ul><li>RightOuterJoin支持：<pre><code>- OPTIMIZER_CHOOSES- BROADCAST_HASH_FIRST- REPARTITION_HASH_FIRST- REPARTITION_SORT_MERGE</code></pre></li><li>FullOuterJoin支持：<ul><li>OPTIMIZER_CHOOSES</li><li>REPARTITION_SORT_MERGE</li></ul></li></ul></li></ul></li></ul><h3 id="CoGroup"><a href="#CoGroup" class="headerlink" title="CoGroup"></a><strong>CoGroup</strong></h3><p> CoGroup是对分组之后的DataSet进行join操作，将两个DataSet数据集合并在一起，会先各自对每个DataSet按照key进行分组，然后将分组之后的DataSet传输到用户定义的CoGroupFunction，将两个数据集根据相同的Key记录组合在一起，相同Key的记录会存放在一个Group中，如果指定key仅在一个数据集中有记录，则co-groupFunction会将这个Group与空的Group关联。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 用户id，购买商品名称，购买商品数量</span></span><br><span class="line">        DataSource&lt;Tuple3&lt;Integer,String,Integer&gt;&gt; source1 = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1</span>,<span class="string">"item1"</span>,<span class="number">2</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2</span>,<span class="string">"item2"</span>,<span class="number">3</span>),</span><br><span class="line">                Tuple3.of(<span class="number">3</span>,<span class="string">"item2"</span>,<span class="number">4</span>));</span><br><span class="line">        <span class="comment">//商品名称与商品单价</span></span><br><span class="line">        DataSource&lt;Tuple2&lt;String, Integer&gt;&gt; source2 = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"item1"</span>, <span class="number">10</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"item2"</span>, <span class="number">20</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"item3"</span>, <span class="number">15</span>));</span><br><span class="line"></span><br><span class="line">        source1.coGroup(source2)</span><br><span class="line">                .where(<span class="number">1</span>)</span><br><span class="line">                .equalTo(<span class="number">0</span>)</span><br><span class="line">                .with(<span class="keyword">new</span> CoGroupFunction&lt;Tuple3&lt;Integer,String,Integer&gt;, Tuple2&lt;String,Integer&gt;, Tuple2&lt;String,Double&gt;&gt;() &#123;</span><br><span class="line">                    <span class="comment">// 每个Iterable存储的是分好组的数据，即相同key的数据组织在一起</span></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">coGroup</span><span class="params">(Iterable&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; first, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; second, Collector&lt;Tuple2&lt;String, Double&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">//存储每种商品购买数量</span></span><br><span class="line">                        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">                        <span class="keyword">for</span>(Tuple3&lt;Integer, String, Integer&gt; val1:first)&#123;</span><br><span class="line">                        sum += val1.f2;</span><br><span class="line"></span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="comment">// 每种商品数量 * 商品单价</span></span><br><span class="line">                    <span class="keyword">for</span>(Tuple2&lt;String, Integer&gt; val2:second)&#123;</span><br><span class="line">                        out.collect(Tuple2.of(val2.f0,sum * val2.f1.doubleValue()));</span><br><span class="line"></span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).print();</span><br></pre></td></tr></table></figure><h3 id="Cross"><a href="#Cross" class="headerlink" title="Cross"></a><strong>Cross</strong></h3><p>将两个数据集合并成一个数据集，返回被连接的两个数据集所有数据行的笛卡儿积，返回的数据行数等于第一个数据集中符合查询条件的数据行数乘以第二个数据集中符合查询条件的数据行数。Cross操作可以通过应用Cross Funciton将关联的数据集合并成目标格式的数据集，如果不指定Cross Funciton则返回Tuple2类型的数据集。Cross操作是计算密集型的算子，建议在使用时加上算法提示，比如<em>crossWithTiny()</em> and <em>crossWithHuge()</em>.</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//[id,x,y],坐标值</span></span><br><span class="line">        DataSet&lt;Tuple3&lt;Integer, Integer, Integer&gt;&gt; coords1 = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1</span>, <span class="number">20</span>, <span class="number">18</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2</span>, <span class="number">15</span>, <span class="number">20</span>),</span><br><span class="line">                Tuple3.of(<span class="number">3</span>, <span class="number">25</span>, <span class="number">10</span>));</span><br><span class="line">        DataSet&lt;Tuple3&lt;Integer, Integer, Integer&gt;&gt; coords2 = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1</span>, <span class="number">20</span>, <span class="number">18</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2</span>, <span class="number">15</span>, <span class="number">20</span>),</span><br><span class="line">                Tuple3.of(<span class="number">3</span>, <span class="number">25</span>, <span class="number">10</span>));</span><br><span class="line">        <span class="comment">// 求任意两点之间的欧氏距离</span></span><br><span class="line"></span><br><span class="line">        coords1.cross(coords2)</span><br><span class="line">                .with(<span class="keyword">new</span> CrossFunction&lt;Tuple3&lt;Integer, Integer, Integer&gt;, Tuple3&lt;Integer, Integer, Integer&gt;, Tuple3&lt;Integer, Integer, Double&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, Integer, Double&gt; <span class="title">cross</span><span class="params">(Tuple3&lt;Integer, Integer, Integer&gt; val1, Tuple3&lt;Integer, Integer, Integer&gt; val2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">// 计算欧式距离</span></span><br><span class="line">                        <span class="keyword">double</span> dist = sqrt(pow(val1.f1 - val2.f1, <span class="number">2</span>) + pow(val1.f2 - val2.f2, <span class="number">2</span>));</span><br><span class="line">                        <span class="comment">// 返回两点之间的欧式距离</span></span><br><span class="line">                        <span class="keyword">return</span> Tuple3.of(val1.f0,val2.f0,dist);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).print();</span><br></pre></td></tr></table></figure><h3 id="Union"><a href="#Union" class="headerlink" title="Union"></a><strong>Union</strong></h3><p>合并两个DataSet数据集，两个数据集的数据元素格式必须相同，多个数据集可以连续合并.</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals1 = env.fromElements(</span><br><span class="line">           Tuple2.of(<span class="string">"jack"</span>,<span class="number">20</span>),</span><br><span class="line">           Tuple2.of(<span class="string">"Tom"</span>,<span class="number">21</span>));</span><br><span class="line">   DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals2 = env.fromElements(</span><br><span class="line">           Tuple2.of(<span class="string">"Robin"</span>,<span class="number">25</span>),</span><br><span class="line">           Tuple2.of(<span class="string">"Bob"</span>,<span class="number">30</span>));</span><br><span class="line">   DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals3 = env.fromElements(</span><br><span class="line">           Tuple2.of(<span class="string">"Jasper"</span>,<span class="number">24</span>),</span><br><span class="line">           Tuple2.of(<span class="string">"jarry"</span>,<span class="number">21</span>));</span><br><span class="line">   DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; unioned = vals1</span><br><span class="line">           .union(vals2)</span><br><span class="line">           .union(vals3);</span><br><span class="line">   unioned.print();</span><br></pre></td></tr></table></figure><h3 id="Rebalance"><a href="#Rebalance" class="headerlink" title="Rebalance"></a><strong>Rebalance</strong></h3><p>对数据集中的数据进行平均分布，使得每个分区上的数据量相同,减轻数据倾斜造成的影响，注意仅仅是<code>Map-like</code>类型的算子(比如map，flatMap)才可以用在Rebalance算子之后。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;String&gt; in = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// rebalance DataSet,然后使用map算子.</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, String&gt;&gt; out = in.rebalance()</span><br><span class="line">                                        .map(<span class="keyword">new</span> Mapper());</span><br></pre></td></tr></table></figure><h3 id="Hash-Partition"><a href="#Hash-Partition" class="headerlink" title="Hash-Partition"></a><strong>Hash-Partition</strong></h3><p>根据给定的Key进行Hash分区，key相同的数据会被放入同一个分区内。可以使用通过元素的位置、元素的名称或者key selector函数指定key。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; in = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// 根据第一个值进行hash分区，然后使用 MapPartition转换操作.</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, String&gt;&gt; out = in.partitionByHash(<span class="number">0</span>)</span><br><span class="line">                                        .mapPartition(<span class="keyword">new</span> PartitionMapper());</span><br></pre></td></tr></table></figure><h3 id="Range-Partition"><a href="#Range-Partition" class="headerlink" title="Range-Partition"></a><strong>Range-Partition</strong></h3><p>根据给定的Key进行Range分区，key相同的数据会被放入同一个分区内。可以使用通过元素的位置、元素的名称或者key selector函数指定key。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; in = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// 根据第一个值进行Range分区，然后使用 MapPartition转换操作.</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, String&gt;&gt; out = in.partitionByRange(<span class="number">0</span>)</span><br><span class="line">                                        .mapPartition(<span class="keyword">new</span> PartitionMapper());</span><br></pre></td></tr></table></figure><h3 id="Custom-Partitioning"><a href="#Custom-Partitioning" class="headerlink" title="Custom Partitioning"></a><strong>Custom Partitioning</strong></h3><p>除了上面的分区外，还支持自定义分区函数。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String,Integer&gt;&gt; in = <span class="comment">// [...]</span></span><br><span class="line">DataSet&lt;Integer&gt; result = in.partitionCustom(partitioner, key)</span><br><span class="line">                            .mapPartition(<span class="keyword">new</span> PartitionMapper());</span><br></pre></td></tr></table></figure><h3 id="Sort-Partition"><a href="#Sort-Partition" class="headerlink" title="Sort Partition"></a><strong>Sort Partition</strong></h3><p>在本地对DataSet数据集中的所有分区根据指定字段进行重排序，排序方式通过Order.ASCENDING以及Order.DESCENDING关键字指定。支持指定多个字段进行分区排序，如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; in = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// 按照第一个字段升序排列，第二个字段降序排列.</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, String&gt;&gt; out = in.sortPartition(<span class="number">1</span>, Order.ASCENDING)</span><br><span class="line">                                        .sortPartition(<span class="number">0</span>, Order.DESCENDING)</span><br><span class="line">                                        .mapPartition(<span class="keyword">new</span> PartitionMapper());</span><br></pre></td></tr></table></figure><h3 id="First-n"><a href="#First-n" class="headerlink" title="First-n"></a>First-n</h3><p>返回数据集的n条随机结果，可以应用于常规类型数据集、Grouped类型数据集以及排序数据集上。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; in = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// 返回数据集中的任意5个元素</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; out1 = in.first(<span class="number">5</span>);</span><br><span class="line"><span class="comment">//返回每个分组内的任意两个元素</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; out2 = in.groupBy(<span class="number">0</span>)</span><br><span class="line">                                          .first(<span class="number">2</span>);</span><br><span class="line"><span class="comment">// 返回每个分组内的前三个元素</span></span><br><span class="line"><span class="comment">// 分组后的数据集按照第二个字段进行升序排序</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; out3 = in.groupBy(<span class="number">0</span>)</span><br><span class="line">                                          .sortGroup(<span class="number">1</span>, Order.ASCENDING)</span><br><span class="line">                                          .first(<span class="number">3</span>);</span><br></pre></td></tr></table></figure><h3 id="MinBy-MaxBy"><a href="#MinBy-MaxBy" class="headerlink" title="MinBy / MaxBy"></a>MinBy / MaxBy</h3><p>从数据集中返回指定字段或组合对应最小或最大的记录，如果选择的字段具有多个相同值，则在集合中随机选择一条记录返回。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; source = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"jack"</span>,<span class="number">20</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Tom"</span>,<span class="number">21</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Robin"</span>,<span class="number">25</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Bob"</span>,<span class="number">30</span>));</span><br><span class="line"><span class="comment">// 按照第2个元素比较，找出第二个元素为最小值的那个tuple</span></span><br><span class="line"><span class="comment">// 在整个DataSet上使用minBy</span></span><br><span class="line">ReduceOperator&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2Reduce = source.minBy(<span class="number">1</span>);</span><br><span class="line">tuple2Reduce.print();<span class="comment">// 返回(jack,20)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 也可以在分组的DataSet上使用minBy</span></span><br><span class="line">source.groupBy(<span class="number">0</span>) <span class="comment">// 按照第一个字段进行分组</span></span><br><span class="line">      .minBy(<span class="number">1</span>)  <span class="comment">// 找出每个分组内的按照第二个元素为最小值的那个tuple</span></span><br><span class="line">      .print();</span><br></pre></td></tr></table></figure><h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>广播变量是分布式计算框架中经常会用到的一种数据共享方式。其主要作用是将小数据集采用网络传输的方式，在每台机器上维护一个只读的缓存变量，所在的计算节点实例均可以在本地内存中直接读取被广播的数据集，这样能够避免在数据计算过程中多次通过远程的方式从其他节点中读取小数据集，从而提升整体任务的计算性能。</p><p>广播变量可以理解为一个公共的共享变量，可以把DataSet广播出去，这样不同的task都可以读取该数据，广播的数据只会在每个节点上存一份。如果不使用广播变量，则会在每个节点上的task中都要复制一份dataset数据集，导致浪费内存。</p><p>使用广播变量的基本步骤如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//第一步创建需要广播的数据集</span></span><br><span class="line">DataSet&lt;Integer&gt; toBroadcast = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">DataSet&lt;String&gt; data = env.fromElements(<span class="string">"a"</span>, <span class="string">"b"</span>);</span><br><span class="line"></span><br><span class="line">data.map(<span class="keyword">new</span> RichMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      <span class="comment">// 第三步访问集合形式的广播变量数据集</span></span><br><span class="line">      Collection&lt;Integer&gt; broadcastSet = getRuntimeContext().getBroadcastVariable(<span class="string">"broadcastSetName"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).withBroadcastSet(toBroadcast, <span class="string">"broadcastSetName"</span>); <span class="comment">// 第二步广播数据集</span></span><br></pre></td></tr></table></figure><p>从上面的代码可以看出，DataSet API支持在RichFunction接口中通过RuntimeContext读取到广播变量。</p><p>首先在RichFunction中实现Open()方法，然后调用getRuntimeContext()方法获取应用的RuntimeContext，接着调用getBroadcastVariable()方法通过广播名称获取广播变量。同时Flink直接通过collect操作将数据集转换为本地Collection。需要注意的是，Collection对象的数据类型必须和定义的数据集的类型保持一致，否则会出现类型转换问题。</p><p>注意事项：</p><ul><li>由于广播变量的内容是保存在每个节点的内存中，所以广播变量数据集不易过大。</li><li>广播变量初始化之后，不支持修改，这样方能保证每个节点的数据都是一样的。</li><li>如果多个算子都要使用一份数据集，那么需要在多个算子的后面分别注册广播变量。</li><li>只能在批处理中使用广播变量。</li></ul><h3 id="使用Demo"><a href="#使用Demo" class="headerlink" title="使用Demo"></a>使用Demo</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BroadcastExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;Tuple2&lt;Integer,String&gt;&gt; RawBroadCastData = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        RawBroadCastData.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>,<span class="string">"jack"</span>));</span><br><span class="line">        RawBroadCastData.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">2</span>,<span class="string">"tom"</span>));</span><br><span class="line">        RawBroadCastData.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">3</span>,<span class="string">"Bob"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 模拟数据源，[userId,userName]</span></span><br><span class="line">        DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; userInfoBroadCastData = env.fromCollection(RawBroadCastData);</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;Tuple2&lt;Integer,Double&gt;&gt; rawUserAount = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        rawUserAount.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>,<span class="number">1000.00</span>));</span><br><span class="line">        rawUserAount.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">2</span>,<span class="number">500.20</span>));</span><br><span class="line">        rawUserAount.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">3</span>,<span class="number">800.50</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理数据：用户id，用户购买金额 ，[UserId,amount]</span></span><br><span class="line">        DataSet&lt;Tuple2&lt;Integer, Double&gt;&gt; userAmount = env.fromCollection(rawUserAount);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 转换为map集合类型的DataSet</span></span><br><span class="line">        DataSet&lt;HashMap&lt;Integer, String&gt;&gt; userInfoBroadCast = userInfoBroadCastData.map(<span class="keyword">new</span> MapFunction&lt;Tuple2&lt;Integer, String&gt;, HashMap&lt;Integer, String&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> HashMap&lt;Integer, String&gt; <span class="title">map</span><span class="params">(Tuple2&lt;Integer, String&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                HashMap&lt;Integer, String&gt; userInfo = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">                userInfo.put(value.f0, value.f1);</span><br><span class="line">                <span class="keyword">return</span> userInfo;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">       DataSet&lt;String&gt; result = userAmount.map(<span class="keyword">new</span> RichMapFunction&lt;Tuple2&lt;Integer, Double&gt;, String&gt;() &#123;</span><br><span class="line">            <span class="comment">// 存放广播变量返回的list集合数据</span></span><br><span class="line">            List&lt;HashMap&lt;String, String&gt;&gt; broadCastList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="comment">// 存放广播变量的值</span></span><br><span class="line">            HashMap&lt;String, String&gt; allMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                <span class="comment">//获取广播数据,返回的是一个list集合</span></span><br><span class="line">                <span class="keyword">this</span>.broadCastList = getRuntimeContext().getBroadcastVariable(<span class="string">"userInfo"</span>);</span><br><span class="line">                <span class="keyword">for</span> (HashMap&lt;String, String&gt; value : broadCastList) &#123;</span><br><span class="line">                    allMap.putAll(value);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(Tuple2&lt;Integer, Double&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String userName = allMap.get(value.f0);</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"用户id： "</span> + value.f0 + <span class="string">" | "</span>+ <span class="string">"用户名： "</span> + userName + <span class="string">" | "</span> + <span class="string">"购买金额： "</span> + value.f1;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).withBroadcastSet(userInfoBroadCast, <span class="string">"userInfo"</span>);</span><br><span class="line"></span><br><span class="line">        result.print();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="分布式缓存"><a href="#分布式缓存" class="headerlink" title="分布式缓存"></a>分布式缓存</h2><h3 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h3><p>Flink提供了一个分布式缓存(distributed cache),类似于Hadoop，以使文件在本地可被用户函数的并行实例访问。分布式缓存的工作机制是为程序注册一个文件或目录(本地或者远程文件系统，如HDFS等)，通过ExecutionEnvironment注册一个缓存文件，并起一个别名。当程序执行的时候，Flink会自动把注册的文件或目录复制到所有TaskManager节点的本地文件系统，用户可以通过注册是起的别名来查找文件或目录，然后在TaskManager节点的本地文件系统访问该文件。</p><p>分布式缓存的使用步骤：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// 注册一个HDFS文件</span></span><br><span class="line">env.registerCachedFile(<span class="string">"hdfs:///path/to/your/file"</span>, <span class="string">"hdfsFile"</span>)</span><br><span class="line"><span class="comment">// 注册一个本地文件</span></span><br><span class="line">env.registerCachedFile(<span class="string">"file:///path/to/exec/file"</span>, <span class="string">"localExecFile"</span>, <span class="keyword">true</span>)</span><br><span class="line"><span class="comment">// 访问数据</span></span><br><span class="line">getRuntimeContext().getDistributedCache().getFile(<span class="string">"hdfsFile"</span>);</span><br></pre></td></tr></table></figure><p>获取缓存文件的方式和广播变量相似，也是实现RichFunction接口，并通过RichFunction接口获得RuntimeContext对象，然后通过RuntimeContext提供的接口获取对应的本地缓存文件。</p><h3 id="使用Demo-1"><a href="#使用Demo-1" class="headerlink" title="使用Demo"></a>使用Demo</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributeCacheExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//获取运行环境</span></span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         *  注册一个本地文件</span></span><br><span class="line"><span class="comment">         *   文件内容为：</span></span><br><span class="line"><span class="comment">         *   1,"jack"</span></span><br><span class="line"><span class="comment">         *   2,"tom"</span></span><br><span class="line"><span class="comment">         *   3,"Bob"</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        env.registerCachedFile(<span class="string">"file:///E://userinfo.txt"</span>, <span class="string">"localFileUserInfo"</span>, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;Tuple2&lt;Integer,Double&gt;&gt; rawUserAount = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        rawUserAount.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>,<span class="number">1000.00</span>));</span><br><span class="line">        rawUserAount.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">2</span>,<span class="number">500.20</span>));</span><br><span class="line">        rawUserAount.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">3</span>,<span class="number">800.50</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理数据：用户id，用户购买金额 ，[UserId,amount]</span></span><br><span class="line">        DataSet&lt;Tuple2&lt;Integer, Double&gt;&gt; userAmount = env.fromCollection(rawUserAount);</span><br><span class="line"></span><br><span class="line">        DataSet&lt;String&gt; result= userAmount.map(<span class="keyword">new</span> RichMapFunction&lt;Tuple2&lt;Integer, Double&gt;, String&gt;() &#123;</span><br><span class="line">            <span class="comment">// 保存缓存数据</span></span><br><span class="line">            HashMap&lt;String, String&gt; allMap = <span class="keyword">new</span> HashMap&lt;String, String&gt;();</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                <span class="comment">// 获取分布式缓存的数据</span></span><br><span class="line">                File userInfoFile = getRuntimeContext().getDistributedCache().getFile(<span class="string">"localFileUserInfo"</span>);</span><br><span class="line">                List&lt;String&gt; userInfo = FileUtils.readLines(userInfoFile);</span><br><span class="line">                <span class="keyword">for</span> (String value : userInfo) &#123;</span><br><span class="line"></span><br><span class="line">                    String[] split = value.split(<span class="string">","</span>);</span><br><span class="line">                    allMap.put(split[<span class="number">0</span>], split[<span class="number">1</span>]);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(Tuple2&lt;Integer, Double&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String userName = allMap.get(value.f0);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> <span class="string">"用户id： "</span> + value.f0 + <span class="string">" | "</span> + <span class="string">"用户名： "</span> + userName + <span class="string">" | "</span> + <span class="string">"购买金额： "</span> + value.f1;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        result.print();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要讲解了Flink DataSet API的基本使用。首先介绍了一个DataSet API的WordCount案例，接着介绍了DataSet API的数据源与Sink操作，以及基本的使用。然后对每一个转换操作进行了详细的解释，并给出了具体的使用案例。最后讲解了广播变量和分布式缓存的概念，并就如何使用这两种高级功能，提供了完整的Demo案例。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink DataStream API 中的多面手——Process Function详解</title>
      <link href="/2020/04/30/Flink-DataStream-API-%E4%B8%AD%E7%9A%84%E5%A4%9A%E9%9D%A2%E6%89%8B%E2%80%94%E2%80%94Process-Function%E8%AF%A6%E8%A7%A3/"/>
      <url>/2020/04/30/Flink-DataStream-API-%E4%B8%AD%E7%9A%84%E5%A4%9A%E9%9D%A2%E6%89%8B%E2%80%94%E2%80%94Process-Function%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>在<a href="https://mp.weixin.qq.com/s/ycz_N5m6RjsW9ZBhNMvACw" target="_blank" rel="noopener">Flink的时间与watermarks详解</a>这篇文章中，阐述了Flink的时间与水位线的相关内容。你可能不禁要发问，该如何访问时间戳和水位线呢？首先通过普通的DataStream API是无法访问的，需要借助Flink提供的一个底层的API——Process  Function。Process Function不仅能够访问时间戳与水位线，而且还可以注册在将来的某个特定时间触发的计时器(timers)。除此之外，还可以将数据通过Side Outputs发送到多个输出流中。这样以来，可以实现数据分流的功能，同时也是处理迟到数据的一种方式。下面我们将从源码入手，结合具体的使用案例来说明该如何使用Process  Function。</p><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Flink提供了很多Process Function，每种Process Function都有各自的功能，这些Process Function主要包括：</p><blockquote><ul><li><p>ProcessFunction</p></li><li><p>KeyedProcessFunction</p></li><li><p>CoProcessFunction</p></li><li><p>ProcessJoinFunction</p></li><li><p>ProcessWindowFunction</p></li><li><p>ProcessAllWindowFunction</p></li><li><p>BaseBroadcastProcessFunction</p><pre><code>* KeyedBroadcastProcessFunction* BroadcastProcessFunction</code></pre></li></ul></blockquote><p>继承关系图如下：</p><p><img src="//jiamaoxiang.top/2020/04/30/Flink-DataStream-API-中的多面手——Process-Function详解/processfunction.png" alt></p><p>从上面的继承关系中可以看出，都实现了RichFunction接口，所以支持使用<code>open()</code>、<code>close()</code>、<code>getRuntimeContext()</code>等方法的调用。从名字上可以看出，这些函数都有不同的适用场景，但是基本的功能是类似的，下面会以KeyedProcessFunction为例来讨论这些函数的通用功能。</p><h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h2><h3 id="KeyedProcessFunction"><a href="#KeyedProcessFunction" class="headerlink" title="KeyedProcessFunction"></a>KeyedProcessFunction</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 处理KeyedStream流的低级API函数</span></span><br><span class="line"><span class="comment"> * 对于输入流中的每个元素都会触发调用processElement方法.该方法会产生0个或多个输出.</span></span><br><span class="line"><span class="comment"> * 其实现类可以通过Context访问数据的时间戳和计时器(timers).当计时器(timers)触发时，会回调onTimer方法.</span></span><br><span class="line"><span class="comment"> * onTimer方法会产生0个或者多个输出，并且会注册一个未来的计时器.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 注意：如果要访问keyed state和计时器(timers)，必须在KeyedStream上使用KeyedProcessFunction.</span></span><br><span class="line"><span class="comment"> * 另外，KeyedProcessFunction的父类AbstractRichFunction实现了RichFunction接口，所以，可以使用</span></span><br><span class="line"><span class="comment"> * open()，close()及getRuntimeContext()方法.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;K&gt; key的类型</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;I&gt; 输入元素的数据类型</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;O&gt; 输出元素的数据类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyedProcessFunction</span>&lt;<span class="title">K</span>, <span class="title">I</span>, <span class="title">O</span>&gt; <span class="keyword">extends</span> <span class="title">AbstractRichFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 处理输入流中的每个元素</span></span><br><span class="line"><span class="comment"> * 该方法会输出0个或者多个输出，类似于FlatMap的功能</span></span><br><span class="line"><span class="comment"> * 除此之外，该方法还可以更新内部状态或者设置计时器(timer)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> value 输入元素</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> ctx  Context，可以访问输入元素的时间戳，并其可以获取一个时间服务器(TimerService)，用于注册计时器(timers)并查询时间</span></span><br><span class="line"><span class="comment"> *  Context只有在processElement被调用期间有效.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> out  返回的结果值</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(I value, Context ctx, Collector&lt;O&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 是一个回调函数，当在TimerService中注册的计时器(timers)被触发时，会回调该函数</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> timestamp 触发计时器(timers)的时间戳</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> ctx  OnTimerContext，允许访问时间戳，TimeDomain枚举类提供了两种时间类型：</span></span><br><span class="line"><span class="comment"> * EVENT_TIME与PROCESSING_TIME</span></span><br><span class="line"><span class="comment"> * 并其可以获取一个时间服务器(TimerService)，用于注册计时器(timers)并查询时间</span></span><br><span class="line"><span class="comment"> * OnTimerContext只有在onTimer方法被调用期间有效</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> out 结果输出</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;O&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 仅仅在processElement()方法或者onTimer方法被调用期间有效</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Context</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 当前被处理元素的时间戳，或者是触发计时器(timers)时的时间戳</span></span><br><span class="line"><span class="comment"> * 该值可能为null，比如当程序中设置的时间语义为：TimeCharacteristic#ProcessingTime</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> Long <span class="title">timestamp</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 访问时间和注册的计时器(timers)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TimerService <span class="title">timerService</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将元素输出到side output (侧输出)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> outputTag 侧输出的标记</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> value 输出的记录</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;X&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> &lt;X&gt; <span class="function"><span class="keyword">void</span> <span class="title">output</span><span class="params">(OutputTag&lt;X&gt; outputTag, X value)</span></span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 获取被处理元素的key</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> K <span class="title">getCurrentKey</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 当onTimer方法被调用时，才可以使用OnTimerContext</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">OnTimerContext</span> <span class="keyword">extends</span> <span class="title">Context</span> </span>&#123;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 触发计时器(timers)的时间类型，包括两种：EVENT_TIME与PROCESSING_TIME</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TimeDomain <span class="title">timeDomain</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 获取触发计时器(timer)元素的key</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> K <span class="title">getCurrentKey</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的源码中，主要有两个方法，分析如下：</p><ul><li>processElement(I value, Context ctx, Collector<o> out)</o></li></ul><p>该方法会对流中的每条记录都调用一次，输出0个或者多个元素，类似于FlatMap的功能，通过Collector将结果发出。除此之外，该函数有一个Context 参数，用户可以通过Context 访问时间戳、当前记录的key值以及TimerService(关于TimerService，下面会详细解释)。另外还可以使用output方法将数据发送到side output，实现分流或者处理迟到数据的功能。</p><ul><li>onTimer(long timestamp, OnTimerContext ctx, Collector<o> out)</o></li></ul><p>该方法是一个回调函数，当在TimerService中注册的计时器(timers)被触发时，会回调该函数。其中<code>@param timestamp</code>参数表示触发计时器(timers)的时间戳，Collector可以将记录发出。细心的你可能会发现，这两个方法都有一个上下文参数，上面的方法传递的是Context 参数，onTimer方法传递的是OnTimerContext参数，这两个参数对象可以实现相似的功能。OnTimerContext还可以返回触发计时器的时间域(EVENT_TIME与PROCESSING_TIME)。</p><h3 id="TimerService"><a href="#TimerService" class="headerlink" title="TimerService"></a>TimerService</h3><p>在KeyedProcessFunction源码中，使用TimerService来访问时间和计时器，下面来看一下源码：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">TimerService</span> </span>&#123;</span><br><span class="line">String UNSUPPORTED_REGISTER_TIMER_MSG = <span class="string">"Setting timers is only supported on a keyed streams."</span>;</span><br><span class="line">String UNSUPPORTED_DELETE_TIMER_MSG = <span class="string">"Deleting timers is only supported on a keyed streams."</span>;</span><br><span class="line"><span class="comment">// 返回当前的处理时间</span></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">currentProcessingTime</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">// 返回当前event-time水位线(watermark)</span></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">currentWatermark</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 注册一个计时器(timers)，当processing time的时间等于该计时器时钟时会被调用</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> time</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">registerProcessingTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 注册一个计时器(timers),当event time的水位线(watermark)到达该时间时会被触发</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> time</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">registerEventTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据给定的触发时间(trigger time)来删除processing-time计时器</span></span><br><span class="line"><span class="comment"> * 如果这个timer不存在，那么该方法不会起作用，</span></span><br><span class="line"><span class="comment"> * 即该计时器(timer)之前已经被注册了，并且没有过时</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> time</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">deleteProcessingTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br><span class="line">    </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据给定的触发时间(trigger time)来删除event-time 计时器</span></span><br><span class="line"><span class="comment"> * 如果这个timer不存在，那么该方法不会起作用，</span></span><br><span class="line"><span class="comment"> * 即该计时器(timer)之前已经被注册了，并且没有过时</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> time</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">deleteEventTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>TimerService提供了以下几种方法：</p><ul><li>currentProcessingTime()</li></ul><p>返回当前的处理时间</p><ul><li>currentWatermark()</li></ul><p>返回当前event-time水位线(watermark)时间戳</p><ul><li>registerProcessingTimeTimer(long time)</li></ul><p>针对当前key，注册一个processing time计时器(timers)，当processing time的时间等于该计时器时钟时会被调用</p><ul><li>registerEventTimeTimer(long time)</li></ul><p>针对当前key，注册一个event time计时器(timers)，当水位线时间戳大于等于该计时器时钟时会被调用</p><ul><li>deleteProcessingTimeTimer(long time)</li></ul><p>针对当前key，删除一个之前注册过的processing time计时器(timers)，如果这个timer不存在，那么该方法不会起作用</p><ul><li>deleteEventTimeTimer(long time)</li></ul><p>针对当前key，删除一个之前注册过的event time计时器(timers)，如果这个timer不存在，那么该方法不会起作用</p><p>当计时器触发时，会回调onTimer()函数，系统对于ProcessElement()方法和onTimer()方法的调用是同步的</p><p>注意:上面的源码中有两个Error 信息,这就说明计时器只能在keyed streams上使用，常见的用途是在某些key值不在使用后清除keyed state，或者实现一些基于时间的自定义窗口逻辑。如果要在一个非KeyedStream上使用计时器，可以使用KeySelector返回一个固定的分区值(比如返回一个常数)，这样所有的数据只会发送到一个分区。</p><h2 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h2><p>下面将使用Process Function的side output功能进行分流处理，具体代码如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessFunctionExample</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义side output标签</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;UserBehaviors&gt; buyTags = <span class="keyword">new</span> OutputTag&lt;UserBehaviors&gt;(<span class="string">"buy"</span>) &#123;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;UserBehaviors&gt; cartTags = <span class="keyword">new</span> OutputTag&lt;UserBehaviors&gt;(<span class="string">"cart"</span>) &#123;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;UserBehaviors&gt; favTags = <span class="keyword">new</span> OutputTag&lt;UserBehaviors&gt;(<span class="string">"fav"</span>) &#123;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitStreamFunction</span> <span class="keyword">extends</span> <span class="title">ProcessFunction</span>&lt;<span class="title">UserBehaviors</span>, <span class="title">UserBehaviors</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(UserBehaviors value, Context ctx, Collector&lt;UserBehaviors&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">switch</span> (value.behavior) &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">"buy"</span>:</span><br><span class="line">                    ctx.output(buyTags, value);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">"cart"</span>:</span><br><span class="line">                    ctx.output(cartTags, value);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">"fav"</span>:</span><br><span class="line">                    ctx.output(favTags, value);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">default</span>:</span><br><span class="line">                    out.collect(value);</span><br><span class="line">           &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 模拟数据源[userId,behavior,product]</span></span><br><span class="line">        SingleOutputStreamOperator&lt;UserBehaviors&gt; splitStream = env.fromElements(</span><br><span class="line">                <span class="keyword">new</span> UserBehaviors(<span class="number">1L</span>, <span class="string">"buy"</span>, <span class="string">"iphone"</span>),</span><br><span class="line">                <span class="keyword">new</span> UserBehaviors(<span class="number">1L</span>, <span class="string">"cart"</span>, <span class="string">"huawei"</span>),</span><br><span class="line">                <span class="keyword">new</span> UserBehaviors(<span class="number">1L</span>, <span class="string">"buy"</span>, <span class="string">"logi"</span>),</span><br><span class="line">                <span class="keyword">new</span> UserBehaviors(<span class="number">1L</span>, <span class="string">"fav"</span>, <span class="string">"oppo"</span>),</span><br><span class="line">                <span class="keyword">new</span> UserBehaviors(<span class="number">2L</span>, <span class="string">"buy"</span>, <span class="string">"huawei"</span>),</span><br><span class="line">                <span class="keyword">new</span> UserBehaviors(<span class="number">2L</span>, <span class="string">"buy"</span>, <span class="string">"onemore"</span>),</span><br><span class="line">                <span class="keyword">new</span> UserBehaviors(<span class="number">2L</span>, <span class="string">"fav"</span>, <span class="string">"iphone"</span>)).process(<span class="keyword">new</span> SplitStreamFunction());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取分流之后购买行为的数据</span></span><br><span class="line">        splitStream.getSideOutput(buyTags).print(<span class="string">"data_buy"</span>);</span><br><span class="line">        <span class="comment">//获取分流之后加购行为的数据</span></span><br><span class="line">        splitStream.getSideOutput(cartTags).print(<span class="string">"data_cart"</span>);</span><br><span class="line">        <span class="comment">//获取分流之后收藏行为的数据</span></span><br><span class="line">        splitStream.getSideOutput(favTags).print(<span class="string">"data_fav"</span>);</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"ProcessFunctionExample"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先介绍了Flink提供的几种底层Process Function API，这些API可以访问时间戳和水位线，同时支持注册一个计时器，进行调用回调函数onTimer()。接着从源码的角度解读了这些API的共同部分，详细解释了每个方法的具体含义和使用方式。最后，给出了一个Process Function常见使用场景案例，使用其实现分流处理。除此之外，用户还可以使用这些函数，通过注册计时器，在回调函数中定义处理逻辑，使用非常的灵活。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink内部Exactly Once三板斧:状态、状态后端与检查点</title>
      <link href="/2020/04/25/Flink%E5%86%85%E9%83%A8Exactly-Once%E4%B8%89%E6%9D%BF%E6%96%A7-%E7%8A%B6%E6%80%81%E3%80%81%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF%E4%B8%8E%E6%A3%80%E6%9F%A5%E7%82%B9/"/>
      <url>/2020/04/25/Flink%E5%86%85%E9%83%A8Exactly-Once%E4%B8%89%E6%9D%BF%E6%96%A7-%E7%8A%B6%E6%80%81%E3%80%81%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF%E4%B8%8E%E6%A3%80%E6%9F%A5%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p>Flink是一个分布式的流处理引擎，而流处理的其中一个特点就是7X24。那么，如何保障Flink作业的持续运行呢？Flink的内部会将应用状态(state)存储到本地内存或者嵌入式的kv数据库(RocksDB)中，由于采用的是分布式架构，Flink需要对本地生成的状态进行持久化存储，以避免因应用或者节点机器故障等原因导致数据的丢失，Flink是通过checkpoint(检查点)的方式将状态写入到远程的持久化存储，从而就可以实现不同语义的结果保障。通过本文，你可以了解到什么是Flink的状态，Flink的状态是怎么存储的，Flink可选择的状态后端(statebackend)有哪些，什么是全局一致性检查点，Flink内部如何通过检查点实现Exactly Once的结果保障。另外，本文内容较长，建议关注加收藏。</p><a id="more"></a><h2 id="什么是状态"><a href="#什么是状态" class="headerlink" title="什么是状态"></a>什么是状态</h2><h3 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h3><p>关于什么是状态，我们先不做过多的分析。首先看一个代码案例，其中案例1是Spark的WordCount代码，案例2是Flink的WorkCount代码。</p><ul><li>案例1：Spark WC</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">object WordCount &#123;</span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args:Array[String])</span></span>&#123;</span><br><span class="line">  val conf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line">  val ssc = <span class="keyword">new</span> StreamingContext(conf, Seconds(<span class="number">5</span>))</span><br><span class="line">  val lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">  val words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">  val pairs = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">  val wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line">  wordCounts.print()</span><br><span class="line">  ssc.start()</span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输入：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">C:\WINDOWS\system32&gt;nc -lp 9999</span><br><span class="line">hello spark</span><br><span class="line">hello spark</span><br></pre></td></tr></table></figure><p>输出：</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/spark.png" alt></p><ul><li>案例2：Flink WC</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(<span class="number">1</span>);</span><br><span class="line">        DataStreamSource&lt;String&gt; streamSource = env.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>);</span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String,Integer&gt;&gt; words = streamSource.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String,Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] splits = value.split(<span class="string">"\\s"</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word : splits) &#123;</span><br><span class="line">                    out.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        words.keyBy(<span class="number">0</span>).sum(<span class="number">1</span>).print();</span><br><span class="line">        env.execute(<span class="string">"WC"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输入：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">C:\WINDOWS\system32&gt;nc -lp <span class="number">9999</span></span><br><span class="line">hello Flink</span><br><span class="line">hello Flink</span><br></pre></td></tr></table></figure><p>输出：</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/Flink.png" alt></p><p>从上面的两个例子可以看出，在使用Spark进行词频统计时，当前的统计结果不受历史统计结果的影响，只计算接收的当前数据的结果，这个就可以理解为无状态的计算。再来看一下Flink的例子，可以看出当第二次词频统计时，把第一次的结果值也统计在了一起，即Flink把上一次的计算结果保存在了状态里，第二次计算的时候会先拿到上一次的结果状态，然后结合新到来的数据再进行计算，这就可以理解成有状态的计算，如下图所示。</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/statetask.png" alt></p><h3 id="状态的类别"><a href="#状态的类别" class="headerlink" title="状态的类别"></a>状态的类别</h3><p>Flink提供了两种基本类型的状态：分别是 <code>Keyed State</code> 和<code>Operator State</code>。根据不同的状态管理方式，每种状态又有两种存在形式，分别为：<code>managed(托管状态)</code>和<code>raw(原生状态)</code>。具体如下表格所示。需要注意的是，由于Flink推荐使用managed state，所以下文主要讨论managed state，对于raw state，本文不会做过多的讨论。</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/state_kinds.png" alt></p><h4 id="managed-state-amp-raw-state区别"><a href="#managed-state-amp-raw-state区别" class="headerlink" title="managed state &amp; raw state区别"></a>managed state &amp; raw state区别</h4><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/managedstate_rowstate.png" alt></p><h4 id="Keyed-State-amp-Operator-State"><a href="#Keyed-State-amp-Operator-State" class="headerlink" title="Keyed State &amp; Operator State"></a>Keyed State &amp; Operator State</h4><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/keystate_operatorstate.png" alt></p><h3 id="Keyed-State"><a href="#Keyed-State" class="headerlink" title="Keyed State"></a>Keyed State</h3><p>Keyed State只能由作用在KeyedStream上面的函数使用，该状态与某个key进行绑定，即每一个key对应一个state。Keyed State按照key进行维护和访问的，Flink会为每一个Key都维护一个状态实例，该状态实例总是位于处理该key记录的算子任务上，因此同一个key的记录可以访问到一样的状态。如下图所示，可以通过在一条流上使用keyBy()方法来生成一个KeyedStream。Flink提供了很多种keyed state，具体如下：</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/keyedstate.png" alt></p><ul><li><strong>ValueState<t></t></strong></li></ul><p>用于保存类型为T的单个值。用户可以通过ValueState.value()来获取该状态值，通过ValueState.update()来更新该状态。使用<code>ValueStateDescriptor</code>来获取状态句柄。</p><ul><li><strong>ListState<t></t></strong></li></ul><p>用于保存类型为T的元素列表，即key的状态值是一个列表。用户可以使用ListState.add()或者ListState.addAll()将新元素添加到列表中，通过ListState.get()访问状态元素，该方法会返回一个可遍历所有元素的Iterable<t>对象，注意ListState不支持删除单个元素，但是用户可以使用update(List<t> values)来更新整个列表。使用 <code>ListStateDescriptor</code>来获取状态句柄。</t></t></p><ul><li><strong>ReducingState<t></t></strong></li></ul><p>调用add()方法添加值时，会立即返回一个使用ReduceFunction聚合后的值，用户可以使用ReducingState.get()来获取该状态值。使用 <code>ReducingStateDescriptor</code>来获取状态句柄。</p><ul><li><strong>AggregatingState&lt;IN, OUT&gt;</strong></li></ul><p>与ReducingState<t>类似，不同的是它使用的是AggregateFunction来聚合内部的值，AggregatingState.get()方法会计算最终的结果并将其返回。使用 <code>AggregatingStateDescriptor</code>来获取状态句柄</t></p><ul><li><strong>MapState&lt;UK, UV&gt;</strong></li></ul><p>用于保存一组key、value的映射，类似于java的Map集合。用户可以通过get(UK key)方法获取key对应的状态，可以通过put(UK k,UV value)方法添加一个键值，可以通过remove(UK key)删除给定key的值，可以通过contains(UK key)判断是否存在对应的key。使用 <code>MapStateDescriptor</code>来获取状态句柄。</p><ul><li><strong>FoldingState&lt;T, ACC&gt;</strong></li></ul><p>在Flink 1.4的版本中标记过时，在未来的版本中会被移除，使用AggregatingState进行代替。</p><p>值得注意的是，上面的状态原语都支持通过State.clear()方法来进行清除状态。另外，上述的状态原语仅用于与状态进行交互，真正的状态是存储在状态后端（后面会介绍状态后端）的，通过该状态原语相当于持有了状态的句柄(handle)。</p><h4 id="keyed-State使用案例"><a href="#keyed-State使用案例" class="headerlink" title="keyed State使用案例"></a>keyed State使用案例</h4><p>下面给出一个MapState的使用案例，关于ValueState的使用情况可以参考官网，具体如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapStateExample</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//统计每个用户每种行为的个数</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UserBehaviorCnt</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>&lt;<span class="title">Tuple3</span>&lt;<span class="title">Long</span>, <span class="title">String</span>, <span class="title">String</span>&gt;, <span class="title">Tuple3</span>&lt;<span class="title">Long</span>, <span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//定义一个MapState句柄</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">transient</span> MapState&lt;String, Integer&gt; behaviorCntState;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化状态</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>.open(parameters);</span><br><span class="line">            MapStateDescriptor&lt;String, Integer&gt; userBehaviorMapStateDesc = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">                    <span class="string">"userBehavior"</span>,  <span class="comment">// 状态描述符的名称</span></span><br><span class="line">                    TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;String&gt;() &#123;&#125;),  <span class="comment">// MapState状态的key的数据类型</span></span><br><span class="line">                    TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Integer&gt;() &#123;&#125;)  <span class="comment">// MapState状态的value的数据类型</span></span><br><span class="line">            );</span><br><span class="line">            behaviorCntState = getRuntimeContext().getMapState(userBehaviorMapStateDesc); <span class="comment">// 获取状态</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Tuple3&lt;Long, String, String&gt; value, Collector&lt;Tuple3&lt;Long, String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            Integer behaviorCnt = <span class="number">1</span>;</span><br><span class="line">            <span class="comment">// 如果当前状态包括该行为，则+1</span></span><br><span class="line">            <span class="keyword">if</span> (behaviorCntState.contains(value.f1)) &#123;</span><br><span class="line">                behaviorCnt = behaviorCntState.get(value.f1) + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 更新状态</span></span><br><span class="line">            behaviorCntState.put(value.f1, behaviorCnt);</span><br><span class="line">            out.collect(Tuple3.of(value.f0, value.f1, behaviorCnt));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 模拟数据源[userId,behavior,product]</span></span><br><span class="line">        DataStreamSource&lt;Tuple3&lt;Long, String, String&gt;&gt; userBehaviors = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"buy"</span>, <span class="string">"iphone"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"cart"</span>, <span class="string">"huawei"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"buy"</span>, <span class="string">"logi"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"fav"</span>, <span class="string">"oppo"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2L</span>, <span class="string">"buy"</span>, <span class="string">"huawei"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2L</span>, <span class="string">"buy"</span>, <span class="string">"onemore"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2L</span>, <span class="string">"fav"</span>, <span class="string">"iphone"</span>));</span><br><span class="line">        userBehaviors</span><br><span class="line">                .keyBy(<span class="number">0</span>)</span><br><span class="line">                .flatMap(<span class="keyword">new</span> UserBehaviorCnt())</span><br><span class="line">                .print();</span><br><span class="line">        env.execute(<span class="string">"MapStateExample"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果输出：</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/MapStateExample.png" alt></p><h4 id="状态的生命周期管理-TTL"><a href="#状态的生命周期管理-TTL" class="headerlink" title="状态的生命周期管理(TTL)"></a>状态的生命周期管理(TTL)</h4><p>对于任何类型Keyed State都可以设定状态的生命周期（TTL）,即状态的存活时间，以确保能够在规定时间内及时地清理状态数据。如果配置了状态的TTL，那么当状态过期时，存储的状态会被清除。状态生命周期功能可以通过StateTtlConfig配置，然后将StateTtlConfig配置传入StateDescriptor中的enableTimeToLive方法中即可。代码示例如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">                 <span class="comment">// 指定TTL时长为10S</span></span><br><span class="line">                .newBuilder(Time.seconds(<span class="number">10</span>))</span><br><span class="line">                 <span class="comment">// 只对创建和写入操作有效</span></span><br><span class="line">                .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)</span><br><span class="line">                 <span class="comment">// 不返回过期的数据</span></span><br><span class="line">                .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) </span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化状态</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>.open(parameters);</span><br><span class="line">            MapStateDescriptor&lt;String, Integer&gt; userBehaviorMapStateDesc = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">                    <span class="string">"userBehavior"</span>,  <span class="comment">// 状态描述符的名称</span></span><br><span class="line">                    TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;String&gt;() &#123;&#125;),  <span class="comment">// MapState状态的key的数据类型</span></span><br><span class="line">                    TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Integer&gt;() &#123;&#125;)  <span class="comment">// MapState状态的value的数据类型</span></span><br><span class="line"></span><br><span class="line">            );</span><br><span class="line">            <span class="comment">// 设置stateTtlConfig</span></span><br><span class="line">            userBehaviorMapStateDesc.enableTimeToLive(ttlConfig);</span><br><span class="line">            behaviorCntState = getRuntimeContext().getMapState(userBehaviorMapStateDesc); <span class="comment">// 获取状态</span></span><br><span class="line"></span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>在StateTtlConfig创建时，newBuilder方法是必须要指定的，newBuilder中设定过期时间的参数。对于其他参数都是可选的或使用默认值。其中setUpdateType方法中传入的类型有三种：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> UpdateType &#123;</span><br><span class="line"><span class="comment">//禁用TTL,永远不会过期</span></span><br><span class="line">Disabled,</span><br><span class="line">    <span class="comment">// 创建和写入时更新TTL</span></span><br><span class="line">OnCreateAndWrite,</span><br><span class="line"><span class="comment">// 与OnCreateAndWrite类似，但是在读操作时也会更新TTL</span></span><br><span class="line">OnReadAndWrite</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>值得注意的是，过期的状态数据根据UpdateType参数进行配置，只有被写入或者读取的时间才会更新TTL，也就是说如果某个状态指标一直不被使用或者更新，则永远不会触发对该状态数据的清理操作，这种情况可能会导致系统中的状态数据越来越大。目前用户可以使用StateTtlConfig.cleanupFullSnapshot设定当触发State Snapshot的时候清理状态数据，但是改配置不适合用于RocksDB做增量Checkpointing的操作。</p><p>上面的StateTtlConfig创建时，可以指定setStateVisibility，用于状态的可见性配置，根据过期数据是否被清理来确定是否返回状态数据。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 是否返回过期的数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> StateVisibility &#123;</span><br><span class="line"><span class="comment">//如果数据没有被清理，就可以返回</span></span><br><span class="line">ReturnExpiredIfNotCleanedUp,</span><br><span class="line"><span class="comment">//永远不返回过期的数据,默认值</span></span><br><span class="line">NeverReturnExpired</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Operator-State"><a href="#Operator-State" class="headerlink" title="Operator State"></a>Operator State</h3><p>Operator State的作用于是某个算子任务，这意味着所有在同一个并行任务之内的记录都能访问到相同的状态 。算子状态不能通过其他任务访问，无论该任务是相同的算子。如下图所示。</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/operatorstate.png" alt></p><p>Operator State是一种non-keyed state，与并行的操作算子实例相关联，例如在Kafka Connector中，每个Kafka消费端算子实例都对应到Kafka的一个分区中，维护Topic分区和Offsets偏移量作为算子的Operator State。在Flink中可以实现ListCheckpointed<t extends serializable>接口或者CheckpointedFunction 接口来实现一个Operator State。</t></p><p>首先，我们先看一下这两个接口的具体实现，然后再给出这两种接口的具体使用案例。先看一下ListCheckpointed接口的源码，如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ListCheckpointed</span>&lt;<span class="title">T</span> <span class="keyword">extends</span> <span class="title">Serializable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 获取某个算子实例的当前状态，该状态包括该算子实例之前被调用时的所有结果</span></span><br><span class="line"><span class="comment"> * 以列表的形式返回一个函数状态的快照</span></span><br><span class="line"><span class="comment"> * Flink触发生成检查点时调用该方法</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> checkpointId checkpoint的ID,是一个唯一的、单调递增的值</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> timestamp Job Manager触发checkpoint时的时间戳</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span>  返回一个operator state list,如果为null时,返回空list</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">List&lt;T&gt; <span class="title">snapshotState</span><span class="params">(<span class="keyword">long</span> checkpointId, <span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 初始化函数状态时调用，可能是在作业启动时或者故障恢复时</span></span><br><span class="line"><span class="comment"> * 根据提供的列表恢复函数状态</span></span><br><span class="line"><span class="comment"> * 注意：当实现该方法时，需要在RichFunction#open()方法之前调用该方法</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> state 被恢复算子实例的state列表 ，可能为空</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">restoreState</span><span class="params">(List&lt;T&gt; state)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用Operator ListState时，在进行扩缩容时，重分布的策略(状态恢复的模式)如下图所示：</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/operatorliststate%E6%89%A9%E7%BC%A9%E5%AE%B9.png" alt></p><p>上面的重分布策略为<strong>Even-split Redistribution</strong>，即每个算子实例中含有部分状态元素的List列表，整个状态数据是所有List列表的合集。当触发restore/redistribution动作时，通过将状态数据平均分配成与算子并行度相同数量的List列表，每个task实例中有一个List，其可以为空或者含有多个元素。</p><p>我们再来看一下CheckpointedFunction接口，源码如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 会在生成检查点之前调用</span></span><br><span class="line"><span class="comment"> * 该方法的目的是确保检查点开始之前所有状态对象都已经更新完毕</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> context 使用FunctionSnapshotContext作为参数</span></span><br><span class="line"><span class="comment"> *                从FunctionSnapshotContext可以获取checkpoint的元数据信息，</span></span><br><span class="line"><span class="comment"> *                比如checkpoint编号，JobManager在初始化checkpoint时的时间戳</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 在创建checkpointedFunction的并行实例时被调用，</span></span><br><span class="line"><span class="comment"> * 在应用启动或者故障重启时触发该方法的调用</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> context 传入FunctionInitializationContext对象，</span></span><br><span class="line"><span class="comment"> *                   可以使用该对象访问OperatorStateStore和 KeyedStateStore对象，</span></span><br><span class="line"><span class="comment"> *                   这两个对象可以获取状态的句柄，即通过Flink runtime来注册函数状态并返回state对象</span></span><br><span class="line"><span class="comment"> *                   比如：ValueState、ListState等</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CheckpointedFunction接口是用于指定有状态函数的最底层的接口，该接口提供了用于注册和维护keyed state 与operator state的hook(即可以同时使用keyed state 和operator state)，另外也是唯一支持使用list union state。关于Union List State,使用的是Flink为Operator state提供的另一种重分布的策略：<strong>Union Redistribution</strong>，即每个算子实例中含有所有状态元素的List列表，当触发restore/redistribution动作时，每个算子都能够获取到完整的状态元素列表。具体如下图所示：</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/operatorunionlist.png" alt></p><h4 id="ListCheckpointed"><a href="#ListCheckpointed" class="headerlink" title="ListCheckpointed"></a>ListCheckpointed</h4><p>ListCheckpointed接口和CheckpointedFunction接口相比在灵活性上相对弱一些，只能支持List类型的状态，并且在数据恢复的时候仅支持<strong>even-redistribution</strong>策略。该接口不像Flink提供的Keyed State(比如Value State、ListState)那样直接在状态后端(state backend)注册，需要将operator state实现为成员变量，然后通过接口提供的回调函数与状态后端进行交互。使用代码案例如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ListCheckpointedExample</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UserBehaviorCnt</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>&lt;<span class="title">Tuple3</span>&lt;<span class="title">Long</span>, <span class="title">String</span>, <span class="title">String</span>&gt;, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;&gt; <span class="keyword">implements</span> <span class="title">ListCheckpointed</span>&lt;<span class="title">Long</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> Long userBuyBehaviorCnt = <span class="number">0L</span>;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Tuple3&lt;Long, String, String&gt; value, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(value.f1.equals(<span class="string">"buy"</span>))&#123;</span><br><span class="line">                userBuyBehaviorCnt ++;</span><br><span class="line">                out.collect(Tuple2.of(<span class="string">"buy"</span>,userBuyBehaviorCnt));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> List&lt;Long&gt; <span class="title">snapshotState</span><span class="params">(<span class="keyword">long</span> checkpointId, <span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">//返回单个元素的List集合，该集合元素是用户购买行为的数量</span></span><br><span class="line">            <span class="keyword">return</span> Collections.singletonList(userBuyBehaviorCnt);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">restoreState</span><span class="params">(List&lt;Long&gt; state)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">// 在进行扩缩容之后，进行状态恢复，需要把其他subtask的状态加在一起</span></span><br><span class="line">            <span class="keyword">for</span> (Long cnt : state) &#123;</span><br><span class="line">                userBuyBehaviorCnt += <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 模拟数据源[userId,behavior,product]</span></span><br><span class="line">        DataStreamSource&lt;Tuple3&lt;Long, String, String&gt;&gt; userBehaviors = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"buy"</span>, <span class="string">"iphone"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"cart"</span>, <span class="string">"huawei"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"buy"</span>, <span class="string">"logi"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"fav"</span>, <span class="string">"oppo"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2L</span>, <span class="string">"buy"</span>, <span class="string">"huawei"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2L</span>, <span class="string">"buy"</span>, <span class="string">"onemore"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2L</span>, <span class="string">"fav"</span>, <span class="string">"iphone"</span>));</span><br><span class="line"></span><br><span class="line">        userBehaviors</span><br><span class="line">                .flatMap(<span class="keyword">new</span> UserBehaviorCnt())</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"ListCheckpointedExample"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="CheckpointedFunction"><a href="#CheckpointedFunction" class="headerlink" title="CheckpointedFunction"></a>CheckpointedFunction</h4><p>CheckpointedFunction接口提供了更加丰富的操作，比如支持Union list state，可以访问keyedState，关于重分布策略，如果使用Even-split Redistribution策略，则通过context. getListState(descriptor)获取Operator State；如果使用UnionRedistribution策略，则通过context. getUnionList State(descriptor)来获取。使用案例如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CheckpointFunctionExample</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UserBehaviorCnt</span> <span class="keyword">implements</span> <span class="title">CheckpointedFunction</span>, <span class="title">FlatMapFunction</span>&lt;<span class="title">Tuple3</span>&lt;<span class="title">Long</span>, <span class="title">String</span>, <span class="title">String</span>&gt;, <span class="title">Tuple3</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>, <span class="title">Long</span>&gt;&gt; </span>&#123;</span><br><span class="line">        <span class="comment">// 统计每个operator实例的用户行为数量的本地变量</span></span><br><span class="line">        <span class="keyword">private</span> Long opUserBehaviorCnt = <span class="number">0L</span>;</span><br><span class="line">        <span class="comment">// 每个key的state,存储key对应的相关状态</span></span><br><span class="line">        <span class="keyword">private</span> ValueState&lt;Long&gt; keyedCntState;</span><br><span class="line">        <span class="comment">// 定义operator state，存储算子的状态</span></span><br><span class="line">        <span class="keyword">private</span> ListState&lt;Long&gt; opCntState;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Tuple3&lt;Long, String, String&gt; value, Collector&lt;Tuple3&lt;Long, Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (value.f1.equals(<span class="string">"buy"</span>)) &#123;</span><br><span class="line">                <span class="comment">// 更新算子状态本地变量值</span></span><br><span class="line">                opUserBehaviorCnt += <span class="number">1</span>;</span><br><span class="line">                Long keyedCount = keyedCntState.value();</span><br><span class="line">                <span class="comment">// 更新keyedstate的状态 ,判断状态是否为null，否则空指针异常</span></span><br><span class="line">                keyedCntState.update(keyedCount == <span class="keyword">null</span> ? <span class="number">1L</span> : keyedCount + <span class="number">1</span> );</span><br><span class="line">                <span class="comment">// 结果输出</span></span><br><span class="line">                out.collect(Tuple3.of(value.f0, keyedCntState.value(), opUserBehaviorCnt));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">// 使用opUserBehaviorCnt本地变量更新operator state</span></span><br><span class="line">            opCntState.clear();</span><br><span class="line">            opCntState.add(opUserBehaviorCnt);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 通过KeyedStateStore,定义keyedState的StateDescriptor描述符</span></span><br><span class="line">            ValueStateDescriptor valueStateDescriptor = <span class="keyword">new</span> ValueStateDescriptor(<span class="string">"keyedCnt"</span>, TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Long&gt;() &#123;</span><br><span class="line">            &#125;));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 通过OperatorStateStore,定义OperatorState的StateDescriptor描述符</span></span><br><span class="line">            ListStateDescriptor opStateDescriptor = <span class="keyword">new</span> ListStateDescriptor(<span class="string">"opCnt"</span>, TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Long&gt;() &#123;</span><br><span class="line">            &#125;));</span><br><span class="line">            <span class="comment">// 初始化keyed state状态值</span></span><br><span class="line">            keyedCntState = context.getKeyedStateStore().getState(valueStateDescriptor);</span><br><span class="line">            <span class="comment">// 初始化operator state状态</span></span><br><span class="line">            opCntState = context.getOperatorStateStore().getListState(opStateDescriptor);</span><br><span class="line">            <span class="comment">// 初始化本地变量operator state</span></span><br><span class="line">            <span class="keyword">for</span> (Long state : opCntState.get()) &#123;</span><br><span class="line">                opUserBehaviorCnt += state;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 模拟数据源[userId,behavior,product]</span></span><br><span class="line">        DataStreamSource&lt;Tuple3&lt;Long, String, String&gt;&gt; userBehaviors = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"buy"</span>, <span class="string">"iphone"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"cart"</span>, <span class="string">"huawei"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"buy"</span>, <span class="string">"logi"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"fav"</span>, <span class="string">"oppo"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2L</span>, <span class="string">"buy"</span>, <span class="string">"huawei"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2L</span>, <span class="string">"buy"</span>, <span class="string">"onemore"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2L</span>, <span class="string">"fav"</span>, <span class="string">"iphone"</span>));</span><br><span class="line"></span><br><span class="line">        userBehaviors</span><br><span class="line">                .keyBy(<span class="number">0</span>)</span><br><span class="line">                .flatMap(<span class="keyword">new</span> UserBehaviorCnt())</span><br><span class="line">                .print();</span><br><span class="line">        env.execute(<span class="string">"CheckpointFunctionExample"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="什么是状态后端"><a href="#什么是状态后端" class="headerlink" title="什么是状态后端"></a>什么是状态后端</h2><p>上面使用的状态都需要存储到状态后端(StateBackend)，然后在checkpoint触发时，将状态持久化到外部存储系统。Flink提供了三种类型的状态后端，分别是基于内存的状态后端(<strong>MemoryStateBackend</strong>、基于文件系统的状态后端(<strong>FsStateBackend</strong>)以及基于RockDB作为存储介质的<strong>RocksDB StateBackend</strong>。这三种类型的StateBackend都能够有效地存储Flink流式计算过程中产生的状态数据，在默认情况下Flink使用的是MemoryStateBackend，区别见下表。下面分别对每种状态后端的特点进行说明。</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/F:%5Cnpm%5Cmywebsite%5Csource_posts%5CFlink%E5%86%85%E9%83%A8Exactly-Once%E4%B8%89%E6%9D%BF%E6%96%A7-%E7%8A%B6%E6%80%81%E3%80%81%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF%E4%B8%8E%E6%A3%80%E6%9F%A5%E7%82%B9%5Cstatebackend.png" alt></p><h3 id="状态后端的类别"><a href="#状态后端的类别" class="headerlink" title="状态后端的类别"></a>状态后端的类别</h3><h4 id="MemoryStateBackend"><a href="#MemoryStateBackend" class="headerlink" title="MemoryStateBackend"></a>MemoryStateBackend</h4><p>MemoryStateBackend将状态数据全部存储在JVM堆内存中，包括用户在使用DataStream API中创建的Key/Value State，窗口中缓存的状态数据，以及触发器等数据。MemoryStateBackend具有非常快速和高效的特点，但也具有非常多的限制，最主要的就是内存的容量限制，一旦存储的状态数据过多就会导致系统内存溢出等问题，从而影响整个应用的正常运行。同时如果机器出现问题，整个主机内存中的状态数据都会丢失，进而无法恢复任务中的状态数据。因此从数据安全的角度建议用户尽可能地避免在生产环境中使用MemoryStateBackend。Flink将MemoryStateBackend作为默认状态后端。</p><p>MemoryStateBackend比较适合用于测试环境中，并用于本地调试和验证，不建议在生产环境中使用。但如果应用状态数据量不是很大，例如使用了大量的非状态计算算子，也可以在生产环境中使MemoryStateBackend.</p><h4 id="FsStateBackend"><a href="#FsStateBackend" class="headerlink" title="FsStateBackend"></a>FsStateBackend</h4><p>FsStateBackend是基于文件系统的一种状态后端，这里的文件系统可以是本地文件系统，也可以是HDFS分布式文件系统。创建FsStateBackend的构造函数如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">FsStateBackend(Path checkpointDataUri, <span class="keyword">boolean</span> asynchronousSnapshots)</span><br></pre></td></tr></table></figure><p>其中path如果为本地路径，其格式为“file:///data/flink/checkpoints”，如果path为HDFS路径，其格式为“hdfs://nameservice/flink/checkpoints”。FsStateBackend中第二个Boolean类型的参数指定是否以同步的方式进行状态数据记录，默认采用异步的方式将状态数据同步到文件系统中，异步方式能够尽可能避免在Checkpoint的过程中影响流式计算任务。如果用户想采用同步的方式进行状态数据的检查点数据，则将第二个参数指定为True即可。</p><p>相比于MemoryStateBackend, FsStateBackend更适合任务状态非常大的情况，例如应用中含有时间范围非常长的窗口计算，或Key/value State状态数据量非常大的场景，这时系统内存不足以支撑状态数据的存储。同时FsStateBackend最大的好处是相对比较稳定，在checkpoint时，将状态持久化到像HDFS分布式文件系统中，能最大程度保证状态数据的安全性。</p><h4 id="RocksDBStateBackend"><a href="#RocksDBStateBackend" class="headerlink" title="RocksDBStateBackend"></a>RocksDBStateBackend</h4><p>与前面的状态后端不同，RocksDBStateBackend需要单独引入相关的依赖包。RocksDB 是一个 key/value 的内存存储系统，类似于HBase，是一种内存磁盘混合的 LSM DB。当写数据时会先写进write buffer(类似于HBase的memstore)，然后在flush到磁盘文件，当读取数据时会现在block cache(类似于HBase的block cache)，所以速度会很快。</p><p>RocksDBStateBackend在性能上要比FsStateBackend高一些，主要是因为借助于RocksDB存储了最新热数据，然后通过异步的方式再同步到文件系统中，但RocksDBStateBackend和MemoryStateBackend相比性能就会较弱一些。</p><p>需要注意 RocksDB 不支持同步的 Checkpoint，构造方法中没有同步快照这个选项。不过 RocksDB 支持增量的 Checkpoint，也是目前唯一增量 Checkpoint 的 Backend，意味着并不需要把所有 sst 文件上传到 Checkpoint 目录，仅需要上传新生成的 sst 文件即可。它的 Checkpoint 存储在外部文件系统（本地或HDFS），其容量限制只要单个 TaskManager 上 State 总量不超过它的内存+磁盘，单 Key最大 2G，总大小不超过配置的文件系统容量即可。对于超大状态的作业，例如天级窗口聚合等场景下可以使会用该状态后端。</p><h3 id="配置状态后端"><a href="#配置状态后端" class="headerlink" title="配置状态后端"></a>配置状态后端</h3><p>Flink默认使用的状态后端是MemoryStateBackend，所以不需要显示配置。对于其他的状态后端，都需要进行显性配置。在Flink中包含了两种级别的StateBackend配置：一种是在程序中进行配置，该配置只对当前应用有效；另外一种是通过 <code>flink-conf.yaml</code>进行全局配置，一旦配置就会对整个Flink集群上的所有应用有效。</p><ul><li>应用级别配置</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> FsStateBackend(<span class="string">"hdfs://namenode:40010/flink/checkpoints"</span>));</span><br></pre></td></tr></table></figure><p>如果使用RocksDBStateBackend则需要单独引入rockdb依赖库,如下：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-statebackend-rocksdb_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>使用方式与FsStateBackend类似，如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> RocksDBStateBackend(<span class="string">"hdfs://namenode:40010/flink/checkpoints"</span>));</span><br></pre></td></tr></table></figure><ul><li>集群级别配置</li></ul><p>具体的配置项在flink-conf.yaml文件中，如下代码所示，参数state.backend指明StateBackend类型，state.checkpoints.dir配置具体的状态存储路径，代码中使用filesystem作为StateBackend，然后指定相应的HDFS文件路径作为state的checkpoint文件夹。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用filesystem存储</span></span><br><span class="line">state.backend: filesystem</span><br><span class="line"><span class="comment"># checkpoint存储路径</span></span><br><span class="line">state.checkpoints.dir: hdfs://namenode:40010/flink/checkpoints</span><br></pre></td></tr></table></figure><p>如果想用RocksDBStateBackend配置集群级别的状态后端，可以使用下面的配置：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 操作RocksDBStateBackend的线程数量，默认值为1</span></span><br><span class="line">state.backend.rocksdb.checkpoint.transfer.thread.num: 1</span><br><span class="line"><span class="comment"># 指定RocksDB存储状态数据的本地文件路径</span></span><br><span class="line">state.backend.rocksdb.localdir: /var/rockdb/checkpoints</span><br><span class="line"><span class="comment"># 用于指定定时器服务的工厂类实现类，默认为“HEAP”，也可以指定为“RocksDB”</span></span><br><span class="line">state.backend.rocksdb.timer-service.factory: HEAP</span><br></pre></td></tr></table></figure><h2 id="什么是Checkpoint-检查点"><a href="#什么是Checkpoint-检查点" class="headerlink" title="什么是Checkpoint(检查点)"></a>什么是Checkpoint(检查点)</h2><p>上面讲解了Flink的状态以及状态后端，状态是存储在状态后端。为了保证state容错，Flink提供了处理故障的措施，这种措施称之为checkpoint(一致性检查点)。checkpoint是Flink实现容错的核心功能，主要是周期性地触发checkpoint，将state生成快照持久化到外部存储系统(比如HDFS)。这样一来，如果Flink程序出现故障，那么就可以从上一次checkpoint中进行状态恢复，从而提供容错保障。另外，通过checkpoint机制，Flink可以实现Exactly-once语义(Flink内部的Exactly-once,关于端到端的exactly_once,Flink是通过两阶段提交协议实现的)。下面将会详细分析Flink的checkpoint机制。</p><h3 id="检查点的生成"><a href="#检查点的生成" class="headerlink" title="检查点的生成"></a>检查点的生成</h3><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/checkpoint_%E6%A6%82%E8%A7%88.png" alt></p><p>如上图，输入流是用户行为数据，包括购买(buy)和加入购物车(cart)两种，每种行为数据都有一个偏移量，统计每种行为的个数。</p><p>第一步：JobManager checkpoint coordinator 触发checkpoint。</p><p>第二步：假设当消费到[cart，3]这条数据时，触发了checkpoint。那么此时数据源会把消费的偏移量3写入持久化存储。</p><p>第三步：当写入结束后，source会将state handle(状态存储路径)反馈给JobManager的checkpoint coordinator。</p><p>第四步：接着算子count buy与count cart也会进行同样的步骤</p><p>第五步：等所有的算子都完成了上述步骤之后，即当 Checkpoint coordinator 收集齐所有 task 的 state handle，就认为这一次的 Checkpoint 全局完成了，向持久化存储中再备份一个 Checkpoint meta 文件，那么整个checkpoint也就完成了，如果中间有一个不成功，那么本次checkpoin就宣告失败。</p><h3 id="检查点的恢复"><a href="#检查点的恢复" class="headerlink" title="检查点的恢复"></a>检查点的恢复</h3><p>通过上面的分析，或许你已经对Flink的checkpoint有了初步的认识。那么接下来，我们看一下是如何从检查点恢复的。</p><ul><li>任务失败</li></ul><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/%E4%BD%9C%E4%B8%9A%E5%A4%B1%E8%B4%A5.png" alt></p><ul><li>重启作业</li></ul><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/1%E9%87%8D%E5%90%AF%E4%BD%9C%E4%B8%9A.png" alt></p><ul><li>恢复检查点</li></ul><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/2%E6%A3%80%E6%9F%A5%E7%82%B9%E6%81%A2%E5%A4%8D.png" alt></p><ul><li>继续处理数据</li></ul><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/%E7%BB%A7%E7%BB%AD%E8%BF%90%E8%A1%8C.png" alt></p><p>上述过程具体总结如下：</p><ul><li>第一步：重启作业</li><li>第二步：从上一次检查点恢复状态数据</li><li>第三步：继续处理新的数据</li></ul><h3 id="Flink内部Exactly-Once实现"><a href="#Flink内部Exactly-Once实现" class="headerlink" title="Flink内部Exactly-Once实现"></a>Flink内部Exactly-Once实现</h3><p>Flink提供了精确一次的处理语义，精确一次的处理语义可以理解为：数据可能会重复计算，但是结果状态只有一个。Flink通过Checkpoint机制实现了精确一次的处理语义，Flink在触发Checkpoint时会向Source端插入checkpoint barrier，checkpoint barriers是从source端插入的，并且会向下游算子进行传递。checkpoint barriers携带一个checkpoint ID，用于标识属于哪一个checkpoint，checkpoint barriers将流逻辑是哪个分为了两部分。对于双流的情况，通过barrier对齐的方式实现精确一次的处理语义。</p><p>关于什么是checkpoint barrier，可以看一下CheckpointBarrier类的源码描述，如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Checkpoint barriers用来在数据流中实现checkpoint对齐的.</span></span><br><span class="line"><span class="comment"> * Checkpoint barrier由JobManager的checkpoint coordinator插入到Source中,</span></span><br><span class="line"><span class="comment"> * Source会把barrier广播发送到下游算子,当一个算子接收到了其中一个输入流的Checkpoint barrier时,</span></span><br><span class="line"><span class="comment"> * 它就会知道已经处理完了本次checkpoint与上次checkpoint之间的数据.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 一旦某个算子接收到了所有输入流的checkpoint barrier时，</span></span><br><span class="line"><span class="comment"> * 意味着该算子的已经处理完了截止到当前checkpoint的数据，</span></span><br><span class="line"><span class="comment"> * 可以触发checkpoint，并将barrier向下游传递</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 根据用户选择的处理语义，在checkpoint完成之前会缓存后一次checkpoint的数据，</span></span><br><span class="line"><span class="comment"> * 直到本次checkpoint完成(exactly once)</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * checkpoint barrier的id是严格单调递增的</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CheckpointBarrier</span> <span class="keyword">extends</span> <span class="title">RuntimeEvent</span> </span>&#123;...&#125;</span><br></pre></td></tr></table></figure><p>可以看出checkpoint barrier主要功能是实现checkpoint对齐的，从而可以实现Exactly-Once处理语义。</p><p>下面将会对checkpoint过程进行分解，具体如下：</p><p>图1，包括两个流，每个任务都会消费一条用户行为数据(包括购买(buy)和加购(cart))，数字代表该数据的偏移量，count buy任务统计购买行为的个数，coun cart统计加购行为的个数。</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/%E7%8A%B6%E6%80%811.png" alt></p><p>图2，触发checkpoint，JobManager会向每个数据源发送一个新的checkpoint编号，以此来启动检查点生成流程。</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/%E7%8A%B6%E6%80%812.png" alt></p><ul><li>图3，当Source任务收到消息后，会停止发出数据，然后利用状态后端触发生成本地状态检查点，并把该checkpoint barrier以及checkpoint id广播至所有传出的数据流分区。状态后端会在checkpoint完成之后通知任务，随后任务会向Job Manager发送确认消息。在将checkpoint barrier发出之后，Source任务恢复正常工作。</li></ul><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/%E7%8A%B6%E6%80%813.png" alt></p><ul><li>图4，Source任务发出的checkpoint barrier会发送到与之相连的下游算子任务，当任务收到一个新的checkpoint barrier时，会继续等待其他输入分区的checkpoint barrier到来，这个过程称之为<strong>barrier 对齐</strong>，checkpoint barrier到来之前会把到来的数据线缓存起来。</li></ul><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/%E7%8A%B6%E6%80%814.png" alt></p><ul><li>图5，任务收齐了全部输入分区的checkpoint barrier之后，会通知状态后端开始生成checkpoint，同时会把checkpoint barrier广播至下游算子。</li></ul><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/%E7%8A%B6%E6%80%815.png" alt></p><ul><li>图6，任务在发出checkpoint barrier之后，开始处理因barrier对齐产生的缓存数据，在缓存的数据处理完之后，就会继续处理输入流数据。</li></ul><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/%E7%8A%B6%E6%80%816.png" alt></p><ul><li>图7，最终checkpoint barrier会被传送到sink端，sink任务接收到checkpoint barrier之后，会向其他算子任务一样，将自身的状态写入checkpoint，之后向Job Manager发送确认消息。Job Manager接收到所有任务返回的确认消息之后，就会将此次检查点标记为完成。</li></ul><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/%E7%8A%B6%E6%80%817.png" alt></p><h3 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// checkpoint的时间间隔，如果状态比较大，可以适当调大该值</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>);</span><br><span class="line"><span class="comment">// 配置处理语义，默认是exactly-once</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line"><span class="comment">// 两个checkpoint之间的最小时间间隔，防止因checkpoint时间过长，导致checkpoint积压</span></span><br><span class="line">env.getCheckpointConfig().setMinPauseBetweenCheckpoints(<span class="number">500</span>);</span><br><span class="line"><span class="comment">// checkpoint执行的上限时间，如果超过该阈值，则会中断checkpoint</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line"><span class="comment">// 最大并行执行的检查点数量，默认为1，可以指定多个，从而同时出发多个checkpoint，提升效率</span></span><br><span class="line">env.getCheckpointConfig().setMaxConcurrentCheckpoints(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// 设定周期性外部检查点，将状态数据持久化到外部系统中，</span></span><br><span class="line"><span class="comment">// 使用该方式不会在任务正常停止的过程中清理掉检查点数据</span></span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line"><span class="comment">// allow job recovery fallback to checkpoint when there is a more recent savepoint</span></span><br><span class="line">env.getCheckpointConfig().setPreferCheckpointForRecovery(<span class="keyword">true</span>);</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先从Flink的状态入手，通过Spark的WordCount和Flink的Work Count进行说明什么是状态。接着对状态的分类以及状态的使用进行了详细说明。然后对Flink提供的三种状态后端进行讨论，并给出了状态后端的使用说明。最后，以图解加文字的形式详细解释了Flink的checkpoint机制，并给出了使用Checkpoint时的程序配置。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的时间与watermarks详解</title>
      <link href="/2020/04/17/Flink%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E4%B8%8E%E7%AA%97%E5%8F%A3%E7%9A%84%E7%AE%97%E5%AD%90/"/>
      <url>/2020/04/17/Flink%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E4%B8%8E%E7%AA%97%E5%8F%A3%E7%9A%84%E7%AE%97%E5%AD%90/</url>
      
        <content type="html"><![CDATA[<p>当我们在使用Flink的时候，避免不了要和时间(time)、水位线(watermarks)打交道，理解这些概念是开发分布式流处理应用的基础。那么Flink支持哪些时间语义？Flink是如何处理乱序事件的？什么是水位线？水位线是如何生成的？水位线的传播方式是什么？让我们带着这些问题来开始本文的内容。</p><a id="more"></a><h2 id="时间语义"><a href="#时间语义" class="headerlink" title="时间语义"></a>时间语义</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>时间是Flink等流处理中最重要的概念之一，在 Flink 中 Time  可以分为三种：Event-Time，Processing-Time 以及 Ingestion-Time，如下图所示：</p><p><img src="//jiamaoxiang.top/2020/04/17/Flink基于时间与窗口的算子/%E6%97%B6%E9%97%B4%E5%9B%BE.png" alt></p><ul><li>Event Time</li></ul><p><strong>事件时间</strong>，事件(Event)本身的时间，即数据流中事件实际发生的时间，通常使用事件发生时的时间戳来描述，这些事件的时间戳通常在进入流处理应用之前就已经存在了，事件时间反映了事件真实的发生时间。所以，基于事件时间的计算操作，其结果是具有确定性的，无论数据流的处理速度如何、事件到达算子的顺序是否会乱，最终生成的结果都是一样的。</p><ul><li>Ingestion Time</li></ul><p><strong>摄入时间</strong>，事件进入Flink的时间，即将每一个事件在数据源算子的处理时间作为事件时间的时间戳，并自动生成水位线(watermarks,关于watermarks下文会详细分析)。</p><p>Ingestion Time从概念上讲介于Event Time和Processing Time之间。与Processing Time相比 ，它的性能消耗更多一些，但结果却更可预测。由于 Ingestion Time使用稳定的时间戳（在数据源处分配了一次），因此对记录的不同窗口操作将引用相同的时间戳，而在Processing Time中每个窗口算子都可以将记录分配给不同的窗口。</p><p>与Event Time相比，Ingestion Time无法处理任何乱序事件或迟到的数据，即无法提供确定的结果，但是程序不必指定如何生成水位线。在内部，Ingestion Time与Event Time非常相似，但是可以实现自动分配时间戳和自动生成水位线的功能。</p><ul><li>Processing Time</li></ul><p><strong>处理时间</strong>，根据处理机器的系统时钟决定数据流当前的时间，即事件被处理时当前系统的时间。还以窗口算子为例(关于window，下文会详细分析)，基于处理时间的窗口操作是以机器时间来进行触发的，由于数据到达窗口的速率不同，所以窗口算子中使用处理时间会导致不确定的结果。在使用处理时间时，无需等待水位线的到来后进行触发窗口，所以可以提供较低的延迟。</p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>经过上面的分析，应该对Flink的时间语义有了大致的了解。不知道你会不会有这样一个疑问：既然事件时间已经能够解决所有的问题了，那为何还要用处理时间呢？其实处理时间有其特定的使用场景，处理时间由于不用考虑事件的延迟与乱序，所以其处理数据的延迟较低。因此如果一些应用比较重视处理速度而非准确性，那么就可以使用处理时间，比如要实时监控仪表盘。总之，虽然处理时间的延迟较低，但是其结果具有不确定性，事件时间虽然有延迟，但是能够保证处理的结果具有准确性，并且可以处理延迟甚至无序的数据。</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>上一小结讲述了三种时间语义的基本概念，接下来将从代码层面讲解在程序中该如何配置这三种时间语义。首先来看一段代码：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/** The time characteristic that is used if none other is set. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> TimeCharacteristic DEFAULT_TIME_CHARACTERISTIC = TimeCharacteristic.ProcessingTime;</span><br><span class="line"><span class="comment">//省略的代码</span></span><br><span class="line"><span class="comment">/** The time characteristic used by the data streams. */</span></span><br><span class="line"><span class="keyword">private</span> TimeCharacteristic timeCharacteristic = DEFAULT_TIME_CHARACTERISTIC;</span><br></pre></td></tr></table></figure><p>上述两行代码摘自StreamExecutionEnvironment类，可以看出，Flink在流处理程序中默认的时间语义是Processing Time，那么该如何修改默认的时间语义呢？很简单，再来看一段代码，下面的代码片段同样来自于StreamExecutionEnvironment类：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果使用Processing Time或者Event Time，默认的水位线间隔时间是200毫秒</span></span><br><span class="line"><span class="comment"> * 可以通过ExecutionConfig#setAutoWatermarkInterval(long)设置</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> characteristic The time characteristic.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setStreamTimeCharacteristic</span><span class="params">(TimeCharacteristic characteristic)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.timeCharacteristic = Preconditions.checkNotNull(characteristic);</span><br><span class="line"><span class="keyword">if</span> (characteristic == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">getConfig().setAutoWatermarkInterval(<span class="number">0</span>);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">getConfig().setAutoWatermarkInterval(<span class="number">200</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述的方法可以配置不同的时间语义，参数TimeCharacteristic是一个枚举类，包括ProcessingTime，IngestionTime，EventTime三个元素。具体使用方式如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);</span></span><br><span class="line"><span class="comment">//env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span></span><br></pre></td></tr></table></figure><h2 id="watermarks"><a href="#watermarks" class="headerlink" title="watermarks"></a>watermarks</h2><p>在解释watermarks(水位线)之前，先看一个我们身边发生的真实案例。高考，是大家非常熟悉的场景。如果把高考的考试安排简单地看作是一个流处理应用，那么，每一个考试科目的开始时间到结束时间就是一个窗口，每个考生可以理解成一条记录，考生到达考场的时间可以理解成记录的时间戳，而考试可以理解成某种算子操作。大家都知道，高考考试在开考后15分钟是不允许进场的，这个规定可以理解成一个水位线，比如，上午第一场语文考试，开考时间是9:30，允许在9:45之前进入考场，那么9:45这个时间可以理解成一个水位线。在开考之前，有的同学喜欢提前到考场，有的同学喜欢卡点到考场。假设有个同学叫<strong>考必胜</strong>,ta是卡着时间点到的考场，但是早上由于吃了不干净的东西，突然感觉肚子不适，无奈之下在厕所里耽误了16分钟，那么按照规定，此时考必胜是不能够进入考场的，因为此时已经默认所有考生都已经在考场了，此时考试也已经触发，那么卡必胜就可以理解为迟到的事件。以上就是对窗口、事件时间以及水位线的简单理解，下面开始详细解释什么水位线。</p><h3 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h3><p>在上一节中，详细讲解了Flink提供的三种时间语义，在讲解这三种时间语义的时候，提到了一个名词—水位线，那么究竟什么是水位线呢？先来看一个例子，假如要每5分钟统计一次过去1个小时内的热门商品的topN，这是一个典型的滑动窗口操作，那么基于事件时间的窗口该在什么时候出发计算呢？换句话说，我们要等多久才能够确定已经接收到了特定时间点之前的所有事件，另一方面，由于网络延迟等原因，会产生乱序的数据，在进行窗口操作时，不能够无限期的等待下去，需要一个机制来告诉窗口在某个特定时间来触发window计算，即认为小于等于该时间点的数据都已经到来了。这个机制就是watermark(水位线)，可以用来处理乱序事件。</p><p>水位线是一个全局的进度指标，表示可以确定不会再有延迟的事件到来的某个时间点。从本质上讲，水位线提供了一个逻辑时钟，用来通知系统当前的事件时间。比如，当一个算子接收到了W(T)时刻的水位线，就可以大胆的认为不会再接收到任何时间戳小于或等于W(T)的事件了。水位线对于基于事件时间的窗口和处理乱序数据是非常关键的，算子一旦接收到了某个水位线，就相当于接到一支穿云箭的信号：所有特定时间区间的数据都已集结完毕，可以进行窗口触发计算。</p><p>既然已经说了，事件是会存在乱序的，那这个乱序的程度究竟有多大呢，这个就不太好确定了，总之总会有些迟到的事件慢慢悠悠的到来。所以，水位线其实是一种在<strong>准确性</strong>与<strong>延迟</strong>之间的权衡，如果水位线设置的非常苛刻，即不允许有掉队的数据出现，虽然准确性提高了，但这在无形之中增加了数据处理的延迟。反之，如果水位线设置的非常激进，即允许有迟到的数据发生，那么虽然降低了数据处理的延迟，但数据的准确性会较低。</p><p>所以，水位线是中庸之道，过犹不及。在很多现实应用中，系统无法获取足够多的信息来确定完美的水位线，那么该怎么办呢？Flink提供了某些机制来处理那些可能晚于水位线的迟到时间，用户可以根据应用的需求不同，可以将这些漏网之鱼(迟到的数据)舍弃掉，或者写入日志，或者利用他们修正之前的结果。</p><p>上面说到没有完美的水位线，可能还是很抽象。接下来，我们再看一幅图，从图中可以很直观地观察真实的水位线与理想中的完美水位线之间的关系，如下图：</p><p><img src="//jiamaoxiang.top/2020/04/17/Flink基于时间与窗口的算子/TimeSkew.png" alt></p><p>上图的浅灰色直虚线表示理想的水位线，深灰色的弯曲虚线表示现实中的水位线，黑色直线表示两者之间的偏差。在理想状态下，这种偏差为0，因为总是在时间发生时就会立即处理，即事件的真实时间与处理事件的时间是一致的。比如，12:01产生的事件刚好在12:01时被处理，12:02产生的事件刚好在12:02时被处理。但是现实总会有迟到的数据产生，比如网络延迟的原因，所以真实的情况会像深灰色的弯曲虚线表示的那样，即12:01产生的数据可能会在12:01之后被处理，12:02产生的数据在12:02时被处理，12:03时产生的数据会被在12:03之后处理。这种动态的偏差在分布式处理系统中是非常常见的。</p><h3 id="水位线图解"><a href="#水位线图解" class="headerlink" title="水位线图解"></a>水位线图解</h3><p>在上一小节，通过语言描述对水位线的概念进行了详细解读，在本小节会通过图解的方式解析水位线的含义，这样更能加深对水位线的理解。如下图所示：</p><p><img src="//jiamaoxiang.top/2020/04/17/Flink基于时间与窗口的算子/%E6%B0%B4%E4%BD%8D%E7%BA%BF%E5%9B%BE%E8%A7%A31.png" alt></p><p>如上图，矩形表示一条记录，三角表示该条记录的时间戳(真实发生时间)，圆圈表示水位线。可以看到上面的数据是乱序的，比如当算子接收到为2的水位线时，就可以认为时间戳小于等于2的数据都已经到来了，此时可以触发计算。同理，接收到为5的水位线时，就可以认为时间戳小于或等于5的数据都已经到来了，此时可以触发计算。</p><p>可以看出水位线是单调递增的，并且和记录的时间戳存在联系，一个时间戳为T的水位线表示接下来所有记录的时间戳一定都会大于T。</p><h3 id="水位线的传播"><a href="#水位线的传播" class="headerlink" title="水位线的传播"></a>水位线的传播</h3><p>现在，或许你已经对水位线是什么有了一个初步的认识，接下来将会介绍水位线是怎么在Flink内部传播的。关于水位线的传播策略可以归纳为3点：</p><ul><li>首先，水位线是以广播的形式在算子之间进行传播</li><li>Long.MAX_VALUE表示事件时间的结束，即未来不会有数据到来了</li><li>单个分区的输入取最大值，多个分区的输入取最小值</li></ul><p>关于Long.MAX_VALUE的解释，先看一段代码，如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="comment">/** </span></span><br><span class="line"><span class="comment"> * 当一个source关闭时，会输出一个Long.MAX_VALUE的水位线，当一个算子接收到该水位线时，</span></span><br><span class="line"><span class="comment"> * 相当于接收到一个信号：未来不会再有数据输入了</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Watermark</span> <span class="keyword">extends</span> <span class="title">StreamElement</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//表示事件时间的结束</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Watermark MAX_WATERMARK = <span class="keyword">new</span> Watermark(Long.MAX_VALUE);</span><br><span class="line">    <span class="comment">//省略的代码</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>关于另外两条策略的解释，可以从下图中得到：</p><p><img src="//jiamaoxiang.top/2020/04/17/Flink基于时间与窗口的算子/%E6%B0%B4%E4%BD%8D%E7%BA%BF%E4%BC%A0%E6%92%AD.png" alt></p><p>如上图，一个任务会为它的每个分区都维护一个分区水位线(partition watermark)，当收到每个分区传来的水位线时，任务首先会让当前分区水位线的值与接收的水位线值相比较，如果新接收的水位线值大于当前分区水位线值，则会将对应的分区水位线值更新为较大的水位线值(如上图中的2步骤)，接着，任务会把事件时钟调整为当前分区水位线值的最小值，如上图步骤2 ，由于当前分区水位线的最小值为3，所以将事件时间时钟更新为3，然后将值为3的水位线广播到下游任务。步骤3与步骤4的处理逻辑同上。</p><p>同时我们可以注意到这种设计其实有一个局限，具体体现在没有对分区(partition)是否来自于不同的流进行区分，比如对于两条流或多条流的Union或Connect操作，同样是按照全部分区水位线中最小值来更新事件时间时钟，这就导致所有的输入记录都会按照基于同一个事件时间时钟来处理，这种一刀切的做法对于同一个流的不同分区而言是无可厚非的，但是对于多条流而言，强制使用一个时钟进行同步会对整个集群带来较大的性能开销，比如当两个流的水位线相差很大是，其中的一个流要等待最慢的那条流，而较快的流的记录会在状态中缓存，直到事件时间时钟到达允许处理它们的那个时间点。</p><h3 id="水位线的生成方式"><a href="#水位线的生成方式" class="headerlink" title="水位线的生成方式"></a>水位线的生成方式</h3><p>通常情况下，在接收到数据源之后应该马上为其生成水位线，即越靠近数据源越好。Flink提供两种方式生成水位线，其中一种方式为在数据源完成的，即利用SourceFunction在应用读入数据流的时候分配时间戳与水位线。另一种方式是通过实现接口的自定义函数，该方式又包括两种实现方式：一种为周期性生成水位线，即实现AssignerWithPeriodicWatermarks接口，另一种为定点生成水位线，即实AssignerWithPunctuatedWatermarks接口。具体如下图所示：</p><p><img src="//jiamaoxiang.top/2020/04/17/Flink基于时间与窗口的算子/%E6%B0%B4%E4%BD%8D%E7%BA%BF%E7%94%9F%E6%88%90%E6%96%B9%E5%BC%8F.png" alt></p><h4 id="数据源方式"><a href="#数据源方式" class="headerlink" title="数据源方式"></a>数据源方式</h4><p>该方式主要是实现自定义数据源，数据源分配时间戳和水位线主要是通过内部的SourceContext对象实现的，先看一下SourceFunction的源码，如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">SourceFunction</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Function</span>, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">SourceContext</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">collect</span><span class="params">(T element)</span></span>;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">* 用于输出记录并附属一个与之关联的时间戳</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">collectWithTimestamp</span><span class="params">(T element, <span class="keyword">long</span> timestamp)</span></span>;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">* 用于输出传入的水位线</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">emitWatermark</span><span class="params">(Watermark mark)</span></span>;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">* 将自身标记为空闲状态</span></span><br><span class="line"><span class="comment">* 某个某个分区不在产生数据，会阻碍全局水位线前进，</span></span><br><span class="line"><span class="comment">* 因为收不到新的记录，意味着不会发出新的水位线，</span></span><br><span class="line"><span class="comment">* 根据水位线的传播策略，会导致整个应用都停止工作</span></span><br><span class="line"><span class="comment">* Flink提供一种机制，将数据源函数暂时标记为空闲，</span></span><br><span class="line"><span class="comment">* 在空闲状态下，Flink水位线的传播机制会忽略掉空闲的数据流分区</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">markAsTemporarilyIdle</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">Object <span class="title">getCheckpointLock</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上面对的代码可以看出，通过SourceContext对象的方法可以实现时间戳与水位线的分配。</p><h4 id="自定义函数的方式"><a href="#自定义函数的方式" class="headerlink" title="自定义函数的方式"></a>自定义函数的方式</h4><p>使用自定义函数的方式分配时间戳，只需要调用assignTimestampsAndWatermarks()方法，传入一个实现AssignerWithPeriodicWatermarks或者AssignerWithPunctuatedWatermarks接口的分配器即可，如下代码所示：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()</span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line">        SingleOutputStreamOperator&lt;UserBehavior&gt; userBehavior = env</span><br><span class="line">                .addSource(<span class="keyword">new</span> MysqlSource())</span><br><span class="line">                .assignTimestampsAndWatermarks(<span class="keyword">new</span> MyTimestampsAndWatermarks());</span><br></pre></td></tr></table></figure><ul><li><strong>周期分配器(AssignerWithPeriodicWatermarks)</strong></li></ul><p>该分配器是实现了一个AssignerWithPeriodicWatermarks的用户自定义函数，通过重写extractTimestamp()方法来提取时间戳，提取出来的时间戳会附加在各自的记录上，查询得到的水位线会注入到数据流中。</p><p>周期性的生成水位线是指以固定的时间间隔来发出水位线并推进事件时间的前进，关于默认的时间间隔在上文中也有提到，根据选择的时间语义确定默认的时间间隔，如果使用Processing Time或者Event Time，默认的水位线间隔时间是200毫秒，当然用户也可以自己设定时间间隔，关于如何设定，先看一段代码，代码来自于ExecutionConfig类：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 设置生成水位线的时间间隔</span></span><br><span class="line"><span class="comment">   * 注：自动生成watermarks的时间间隔不能是负数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ExecutionConfig <span class="title">setAutoWatermarkInterval</span><span class="params">(<span class="keyword">long</span> interval)</span> </span>&#123;</span><br><span class="line">Preconditions.checkArgument(interval &gt;= <span class="number">0</span>, <span class="string">"Auto watermark interval must not be negative."</span>);</span><br><span class="line"><span class="keyword">this</span>.autoWatermarkInterval = interval;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所以，如果要调整默认的200毫秒的间隔，可以调用setAutoWatermarkInterval()方法，具体使用如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">//每3秒生成一次水位线</span></span><br><span class="line">env.getConfig().setAutoWatermarkInterval(<span class="number">3000</span>);</span><br></pre></td></tr></table></figure><p>上面指定了每隔3秒生成一次水位线，即每隔3秒会自动向流里注入一个水位线，在代码层面，Flink会每隔3秒钟调用一次AssignerWithPeriodicWatermarks的getCurrentWatermark()方法，每次调用该方法时，如果得到的值不为空并且大于上一个水位线的时间戳，那么就会向流中注入一个新的水位线。这项检查可以有效地保证了事件时间的递增的特性，一旦检查失败也就不会生成水位线。下面给出一个实现周期分配水位线的例子：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyTimestampsAndWatermarks</span> <span class="keyword">implements</span> <span class="title">AssignerWithPeriodicWatermarks</span>&lt;<span class="title">UserBehavior</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// 定义1分钟的容忍间隔时间，即允许数据的最大乱序时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> maxOutofOrderness = <span class="number">60</span> * <span class="number">1000</span>;</span><br><span class="line">    <span class="comment">// 观察到的最大时间戳</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> currentMaxTs = Long.MIN_VALUE;      </span><br><span class="line"></span><br><span class="line">    <span class="meta">@Nullable</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Watermark <span class="title">getCurrentWatermark</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 生成具有1分钟容忍度的水位线</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Watermark(currentMaxTs - maxOutofOrderness);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(UserBehavior element, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//获取当前记录的时间戳</span></span><br><span class="line">        <span class="keyword">long</span> currentTs = element.timestamp;</span><br><span class="line">        <span class="comment">// 更新最大的时间戳</span></span><br><span class="line">        currentMaxTs = Math.max(currentMaxTs, currentTs);</span><br><span class="line">        <span class="comment">// 返回记录的时间戳</span></span><br><span class="line">        <span class="keyword">return</span> currentTs;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过查看TimestampAssignerd 继承关系可以发现(继承关系如下图)，除此之外，Flink还提供了两种内置的水位线分配器，分别为：AscendingTimestampExtractor和BoundedOutOfOrdernessTimestampExtractor两个抽象类。</p><p><img src="//jiamaoxiang.top/2020/04/17/Flink基于时间与窗口的算子/%E5%86%85%E7%BD%AE%E6%B0%B4%E4%BD%8D%E7%BA%BF%E5%88%86%E9%85%8D%E5%99%A8.png" alt></p><p>关于<strong>AscendingTimestampExtractor</strong>，一般是在数据集的时间戳是单调递增的且没有乱序时使用，该方法使用当前的时间戳生成水位线，使用方式如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;UserBehavior&gt; userBehavior = env</span><br><span class="line">                .addSource(<span class="keyword">new</span> MysqlSource())</span><br><span class="line">                .assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;UserBehavior&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(UserBehavior element)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> element.timestamp*<span class="number">1000</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br></pre></td></tr></table></figure><p>关于<strong>BoundedOutOfOrdernessTimestampExtractor</strong>，是在数据集中存在乱序数据的情况下使用，即数据有延迟(任意新到来的元素与已经到来的时间戳最大的元素之间的时间差)，这种方式可以接收一个表示最大预期延迟参数，具体如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;UserBehavior&gt; userBehavior = env</span><br><span class="line">                .addSource(<span class="keyword">new</span> MysqlSource())</span><br><span class="line">                .assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;UserBehavior&gt;(Time.seconds(<span class="number">10</span>)) &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(UserBehavior element)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> element.timestamp*<span class="number">1000</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; );</span><br></pre></td></tr></table></figure><p>上述的代码接收了一个10秒钟延迟的参数，这10秒钟意味着如果当前元素的事件时间与到达的元素的最大时间戳的差值在10秒之内，那么该元素会被处理，如果差值超过10秒，表示其本应该参与的计算，已经完成了，Flink称之为迟到的数据，Flink提供了不同的策略来处理这些迟到的数据。</p><ul><li><strong>定点水位线分配器(AssignerWithPunctuatedWatermarks)</strong></li></ul><p>该方式是基于某些事件(指示系统进度的特殊元祖或标记)触发水位线的生成与发送，基于特定的事件向流中注入一个水位线，流中的每一个元素都有机会判断是否生成一个水位线，如果得到的水位线不为空并且大于之前的水位线，就生成水位线并注入流中。</p><p>实现AssignerWithPunctuatedWatermarks接口，重写checkAndGetNextWatermark()方法，该方法会在针对每个事件的extractTimestamp()方法后立即调用，以此来决定是否生成一个新的水位线，如果该方法返回一个非空并且大于之前值的水位线，就会将这个新的水位线发出。</p><p>下面将会实现一个简单的定点水位线分配器</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyPunctuatedAssigner</span> <span class="keyword">implements</span> <span class="title">AssignerWithPunctuatedWatermarks</span>&lt;<span class="title">UserBehavior</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// 定义1分钟的容忍间隔时间，即允许数据的最大乱序时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> maxOutofOrderness = <span class="number">60</span> * <span class="number">1000</span>;      </span><br><span class="line">    <span class="meta">@Nullable</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Watermark <span class="title">checkAndGetNextWatermark</span><span class="params">(UserBehavior element, <span class="keyword">long</span> extractedTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 如果读取数据的用户行为是购买，就生成水位线</span></span><br><span class="line">        <span class="keyword">if</span>(element.action.equals(<span class="string">"buy"</span>))&#123;</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">new</span> Watermark(extractedTimestamp - maxOutofOrderness);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="comment">// 不发出水位线</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(UserBehavior element, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> element.timestamp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="迟到的数据"><a href="#迟到的数据" class="headerlink" title="迟到的数据"></a>迟到的数据</h3><p>上文已经说过，现实中很难生成一个完美的水位线，水位线就是在延迟与准确性之前做的一种权衡。那么，如果生成的水位线过于紧迫，即水位线可能会大于后来数据的时间戳，这就意味着数据有延迟，关于延迟数据的处理，Flink提供了一些机制，具体如下：</p><ul><li>直接将迟到的数据丢弃</li><li>将迟到的数据输出到单独的数据流中，即使用sideOutputLateData(new OutputTag&lt;&gt;()）实现侧输出</li><li>根据迟到的事件更新并发出结果</li></ul><p>由于篇幅限制，关于迟到数据的具体处理在本文先不做太多的讨论，在后续的文章中会对其详细进行说明。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文从Flink的时间语义开始说起，详细介绍了三种时间语义的概念、特点及使用方式，接着对Flink处理乱序数据的一种机制—水位线进行详细说明，主要描述了水位线的基本概念，传播方式、生成方式，并对其中的细节部分进行了图解，可以加深对水位线的理解。最后，简单说明了一下Flink对于迟到数据的处理方式。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink DataStream API编程指南</title>
      <link href="/2020/04/12/Flink-DataStream-API%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/"/>
      <url>/2020/04/12/Flink-DataStream-API%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<p>Flink DataStream API主要分为三个部分，分别为Source、Transformation以及Sink，其中Source是数据源，Flink内置了很多数据源，比如最常用的Kafka。Transformation是具体的转换操作，主要是用户定义的处理数据的逻辑，比如Map，FlatMap等。Sink(数据汇)是数据的输出，可以把处理之后的数据输出到存储设备上，Flink内置了许多的Sink，比如Kafka，HDFS等。另外除了Flink内置的Source和Sink外，用户可以实现自定义的Source与Sink。考虑到内置的Source与Sink使用起来比较简单且方便，所以，关于内置的Source与Sink的使用方式不在本文的讨论范围之内，本文会先从自定义Source开始说起，然后详细描述一些常见算子的使用方式，最后会实现一个自定义的Sink。</p><a id="more"></a><h2 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h2><p>Flink内部实现了比较常用的数据源，比如基于文件的，基于Socket的，基于集合的等等，如果这些都不能满足需求，用户可以自定义数据源，下面将会以MySQL为例，实现一个自定义的数据源。本文的所有操作将使用该数据源，具体代码如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Created</span> with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@author</span> : jmx</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Date</span>: 2020/4/14</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Time</span>: 17:34</span></span><br><span class="line"><span class="comment"> * note: RichParallelSourceFunction与SourceContext必须加泛型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MysqlSource</span> <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>&lt;<span class="title">UserBehavior</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> Connection conn;</span><br><span class="line">    <span class="keyword">public</span> PreparedStatement pps;</span><br><span class="line">    <span class="keyword">private</span> String driver;</span><br><span class="line">    <span class="keyword">private</span> String url;</span><br><span class="line">    <span class="keyword">private</span> String user;</span><br><span class="line">    <span class="keyword">private</span> String pass;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 该方法只会在最开始的时候被调用一次</span></span><br><span class="line"><span class="comment">     * 此方法用于实现获取连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//初始化数据库连接参数</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        URL fileUrl = TestProperties.class.getClassLoader().getResource(<span class="string">"mysql.ini"</span>);</span><br><span class="line">        FileInputStream inputStream = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(fileUrl.toURI()));</span><br><span class="line">        properties.load(inputStream);</span><br><span class="line">        inputStream.close();</span><br><span class="line">        driver = properties.getProperty(<span class="string">"driver"</span>);</span><br><span class="line">        url = properties.getProperty(<span class="string">"url"</span>);</span><br><span class="line">        user = properties.getProperty(<span class="string">"user"</span>);</span><br><span class="line">        pass = properties.getProperty(<span class="string">"pass"</span>);</span><br><span class="line">        <span class="comment">//获取数据连接</span></span><br><span class="line">        conn = getConection();</span><br><span class="line">        String scanSQL = <span class="string">"SELECT * FROM user_behavior_log"</span>;</span><br><span class="line">        pps = conn.prepareStatement(scanSQL);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;UserBehavior&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ResultSet resultSet = pps.executeQuery();</span><br><span class="line">        <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">            ctx.collect(UserBehavior.of(</span><br><span class="line">                    resultSet.getLong(<span class="string">"user_id"</span>),</span><br><span class="line">                    resultSet.getLong(<span class="string">"item_id"</span>),</span><br><span class="line">                    resultSet.getInt(<span class="string">"cat_id"</span>),</span><br><span class="line">                    resultSet.getInt(<span class="string">"merchant_id"</span>),</span><br><span class="line">                    resultSet.getInt(<span class="string">"brand_id"</span>),</span><br><span class="line">                    resultSet.getString(<span class="string">"action"</span>),</span><br><span class="line">                    resultSet.getString(<span class="string">"gender"</span>),</span><br><span class="line">                    resultSet.getLong(<span class="string">"timestamp"</span>)));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 实现关闭连接</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (pps != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                pps.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                conn.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取数据库连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> SQLException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Connection <span class="title">getConection</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        Connection connnection = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//加载驱动</span></span><br><span class="line">            Class.forName(driver);</span><br><span class="line">            <span class="comment">//获取连接</span></span><br><span class="line">            connnection = DriverManager.getConnection(</span><br><span class="line">                    url,</span><br><span class="line">                    user,</span><br><span class="line">                    pass);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> connnection;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>首先继承RichParallelSourceFunction，实现继承的方法，主要包括open()方法、run()方法及close方法。上述的</p><p>RichParallelSourceFunction是支持设置多并行度的，关于RichParallelSourceFunction与RichSourceFunction的区别，前者支持用户设置多并行度，后者不支持通过setParallelism()方法设置并行度，默认的并行度为1，否则会报如下错误：<figure class="highlight plain"><figcaption><span>in thread "main" java.lang.IllegalArgumentException: The maximum parallelism of non parallel operator must be 1.```</span></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">另外，RichParallelSourceFunction提供了额外的open()方法与close()方法，如果定义Source时需要获取链接，那么可以在open()方法中进行初始化，然后在close()方法中关闭资源链接，关于Rich***Function与普通Function的区别，下文会详细解释，在这里先有个印象。上述的代码中的配置信息是通过配置文件传递的，由于篇幅限制，我会把本文的代码放置在github，见文末github地址。</span><br><span class="line"></span><br><span class="line">## 基本转换</span><br><span class="line"></span><br><span class="line">Flink提供了大量的算子操作供用户使用，常见的算子主要包括以下几种，注意：本文不讨论关于基于时间与窗口的算子，这些内容会在《Flink基于时间与窗口的算子》中进行详细介绍。</span><br><span class="line"></span><br><span class="line">**说明**：本文的操作是基于上文自定义的MySQL Source，对应的数据解释如下：</span><br><span class="line"></span><br><span class="line">```java</span><br><span class="line">userId;     // 用户ID</span><br><span class="line">itemId;     // 商品ID</span><br><span class="line">catId;      // 商品类目ID</span><br><span class="line">merchantId; // 卖家ID</span><br><span class="line">brandId;    // 品牌ID</span><br><span class="line">action;     // 用户行为, 包括(&quot;pv&quot;, &quot;buy&quot;, &quot;cart&quot;, &quot;fav&quot;)</span><br><span class="line">gender;     // 性别</span><br><span class="line">timestamp;  // 行为发生的时间戳，单位秒</span><br></pre></td></tr></table></figure></p><h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><h4 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h4><p> DataStream → DataStream 的转换，输入一个元素，返回一个元素，如下操作：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;String&gt; userBehaviorMap = userBehavior.map(<span class="keyword">new</span> RichMapFunction&lt;UserBehavior, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(UserBehavior value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String action = <span class="string">""</span>;</span><br><span class="line">                <span class="keyword">switch</span> (value.action) &#123;</span><br><span class="line">                    <span class="keyword">case</span> <span class="string">"pv"</span>:</span><br><span class="line">                        action = <span class="string">"浏览"</span>;</span><br><span class="line">                    <span class="keyword">case</span> <span class="string">"cart"</span>:</span><br><span class="line">                        action = <span class="string">"加购"</span>;</span><br><span class="line">                    <span class="keyword">case</span> <span class="string">"fav"</span>:</span><br><span class="line">                        action = <span class="string">"收藏"</span>;</span><br><span class="line">                    <span class="keyword">case</span> <span class="string">"buy"</span>:</span><br><span class="line">                        action = <span class="string">"购买"</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> action;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><h4 id="示意图"><a href="#示意图" class="headerlink" title="示意图"></a>示意图</h4><p>将雨滴形状转换成相对应的圆形形状的map操作</p><p><img src="//jiamaoxiang.top/2020/04/12/Flink-DataStream-API编程指南/map.png" alt></p><h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><h4 id="解释-1"><a href="#解释-1" class="headerlink" title="解释"></a>解释</h4><p>DataStream → DataStream，输入一个元素，返回零个、一个或多个元素。事实上，flatMap算子可以看做是filter与map的泛化，即它能够实现这两种操作。flatMap算子对应的FlatMapFunction定义了flatMap方法，可以通过向collector对象传递数据的方式返回0个，1个或者多个事件作为结果。如下操作：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;UserBehavior&gt; userBehaviorflatMap = userBehavior.flatMap(<span class="keyword">new</span> RichFlatMapFunction&lt;UserBehavior, UserBehavior&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(UserBehavior value, Collector&lt;UserBehavior&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">if</span> (value.gender.equals(<span class="string">"女"</span>)) &#123;</span><br><span class="line">                    out.collect(value);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><h4 id="示意图-1"><a href="#示意图-1" class="headerlink" title="示意图"></a>示意图</h4><p>将黄色的雨滴过滤掉，将蓝色雨滴转为圆形，保留绿色雨滴</p><p><img src="//jiamaoxiang.top/2020/04/12/Flink-DataStream-API编程指南/flatmap.png" alt></p><h3 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h3><h4 id="解释-2"><a href="#解释-2" class="headerlink" title="解释"></a>解释</h4><p>DataStream → DataStream，过滤算子，对数据进行判断，符合条件即返回true的数据会被保留，否则被过滤。如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;UserBehavior&gt; userBehaviorFilter = userBehavior.filter(<span class="keyword">new</span> RichFilterFunction&lt;UserBehavior&gt;() &#123;</span><br><span class="line">          <span class="meta">@Override</span></span><br><span class="line">          <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(UserBehavior value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">              <span class="keyword">return</span> value.action.equals(<span class="string">"buy"</span>);<span class="comment">//保留购买行为的数据</span></span><br><span class="line">          &#125;</span><br><span class="line">      &#125;);</span><br></pre></td></tr></table></figure><h4 id="示意图-2"><a href="#示意图-2" class="headerlink" title="示意图"></a>示意图</h4><p>将红色与绿色雨滴过滤掉，保留蓝色雨滴。</p><p><img src="//jiamaoxiang.top/2020/04/12/Flink-DataStream-API编程指南/filter.png" alt></p><h3 id="keyBy"><a href="#keyBy" class="headerlink" title="keyBy"></a>keyBy</h3><h4 id="解释-3"><a href="#解释-3" class="headerlink" title="解释"></a>解释</h4><p>DataStream→KeyedStream，从逻辑上将流划分为不相交的分区。具有相同键的所有记录都分配给同一分区。在内部，keyBy（）是通过哈希分区实现的。<br>定义键值有3中方式：<br>(1)使用字段位置，如keyBy(1)，此方式是针对元组数据类型，比如tuple，使用元组相应元素的位置来定义键值;<br>(2)字段表达式,用于元组、POJO以及样例类;<br>(3)键值选择器，即keySelector，可以从输入事件中提取键值</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; userBehaviorkeyBy = userBehavior.map(<span class="keyword">new</span> RichMapFunction&lt;UserBehavior, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">map</span><span class="params">(UserBehavior value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Tuple2.of(value.action.toString(), <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(<span class="number">0</span>) <span class="comment">// scala元组编号从1开始，java元组编号是从0开始</span></span><br><span class="line">           .sum(<span class="number">1</span>); <span class="comment">//滚动聚合</span></span><br></pre></td></tr></table></figure><h4 id="示意图-3"><a href="#示意图-3" class="headerlink" title="示意图"></a>示意图</h4><p>基于形状对事件进行分区的keyBy操作</p><p><img src="//jiamaoxiang.top/2020/04/12/Flink-DataStream-API编程指南/keyBy.png" alt></p><h3 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h3><h4 id="解释-4"><a href="#解释-4" class="headerlink" title="解释"></a>解释</h4><p>KeyedStream → DataStream，对数据进行滚动聚合操作，结合当前元素和上一次Reduce返回的值进行聚合，然后返回一个新的值.将一个ReduceFunction应用在一个keyedStream上,每到来一个事件都会与当前reduce的结果进行聚合，<br>产生一个新的DataStream,该算子不会改变数据类型，因此输入流与输出流的类型永远保持一致。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; userBehaviorReduce = userBehavior.map(<span class="keyword">new</span> RichMapFunction&lt;UserBehavior, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">map</span><span class="params">(UserBehavior value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Tuple2.of(value.action.toString(), <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(<span class="number">0</span>) <span class="comment">// scala元组编号从1开始，java元组编号是从0开始</span></span><br><span class="line">          .reduce(<span class="keyword">new</span> RichReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">              <span class="meta">@Override</span></span><br><span class="line">              <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                  <span class="keyword">return</span> Tuple2.of(value1.f0,value1.f1 + value2.f1);<span class="comment">//滚动聚合,功能与sum类似</span></span><br><span class="line">              &#125;</span><br><span class="line">          &#125;);</span><br></pre></td></tr></table></figure><h4 id="示意图-4"><a href="#示意图-4" class="headerlink" title="示意图"></a>示意图</h4><p><img src="//jiamaoxiang.top/2020/04/12/Flink-DataStream-API编程指南/reduce.png" alt></p><h3 id="Aggregations-滚动聚合"><a href="#Aggregations-滚动聚合" class="headerlink" title="Aggregations(滚动聚合)"></a>Aggregations(滚动聚合)</h3><p>KeyedStream → DataStream，Aggregations(滚动聚合),滚动聚合转换作用于KeyedStream流上，生成一个包含聚合结果(比如sum求和，min最小值)的DataStream，滚动聚合的转换会为每个流过该算子的key值保存一个聚合结果，<br>当有新的元素流过该算子时，会根据之前的结果值和当前的元素值，更新相应的结果值</p><ul><li><p>sum():滚动聚合流过该算子的指定字段的和；</p></li><li><p>min():滚动计算流过该算子的指定字段的最小值</p></li><li><p>max():滚动计算流过该算子的指定字段的最大值</p></li><li><p>minBy():滚动计算当目前为止流过该算子的最小值，返回该值对应的事件；</p></li><li><p>maxBy():滚动计算当目前为止流过该算子的最大值，返回该值对应的事件；</p></li></ul><h3 id="union"><a href="#union" class="headerlink" title="union"></a>union</h3><h4 id="解释-5"><a href="#解释-5" class="headerlink" title="解释"></a>解释</h4><p>DataStream* → DataStream，将多条流合并，新的的流会包括所有流的数据，值得注意的是，两个流的数据类型必须一致，另外，来自两条流的事件会以FIFO(先进先出)的方式合并，所以并不能保证两条流的顺序，此外，union算子不会对数据去重，每个输入事件都会被发送到下游算子。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">userBehaviorkeyBy.union(userBehaviorReduce).print();<span class="comment">//将两条流union在一起，可以支持多条流(大于2)的union</span></span><br></pre></td></tr></table></figure><h4 id="示意图-5"><a href="#示意图-5" class="headerlink" title="示意图"></a>示意图</h4><p><img src="//jiamaoxiang.top/2020/04/12/Flink-DataStream-API编程指南/union.png" alt></p><h3 id="connect"><a href="#connect" class="headerlink" title="connect"></a>connect</h3><h4 id="解释-6"><a href="#解释-6" class="headerlink" title="解释"></a>解释</h4><p>DataStream,DataStream → ConnectedStreams，将两个流的事件进行组合，返回一个ConnectedStreams对象，两个流的数据类型可以不一致,ConnectedStreams对象提供了类似于map(),flatMap()功能的算子，如CoMapFunction与CoFlatMapFunction分别表示map()与flatMap算子，这两个算子会分别作用于两条流，注意：CoMapFunction 或CoFlatMapFunction被调用的时候并不能控制事件的顺序只要有事件流过该算子，该算子就会被调用。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ConnectedStreams&lt;UserBehavior, Tuple2&lt;String, Integer&gt;&gt; behaviorConnectedStreams = userBehaviorFilter.connect(userBehaviorkeyBy);</span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple3&lt;String, String, Integer&gt;&gt; behaviorConnectedStreamsmap = behaviorConnectedStreams.map(<span class="keyword">new</span> RichCoMapFunction&lt;UserBehavior, Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple3&lt;String, String, Integer&gt; <span class="title">map1</span><span class="params">(UserBehavior value1)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Tuple3.of(<span class="string">"first"</span>, value1.action, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple3&lt;String, String, Integer&gt; <span class="title">map2</span><span class="params">(Tuple2&lt;String, Integer&gt; value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Tuple3.of(<span class="string">"second"</span>, value2.f0, value2.f1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><h3 id="split"><a href="#split" class="headerlink" title="split"></a>split</h3><h4 id="解释-7"><a href="#解释-7" class="headerlink" title="解释"></a>解释</h4><p>DataStream → SplitStream，将流分割成两条或多条流，与union相反。分割之后的流与输入流的数据类型一致，<br>对于每个到来的事件可以被路由到0个、1个或多个输出流中。可以实现过滤与复制事件的功能，DataStream.split()接收一个OutputSelector函数，用来定义分流的规则，即将满足不同条件的流分配到用户命名的一个输出。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> SplitStream&lt;UserBehavior&gt; userBehaviorSplitStream = userBehavior.split(<span class="keyword">new</span> OutputSelector&lt;UserBehavior&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(UserBehavior value)</span> </span>&#123;</span><br><span class="line">                ArrayList&lt;String&gt; userBehaviors = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">                <span class="keyword">if</span> (value.action.equals(<span class="string">"buy"</span>)) &#123;</span><br><span class="line">                    userBehaviors.add(<span class="string">"buy"</span>);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    userBehaviors.add(<span class="string">"other"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> userBehaviors;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">userBehaviorSplitStream.select(<span class="string">"buy"</span>).print();</span><br></pre></td></tr></table></figure><h4 id="示意图-6"><a href="#示意图-6" class="headerlink" title="示意图"></a>示意图</h4><p><img src="//jiamaoxiang.top/2020/04/12/Flink-DataStream-API编程指南/split.png" alt></p><h2 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h2><p>Flink提供了许多内置的Sink，比如writeASText，print，HDFS，Kaka等等，下面将基于MySQL实现一个自定义的Sink，可以与自定义的MysqlSource进行对比，具体如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Created</span> with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@author</span> : jmx</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Date</span>: 2020/4/16</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Time</span>: 22:53</span></span><br><span class="line"><span class="comment"> *  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MysqlSink</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">UserBehavior</span>&gt; </span>&#123;</span><br><span class="line">    PreparedStatement pps;</span><br><span class="line">    <span class="keyword">public</span> Connection conn;</span><br><span class="line">    <span class="keyword">private</span> String driver;</span><br><span class="line">    <span class="keyword">private</span> String url;</span><br><span class="line">    <span class="keyword">private</span> String user;</span><br><span class="line">    <span class="keyword">private</span> String pass;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 在open() 方法初始化连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//初始化数据库连接参数</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        URL fileUrl = TestProperties.class.getClassLoader().getResource(<span class="string">"mysql.ini"</span>);</span><br><span class="line">        FileInputStream inputStream = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(fileUrl.toURI()));</span><br><span class="line">        properties.load(inputStream);</span><br><span class="line">        inputStream.close();</span><br><span class="line">        driver = properties.getProperty(<span class="string">"driver"</span>);</span><br><span class="line">        url = properties.getProperty(<span class="string">"url"</span>);</span><br><span class="line">        user = properties.getProperty(<span class="string">"user"</span>);</span><br><span class="line">        pass = properties.getProperty(<span class="string">"pass"</span>);</span><br><span class="line">        <span class="comment">//获取数据连接</span></span><br><span class="line">        conn = getConnection();</span><br><span class="line">        String insertSql = <span class="string">"insert into user_behavior values(?, ?, ?, ?,?, ?, ?, ?);"</span>;</span><br><span class="line">        pps = conn.prepareStatement(insertSql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 实现关闭连接</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                conn.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (pps != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                pps.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 调用invoke() 方法，进行数据插入</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(UserBehavior value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        pps.setLong(<span class="number">1</span>, value.userId);</span><br><span class="line">        pps.setLong(<span class="number">2</span>, value.itemId);</span><br><span class="line">        pps.setInt(<span class="number">3</span>, value.catId);</span><br><span class="line">        pps.setInt(<span class="number">4</span>, value.merchantId);</span><br><span class="line">        pps.setInt(<span class="number">5</span>, value.brandId);</span><br><span class="line">        pps.setString(<span class="number">6</span>, value.action);</span><br><span class="line">        pps.setString(<span class="number">7</span>, value.gender);</span><br><span class="line">        pps.setLong(<span class="number">8</span>, value.timestamp);</span><br><span class="line">        pps.executeUpdate();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取数据库连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> SQLException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Connection <span class="title">getConnection</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        Connection connnection = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//加载驱动</span></span><br><span class="line">            Class.forName(driver);</span><br><span class="line">            <span class="comment">//获取连接</span></span><br><span class="line">            connnection = DriverManager.getConnection(</span><br><span class="line">                    url,</span><br><span class="line">                    user,</span><br><span class="line">                    pass);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> connnection;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="关于RichFunction"><a href="#关于RichFunction" class="headerlink" title="关于RichFunction"></a>关于RichFunction</h2><p>细心的读者可以发现，在前文的算子操作案例中，使用的都是RichFunction，因为在很多时候需要在函数处理数据之前先进行一些初始化操作，或者获取函数的上下文信息，DataStream API提供了一类RichFunction，与普通的函数相比，该函数提供了许多额外的功能。</p><p>使用RichFunction的时候，可以实现两个额外的方法：</p><ul><li>open(),是初始化方法，会在每个人物首次调用转换方法(比如map)前调用一次。通常用于进行一次的设置工作，注意Configuration参数只在DataSet API中使用，而并没有在DataStream API中使用，因此在使用DataStream API时，可以将其忽略。</li><li>close()，函数的终止方法 ，会在每个任务最后一次调用转换方法后调用一次，通常用于资源释放等操作。</li></ul><p>此外用户还可以通过getRuntimeContext()方法访问函数的上下文信息(RuntimeContext),例如函数的并行度，函数所在subtask的编号以及执行函数的任务名称，同时也可以访问分区状态。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先实现了自定义MySQL Source，然后基于MySql 的Source进行了一系列的算子操作，并对常见的算子操作进行详细剖析，最后实现了一个自定义MySQL Sink，并对RichFunction进行了解释。</p><p><strong>代码地址</strong>:<a href="https://github.com/jiamx/study-flink" target="_blank" rel="noopener">https://github.com/jiamx/study-flink</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何使用Hive进行OLAP分析</title>
      <link href="/2020/04/09/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Hive%E8%BF%9B%E8%A1%8COLAP%E5%88%86%E6%9E%90/"/>
      <url>/2020/04/09/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Hive%E8%BF%9B%E8%A1%8COLAP%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>本文首先介绍了什么是OLAP，接着介绍Hive中提供的几种OLAP分析的函数，并对每一种函数进行了详细说明，并给出了相关的图示解释，最后以一个案例说明了这几种函数的使用方式，可以进一步加深理解。</p><a id="more"></a><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>在线分析处理(OLAP,Online Analytical Processing)是通过带层次的维度和跨维度进行多维分析的，简单理解为一种多维数据分析的方式，通过OLAP可以展示数据仓库中数据的多维逻辑视图。在多维分析中，数据是按照维度(观察数据的角度)来表示的，比如商品、城市、客户。而维通常按层次(层次维度)组织的，如城市、省、国家，再比如时间也是有层次的，如天、周、月、季度和年。不同的管理者可以从不同的维度(视角)去观察这些数据，这些在多个不同维度上对数据进行综合考察的手段就是通常所说的数据仓库多维查询，最常见的就如上卷(roll-up)和下钻(drill-down)了,所谓上卷，指的是选定特定的数据范围之后，对其进行汇总统计以获取更高层次的信息。所谓下钻，指的是选定特定的数据范围之后，需要进一步查看细节的数据。从另一种意义上说，钻取就是针对多维展现的数据，进一步探究其内部的组成和来源。值得注意的是，上卷和下钻要求维度具有层级结构，即数仓中所说的层次维度。</p><h2 id="如何实现数据的多维分析"><a href="#如何实现数据的多维分析" class="headerlink" title="如何实现数据的多维分析"></a>如何实现数据的多维分析</h2><p>Hive提供了多维数据分析的函数，如GROUPING SETS,GROUPING_ID,CUBE,ROLLUP,通过这些分析函数，可以轻而易举的实现多维数据分析。下面将会通过一个案例来了解这些函数的具体含义以及该如何使用这些函数。注意：在hive中使用这些函数之前，要确保开启了map端聚合，即set hive.map.aggr=true，否则会报如下错误：</p><p><img src="//jiamaoxiang.top/2020/04/09/如何使用Hive进行OLAP分析/map%E7%AB%AF%E8%81%9A%E5%90%88.png" alt></p><h3 id="简单介绍"><a href="#简单介绍" class="headerlink" title="简单介绍"></a>简单介绍</h3><ul><li>GROUPING SETS</li></ul><p>在一个group by查询中，通过该子句可以对不同维度或同一维度的不同层次进行聚合，简单理解为一条sql可以实现多种不同的分组规则，用户可以在该函数中传入自己定义的多种分组字段，本质上等价于多个group by语句进行UNION，对于GROUPING SETS子句中的空白集’（）’表示对总体进行聚集。</p><p><strong>示例模板</strong></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 使用GROUPING SETS查询</span></span><br><span class="line"><span class="keyword">SELECT</span> a,</span><br><span class="line">       b,</span><br><span class="line">       <span class="keyword">SUM</span>(c)</span><br><span class="line"><span class="keyword">FROM</span> tab1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> a,b</span><br><span class="line"><span class="keyword">GROUPING</span> <span class="keyword">SETS</span> ((a,b), a,b, ());</span><br><span class="line"><span class="comment">-- 与GROUP BY等价关系</span></span><br><span class="line"><span class="keyword">SELECT</span> a, b, <span class="keyword">SUM</span>( c ) <span class="keyword">FROM</span> tab1 <span class="keyword">GROUP</span> <span class="keyword">BY</span> a, b</span><br><span class="line"><span class="keyword">UNION</span></span><br><span class="line"><span class="keyword">SELECT</span> a, <span class="literal">null</span>, <span class="keyword">SUM</span>( c ) <span class="keyword">FROM</span> tab1 <span class="keyword">GROUP</span> <span class="keyword">BY</span> a, <span class="literal">null</span></span><br><span class="line"><span class="keyword">UNION</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="literal">null</span>, b, <span class="keyword">SUM</span>( c ) <span class="keyword">FROM</span> tab1 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="literal">null</span>, b</span><br><span class="line"><span class="keyword">UNION</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="literal">null</span>, <span class="literal">null</span>, <span class="keyword">SUM</span>( c ) <span class="keyword">FROM</span> tab1;</span><br></pre></td></tr></table></figure><ul><li>GROUPING__ID</li></ul><p>当使用聚合时，有时候会出现数据本身为null值，很难区分究竟是数据列本身为null值还是聚合数据行为null，即无法区分查询结果中的null值是属于列本身的还是聚合的结果行，因此需要一种方法识别出列中的null值。grouping_id 函数就是此场景下的解决方案。注意该函数是有两个下划线。这个函数为每种聚合数据行生成唯一的组id。它的返回值看起来像整型数值，其实是字符串类型，这个值使用了位图策略（bitvector，位向量），即它的二进制形式中的每一位表示对应列是否参与分组，如果某一列参与了分组，对应位就被置为1，否则为0。通过这种方式可以区分出数据本身中的null值。看到这是不是还是一头雾水，没关系，来看下面的示例：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_grouping__id(<span class="keyword">id</span> <span class="built_in">int</span>,amount <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>));</span><br><span class="line"><span class="comment">-- 插入数据</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> test_grouping__id <span class="keyword">values</span>(<span class="number">1</span>,<span class="literal">null</span>),(<span class="number">1</span>,<span class="number">1</span>),(<span class="number">2</span>,<span class="number">2</span>),(<span class="number">3</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="literal">null</span>),(<span class="number">4</span>,<span class="number">5</span>);</span><br><span class="line"><span class="comment">--执行查询</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">id</span>,</span><br><span class="line">       amount,</span><br><span class="line">       grouping__id,</span><br><span class="line">       <span class="keyword">count</span>(*) cnt</span><br><span class="line"><span class="keyword">FROM</span> test_grouping__id</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">id</span>,</span><br><span class="line">         amount</span><br><span class="line"><span class="keyword">GROUPING</span> <span class="keyword">sets</span>(<span class="keyword">id</span>,(<span class="keyword">id</span>,amount),())</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> grouping__id</span><br></pre></td></tr></table></figure><p><strong>查询结果分析</strong></p><p>查询结果如下图所示：绿色框表示未进行分组，即进行全局聚合，grouping_id等于0，表示没有字段参与分组。蓝色框表示按照id进行分组，对应的grouping_id为1，表示只有一个字段参与了分组。橘色的框表示按照id和amount两个字段进行分组，grouping_id为3，即有两个字段参与了分组，转成十进制为2^0 + 2^1  = 3。</p><p><img src="//jiamaoxiang.top/2020/04/09/如何使用Hive进行OLAP分析/grouping__id.png" alt></p><p>以上面为例，分组字段为id、amount，转成二进制表示形式为：</p><p><img src="//jiamaoxiang.top/2020/04/09/如何使用Hive进行OLAP分析/%E4%BA%8C%E8%BF%9B%E5%88%B6.png" alt></p><ul><li>ROLLUP</li></ul><p>通用的语法为WITH ROLLUP,需要与group by一起用于在维的层次结构级别上计算聚合。功能为可以按照group by的分组字段进行组合，计算出不同分组的结果。注意对于分组字段的组合会与最左边的字段为主。使用ROLLUP的GROUP BY a，b，c假定层次结构是“ a”向下钻取到“ b”，“ b”向下钻取到“ c”。则可以通过GROUP BY a，b，c，WITH ROLLUP进行实现，该语句等价于GROUP BY a，b，c GROUPING SETS（（a，b，c），（a，b），（a），（））。即使用WITH ROLLUP，首先会对全局聚合(不分组)，然后会按GROUP BY字段组合，进行聚合，但是最左侧的分组字段必须参与分组，比如a字段是最左侧的字段，则a必定参与分组组合。</p><p><strong>示例模板</strong></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 使用WITH ROLLUP查询</span></span><br><span class="line"><span class="keyword">SELECT</span> a,</span><br><span class="line">       b,</span><br><span class="line">       c</span><br><span class="line">       <span class="keyword">SUM</span>(d)</span><br><span class="line"><span class="keyword">FROM</span> tab1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> a,b,c</span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">ROLLUP</span></span><br><span class="line"><span class="comment">-- 等价于下面的方式</span></span><br><span class="line"><span class="keyword">SELECT</span> a,</span><br><span class="line">       b,</span><br><span class="line">       c,</span><br><span class="line">       <span class="keyword">SUM</span>(d)</span><br><span class="line"><span class="keyword">FROM</span> tab1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> a,b,c</span><br><span class="line"><span class="keyword">GROUPING</span> <span class="keyword">SETS</span> ((a,b,c), (a,b), (a),());</span><br></pre></td></tr></table></figure><ul><li>CUBE</li></ul><p>CUBE表示一个立方体，apache的kylin使用就是这种预计算方式。即会对给定的维度(分组字段)进行多种组合之后，形成不同分组规则的数据结果。一旦我们在一组维度上计算出CUBE，就可以得到这些维度上所有可能的聚合聚合结果。比如：<strong>GROUP BY a，b，c WITH CUBE</strong>，等价于<strong>GROUP BY a，b，c GROUPING SETS（（a，b，c），（a，b），（b，c）， （a，c），（a），（b），（c），（））</strong>。</p><p>其实，可以将上面的情况抽象成排列组合的问题，即从分组字段集合(假设有n个字段)中随意取出0~n个字段，那么会有多少中组合方式，如下面公式所示：</p><p><img src="//jiamaoxiang.top/2020/04/09/如何使用Hive进行OLAP分析/%E7%BB%84%E5%90%881.png" alt></p><p>结合上面的例子，<strong>GROUP BY a，b，c WITH CUBE</strong>，那么所有的组合方式有：（a，b，c），（a，b），（b，c）， （a，c），（a），（b），（c），（）,一共有8种组合，即2^3 = 8。</p><p><strong>示例模板</strong></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 使用WITH CUBE查询</span></span><br><span class="line"><span class="keyword">SELECT</span> a,</span><br><span class="line">       b,</span><br><span class="line">       c</span><br><span class="line">       <span class="keyword">SUM</span>(d)</span><br><span class="line"><span class="keyword">FROM</span> tab1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> a,b,c</span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">CUBE</span></span><br><span class="line"><span class="comment">-- 等价于下面的方式</span></span><br><span class="line"><span class="keyword">SELECT</span> a,</span><br><span class="line">       b,</span><br><span class="line">       c,</span><br><span class="line">       <span class="keyword">SUM</span>(d)</span><br><span class="line"><span class="keyword">FROM</span> tab1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> a,b,c</span><br><span class="line"><span class="keyword">GROUPING</span> <span class="keyword">SETS</span> ((a,b,c),(a,b),(b,c), (a,c),(a),(b),(c),());</span><br></pre></td></tr></table></figure><h3 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h3><h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><p>有一份用户行为数据集，包括用户的所有行为（包括pv点击、buy购买、cart加购、fav收藏），具体如下表所示：</p><table><thead><tr><th>字段名</th><th align="left">列名称</th><th align="left">说明</th></tr></thead><tbody><tr><td>user_id</td><td align="left">用户ID</td><td align="left">整数类型，用户ID</td></tr><tr><td>item_id</td><td align="left">商品ID</td><td align="left">整数类型，商品ID</td></tr><tr><td>category_id</td><td align="left">商品类目ID</td><td align="left">整数类型，商品所属类目ID</td></tr><tr><td>behavior</td><td align="left">行为类型</td><td align="left">字符串，枚举类型，包括(‘pv’, ‘buy’, ‘cart’, ‘fav’)</td></tr><tr><td>access_time</td><td align="left">时间戳</td><td align="left">行为发生的时间戳，单位秒</td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_behavior</span><br><span class="line">             (</span><br><span class="line">                user_id <span class="built_in">int</span> ,</span><br><span class="line">                item_id <span class="built_in">int</span>,</span><br><span class="line">                category_id <span class="built_in">int</span>,</span><br><span class="line">                behavior <span class="keyword">string</span>,</span><br><span class="line">               access_time <span class="keyword">string</span></span><br><span class="line">               )</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span>;</span><br><span class="line"><span class="comment">-- 装载数据</span></span><br><span class="line">1,101,1,pv,1511658000</span><br><span class="line">2,102,1,pv,1511658000</span><br><span class="line">3,103,1,pv,1511658000</span><br><span class="line">4,104,2,cart,1511659329</span><br><span class="line">5,105,2,buy,1511659326</span><br><span class="line">6,106,3,fav,1511659323</span><br><span class="line">7,101,1,pv,1511658010</span><br><span class="line">8,102,1,buy,1511658200</span><br><span class="line">9,103,1,cart,1511658030</span><br><span class="line">10,107,3,fav,1511659332</span><br></pre></td></tr></table></figure><h4 id="GROUPING-SETS使用"><a href="#GROUPING-SETS使用" class="headerlink" title="GROUPING SETS使用"></a>GROUPING SETS使用</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询每种商品品类、每种用户行为的访问次数</span></span><br><span class="line"><span class="comment">-- 查询每种用户行为的访问次数</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">item_id,</span><br><span class="line">category_id,</span><br><span class="line">behavior,</span><br><span class="line"><span class="keyword">COUNT</span>(*) <span class="keyword">AS</span> cnt,</span><br><span class="line">GROUPING__ID </span><br><span class="line"><span class="keyword">FROM</span> user_behavior </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> item_id,category_id,behavior </span><br><span class="line"><span class="keyword">GROUPING</span> <span class="keyword">SETS</span> ((category_id,behavior),behavior)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br></pre></td></tr></table></figure><p><strong>结果如下：</strong></p><p><img src="//jiamaoxiang.top/2020/04/09/如何使用Hive进行OLAP分析/grouping_set.png" alt></p><h4 id="ROLLUP使用"><a href="#ROLLUP使用" class="headerlink" title="ROLLUP使用"></a>ROLLUP使用</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询每种商品品类的访问次数</span></span><br><span class="line"><span class="comment">-- 查询每种商品品类、每种用户行为的次数</span></span><br><span class="line"><span class="comment">-- 查询用户的总访问次数</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">category_id,</span><br><span class="line">behavior,</span><br><span class="line"><span class="keyword">COUNT</span>(*) <span class="keyword">AS</span> cnt,</span><br><span class="line">GROUPING__ID </span><br><span class="line"><span class="keyword">FROM</span> user_behavior </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> category_id,behavior </span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">ROLLUP</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br></pre></td></tr></table></figure><p><strong>结果如下：</strong></p><p><img src="//jiamaoxiang.top/2020/04/09/如何使用Hive进行OLAP分析/rollup.png" alt></p><h4 id="CUBE使用"><a href="#CUBE使用" class="headerlink" title="CUBE使用"></a>CUBE使用</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询每种商品品类的访问次数</span></span><br><span class="line"><span class="comment">-- 查询每种用户行为的次数</span></span><br><span class="line"><span class="comment">-- 查询每种商品品类、每种用户行为的次数</span></span><br><span class="line"><span class="comment">-- 查询用户的总访问次数</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">category_id,</span><br><span class="line">behavior,</span><br><span class="line"><span class="keyword">COUNT</span>(*) <span class="keyword">AS</span> cnt,</span><br><span class="line">GROUPING__ID </span><br><span class="line"><span class="keyword">FROM</span> user_behavior </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> category_id,behavior </span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">CUBE</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br></pre></td></tr></table></figure><p><strong>结果如下：</strong></p><p><img src="//jiamaoxiang.top/2020/04/09/如何使用Hive进行OLAP分析/cube.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先介绍了什么是OLAP，接着介绍Hive中提供的几种OLAP分析的函数，并对每一种函数进行了详细说明，并给出了相关的图示解释，最后以一个案例说明了这几种函数的使用方式，可以进一步加深理解。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>你真的了解Flink Kafka source吗？</title>
      <link href="/2020/04/02/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3Flink-Kafka-connector%E5%90%97%EF%BC%9F/"/>
      <url>/2020/04/02/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3Flink-Kafka-connector%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>Flink 提供了专门的 Kafka 连接器，向 Kafka topic 中读取或者写入数据。Flink Kafka Consumer 集成了 Flink 的 Checkpoint 机制，可提供 exactly-once 的处理语义。为此，Flink 并不完全依赖于跟踪 Kafka 消费组的偏移量，而是在内部跟踪和检查偏移量。</p><a id="more"></a><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>当我们在使用Spark Streaming、Flink等计算框架进行数据实时处理时，使用Kafka作为一款发布与订阅的消息系统成为了标配。Spark Streaming与Flink都提供了相对应的Kafka Consumer，使用起来非常的方便，只需要设置一下Kafka的参数，然后添加kafka的source就万事大吉了。如果你真的觉得事情就是如此的so easy，感觉妈妈再也不用担心你的学习了，那就真的是too young too simple sometimes naive了。本文以Flink 的Kafka Source为讨论对象，首先从基本的使用入手，然后深入源码逐一剖析，一并为你拨开Flink Kafka connector的神秘面纱。值得注意的是，本文假定读者具备了Kafka的相关知识，关于Kafka的相关细节问题，不在本文的讨论范围之内。</p><h2 id="Flink-Kafka-Consumer介绍"><a href="#Flink-Kafka-Consumer介绍" class="headerlink" title="Flink Kafka Consumer介绍"></a>Flink Kafka Consumer介绍</h2><p>Flink Kafka Connector有很多个版本，可以根据你的kafka和Flink的版本选择相应的包（maven artifact id）和类名。本文所涉及的Flink版本为1.10，Kafka的版本为2.3.4。Flink所提供的Maven依赖于类名如下表所示：</p><table><thead><tr><th align="left">Maven 依赖</th><th align="left">自从哪个版本 开始支持</th><th align="left">类名</th><th align="left">Kafka 版本</th><th align="left">注意</th></tr></thead><tbody><tr><td align="left">flink-connector-kafka-0.8_2.11</td><td align="left">1.0.0</td><td align="left">FlinkKafkaConsumer08 FlinkKafkaProducer08</td><td align="left">0.8.x</td><td align="left">这个连接器在内部使用 Kafka 的 <a href="https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+SimpleConsumer+Example" target="_blank" rel="noopener">SimpleConsumer</a> API。偏移量由 Flink 提交给 ZK。</td></tr><tr><td align="left">flink-connector-kafka-0.9_2.11</td><td align="left">1.0.0</td><td align="left">FlinkKafkaConsumer09 FlinkKafkaProducer09</td><td align="left">0.9.x</td><td align="left">这个连接器使用新的 Kafka <a href="http://kafka.apache.org/documentation.html#newconsumerapi" target="_blank" rel="noopener">Consumer API</a></td></tr><tr><td align="left">flink-connector-kafka-0.10_2.11</td><td align="left">1.2.0</td><td align="left">FlinkKafkaConsumer010 FlinkKafkaProducer010</td><td align="left">0.10.x</td><td align="left">这个连接器支持 <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message" target="_blank" rel="noopener">带有时间戳的 Kafka 消息</a>，用于生产和消费。</td></tr><tr><td align="left">flink-connector-kafka-0.11_2.11</td><td align="left">1.4.0</td><td align="left">FlinkKafkaConsumer011 FlinkKafkaProducer011</td><td align="left">&gt;=  0.11.x</td><td align="left">Kafka 从 0.11.x 版本开始不支持 Scala 2.10。此连接器支持了 <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging" target="_blank" rel="noopener">Kafka 事务性的消息传递</a>来为生产者提供 Exactly once 语义。</td></tr><tr><td align="left">flink-connector-kafka_2.11</td><td align="left">1.7.0</td><td align="left">FlinkKafkaConsumer FlinkKafkaProducer</td><td align="left">&gt;= 1.0.0</td><td align="left">这个通用的 Kafka 连接器尽力与 Kafka client 的最新版本保持同步。该连接器使用的 Kafka client 版本可能会在 Flink 版本之间发生变化。从 Flink 1.9 版本开始，它使用 Kafka 2.2.0 client。当前 Kafka 客户端向后兼容 0.10.0 或更高版本的 Kafka broker。 但是对于 Kafka 0.11.x 和 0.10.x 版本，我们建议你分别使用专用的 flink-connector-kafka-0.11_2.11 和 flink-connector-kafka-0.10_2.11 连接器。</td></tr></tbody></table><h2 id="Demo示例"><a href="#Demo示例" class="headerlink" title="Demo示例"></a>Demo示例</h2><h3 id="添加Maven依赖"><a href="#添加Maven依赖" class="headerlink" title="添加Maven依赖"></a>添加Maven依赖</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;!--本文使用的是通用型的connector--&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="简单代码案例"><a href="#简单代码案例" class="headerlink" title="简单代码案例"></a>简单代码案例</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConnector</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment senv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">// 开启checkpoint，时间间隔为毫秒</span></span><br><span class="line">        senv.enableCheckpointing(<span class="number">5000L</span>);</span><br><span class="line">        <span class="comment">// 选择状态后端</span></span><br><span class="line">        senv.setStateBackend((StateBackend) <span class="keyword">new</span> FsStateBackend(<span class="string">"file:///E://checkpoint"</span>));</span><br><span class="line">        <span class="comment">//senv.setStateBackend((StateBackend) new FsStateBackend("hdfs://kms-1:8020/checkpoint"));</span></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// kafka broker地址</span></span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"kms-2:9092,kms-3:9092,kms-4:9092"</span>);</span><br><span class="line">        <span class="comment">// 仅kafka0.8版本需要配置</span></span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"kms-2:2181,kms-3:2181,kms-4:2181"</span>);</span><br><span class="line">        <span class="comment">// 消费者组</span></span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line">        <span class="comment">// 自动偏移量提交</span></span><br><span class="line">        props.put(<span class="string">"enable.auto.commit"</span>, <span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">// 偏移量提交的时间间隔，毫秒</span></span><br><span class="line">        props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="number">5000</span>);</span><br><span class="line">        <span class="comment">// kafka 消息的key序列化器</span></span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        <span class="comment">// kafka 消息的value序列化器</span></span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        <span class="comment">// 指定kafka的消费者从哪里开始消费数据</span></span><br><span class="line">        <span class="comment">// 共有三种方式，</span></span><br><span class="line">        <span class="comment">// #earliest</span></span><br><span class="line">        <span class="comment">// 当各分区下有已提交的offset时，从提交的offset开始消费；</span></span><br><span class="line">        <span class="comment">// 无提交的offset时，从头开始消费</span></span><br><span class="line">        <span class="comment">// #latest</span></span><br><span class="line">        <span class="comment">// 当各分区下有已提交的offset时，从提交的offset开始消费；</span></span><br><span class="line">        <span class="comment">// 无提交的offset时，消费新产生的该分区下的数据</span></span><br><span class="line">        <span class="comment">// #none</span></span><br><span class="line">        <span class="comment">// topic各分区都存在已提交的offset时，</span></span><br><span class="line">        <span class="comment">// 从offset后开始消费；</span></span><br><span class="line">        <span class="comment">// 只要有一个分区不存在已提交的offset，则抛出异常</span></span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; consumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(</span><br><span class="line">                <span class="string">"qfbap_ods.code_city"</span>,</span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props);</span><br><span class="line">        <span class="comment">//设置checkpoint后在提交offset，即oncheckpoint模式</span></span><br><span class="line">        <span class="comment">// 该值默认为true，</span></span><br><span class="line">        consumer.setCommitOffsetsOnCheckpoints(<span class="keyword">true</span>);</span><br><span class="line">     </span><br><span class="line">        <span class="comment">// 最早的数据开始消费</span></span><br><span class="line">        <span class="comment">// 该模式下，Kafka 中的 committed offset 将被忽略，不会用作起始位置。</span></span><br><span class="line">        <span class="comment">//consumer.setStartFromEarliest();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 消费者组最近一次提交的偏移量，默认。</span></span><br><span class="line">        <span class="comment">// 如果找不到分区的偏移量，那么将会使用配置中的 auto.offset.reset 设置</span></span><br><span class="line">        <span class="comment">//consumer.setStartFromGroupOffsets();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 最新的数据开始消费</span></span><br><span class="line">        <span class="comment">// 该模式下，Kafka 中的 committed offset 将被忽略，不会用作起始位置。</span></span><br><span class="line">        <span class="comment">//consumer.setStartFromLatest();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定具体的偏移量时间戳,毫秒</span></span><br><span class="line">        <span class="comment">// 对于每个分区，其时间戳大于或等于指定时间戳的记录将用作起始位置。</span></span><br><span class="line">        <span class="comment">// 如果一个分区的最新记录早于指定的时间戳，则只从最新记录读取该分区数据。</span></span><br><span class="line">        <span class="comment">// 在这种模式下，Kafka 中的已提交 offset 将被忽略，不会用作起始位置。</span></span><br><span class="line">        <span class="comment">//consumer.setStartFromTimestamp(1585047859000L);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 为每个分区指定偏移量</span></span><br><span class="line">        <span class="comment">/*Map&lt;KafkaTopicPartition, Long&gt; specificStartOffsets = new HashMap&lt;&gt;();</span></span><br><span class="line"><span class="comment">        specificStartOffsets.put(new KafkaTopicPartition("qfbap_ods.code_city", 0), 23L);</span></span><br><span class="line"><span class="comment">        specificStartOffsets.put(new KafkaTopicPartition("qfbap_ods.code_city", 1), 31L);</span></span><br><span class="line"><span class="comment">        specificStartOffsets.put(new KafkaTopicPartition("qfbap_ods.code_city", 2), 43L);</span></span><br><span class="line"><span class="comment">        consumer1.setStartFromSpecificOffsets(specificStartOffsets);*/</span></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 请注意：当 Job 从故障中自动恢复或使用 savepoint 手动恢复时，</span></span><br><span class="line"><span class="comment">         * 这些起始位置配置方法不会影响消费的起始位置。</span></span><br><span class="line"><span class="comment">         * 在恢复时，每个 Kafka 分区的起始位置由存储在 savepoint 或 checkpoint 中的 offset 确定</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; source = senv.addSource(consumer);</span><br><span class="line">        <span class="comment">// TODO</span></span><br><span class="line">        source.print();</span><br><span class="line">        senv.execute(<span class="string">"test kafka connector"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="参数配置解读"><a href="#参数配置解读" class="headerlink" title="参数配置解读"></a>参数配置解读</h3><p>在Demo示例中，给出了详细的配置信息，下面将对上面的参数配置进行逐一分析。</p><h4 id="kakfa的properties参数配置"><a href="#kakfa的properties参数配置" class="headerlink" title="kakfa的properties参数配置"></a>kakfa的properties参数配置</h4><ul><li><p>bootstrap.servers：kafka broker地址</p></li><li><p>zookeeper.connect：仅kafka0.8版本需要配置</p></li><li><p>group.id：消费者组</p></li><li><p>enable.auto.commit：</p><p>自动偏移量提交，该值的配置不是最终的偏移量提交模式，需要考虑用户是否开启了checkpoint，</p><p>在下面的源码分析中会进行解读</p></li><li><p>auto.commit.interval.ms：偏移量提交的时间间隔，毫秒</p></li><li><p>key.deserializer：</p><p>kafka 消息的key序列化器，如果不指定会使用ByteArrayDeserializer序列化器</p></li><li><p>value.deserializer：</p></li></ul><p>kafka 消息的value序列化器，如果不指定会使用ByteArrayDeserializer序列化器</p><ul><li><p>auto.offset.reset：</p><p>指定kafka的消费者从哪里开始消费数据，共有三种方式，</p><ul><li>第一种：earliest<br>当各分区下有已提交的offset时，从提交的offset开始消费； 无提交的offset时，从头开始消费</li><li>第二种：latest<br>当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据</li><li>第三种：none<br>topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常</li></ul><p>注意：上面的指定消费模式并不是最终的消费模式，取决于用户在Flink程序中配置的消费模式</p></li></ul><h4 id="Flink程序用户配置的参数"><a href="#Flink程序用户配置的参数" class="headerlink" title="Flink程序用户配置的参数"></a>Flink程序用户配置的参数</h4><ul><li>consumer.setCommitOffsetsOnCheckpoints(true)</li></ul><p>​    解释：设置checkpoint后在提交offset，即oncheckpoint模式，该值默认为true，该参数会影响偏移量的提交方式，下面的源码中会进行分析</p><ul><li><p>consumer.setStartFromEarliest()</p><p>解释： 最早的数据开始消费 ，该模式下，Kafka 中的 committed offset 将被忽略，不会用作起始位置。该方法为继承父类FlinkKafkaConsumerBase的方法。</p></li><li><p>consumer.setStartFromGroupOffsets()</p><p>解释：消费者组最近一次提交的偏移量，默认。 如果找不到分区的偏移量，那么将会使用配置中的 auto.offset.reset 设置，该方法为继承父类FlinkKafkaConsumerBase的方法。</p></li><li><p>consumer.setStartFromLatest()</p><p>解释：最新的数据开始消费，该模式下，Kafka 中的 committed offset 将被忽略，不会用作起始位置。该方法为继承父类FlinkKafkaConsumerBase的方法。</p></li><li><p>consumer.setStartFromTimestamp(1585047859000L)</p><p>解释：指定具体的偏移量时间戳,毫秒。对于每个分区，其时间戳大于或等于指定时间戳的记录将用作起始位置。 如果一个分区的最新记录早于指定的时间戳，则只从最新记录读取该分区数据。在这种模式下，Kafka 中的已提交 offset 将被忽略，不会用作起始位置。</p></li><li><p>consumer.setStartFromSpecificOffsets(specificStartOffsets)</p></li></ul><p>解释：为每个分区指定偏移量，该方法为继承父类FlinkKafkaConsumerBase的方法。</p><p>请注意：当 Job 从故障中自动恢复或使用 savepoint 手动恢复时，这些起始位置配置方法不会影响消费的起始位置。在恢复时，每个 Kafka 分区的起始位置由存储在 savepoint 或 checkpoint 中的 offset 确定。</p><h2 id="Flink-Kafka-Consumer源码解读"><a href="#Flink-Kafka-Consumer源码解读" class="headerlink" title="Flink Kafka Consumer源码解读"></a>Flink Kafka Consumer源码解读</h2><h3 id="继承关系"><a href="#继承关系" class="headerlink" title="继承关系"></a>继承关系</h3><p>Flink Kafka Consumer继承了FlinkKafkaConsumerBase抽象类，而FlinkKafkaConsumerBase抽象类又继承了RichParallelSourceFunction，所以要实现一个自定义的source时，有两种实现方式：一种是通过实现SourceFunction接口来自定义并行度为1的数据源；另一种是通过实现ParallelSourceFunction接口或者继承RichParallelSourceFunction来自定义具有并行度的数据源。FlinkKafkaConsumer的继承关系如下图所示。</p><p><img src="//jiamaoxiang.top/2020/04/02/你真的了解Flink-Kafka-connector吗？/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3Flink-Kafka-connector%E5%90%97%EF%BC%9F%5C%E7%BB%A7%E6%89%BF%E5%9B%BE.png" alt></p><h3 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h3><h4 id="FlinkKafkaConsumer源码"><a href="#FlinkKafkaConsumer源码" class="headerlink" title="FlinkKafkaConsumer源码"></a>FlinkKafkaConsumer源码</h4><p>先看一下FlinkKafkaConsumer的源码，为了方面阅读，本文将尽量给出本比较完整的源代码片段，具体如下所示：代码较长，在这里可以先有有一个总体的印象，下面会对重要的代码片段详细进行分析。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkKafkaConsumer</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">FlinkKafkaConsumerBase</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 配置轮询超时超时时间，使用flink.poll-timeout参数在properties进行配置</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String KEY_POLL_TIMEOUT = <span class="string">"flink.poll-timeout"</span>;</span><br><span class="line"><span class="comment">// 如果没有可用数据，则等待轮询所需的时间（以毫秒为单位）。 如果为0，则立即返回所有可用的记录</span></span><br><span class="line"><span class="comment">//默认轮询超时时间</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> DEFAULT_POLL_TIMEOUT = <span class="number">100L</span>;</span><br><span class="line"><span class="comment">// 用户提供的kafka 参数配置</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> Properties properties;</span><br><span class="line"><span class="comment">// 如果没有可用数据，则等待轮询所需的时间（以毫秒为单位）。 如果为0，则立即返回所有可用的记录</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">long</span> pollTimeout;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topic                   消费的主题名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> valueDeserializer       反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props                   用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(String topic, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(Collections.singletonList(topic), valueDeserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * 该构造方法允许传入KafkaDeserializationSchema，该反序列化类支持访问kafka消费的额外信息</span></span><br><span class="line"><span class="comment"> * 比如：key/value对，offsets(偏移量)，topic(主题名称)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topic                消费的主题名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer         反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props                用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(String topic, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(Collections.singletonList(topic), deserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * 该构造方法允许传入多个topic(主题)，支持消费多个主题</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topics          消费的主题名称，多个主题为List集合</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer    反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props           用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(List&lt;String&gt; topics, DeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(topics, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(deserializer), props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * 该构造方法允许传入多个topic(主题)，支持消费多个主题,</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topics         消费的主题名称，多个主题为List集合</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer   反序列化类型，用于将kafka的字节消息转换为Flink的对象,支持获取额外信息</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props          用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(List&lt;String&gt; topics, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(topics, <span class="keyword">null</span>, deserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于正则表达式订阅多个topic</span></span><br><span class="line"><span class="comment"> * 如果开启了分区发现，即FlinkKafkaConsumer.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值为非负数</span></span><br><span class="line"><span class="comment"> * 只要是能够正则匹配上，主题一旦被创建就会立即被订阅</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> subscriptionPattern   主题的正则表达式</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> valueDeserializer   反序列化类型，用于将kafka的字节消息转换为Flink的对象,支持获取额外信息</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props               用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(Pattern subscriptionPattern, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(<span class="keyword">null</span>, subscriptionPattern, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(valueDeserializer), props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于正则表达式订阅多个topic</span></span><br><span class="line"><span class="comment"> * 如果开启了分区发现，即FlinkKafkaConsumer.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值为非负数</span></span><br><span class="line"><span class="comment"> * 只要是能够正则匹配上，主题一旦被创建就会立即被订阅</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> subscriptionPattern   主题的正则表达式</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer          该反序列化类支持访问kafka消费的额外信息,比如：key/value对，offsets(偏移量)，topic(主题名称)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props                 用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(Pattern subscriptionPattern, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(<span class="keyword">null</span>, subscriptionPattern, deserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">List&lt;String&gt; topics,</span></span></span><br><span class="line"><span class="function"><span class="params">Pattern subscriptionPattern,</span></span></span><br><span class="line"><span class="function"><span class="params">KafkaDeserializationSchema&lt;T&gt; deserializer,</span></span></span><br><span class="line"><span class="function"><span class="params">Properties props)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 调用父类(FlinkKafkaConsumerBase)构造方法，PropertiesUtil.getLong方法第一个参数为Properties，第二个参数为key，第三个参数为value默认值</span></span><br><span class="line"><span class="keyword">super</span>(</span><br><span class="line">topics,</span><br><span class="line">subscriptionPattern,</span><br><span class="line">deserializer,</span><br><span class="line">getLong(</span><br><span class="line">checkNotNull(props, <span class="string">"props"</span>),</span><br><span class="line">KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS, PARTITION_DISCOVERY_DISABLED),</span><br><span class="line">!getBoolean(props, KEY_DISABLE_METRICS, <span class="keyword">false</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">this</span>.properties = props;</span><br><span class="line">setDeserializer(<span class="keyword">this</span>.properties);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 配置轮询超时时间，如果在properties中配置了KEY_POLL_TIMEOUT参数，则返回具体的配置值，否则返回默认值DEFAULT_POLL_TIMEOUT</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (properties.containsKey(KEY_POLL_TIMEOUT)) &#123;</span><br><span class="line"><span class="keyword">this</span>.pollTimeout = Long.parseLong(properties.getProperty(KEY_POLL_TIMEOUT));</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">this</span>.pollTimeout = DEFAULT_POLL_TIMEOUT;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Cannot parse poll timeout for '"</span> + KEY_POLL_TIMEOUT + <span class="string">'\''</span>, e);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">   <span class="comment">// 父类(FlinkKafkaConsumerBase)方法重写，该方法的作用是返回一个fetcher实例，</span></span><br><span class="line"><span class="comment">// fetcher的作用是连接kafka的broker，拉去数据并进行反序列化，然后将数据输出为数据流(data stream)</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">protected</span> AbstractFetcher&lt;T, ?&gt; createFetcher(</span><br><span class="line">SourceContext&lt;T&gt; sourceContext,</span><br><span class="line">Map&lt;KafkaTopicPartition, Long&gt; assignedPartitionsWithInitialOffsets,</span><br><span class="line">SerializedValue&lt;AssignerWithPeriodicWatermarks&lt;T&gt;&gt; watermarksPeriodic,</span><br><span class="line">SerializedValue&lt;AssignerWithPunctuatedWatermarks&lt;T&gt;&gt; watermarksPunctuated,</span><br><span class="line">StreamingRuntimeContext runtimeContext,</span><br><span class="line">OffsetCommitMode offsetCommitMode,</span><br><span class="line">MetricGroup consumerMetricGroup,</span><br><span class="line"><span class="keyword">boolean</span> useMetrics) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 确保当偏移量的提交模式为ON_CHECKPOINTS(条件1：开启checkpoint，条件2：consumer.setCommitOffsetsOnCheckpoints(true))时，禁用自动提交</span></span><br><span class="line"><span class="comment">// 该方法为父类(FlinkKafkaConsumerBase)的静态方法</span></span><br><span class="line"><span class="comment">// 这将覆盖用户在properties中配置的任何设置</span></span><br><span class="line"><span class="comment">// 当offset的模式为ON_CHECKPOINTS，或者为DISABLED时，会将用户配置的properties属性进行覆盖</span></span><br><span class="line"><span class="comment">// 具体是将ENABLE_AUTO_COMMIT_CONFIG = "enable.auto.commit"的值重置为"false</span></span><br><span class="line">        <span class="comment">// 可以理解为：如果开启了checkpoint，并且设置了consumer.setCommitOffsetsOnCheckpoints(true)，默认为true，</span></span><br><span class="line"><span class="comment">// 就会将kafka properties的enable.auto.commit强制置为false</span></span><br><span class="line">adjustAutoCommitConfig(properties, offsetCommitMode);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> KafkaFetcher&lt;&gt;(</span><br><span class="line">sourceContext,</span><br><span class="line">assignedPartitionsWithInitialOffsets,</span><br><span class="line">watermarksPeriodic,</span><br><span class="line">watermarksPunctuated,</span><br><span class="line">runtimeContext.getProcessingTimeService(),</span><br><span class="line">runtimeContext.getExecutionConfig().getAutoWatermarkInterval(),</span><br><span class="line">runtimeContext.getUserCodeClassLoader(),</span><br><span class="line">runtimeContext.getTaskNameWithSubtasks(),</span><br><span class="line">deserializer,</span><br><span class="line">properties,</span><br><span class="line">pollTimeout,</span><br><span class="line">runtimeContext.getMetricGroup(),</span><br><span class="line">consumerMetricGroup,</span><br><span class="line">useMetrics);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//父类(FlinkKafkaConsumerBase)方法重写</span></span><br><span class="line"><span class="comment">// 返回一个分区发现类，分区发现可以使用kafka broker的高级consumer API发现topic和partition的元数据</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> AbstractPartitionDiscoverer <span class="title">createPartitionDiscoverer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">KafkaTopicsDescriptor topicsDescriptor,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> indexOfThisSubtask,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> numParallelSubtasks)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> KafkaPartitionDiscoverer(topicsDescriptor, indexOfThisSubtask, numParallelSubtasks, properties);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *判断是否在kafka的参数开启了自动提交，即enable.auto.commit=true，</span></span><br><span class="line"><span class="comment"> * 并且auto.commit.interval.ms&gt;0,</span></span><br><span class="line"><span class="comment"> * 注意：如果没有没有设置enable.auto.commit的参数，则默认为true</span></span><br><span class="line"><span class="comment"> *       如果没有设置auto.commit.interval.ms的参数，则默认为5000毫秒</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">getIsAutoCommitEnabled</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="keyword">return</span> getBoolean(properties, ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">true</span>) &amp;&amp;</span><br><span class="line">PropertiesUtil.getLong(properties, ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="number">5000</span>) &gt; <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 确保配置了kafka消息的key与value的反序列化方式，</span></span><br><span class="line"><span class="comment"> * 如果没有配置，则使用ByteArrayDeserializer序列化器，</span></span><br><span class="line"><span class="comment"> * 该类的deserialize方法是直接将数据进行return，未做任何处理</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">setDeserializer</span><span class="params">(Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">final</span> String deSerName = ByteArrayDeserializer.class.getName();</span><br><span class="line"></span><br><span class="line">Object keyDeSer = props.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">Object valDeSer = props.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (keyDeSer != <span class="keyword">null</span> &amp;&amp; !keyDeSer.equals(deSerName)) &#123;</span><br><span class="line">LOG.warn(<span class="string">"Ignoring configured key DeSerializer (&#123;&#125;)"</span>, ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (valDeSer != <span class="keyword">null</span> &amp;&amp; !valDeSer.equals(deSerName)) &#123;</span><br><span class="line">LOG.warn(<span class="string">"Ignoring configured value DeSerializer (&#123;&#125;)"</span>, ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">&#125;</span><br><span class="line">props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, deSerName);</span><br><span class="line">props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, deSerName);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>上面的代码已经给出了非常详细的注释，下面将对比较关键的部分进行分析。</p><ul><li><p>构造方法分析</p><p><img src="//jiamaoxiang.top/2020/04/02/你真的了解Flink-Kafka-connector吗？/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3Flink-Kafka-connector%E5%90%97%EF%BC%9F%5C%E6%9E%84%E9%80%A0%E6%96%B9%E6%B3%95%E9%87%8D%E5%86%99.png" alt></p></li></ul><p>FlinkKakfaConsumer提供了7种构造方法，如上图所示。不同的构造方法分别具有不同的功能，通过传递的参数也可以大致分析出每种构造方法特有的功能，为了方便理解，本文将对其进行分组讨论，具体如下：</p><p><strong>单topic</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topic                   消费的主题名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> valueDeserializer       反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props                   用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(String topic, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(Collections.singletonList(topic), valueDeserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * 该构造方法允许传入KafkaDeserializationSchema，该反序列化类支持访问kafka消费的额外信息</span></span><br><span class="line"><span class="comment"> * 比如：key/value对，offsets(偏移量)，topic(主题名称)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topic                消费的主题名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer         反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props                用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(String topic, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(Collections.singletonList(topic), deserializer, props);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面两种构造方法只支持单个topic，区别在于反序列化的方式不一样。第一种使用的是DeserializationSchema，第二种使用的是KafkaDeserializationSchema，其中使用带有KafkaDeserializationSchema参数的构造方法可以获取更多的附属信息，比如在某些场景下需要获取key/value对，offsets(偏移量)，topic(主题名称)等信息，可以选择使用此方式的构造方法。以上两种方法都调用了私有的构造方法，私有构造方法的分析见下面。</p><p><strong>多topic</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * 该构造方法允许传入多个topic(主题)，支持消费多个主题</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topics          消费的主题名称，多个主题为List集合</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer    反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props           用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(List&lt;String&gt; topics, DeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(topics, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(deserializer), props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * 该构造方法允许传入多个topic(主题)，支持消费多个主题,</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topics         消费的主题名称，多个主题为List集合</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer   反序列化类型，用于将kafka的字节消息转换为Flink的对象,支持获取额外信息</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props          用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(List&lt;String&gt; topics, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(topics, <span class="keyword">null</span>, deserializer, props);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的两种多topic的构造方法，可以使用一个list集合接收多个topic进行消费，区别在于反序列化的方式不一样。第一种使用的是DeserializationSchema，第二种使用的是KafkaDeserializationSchema，其中使用带有KafkaDeserializationSchema参数的构造方法可以获取更多的附属信息，比如在某些场景下需要获取key/value对，offsets(偏移量)，topic(主题名称)等信息，可以选择使用此方式的构造方法。以上两种方法都调用了私有的构造方法，私有构造方法的分析见下面。</p><p><strong>正则匹配topic</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于正则表达式订阅多个topic</span></span><br><span class="line"><span class="comment"> * 如果开启了分区发现，即FlinkKafkaConsumer.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值为非负数</span></span><br><span class="line"><span class="comment"> * 只要是能够正则匹配上，主题一旦被创建就会立即被订阅</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> subscriptionPattern   主题的正则表达式</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> valueDeserializer   反序列化类型，用于将kafka的字节消息转换为Flink的对象,支持获取额外信息</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props               用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(Pattern subscriptionPattern, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(<span class="keyword">null</span>, subscriptionPattern, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(valueDeserializer), props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于正则表达式订阅多个topic</span></span><br><span class="line"><span class="comment"> * 如果开启了分区发现，即FlinkKafkaConsumer.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值为非负数</span></span><br><span class="line"><span class="comment"> * 只要是能够正则匹配上，主题一旦被创建就会立即被订阅</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> subscriptionPattern   主题的正则表达式</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer          该反序列化类支持访问kafka消费的额外信息,比如：key/value对，offsets(偏移量)，topic(主题名称)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props                 用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(Pattern subscriptionPattern, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(<span class="keyword">null</span>, subscriptionPattern, deserializer, props);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实际的生产环境中可能有这样一些需求，比如有一个flink作业需要将多种不同的数据聚合到一起，而这些数据对应着不同的kafka topic，随着业务增长，新增一类数据，同时新增了一个kafka topic，如何在不重启作业的情况下作业自动感知新的topic。首先需要在构建FlinkKafkaConsumer时的properties中设置flink.partition-discovery.interval-millis参数为非负值，表示开启动态发现的开关，以及设置的时间间隔。此时FLinkKafkaConsumer内部会启动一个单独的线程定期去kafka获取最新的meta信息。具体的调用执行信息，参见下面的私有构造方法</p><p><strong>私有构造方法</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">List&lt;String&gt; topics,</span></span></span><br><span class="line"><span class="function"><span class="params">Pattern subscriptionPattern,</span></span></span><br><span class="line"><span class="function"><span class="params">KafkaDeserializationSchema&lt;T&gt; deserializer,</span></span></span><br><span class="line"><span class="function"><span class="params">Properties props)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用父类(FlinkKafkaConsumerBase)构造方法，PropertiesUtil.getLong方法第一个参数为Properties，第二个参数为key，第三个参数为value默认值。KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值是开启分区发现的配置参数，在properties里面配置flink.partition-discovery.interval-millis=5000(大于0的数),如果没有配置则使用PARTITION_DISCOVERY_DISABLED=Long.MIN_VALUE(表示禁用分区发现)</span></span><br><span class="line"><span class="keyword">super</span>(</span><br><span class="line">topics,</span><br><span class="line">subscriptionPattern,</span><br><span class="line">deserializer,</span><br><span class="line">getLong(</span><br><span class="line">checkNotNull(props, <span class="string">"props"</span>),</span><br><span class="line">KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS, PARTITION_DISCOVERY_DISABLED),</span><br><span class="line">!getBoolean(props, KEY_DISABLE_METRICS, <span class="keyword">false</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">this</span>.properties = props;</span><br><span class="line">setDeserializer(<span class="keyword">this</span>.properties);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 配置轮询超时时间，如果在properties中配置了KEY_POLL_TIMEOUT参数，则返回具体的配置值，否则返回默认值DEFAULT_POLL_TIMEOUT</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (properties.containsKey(KEY_POLL_TIMEOUT)) &#123;</span><br><span class="line"><span class="keyword">this</span>.pollTimeout = Long.parseLong(properties.getProperty(KEY_POLL_TIMEOUT));</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">this</span>.pollTimeout = DEFAULT_POLL_TIMEOUT;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Cannot parse poll timeout for '"</span> + KEY_POLL_TIMEOUT + <span class="string">'\''</span>, e);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>其他方法分析</li></ul><p><strong>KafkaFetcher对象创建</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">// 父类(FlinkKafkaConsumerBase)方法重写，该方法的作用是返回一个fetcher实例，</span></span><br><span class="line"><span class="comment">// fetcher的作用是连接kafka的broker，拉去数据并进行反序列化，然后将数据输出为数据流(data stream)</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">protected</span> AbstractFetcher&lt;T, ?&gt; createFetcher(</span><br><span class="line">SourceContext&lt;T&gt; sourceContext,</span><br><span class="line">Map&lt;KafkaTopicPartition, Long&gt; assignedPartitionsWithInitialOffsets,</span><br><span class="line">SerializedValue&lt;AssignerWithPeriodicWatermarks&lt;T&gt;&gt; watermarksPeriodic,</span><br><span class="line">SerializedValue&lt;AssignerWithPunctuatedWatermarks&lt;T&gt;&gt; watermarksPunctuated,</span><br><span class="line">StreamingRuntimeContext runtimeContext,</span><br><span class="line">OffsetCommitMode offsetCommitMode,</span><br><span class="line">MetricGroup consumerMetricGroup,</span><br><span class="line"><span class="keyword">boolean</span> useMetrics) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">       <span class="comment">// 确保当偏移量的提交模式为ON_CHECKPOINTS(条件1：开启checkpoint，条件2：consumer.setCommitOffsetsOnCheckpoints(true))时，禁用自动提交</span></span><br><span class="line"><span class="comment">// 该方法为父类(FlinkKafkaConsumerBase)的静态方法</span></span><br><span class="line"><span class="comment">// 这将覆盖用户在properties中配置的任何设置</span></span><br><span class="line"><span class="comment">// 当offset的模式为ON_CHECKPOINTS，或者为DISABLED时，会将用户配置的properties属性进行覆盖</span></span><br><span class="line"><span class="comment">// 具体是将ENABLE_AUTO_COMMIT_CONFIG = "enable.auto.commit"的值重置为"false</span></span><br><span class="line">       <span class="comment">// 可以理解为：如果开启了checkpoint，并且设置了consumer.setCommitOffsetsOnCheckpoints(true)，默认为true，</span></span><br><span class="line"><span class="comment">// 就会将kafka properties的enable.auto.commit强制置为false</span></span><br><span class="line">adjustAutoCommitConfig(properties, offsetCommitMode);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> KafkaFetcher&lt;&gt;(</span><br><span class="line">sourceContext,</span><br><span class="line">assignedPartitionsWithInitialOffsets,</span><br><span class="line">watermarksPeriodic,</span><br><span class="line">watermarksPunctuated,</span><br><span class="line">runtimeContext.getProcessingTimeService(),</span><br><span class="line">runtimeContext.getExecutionConfig().getAutoWatermarkInterval(),</span><br><span class="line">runtimeContext.getUserCodeClassLoader(),</span><br><span class="line">runtimeContext.getTaskNameWithSubtasks(),</span><br><span class="line">deserializer,</span><br><span class="line">properties,</span><br><span class="line">pollTimeout,</span><br><span class="line">runtimeContext.getMetricGroup(),</span><br><span class="line">consumerMetricGroup,</span><br><span class="line">useMetrics);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该方法的作用是返回一个fetcher实例，fetcher的作用是连接kafka的broker，拉去数据并进行反序列化，然后将数据输出为数据流(data stream)，在这里对自动偏移量提交模式进行了强制调整，即确保当偏移量的提交模式为ON_CHECKPOINTS(条件1：开启checkpoint，条件2：consumer.setCommitOffsetsOnCheckpoints(true))时，禁用自动提交。这将覆盖用户在properties中配置的任何设置，简单可以理解为：如果开启了checkpoint，并且设置了consumer.setCommitOffsetsOnCheckpoints(true)，默认为true，就会将kafka properties的enable.auto.commit强制置为false。关于offset的提交模式，见下文的偏移量提交模式分析。</p><p><strong>判断是否设置了自动提交</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">getIsAutoCommitEnabled</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="keyword">return</span> getBoolean(properties, ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">true</span>) &amp;&amp;</span><br><span class="line">PropertiesUtil.getLong(properties, ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="number">5000</span>) &gt; <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>判断是否在kafka的参数开启了自动提交，即enable.auto.commit=true，并且auto.commit.interval.ms&gt;0, 注意：如果没有没有设置enable.auto.commit的参数，则默认为true, 如果没有设置auto.commit.interval.ms的参数，则默认为5000毫秒。该方法会在FlinkKafkaConsumerBase的open方法进行初始化的时候调用。</p><p><strong>反序列化</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">setDeserializer</span><span class="params">(Properties props)</span> </span>&#123;</span><br><span class="line">         <span class="comment">// 默认的反序列化方式 </span></span><br><span class="line"><span class="keyword">final</span> String deSerName = ByteArrayDeserializer.class.getName();</span><br><span class="line">         <span class="comment">//获取用户配置的properties关于key与value的反序列化模式</span></span><br><span class="line">Object keyDeSer = props.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">Object valDeSer = props.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">         <span class="comment">// 如果配置了，则使用用户配置的值</span></span><br><span class="line"><span class="keyword">if</span> (keyDeSer != <span class="keyword">null</span> &amp;&amp; !keyDeSer.equals(deSerName)) &#123;</span><br><span class="line">LOG.warn(<span class="string">"Ignoring configured key DeSerializer (&#123;&#125;)"</span>, ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (valDeSer != <span class="keyword">null</span> &amp;&amp; !valDeSer.equals(deSerName)) &#123;</span><br><span class="line">LOG.warn(<span class="string">"Ignoring configured value DeSerializer (&#123;&#125;)"</span>, ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">&#125;</span><br><span class="line">        <span class="comment">// 没有配置，则使用ByteArrayDeserializer进行反序列化</span></span><br><span class="line">props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, deSerName);</span><br><span class="line">props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, deSerName);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>确保配置了kafka消息的key与value的反序列化方式，如果没有配置，则使用ByteArrayDeserializer序列化器，<br>ByteArrayDeserializer类的deserialize方法是直接将数据进行return，未做任何处理。</p><h4 id="FlinkKafkaConsumerBase源码"><a href="#FlinkKafkaConsumerBase源码" class="headerlink" title="FlinkKafkaConsumerBase源码"></a>FlinkKafkaConsumerBase源码</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkKafkaConsumerBase</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span></span></span><br><span class="line"><span class="class"><span class="title">CheckpointListener</span>,</span></span><br><span class="line"><span class="class"><span class="title">ResultTypeQueryable</span>&lt;<span class="title">T</span>&gt;,</span></span><br><span class="line"><span class="class"><span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAX_NUM_PENDING_CHECKPOINTS = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> PARTITION_DISCOVERY_DISABLED = Long.MIN_VALUE;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String KEY_DISABLE_METRICS = <span class="string">"flink.disable-metrics"</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS = <span class="string">"flink.partition-discovery.interval-millis"</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String OFFSETS_STATE_NAME = <span class="string">"topic-partition-offset-states"</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">boolean</span> enableCommitOnCheckpoints = <span class="keyword">true</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 偏移量的提交模式，仅能通过在FlinkKafkaConsumerBase#open(Configuration)进行配置</span></span><br><span class="line"><span class="comment"> * 该值取决于用户是否开启了checkpoint</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> OffsetCommitMode offsetCommitMode;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 配置从哪个位置开始消费kafka的消息，</span></span><br><span class="line"><span class="comment"> * 默认为StartupMode#GROUP_OFFSETS，即从当前提交的偏移量开始消费</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> StartupMode startupMode = StartupMode.GROUP_OFFSETS;</span><br><span class="line"><span class="keyword">private</span> Map&lt;KafkaTopicPartition, Long&gt; specificStartupOffsets;</span><br><span class="line"><span class="keyword">private</span> Long startupOffsetsTimestamp;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 确保当偏移量的提交模式为ON_CHECKPOINTS时，禁用自动提交，</span></span><br><span class="line"><span class="comment"> * 这将覆盖用户在properties中配置的任何设置。</span></span><br><span class="line"><span class="comment"> * 当offset的模式为ON_CHECKPOINTS，或者为DISABLED时，会将用户配置的properties属性进行覆盖</span></span><br><span class="line"><span class="comment"> * 具体是将ENABLE_AUTO_COMMIT_CONFIG = "enable.auto.commit"的值重置为"false，即禁用自动提交</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> properties       kafka配置的properties，会通过该方法进行覆盖</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> offsetCommitMode    offset提交模式</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">adjustAutoCommitConfig</span><span class="params">(Properties properties, OffsetCommitMode offsetCommitMode)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS || offsetCommitMode == OffsetCommitMode.DISABLED) &#123;</span><br><span class="line">properties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="string">"false"</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 决定是否在开启checkpoint时，在checkpoin之后提交偏移量，</span></span><br><span class="line"><span class="comment"> * 只有用户配置了启用checkpoint，该参数才会其作用</span></span><br><span class="line"><span class="comment"> * 如果没有开启checkpoint，则使用kafka的配置参数：enable.auto.commit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> commitOnCheckpoints</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setCommitOffsetsOnCheckpoints</span><span class="params">(<span class="keyword">boolean</span> commitOnCheckpoints)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.enableCommitOnCheckpoints = commitOnCheckpoints;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 从最早的偏移量开始消费，</span></span><br><span class="line"><span class="comment"> *该模式下，Kafka 中的已经提交的偏移量将被忽略，不会用作起始位置。</span></span><br><span class="line"><span class="comment"> *可以通过consumer1.setStartFromEarliest()进行设置</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromEarliest</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.startupMode = StartupMode.EARLIEST;</span><br><span class="line"><span class="keyword">this</span>.startupOffsetsTimestamp = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">this</span>.specificStartupOffsets = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 从最新的数据开始消费,</span></span><br><span class="line"><span class="comment"> *  该模式下，Kafka 中的 已提交的偏移量将被忽略，不会用作起始位置。</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromLatest</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.startupMode = StartupMode.LATEST;</span><br><span class="line"><span class="keyword">this</span>.startupOffsetsTimestamp = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">this</span>.specificStartupOffsets = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *指定具体的偏移量时间戳,毫秒</span></span><br><span class="line"><span class="comment"> *对于每个分区，其时间戳大于或等于指定时间戳的记录将用作起始位置。</span></span><br><span class="line"><span class="comment"> * 如果一个分区的最新记录早于指定的时间戳，则只从最新记录读取该分区数据。</span></span><br><span class="line"><span class="comment"> * 在这种模式下，Kafka 中的已提交 offset 将被忽略，不会用作起始位置。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromTimestamp</span><span class="params">(<span class="keyword">long</span> startupOffsetsTimestamp)</span> </span>&#123;</span><br><span class="line">checkArgument(startupOffsetsTimestamp &gt;= <span class="number">0</span>, <span class="string">"The provided value for the startup offsets timestamp is invalid."</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">long</span> currentTimestamp = System.currentTimeMillis();</span><br><span class="line">checkArgument(startupOffsetsTimestamp &lt;= currentTimestamp,</span><br><span class="line"><span class="string">"Startup time[%s] must be before current time[%s]."</span>, startupOffsetsTimestamp, currentTimestamp);</span><br><span class="line"></span><br><span class="line"><span class="keyword">this</span>.startupMode = StartupMode.TIMESTAMP;</span><br><span class="line"><span class="keyword">this</span>.startupOffsetsTimestamp = startupOffsetsTimestamp;</span><br><span class="line"><span class="keyword">this</span>.specificStartupOffsets = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 从具体的消费者组最近提交的偏移量开始消费，为默认方式</span></span><br><span class="line"><span class="comment"> * 如果没有发现分区的偏移量，使用auto.offset.reset参数配置的值</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromGroupOffsets</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.startupMode = StartupMode.GROUP_OFFSETS;</span><br><span class="line"><span class="keyword">this</span>.startupOffsetsTimestamp = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">this</span>.specificStartupOffsets = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *为每个分区指定偏移量进行消费</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromSpecificOffsets</span><span class="params">(Map&lt;KafkaTopicPartition, Long&gt; specificStartupOffsets)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.startupMode = StartupMode.SPECIFIC_OFFSETS;</span><br><span class="line"><span class="keyword">this</span>.startupOffsetsTimestamp = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">this</span>.specificStartupOffsets = checkNotNull(specificStartupOffsets);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration configuration)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// determine the offset commit mode</span></span><br><span class="line"><span class="comment">// 决定偏移量的提交模式，</span></span><br><span class="line"><span class="comment">// 第一个参数为是否开启了自动提交，</span></span><br><span class="line"><span class="comment">// 第二个参数为是否开启了CommitOnCheckpoint模式</span></span><br><span class="line"><span class="comment">// 第三个参数为是否开启了checkpoint</span></span><br><span class="line"><span class="keyword">this</span>.offsetCommitMode = OffsetCommitModes.fromConfiguration(</span><br><span class="line">getIsAutoCommitEnabled(),</span><br><span class="line">enableCommitOnCheckpoints,</span><br><span class="line">((StreamingRuntimeContext) getRuntimeContext()).isCheckpointingEnabled());</span><br><span class="line">       </span><br><span class="line">   <span class="comment">// 省略的代码</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 省略的代码</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个fetcher用于连接kafka的broker，拉去数据并进行反序列化，然后将数据输出为数据流(data stream)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> sourceContext   数据输出的上下文</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> subscribedPartitionsToStartOffsets  当前sub task需要处理的topic分区集合，即topic的partition与offset的Map集合</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> watermarksPeriodic    可选,一个序列化的时间戳提取器，生成periodic类型的 watermark</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> watermarksPunctuated  可选,一个序列化的时间戳提取器，生成punctuated类型的 watermark</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> runtimeContext        task的runtime context上下文</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> offsetCommitMode      offset的提交模式,有三种，分别为：DISABLED(禁用偏移量自动提交),ON_CHECKPOINTS(仅仅当checkpoints完成之后，才提交偏移量给kafka)</span></span><br><span class="line"><span class="comment"> * KAFKA_PERIODIC(使用kafka自动提交函数，周期性自动提交偏移量)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> kafkaMetricGroup   Flink的Metric</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> useMetrics         是否使用Metric</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span>                   返回一个fetcher实例</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">abstract</span> AbstractFetcher&lt;T, ?&gt; createFetcher(</span><br><span class="line">SourceContext&lt;T&gt; sourceContext,</span><br><span class="line">Map&lt;KafkaTopicPartition, Long&gt; subscribedPartitionsToStartOffsets,</span><br><span class="line">SerializedValue&lt;AssignerWithPeriodicWatermarks&lt;T&gt;&gt; watermarksPeriodic,</span><br><span class="line">SerializedValue&lt;AssignerWithPunctuatedWatermarks&lt;T&gt;&gt; watermarksPunctuated,</span><br><span class="line">StreamingRuntimeContext runtimeContext,</span><br><span class="line">OffsetCommitMode offsetCommitMode,</span><br><span class="line">MetricGroup kafkaMetricGroup,</span><br><span class="line"><span class="keyword">boolean</span> useMetrics) <span class="keyword">throws</span> Exception;</span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">getIsAutoCommitEnabled</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">// 省略的代码</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述代码是FlinkKafkaConsumerBase的部分代码片段，基本上对其做了详细注释，里面的有些方法是FlinkKafkaConsumer继承的，有些是重写的。之所以在这里给出，可以对照FlinkKafkaConsumer的源码，从而方便理解。</p><h3 id="偏移量提交模式分析"><a href="#偏移量提交模式分析" class="headerlink" title="偏移量提交模式分析"></a>偏移量提交模式分析</h3><p>Flink Kafka Consumer 允许有配置如何将 offset 提交回 Kafka broker（或 0.8 版本的 Zookeeper）的行为。请注意：Flink Kafka Consumer 不依赖于提交的 offset 来实现容错保证。提交的 offset 只是一种方法，用于公开 consumer 的进度以便进行监控。</p><p>配置 offset 提交行为的方法是否相同，取决于是否为 job 启用了 checkpointing。在这里先给出提交模式的具体结论，下面会对两种方式进行具体的分析。基本的结论为：</p><ul><li><p>开启checkpoint</p><ul><li><p>情况1：用户通过调用 consumer 上的 setCommitOffsetsOnCheckpoints(true) 方法来启用 offset 的提交(默认情况下为 true )<br>那么当 checkpointing 完成时，Flink Kafka Consumer 将提交的 offset 存储在 checkpoint 状态中。<br>这确保 Kafka broker 中提交的 offset 与 checkpoint 状态中的 offset 一致。<br>注意，在这个场景中，Properties 中的自动定期 offset 提交设置会被完全忽略。<br>此情况使用的是ON_CHECKPOINTS</p></li><li><p>情况2：用户通过调用 consumer 上的 setCommitOffsetsOnCheckpoints(“false”) 方法来禁用 offset 的提交，则使用DISABLED模式提交offset</p></li></ul></li><li><p>未开启checkpoint<br>Flink Kafka Consumer 依赖于内部使用的 Kafka client 自动定期 offset 提交功能，因此，要禁用或启用 offset 的提交</p></li><li><p>情况1：配置了Kafka properties的参数配置了”enable.auto.commit” = “true”或者 Kafka 0.8 的 auto.commit.enable=true，使用KAFKA_PERIODIC模式提交offset，即自动提交offset</p><ul><li>情况2：没有配置enable.auto.commit参数，使用DISABLED模式提交offset，这意味着kafka不知道当前的消费者组的消费者每次消费的偏移量。</li></ul></li></ul><h4 id="提交模式源码分析"><a href="#提交模式源码分析" class="headerlink" title="提交模式源码分析"></a>提交模式源码分析</h4><ul><li>offset的提交模式</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> OffsetCommitMode &#123;</span><br><span class="line"><span class="comment">// 禁用偏移量自动提交</span></span><br><span class="line">DISABLED,</span><br><span class="line"><span class="comment">// 仅仅当checkpoints完成之后，才提交偏移量给kafka</span></span><br><span class="line">ON_CHECKPOINTS,</span><br><span class="line"><span class="comment">// 使用kafka自动提交函数，周期性自动提交偏移量</span></span><br><span class="line">KAFKA_PERIODIC;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>提交模式的调用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OffsetCommitModes</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> OffsetCommitMode <span class="title">fromConfiguration</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">boolean</span> enableAutoCommit,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">boolean</span> enableCommitOnCheckpoint,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">boolean</span> enableCheckpointing)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 如果开启了checkinpoint，执行下面判断</span></span><br><span class="line"><span class="keyword">if</span> (enableCheckpointing) &#123;</span><br><span class="line"><span class="comment">// 如果开启了checkpoint，进一步判断是否在checkpoin启用时提交(setCommitOffsetsOnCheckpoints(true))，如果是则使用ON_CHECKPOINTS模式</span></span><br><span class="line"><span class="comment">// 否则使用DISABLED模式</span></span><br><span class="line"><span class="keyword">return</span> (enableCommitOnCheckpoint) ? OffsetCommitMode.ON_CHECKPOINTS : OffsetCommitMode.DISABLED;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// 若Kafka properties的参数配置了"enable.auto.commit" = "true"，则使用KAFKA_PERIODIC模式提交offset</span></span><br><span class="line"><span class="comment">// 否则使用DISABLED模式</span></span><br><span class="line"><span class="keyword">return</span> (enableAutoCommit) ? OffsetCommitMode.KAFKA_PERIODIC : OffsetCommitMode.DISABLED;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了Flink Kafka Consumer，首先对FlinkKafkaConsumer的不同版本进行了对比，然后给出了一个完整的Demo案例，并对案例的配置参数进行了详细解释，接着分析了FlinkKafkaConsumer的继承关系，并分别对FlinkKafkaConsumer以及其父类FlinkKafkaConsumerBase的源码进行了解读，最后从源码层面分析了Flink Kafka Consumer的偏移量提交模式，并对每一种提交模式进行了梳理。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink1.10集成Hive快速入门</title>
      <link href="/2020/03/31/Flink1-10%E9%9B%86%E6%88%90Hive%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"/>
      <url>/2020/03/31/Flink1-10%E9%9B%86%E6%88%90Hive%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<p>Hive 是大数据领域最早出现的 SQL 引擎，发展至今有着丰富的功能和广泛的用户基础。之后出现的 SQL 引擎，如 Spark SQL、Impala 等，都在一定程度上提供了与 Hive 集成的功能，从而方便用户使用现有的数据仓库、进行作业迁移等。</p><a id="more"></a><p>Flink从1.9开始支持集成Hive，不过1.9版本为beta版，不推荐在生产环境中使用。在最新版Flink1.10版本，标志着对 Blink的整合宣告完成，随着对 Hive 的生产级别集成，Hive作为数据仓库系统的绝对核心，承担着绝大多数的离线数据ETL计算和数据管理，期待Flink未来对Hive的完美支持。</p><p>而 HiveCatalog 会与一个 Hive Metastore 的实例连接，提供元数据持久化的能力。要使用 Flink 与 Hive 进行交互，用户需要配置一个 HiveCatalog，并通过 HiveCatalog 访问 Hive 中的元数据。</p><h2 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h2><p>要与Hive集成，需要在Flink的lib目录下添加额外的依赖jar包，以使集成在Table API程序或SQL Client中的SQL中起作用。或者，可以将这些依赖项放在文件夹中，并分别使用Table API程序或SQL Client 的<code>-C</code> 或<code>-l</code>选项将它们添加到classpath中。本文使用第一种方式，即将jar包直接复制到$FLINK_HOME/lib目录下。本文使用的Hive版本为2.3.4(对于不同版本的Hive，可以参照官网选择不同的jar包依赖)，总共需要3个jar包，如下：</p><ul><li>flink-connector-hive_2.11-1.10.0.jar</li><li>flink-shaded-hadoop-2-uber-2.7.5-8.0.jar</li><li>hive-exec-2.3.4.jar</li></ul><p>其中hive-exec-2.3.4.jar在hive的lib文件夹下，另外两个需要自行下载，下载地址：<a href="https://repo1.maven.org/maven2/org/apache/flink/flink-connector-hive_2.11/1.10.0/" target="_blank" rel="noopener">flink-connector-hive_2.11-1.10.0.jar</a>，<a href="//https://maven.aliyun.com/mvn/search">flink-shaded-hadoop-2-uber-2.7.5-8.0.jar</a></p><p><img src="//jiamaoxiang.top/2020/03/31/Flink1-10集成Hive快速入门/demo.png" alt></p><p><strong>切莫拔剑四顾心茫然，话不多说，直接上代码。</strong></p><h2 id="构建程序"><a href="#构建程序" class="headerlink" title="构建程序"></a>构建程序</h2><h3 id="添加Maven依赖"><a href="#添加Maven依赖" class="headerlink" title="添加Maven依赖"></a>添加Maven依赖</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Flink Dependency --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-java-bridge_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Hive Dependency --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="实例代码"><a href="#实例代码" class="headerlink" title="实例代码"></a>实例代码</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.flink.sql.hiveintegration;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.catalog.hive.HiveCatalog;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Created</span> with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@author</span> : jmx</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Date</span>: 2020/3/31</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Time</span>: 13:22</span></span><br><span class="line"><span class="comment"> *  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkHiveIntegration</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">                .newInstance()</span><br><span class="line">                .useBlinkPlanner() <span class="comment">// 使用BlinkPlanner</span></span><br><span class="line">                .inBatchMode() <span class="comment">// Batch模式，默认为StreamingMode</span></span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用StreamingMode</span></span><br><span class="line">       <span class="comment">/* EnvironmentSettings settings = EnvironmentSettings</span></span><br><span class="line"><span class="comment">                .newInstance()</span></span><br><span class="line"><span class="comment">                .useBlinkPlanner() // 使用BlinkPlanner</span></span><br><span class="line"><span class="comment">                .inStreamingMode() // StreamingMode</span></span><br><span class="line"><span class="comment">                .build();*/</span></span><br><span class="line"></span><br><span class="line">        TableEnvironment tableEnv = TableEnvironment.create(settings);</span><br><span class="line"></span><br><span class="line">        String name = <span class="string">"myhive"</span>;      <span class="comment">// Catalog名称，定义一个唯一的名称表示</span></span><br><span class="line">        String defaultDatabase = <span class="string">"qfbap_ods"</span>;  <span class="comment">// 默认数据库名称</span></span><br><span class="line">        String hiveConfDir = <span class="string">"/opt/modules/apache-hive-2.3.4-bin/conf"</span>;  <span class="comment">// hive-site.xml路径</span></span><br><span class="line">        String version = <span class="string">"2.3.4"</span>;       <span class="comment">// Hive版本号</span></span><br><span class="line"></span><br><span class="line">        HiveCatalog hive = <span class="keyword">new</span> HiveCatalog(name, defaultDatabase, hiveConfDir, version);</span><br><span class="line"></span><br><span class="line">        tableEnv.registerCatalog(<span class="string">"myhive"</span>, hive);</span><br><span class="line">        tableEnv.useCatalog(<span class="string">"myhive"</span>);</span><br><span class="line">        <span class="comment">// 创建数据库，目前不支持创建hive表</span></span><br><span class="line">        String createDbSql = <span class="string">"CREATE DATABASE IF NOT EXISTS myhive.test123"</span>;</span><br><span class="line"></span><br><span class="line">        tableEnv.sqlUpdate(createDbSql);  </span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Flink-SQL-Client集成Hive"><a href="#Flink-SQL-Client集成Hive" class="headerlink" title="Flink SQL Client集成Hive"></a>Flink SQL Client集成Hive</h2><p>Flink的表和SQL API可以处理用SQL语言编写的查询，但是这些查询需要嵌入到用Java或Scala编写的程序中。此外，这些程序在提交到集群之前需要与构建工具打包。这或多或少地限制了Java/Scala程序员对Flink的使用。</p><p>SQL客户端旨在提供一种简单的方式，无需一行Java或Scala代码，即可将表程序编写、调试和提交到Flink集群。Flink SQL客户端CLI允许通过命令行的形式运行分布式程序。使用Flink SQL cli访问Hive，需要配置sql-client-defaults.yaml文件。</p><h3 id="sql-client-defaults-yaml配置"><a href="#sql-client-defaults-yaml配置" class="headerlink" title="sql-client-defaults.yaml配置"></a>sql-client-defaults.yaml配置</h3><p>目前 HiveTableSink 不支持流式写入（未实现 AppendStreamTableSink）。需要将执行模式改成 batch<br>模式，否则会报如下错误：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">org.apache.flink.table.api.TableException: Stream Tables can only be emitted by AppendStreamTableSink, RetractStreamTableSink, or UpsertStreamTableSink.</span><br></pre></td></tr></table></figure><p>需要修改的配置内容如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#...省略的配置项...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line"><span class="comment"># Catalogs</span></span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line"><span class="comment"># 配置catalogs,可以配置多个.</span></span><br><span class="line">catalogs: <span class="comment"># empty list</span></span><br><span class="line">  - name: myhive</span><br><span class="line">    <span class="built_in">type</span>: hive</span><br><span class="line">    hive-conf-dir: /opt/modules/apache-hive-2.3.4-bin/conf</span><br><span class="line">    hive-version: 2.3.4</span><br><span class="line">    default-database: qfbap_ods</span><br><span class="line"></span><br><span class="line"><span class="comment">#...省略的配置项...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line"><span class="comment"># Execution properties</span></span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Properties that change the fundamental execution behavior of a table program.</span></span><br><span class="line"></span><br><span class="line">execution:</span><br><span class="line">  <span class="comment"># select the implementation responsible for planning table programs</span></span><br><span class="line">  <span class="comment"># possible values are 'blink' (used by default) or 'old'</span></span><br><span class="line">  planner: blink</span><br><span class="line">  <span class="comment"># 'batch' or 'streaming' execution</span></span><br><span class="line">  <span class="built_in">type</span>: batch</span><br></pre></td></tr></table></figure><h3 id="启动Flink-SQL-Cli"><a href="#启动Flink-SQL-Cli" class="headerlink" title="启动Flink SQL Cli"></a>启动Flink SQL Cli</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/sql-client.sh  embedded</span><br></pre></td></tr></table></figure><p>启动之后，就可以在此Cli下执行SQL命令访问Hive的表了，基本的操作如下：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 命令行帮助</span></span><br><span class="line">Flink SQL&gt; help</span><br><span class="line"><span class="comment">-- 查看当前会话的catalog，其中myhive为自己配置的，default_catalog为默认的</span></span><br><span class="line">Flink SQL&gt; show catalogs;</span><br><span class="line">default_catalog</span><br><span class="line">myhive</span><br><span class="line"><span class="comment">-- 使用catalog</span></span><br><span class="line">Flink SQL&gt; use catalog myhive;</span><br><span class="line"><span class="comment">-- 查看当前catalog的数据库</span></span><br><span class="line">Flink SQL&gt; show databases;</span><br><span class="line"><span class="comment">-- 创建数据库</span></span><br><span class="line">Flink SQL&gt; create database testdb;</span><br><span class="line"><span class="comment">-- 删除数据库</span></span><br><span class="line">Flink SQL&gt; drop database testdb;</span><br><span class="line"><span class="comment">-- 创建表</span></span><br><span class="line">Flink SQL&gt; create table tbl(id int,name string);</span><br><span class="line"><span class="comment">-- 删除表</span></span><br><span class="line">Flink SQL&gt; drop table tbl;</span><br><span class="line"><span class="comment">-- 查询表</span></span><br><span class="line">Flink SQL&gt; select * from  code_city;</span><br><span class="line"><span class="comment">-- 插入数据</span></span><br><span class="line">Flink SQL&gt; insert overwrite code_city select id,city,province,event_time from code_city_delta ;</span><br><span class="line">Flink SQL&gt; INSERT into code_city values(1,'南京','江苏','');</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文以最新版本的Flink为例，对Flink集成Hive进行了实操。首先通过代码的方式与Hive进行集成，然后介绍了如何使用Flink SQL 客户端访问Hive，并对其中会遇到的坑进行了描述，最后给出了Flink SQL Cli的详细使用。相信在未来的版本中Flink SQL会越来越完善，期待Flink未来对Hive的完美支持。</p><p>欢迎添加我的公众号，随时随地了解更多精彩内容。</p><p><img src="//jiamaoxiang.top/2020/03/31/Flink1-10集成Hive快速入门/%E4%B8%89%E7%BB%B4%E7%A0%81.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的八种分区策略源码解读</title>
      <link href="/2020/03/30/Flink%E7%9A%84%E5%85%AB%E7%A7%8D%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/"/>
      <url>/2020/03/30/Flink%E7%9A%84%E5%85%AB%E7%A7%8D%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p>Flink包含8中分区策略，这8中分区策略(分区器)分别如下面所示，本文将从源码的角度一一解读每个分区器的实现方式。</p><a id="more"></a><ul><li><strong>GlobalPartitioner</strong></li><li><strong>ShufflePartitioner</strong></li><li><strong>RebalancePartitioner</strong></li><li><strong>RescalePartitioner</strong></li><li><strong>BroadcastPartitioner</strong></li><li><strong>ForwardPartitioner</strong></li><li><strong>KeyGroupStreamPartitioner</strong></li><li><strong>CustomPartitionerWrapper</strong></li></ul><h2 id="继承关系图"><a href="#继承关系图" class="headerlink" title="继承关系图"></a>继承关系图</h2><h3 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h3><h4 id="名称"><a href="#名称" class="headerlink" title="名称"></a>名称</h4><p><strong>ChannelSelector</strong></p><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ChannelSelector</span>&lt;<span class="title">T</span> <span class="keyword">extends</span> <span class="title">IOReadableWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 初始化channels数量，channel可以理解为下游Operator的某个实例(并行算子的某个subtask).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setup</span><span class="params">(<span class="keyword">int</span> numberOfChannels)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *根据当前的record以及Channel总数，</span></span><br><span class="line"><span class="comment"> *决定应将record发送到下游哪个Channel。</span></span><br><span class="line"><span class="comment"> *不同的分区策略会实现不同的该方法。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(T record)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*是否以广播的形式发送到下游所有的算子实例</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">isBroadcast</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="抽象类"><a href="#抽象类" class="headerlink" title="抽象类"></a>抽象类</h3><h4 id="名称-1"><a href="#名称-1" class="headerlink" title="名称"></a>名称</h4><p><strong>StreamPartitioner</strong></p><h4 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span></span></span><br><span class="line"><span class="class"><span class="title">ChannelSelector</span>&lt;<span class="title">SerializationDelegate</span>&lt;<span class="title">StreamRecord</span>&lt;<span class="title">T</span>&gt;&gt;&gt;, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">int</span> numberOfChannels;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(<span class="keyword">int</span> numberOfChannels)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.numberOfChannels = numberOfChannels;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isBroadcast</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="继承关系图-1"><a href="#继承关系图-1" class="headerlink" title="继承关系图"></a>继承关系图</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/Flink%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E7%B1%BB%E7%BB%A7%E6%89%BF%E5%85%B3%E7%B3%BB%E5%9B%BE.png" alt></p><h2 id="GlobalPartitioner"><a href="#GlobalPartitioner" class="headerlink" title="GlobalPartitioner"></a>GlobalPartitioner</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>该分区器会将所有的数据都发送到下游的某个算子实例(subtask id = 0)</p><h3 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 发送所有的数据到下游算子的第一个task(ID = 0)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GlobalPartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="comment">//只返回0，即只发送给下游算子的第一个task</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"GLOBAL"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解"><a href="#图解" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/globa.png" alt></p><h2 id="ShufflePartitioner"><a href="#ShufflePartitioner" class="headerlink" title="ShufflePartitioner"></a>ShufflePartitioner</h2><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p>随机选择一个下游算子实例进行发送</p><h3 id="源码解读-1"><a href="#源码解读-1" class="headerlink" title="源码解读"></a>源码解读</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 随机的选择一个channel进行发送</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ShufflePartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Random random = <span class="keyword">new</span> Random();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="comment">//产生[0,numberOfChannels)伪随机数，随机发送到下游的某个task</span></span><br><span class="line"><span class="keyword">return</span> random.nextInt(numberOfChannels);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> ShufflePartitioner&lt;T&gt;();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"SHUFFLE"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解-1"><a href="#图解-1" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/shuffle.png" alt></p><h2 id="BroadcastPartitioner"><a href="#BroadcastPartitioner" class="headerlink" title="BroadcastPartitioner"></a>BroadcastPartitioner</h2><h3 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h3><p>发送到下游所有的算子实例</p><h3 id="源码解读-2"><a href="#源码解读-2" class="headerlink" title="源码解读"></a>源码解读</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 发送到所有的channel</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BroadcastPartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Broadcast模式是直接发送到下游的所有task，所以不需要通过下面的方法选择发送的通道</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">"Broadcast partitioner does not support select channels."</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isBroadcast</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"BROADCAST"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解-2"><a href="#图解-2" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/broadcast.png" alt></p><h2 id="RebalancePartitioner"><a href="#RebalancePartitioner" class="headerlink" title="RebalancePartitioner"></a>RebalancePartitioner</h2><h3 id="简介-3"><a href="#简介-3" class="headerlink" title="简介"></a>简介</h3><p>通过循环的方式依次发送到下游的task</p><h3 id="源码解读-3"><a href="#源码解读-3" class="headerlink" title="源码解读"></a>源码解读</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *通过循环的方式依次发送到下游的task</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RebalancePartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> nextChannelToSendTo;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(<span class="keyword">int</span> numberOfChannels)</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>.setup(numberOfChannels);</span><br><span class="line"><span class="comment">//初始化channel的id，返回[0,numberOfChannels)的伪随机数</span></span><br><span class="line">nextChannelToSendTo = ThreadLocalRandom.current().nextInt(numberOfChannels);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="comment">//循环依次发送到下游的task，比如：nextChannelToSendTo初始值为0，numberOfChannels(下游算子的实例个数，并行度)值为2</span></span><br><span class="line"><span class="comment">//则第一次发送到ID = 1的task，第二次发送到ID = 0的task，第三次发送到ID = 1的task上...依次类推</span></span><br><span class="line">nextChannelToSendTo = (nextChannelToSendTo + <span class="number">1</span>) % numberOfChannels;</span><br><span class="line"><span class="keyword">return</span> nextChannelToSendTo;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"REBALANCE"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解-3"><a href="#图解-3" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/rebalance.png" alt></p><h2 id="RescalePartitioner"><a href="#RescalePartitioner" class="headerlink" title="RescalePartitioner"></a>RescalePartitioner</h2><h3 id="简介-4"><a href="#简介-4" class="headerlink" title="简介"></a>简介</h3><p>基于上下游Operator的并行度，将记录以循环的方式输出到下游Operator的每个实例。<br>  举例: 上游并行度是2，下游是4，则上游一个并行度以循环的方式将记录输出到下游的两个并行度上;上游另一个并行度以循环的方式将记录输出到下游另两个并行度上。<br> 若上游并行度是4，下游并行度是2，则上游两个并行度将记录输出到下游一个并行度上；上游另两个并行度将记录输出到下游另一个并行度上。</p><h3 id="源码解读-4"><a href="#源码解读-4" class="headerlink" title="源码解读"></a>源码解读</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RescalePartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> nextChannelToSendTo = -<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (++nextChannelToSendTo &gt;= numberOfChannels) &#123;</span><br><span class="line">nextChannelToSendTo = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> nextChannelToSendTo;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"RESCALE"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解-4"><a href="#图解-4" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/rescale.png" alt></p><h4 id="尖叫提示"><a href="#尖叫提示" class="headerlink" title="尖叫提示"></a>尖叫提示</h4><p>Flink 中的执行图可以分成四层：StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图。</p><p><strong>StreamGraph</strong>：是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构。</p><p><strong>JobGraph</strong>：StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。</p><p><strong>ExecutionGraph</strong>：JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。</p><p>物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。</p><p> 而StreamingJobGraphGenerator就是StreamGraph转换为JobGraph。在这个类中，把ForwardPartitioner和RescalePartitioner列为POINTWISE分配模式，其他的为ALL_TO_ALL分配模式。代码如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (partitioner <span class="keyword">instanceof</span> ForwardPartitioner || partitioner <span class="keyword">instanceof</span> RescalePartitioner) &#123;</span><br><span class="line">jobEdge = downStreamVertex.connectNewDataSetAsInput(</span><br><span class="line">headVertex,</span><br><span class="line"></span><br><span class="line">   <span class="comment">// 上游算子(生产端)的实例(subtask)连接下游算子(消费端)的一个或者多个实例(subtask)</span></span><br><span class="line">DistributionPattern.POINTWISE,</span><br><span class="line">resultPartitionType);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">jobEdge = downStreamVertex.connectNewDataSetAsInput(</span><br><span class="line">headVertex,</span><br><span class="line"><span class="comment">// 上游算子(生产端)的实例(subtask)连接下游算子(消费端)的所有实例(subtask)</span></span><br><span class="line">DistributionPattern.ALL_TO_ALL,</span><br><span class="line">resultPartitionType);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="ForwardPartitioner"><a href="#ForwardPartitioner" class="headerlink" title="ForwardPartitioner"></a>ForwardPartitioner</h2><h3 id="简介-5"><a href="#简介-5" class="headerlink" title="简介"></a>简介</h3><p>发送到下游对应的第一个task，保证上下游算子并行度一致，即上有算子与下游算子是1:1的关系</p><h3 id="源码解读-5"><a href="#源码解读-5" class="headerlink" title="源码解读"></a>源码解读</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 发送到下游对应的第一个task</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ForwardPartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"FORWARD"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解-5"><a href="#图解-5" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/forward.png" alt></p><h4 id="尖叫提示-1"><a href="#尖叫提示-1" class="headerlink" title="尖叫提示"></a>尖叫提示</h4><p>在上下游的算子没有指定分区器的情况下，如果上下游的算子并行度一致，则使用ForwardPartitioner，否则使用RebalancePartitioner，对于ForwardPartitioner，必须保证上下游算子并行度一致，否则会抛出异常</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//在上下游的算子没有指定分区器的情况下，如果上下游的算子并行度一致，则使用ForwardPartitioner，否则使用RebalancePartitioner</span></span><br><span class="line"><span class="keyword">if</span> (partitioner == <span class="keyword">null</span> &amp;&amp; upstreamNode.getParallelism() == downstreamNode.getParallelism()) &#123;</span><br><span class="line">partitioner = <span class="keyword">new</span> ForwardPartitioner&lt;Object&gt;();</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (partitioner == <span class="keyword">null</span>) &#123;</span><br><span class="line">partitioner = <span class="keyword">new</span> RebalancePartitioner&lt;Object&gt;();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (partitioner <span class="keyword">instanceof</span> ForwardPartitioner) &#123;</span><br><span class="line"><span class="comment">//如果上下游的并行度不一致，会抛出异常</span></span><br><span class="line"><span class="keyword">if</span> (upstreamNode.getParallelism() != downstreamNode.getParallelism()) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">"Forward partitioning does not allow "</span> +</span><br><span class="line"><span class="string">"change of parallelism. Upstream operation: "</span> + upstreamNode + <span class="string">" parallelism: "</span> + upstreamNode.getParallelism() +</span><br><span class="line"><span class="string">", downstream operation: "</span> + downstreamNode + <span class="string">" parallelism: "</span> + downstreamNode.getParallelism() +</span><br><span class="line"><span class="string">" You must use another partitioning strategy, such as broadcast, rebalance, shuffle or global."</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="KeyGroupStreamPartitioner"><a href="#KeyGroupStreamPartitioner" class="headerlink" title="KeyGroupStreamPartitioner"></a>KeyGroupStreamPartitioner</h2><h3 id="简介-6"><a href="#简介-6" class="headerlink" title="简介"></a>简介</h3><p>根据key的分组索引选择发送到相对应的下游subtask</p><h3 id="源码解读-6"><a href="#源码解读-6" class="headerlink" title="源码解读"></a>源码解读</h3><ul><li>org.apache.flink.streaming.runtime.partitioner.KeyGroupStreamPartitioner</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据key的分组索引选择发送到相对应的下游subtask</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;K&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyGroupStreamPartitioner</span>&lt;<span class="title">T</span>, <span class="title">K</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span> <span class="title">ConfigurableStreamPartitioner</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line">K key;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">key = keySelector.getKey(record.getInstance().getValue());</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Could not extract key from "</span> + record.getInstance().getValue(), e);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//调用KeyGroupRangeAssignment类的assignKeyToParallelOperator方法,代码如下所示</span></span><br><span class="line"><span class="keyword">return</span> KeyGroupRangeAssignment.assignKeyToParallelOperator(key, maxParallelism, numberOfChannels);</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>org.apache.flink.runtime.state.KeyGroupRangeAssignment</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyGroupRangeAssignment</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据key分配一个并行算子实例的索引，该索引即为该key要发送的下游算子实例的路由信息，</span></span><br><span class="line"><span class="comment"> * 即该key发送到哪一个task</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">assignKeyToParallelOperator</span><span class="params">(Object key, <span class="keyword">int</span> maxParallelism, <span class="keyword">int</span> parallelism)</span> </span>&#123;</span><br><span class="line">Preconditions.checkNotNull(key, <span class="string">"Assigned key must not be null!"</span>);</span><br><span class="line"><span class="keyword">return</span> computeOperatorIndexForKeyGroup(maxParallelism, parallelism, assignToKeyGroup(key, maxParallelism));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *根据key分配一个分组id(keyGroupId)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">assignToKeyGroup</span><span class="params">(Object key, <span class="keyword">int</span> maxParallelism)</span> </span>&#123;</span><br><span class="line">Preconditions.checkNotNull(key, <span class="string">"Assigned key must not be null!"</span>);</span><br><span class="line"><span class="comment">//获取key的hashcode</span></span><br><span class="line"><span class="keyword">return</span> computeKeyGroupForKeyHash(key.hashCode(), maxParallelism);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据key分配一个分组id(keyGroupId),</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">computeKeyGroupForKeyHash</span><span class="params">(<span class="keyword">int</span> keyHash, <span class="keyword">int</span> maxParallelism)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//与maxParallelism取余，获取keyGroupId</span></span><br><span class="line"><span class="keyword">return</span> MathUtils.murmurHash(keyHash) % maxParallelism;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//计算分区index，即该key group应该发送到下游的哪一个算子实例</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">computeOperatorIndexForKeyGroup</span><span class="params">(<span class="keyword">int</span> maxParallelism, <span class="keyword">int</span> parallelism, <span class="keyword">int</span> keyGroupId)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> keyGroupId * parallelism / maxParallelism;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="图解-6"><a href="#图解-6" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/key.png" alt></p><h2 id="CustomPartitionerWrapper"><a href="#CustomPartitionerWrapper" class="headerlink" title="CustomPartitionerWrapper"></a>CustomPartitionerWrapper</h2><h3 id="简介-7"><a href="#简介-7" class="headerlink" title="简介"></a>简介</h3><p>通过<code>Partitioner</code>实例的<code>partition</code>方法(自定义的)将记录输出到下游。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomPartitionerWrapper</span>&lt;<span class="title">K</span>, <span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">Partitioner&lt;K&gt; partitioner;</span><br><span class="line">KeySelector&lt;T, K&gt; keySelector;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">CustomPartitionerWrapper</span><span class="params">(Partitioner&lt;K&gt; partitioner, KeySelector&lt;T, K&gt; keySelector)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.partitioner = partitioner;</span><br><span class="line"><span class="keyword">this</span>.keySelector = keySelector;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line">K key;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">key = keySelector.getKey(record.getInstance().getValue());</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Could not extract key from "</span> + record.getInstance(), e);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//实现Partitioner接口，重写partition方法</span></span><br><span class="line"><span class="keyword">return</span> partitioner.partition(key, numberOfChannels);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"CUSTOM"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>比如：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">      <span class="comment">// key: 根据key的值来分区</span></span><br><span class="line">      <span class="comment">// numPartitions: 下游算子并行度</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String key, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> key.length() % numPartitions;<span class="comment">//在此处定义分区策略</span></span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/%E6%B1%87%E6%80%BB.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于Canal与Flink实现数据实时增量同步(二)</title>
      <link href="/2020/03/24/%E5%9F%BA%E4%BA%8ECanal%E4%B8%8EFlink%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%A2%9E%E9%87%8F%E5%90%8C%E6%AD%A5-%E4%BA%8C/"/>
      <url>/2020/03/24/%E5%9F%BA%E4%BA%8ECanal%E4%B8%8EFlink%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%A2%9E%E9%87%8F%E5%90%8C%E6%AD%A5-%E4%BA%8C/</url>
      
        <content type="html"><![CDATA[<p>本文主要从Binlog实时采集和离线处理Binlog还原业务数据两个方面，来介绍如何实现DB数据准确、高效地进入Hive数仓。</p><a id="more"></a><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在数据仓库建模中，未经任何加工处理的原始业务层数据，我们称之为ODS(Operational Data Store)数据。在互联网企业中，常见的ODS数据有业务日志数据（Log）和业务DB数据（DB）两类。对于业务DB数据来说，从MySQL等关系型数据库的业务数据进行采集，然后导入到Hive中，是进行数据仓库生产的重要环节。如何准确、高效地把MySQL数据同步到Hive中？一般常用的解决方案是批量取数并Load：直连MySQL去Select表中的数据，然后存到本地文件作为中间存储，最后把文件Load到Hive表中。这种方案的优点是实现简单，但是随着业务的发展，缺点也逐渐暴露出来：</p><ul><li><p>性能瓶颈：随着业务规模的增长，Select From MySQL -&gt; Save to Localfile -&gt; Load to Hive这种数据流花费的时间越来越长，无法满足下游数仓生产的时间要求。</p></li><li><p>直接从MySQL中Select大量数据，对MySQL的影响非常大，容易造成慢查询，影响业务线上的正常服务。</p></li><li><p>由于Hive本身的语法不支持更新、删除等SQL原语(高版本Hive支持，但是需要分桶+ORC存储格式)，对于MySQL中发生Update/Delete的数据无法很好地进行支持。</p></li></ul><p>为了彻底解决这些问题，我们逐步转向CDC (Change Data Capture) + Merge的技术方案，即实时Binlog采集 + 离线处理Binlog还原业务数据这样一套解决方案。Binlog是MySQL的二进制日志，记录了MySQL中发生的所有数据变更，MySQL集群自身的主从同步就是基于Binlog做的。</p><h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><p>首先，采用Flink负责把Kafka上的Binlog数据拉取到HDFS上。</p><p>然后，对每张ODS表，首先需要一次性制作快照（Snapshot），把MySQL里的存量数据读取到Hive上，这一过程底层采用直连MySQL去Select数据的方式，可以使用Sqoop进行一次性全量导入。</p><p>最后，对每张ODS表，每天基于存量数据和当天增量产生的Binlog做Merge，从而还原出业务数据。</p><p>Binlog是流式产生的，通过对Binlog的实时采集，把部分数据处理需求由每天一次的批处理分摊到实时流上。无论从性能上还是对MySQL的访问压力上，都会有明显地改善。Binlog本身记录了数据变更的类型（Insert/Update/Delete），通过一些语义方面的处理，完全能够做到精准的数据还原。</p><h2 id="实现方案"><a href="#实现方案" class="headerlink" title="实现方案"></a>实现方案</h2><h3 id="Flink处理Kafka的binlog日志"><a href="#Flink处理Kafka的binlog日志" class="headerlink" title="Flink处理Kafka的binlog日志"></a>Flink处理Kafka的binlog日志</h3><p>使用kafka source，对读取的数据进行JSON解析，将解析的字段拼接成字符串，符合Hive的schema格式，具体代码如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.etl.kafka2hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONArray;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.parser.Feature;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FilterFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringEncoder;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.core.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.StateBackend;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.filesystem.FsStateBackend;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.CheckpointConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.RollingPolicy;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Created</span> with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@author</span> : jmx</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Date</span>: 2020/3/27</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Time</span>: 12:52</span></span><br><span class="line"><span class="comment"> *  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsSink</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String fieldDelimiter = <span class="string">","</span>;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// checkpoint</span></span><br><span class="line">        env.enableCheckpointing(<span class="number">10_000</span>);</span><br><span class="line">        <span class="comment">//env.setStateBackend((StateBackend) new FsStateBackend("file:///E://checkpoint"));</span></span><br><span class="line">        env.setStateBackend((StateBackend) <span class="keyword">new</span> FsStateBackend(<span class="string">"hdfs://kms-1:8020/checkpoint"</span>));</span><br><span class="line">        CheckpointConfig config = env.getCheckpointConfig();</span><br><span class="line">        config.enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// source</span></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.setProperty(<span class="string">"bootstrap.servers"</span>, <span class="string">"kms-2:9092,kms-3:9092,kms-4:9092"</span>);</span><br><span class="line">        <span class="comment">// only required for Kafka 0.8</span></span><br><span class="line">        props.setProperty(<span class="string">"zookeeper.connect"</span>, <span class="string">"kms-2:2181,kms-3:2181,kms-4:2181"</span>);</span><br><span class="line">        props.setProperty(<span class="string">"group.id"</span>, <span class="string">"test123"</span>);</span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; consumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(</span><br><span class="line">                <span class="string">"qfbap_ods.code_city"</span>, <span class="keyword">new</span> SimpleStringSchema(), props);</span><br><span class="line">        consumer.setStartFromEarliest();</span><br><span class="line">        DataStream&lt;String&gt; stream = env.addSource(consumer);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// transform</span></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; cityDS = stream</span><br><span class="line">                .filter(<span class="keyword">new</span> FilterFunction&lt;String&gt;() &#123;</span><br><span class="line">                    <span class="comment">// 过滤掉DDL操作</span></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(String jsonVal)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        JSONObject record = JSON.parseObject(jsonVal, Feature.OrderedField);</span><br><span class="line">                        <span class="keyword">return</span> record.getString(<span class="string">"isDdl"</span>).equals(<span class="string">"false"</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        StringBuilder fieldsBuilder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">                        <span class="comment">// 解析JSON数据</span></span><br><span class="line">                        JSONObject record = JSON.parseObject(value, Feature.OrderedField);</span><br><span class="line">                        <span class="comment">// 获取最新的字段值</span></span><br><span class="line">                        JSONArray data = record.getJSONArray(<span class="string">"data"</span>);</span><br><span class="line">                        <span class="comment">// 遍历，字段值的JSON数组，只有一个元素</span></span><br><span class="line">                        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; data.size(); i++) &#123;</span><br><span class="line">                            <span class="comment">// 获取到JSON数组的第i个元素</span></span><br><span class="line">                            JSONObject obj = data.getJSONObject(i);</span><br><span class="line">                            <span class="keyword">if</span> (obj != <span class="keyword">null</span>) &#123;</span><br><span class="line">                                fieldsBuilder.append(record.getLong(<span class="string">"id"</span>)); <span class="comment">// 序号id</span></span><br><span class="line">                                fieldsBuilder.append(fieldDelimiter); <span class="comment">// 字段分隔符</span></span><br><span class="line">                                fieldsBuilder.append(record.getLong(<span class="string">"es"</span>)); <span class="comment">//业务时间戳</span></span><br><span class="line">                                fieldsBuilder.append(fieldDelimiter);</span><br><span class="line">                                fieldsBuilder.append(record.getLong(<span class="string">"ts"</span>)); <span class="comment">// 日志时间戳</span></span><br><span class="line">                                fieldsBuilder.append(fieldDelimiter);</span><br><span class="line">                                fieldsBuilder.append(record.getString(<span class="string">"type"</span>)); <span class="comment">// 操作类型</span></span><br><span class="line">                                <span class="keyword">for</span> (Map.Entry&lt;String, Object&gt; entry : obj.entrySet()) &#123;</span><br><span class="line"></span><br><span class="line">                                    fieldsBuilder.append(fieldDelimiter);</span><br><span class="line">                                    fieldsBuilder.append(entry.getValue()); <span class="comment">// 表字段数据</span></span><br><span class="line">                                &#125;</span><br><span class="line"></span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">return</span> fieldsBuilder.toString();</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//cityDS.print();</span></span><br><span class="line">        <span class="comment">//stream.print();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// sink</span></span><br><span class="line">        <span class="comment">// 以下条件满足其中之一就会滚动生成新的文件</span></span><br><span class="line">        RollingPolicy&lt;String, String&gt; rollingPolicy = DefaultRollingPolicy.create()</span><br><span class="line">                .withRolloverInterval(<span class="number">60L</span> * <span class="number">1000L</span>) <span class="comment">//滚动写入新文件的时间，默认60s。根据具体情况调节</span></span><br><span class="line">                .withMaxPartSize(<span class="number">1024</span> * <span class="number">1024</span> * <span class="number">128L</span>) <span class="comment">//设置每个文件的最大大小 ,默认是128M，这里设置为128M</span></span><br><span class="line">                .withInactivityInterval(<span class="number">60L</span> * <span class="number">1000L</span>) <span class="comment">//默认60秒,未写入数据处于不活跃状态超时会滚动新文件</span></span><br><span class="line">                .build();</span><br><span class="line">        </span><br><span class="line">        StreamingFileSink&lt;String&gt; sink = StreamingFileSink</span><br><span class="line">                <span class="comment">//.forRowFormat(new Path("file:///E://binlog_db/city"), new SimpleStringEncoder&lt;String&gt;())</span></span><br><span class="line">                .forRowFormat(<span class="keyword">new</span> Path(<span class="string">"hdfs://kms-1:8020/binlog_db/code_city_delta"</span>), <span class="keyword">new</span> SimpleStringEncoder&lt;String&gt;())</span><br><span class="line">                .withBucketAssigner(<span class="keyword">new</span> EventTimeBucketAssigner())</span><br><span class="line">                .withRollingPolicy(rollingPolicy)</span><br><span class="line">                .withBucketCheckInterval(<span class="number">1000</span>)  <span class="comment">// 桶检查间隔，这里设置1S</span></span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        cityDS.addSink(sink);</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于Flink Sink到HDFS，<code>StreamingFileSink</code> 替代了先前的 <code>BucketingSink</code>，用来将上游数据存储到 HDFS 的不同目录中。它的核心逻辑是分桶，默认的分桶方式是 <code>DateTimeBucketAssigner</code>，即按照处理时间分桶。处理时间指的是消息到达 Flink 程序的时间，这点并不符合我们的需求。因此，我们需要自己编写代码将事件时间从消息体中解析出来，按规则生成分桶的名称，具体代码如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.etl.kafka2hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.core.io.SimpleVersionedSerializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.BucketAssigner;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.SimpleVersionedStringSerializer;</span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Created</span> with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@author</span> : jmx</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Date</span>: 2020/3/27</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Time</span>: 12:49</span></span><br><span class="line"><span class="comment"> *  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EventTimeBucketAssigner</span> <span class="keyword">implements</span> <span class="title">BucketAssigner</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getBucketId</span><span class="params">(String element, Context context)</span> </span>&#123;</span><br><span class="line">        String partitionValue;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            partitionValue = getPartitionValue(element);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            partitionValue = <span class="string">"00000000"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"dt="</span> + partitionValue;<span class="comment">//分区目录名称</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SimpleVersionedSerializer&lt;String&gt; <span class="title">getSerializer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> SimpleVersionedStringSerializer.INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> String <span class="title">getPartitionValue</span><span class="params">(String element)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 取出最后拼接字符串的es字段值，该值为业务时间</span></span><br><span class="line">        <span class="keyword">long</span> eventTime = Long.parseLong(element.split(<span class="string">","</span>)[<span class="number">1</span>]);</span><br><span class="line">        Date eventDate = <span class="keyword">new</span> Date(eventTime);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> SimpleDateFormat(<span class="string">"yyyyMMdd"</span>).format(eventDate);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="离线还原MySQL数据"><a href="#离线还原MySQL数据" class="headerlink" title="离线还原MySQL数据"></a>离线还原MySQL数据</h3><p>经过上述步骤，即可将Binlog日志记录写入到HDFS的对应的分区中，接下来就需要根据增量的数据和存量的数据还原最新的数据。Hive 表保存在 HDFS 上，该文件系统不支持修改，因此我们需要一些额外工作来写入数据变更。常用的方式包括：JOIN、Hive 事务、或改用 HBase、kudu。</p><p>如昨日的存量数据code_city,今日增量的数据为code_city_delta，可以通过 <code>FULL OUTER JOIN</code>，将存量和增量数据合并成一张最新的数据表，并作为明天的存量数据：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> code_city</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">        <span class="keyword">COALESCE</span>( t2.id, t1.id ) <span class="keyword">AS</span> <span class="keyword">id</span>,</span><br><span class="line">        <span class="keyword">COALESCE</span> ( t2.city, t1.city ) <span class="keyword">AS</span> city,</span><br><span class="line">        <span class="keyword">COALESCE</span> ( t2.province, t1.province ) <span class="keyword">AS</span> province,</span><br><span class="line">        <span class="keyword">COALESCE</span> ( t2.event_time, t1.event_time ) <span class="keyword">AS</span> event_time </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">        code_city t1</span><br><span class="line">        <span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> (</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">        <span class="keyword">id</span>,</span><br><span class="line">        city,</span><br><span class="line">        province,</span><br><span class="line">        event_time </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">        (<span class="comment">-- 取最后一条状态数据</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">        <span class="keyword">id</span>,</span><br><span class="line">        city,</span><br><span class="line">        province,</span><br><span class="line">        dml_type,</span><br><span class="line">        event_time,</span><br><span class="line">        row_number ( ) <span class="keyword">over</span> ( <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">id</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> event_time <span class="keyword">DESC</span> ) <span class="keyword">AS</span> <span class="keyword">rank</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">        code_city_delta </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">        dt = <span class="string">'20200324'</span> <span class="comment">-- 分区数据</span></span><br><span class="line">        ) temp </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">        <span class="keyword">rank</span> = <span class="number">1</span> </span><br><span class="line">        ) t2 <span class="keyword">ON</span> t1.id = t2.id;</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要从Binlog流式采集和基于Binlog的ODS数据还原两方面，介绍了通过Flink实现实时的ETL，此外还可以将binlog日志写入kudu、HBase等支持事务操作的NoSQL中，这样就可以省去数据表还原的步骤。本文是《基于Canal与Flink实现数据实时增量同步》的第二篇，关于canal解析Binlog日志写入kafka的实现步骤，参见《基于Canal与Flink实现数据实时增量同步一》。</p><p><strong>refrence：</strong></p><p>[1]<a href="https://tech.meituan.com/2018/12/06/binlog-dw.html" target="_blank" rel="noopener">https://tech.meituan.com/2018/12/06/binlog-dw.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式数据集成框架gobblin快速入门</title>
      <link href="/2020/03/22/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90%E6%A1%86%E6%9E%B6gobblin%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"/>
      <url>/2020/03/22/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90%E6%A1%86%E6%9E%B6gobblin%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/apache/incubator-gobblin" target="_blank" rel="noopener">Apache Gobblin </a>是一个通用的分布式数据集成框架，用于从各种数据源（数据库，REST API，FTP / SFTP服务器，文件管理器等）提取，转换和加载大量数据到Hadoop上。使得大数据集成变得更加简单，例如<strong>流</strong>和<strong>批处理</strong>数据生态系统的数据摄取，复制，组织和生命周期管理。gobblin由LinkedIn开源，现为Apache的孵化项目。</p><hr><h2 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h2><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><p>下载gobblin的发布包，下载地址为：<a href="https://github.com/apache/incubator-gobblin/releases" target="_blank" rel="noopener">https://github.com/apache/incubator-gobblin/releases</a>,本文档下载的版本为<a href="https://github.com/apache/incubator-gobblin/releases/tag/release-0.14.0" target="_blank" rel="noopener">release-0.14.0</a>，包名称为：incubator-gobblin-release-0.14.0.tar.gz</p><h3 id="解压编译"><a href="#解压编译" class="headerlink" title="解压编译"></a>解压编译</h3><ul><li>解压源码tar包，进入解压文件夹</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar -xzvf incubator-gobblin-release-0.14.0.tar.gz -C /opt/module/</span></span><br><span class="line"><span class="comment"># cd /opt/module/incubator-gobblin-release-0.14.0</span></span><br></pre></td></tr></table></figure><ul><li>编译</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./gradlew :gobblin-distribution:buildDistributionTar</span><br></pre></td></tr></table></figure><p>编译过程大概几分钟，编译完成后会生成一个build文件夹，进入该文件夹会看到生成的tar包名称为：apache-gobblin-incubating-bin-0.14.0.tar.gz</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cd /opt/module/incubator-gobblin-release-0.14.0/build/gobblin-distribution/distributions</span></span><br></pre></td></tr></table></figure><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>将上面编译好的tar包解压，解压后的文件名称为gobblin-dist</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar -xzvf gobblin-incubating-bin-0.14.0.tar.gz  -C /opt/module/</span></span><br><span class="line"><span class="comment"># ll</span></span><br><span class="line">drwxr-xr-x 2 root root  4096 3月  22 17:35 bin</span><br><span class="line">drwxr-xr-x 6 root root  4096 3月  22 17:35 conf</span><br><span class="line">drwxr-xr-x 2 root root 16384 3月  22 17:35 lib</span><br></pre></td></tr></table></figure><h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2>]]></content>
      
      
      <categories>
          
          <category> gobblin </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gobblin </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于Canal与Flink实现数据实时增量同步(一)</title>
      <link href="/2020/03/05/%E5%9F%BA%E4%BA%8ECanal%E4%B8%8EFlink%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%A2%9E%E9%87%8F%E5%90%8C%E6%AD%A5-%E4%B8%80/"/>
      <url>/2020/03/05/%E5%9F%BA%E4%BA%8ECanal%E4%B8%8EFlink%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%A2%9E%E9%87%8F%E5%90%8C%E6%AD%A5-%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<p>canal是阿里巴巴旗下的一款开源项目，纯Java开发。基于数据库增量日志解析，提供增量数据订阅&amp;消费，目前主要支持了MySQL（也支持mariaDB）。</p><a id="more"></a><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><h3 id="常见的binlog命令"><a href="#常见的binlog命令" class="headerlink" title="常见的binlog命令"></a>常见的binlog命令</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 是否启用binlog日志</span></span><br><span class="line">show variables like <span class="string">'log_bin'</span>;</span><br><span class="line"><span class="comment"># 查看binlog类型</span></span><br><span class="line">show global variables like <span class="string">'binlog_format'</span>;</span><br><span class="line"><span class="comment"># 查看详细的日志配置信息</span></span><br><span class="line">show global variables like <span class="string">'%log%'</span>;</span><br><span class="line"><span class="comment"># mysql数据存储目录</span></span><br><span class="line">show variables like <span class="string">'%dir%'</span>;</span><br><span class="line"><span class="comment"># 查看binlog的目录</span></span><br><span class="line">show global variables like <span class="string">"%log_bin%"</span>;</span><br><span class="line"><span class="comment"># 查看当前服务器使用的biglog文件及大小</span></span><br><span class="line">show binary logs;</span><br><span class="line"><span class="comment"># 查看最新一个binlog日志文件名称和Position</span></span><br><span class="line">show master status;</span><br></pre></td></tr></table></figure><h3 id="配置MySQL的binlog"><a href="#配置MySQL的binlog" class="headerlink" title="配置MySQL的binlog"></a>配置MySQL的binlog</h3><p>对于自建 MySQL , 需要先开启 Binlog 写入功能，配置 binlog-format 为 ROW 模式，my.cnf 中配置如下</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line"><span class="built_in">log</span>-bin=mysql-bin <span class="comment"># 开启 binlog</span></span><br><span class="line">binlog-format=ROW <span class="comment"># 选择 ROW 模式</span></span><br><span class="line">server_id=1 <span class="comment"># 配置 MySQL replaction 需要定义，不要和 canal 的 slaveId 重复</span></span><br></pre></td></tr></table></figure><h3 id="授权"><a href="#授权" class="headerlink" title="授权"></a>授权</h3><p>授权 canal 链接 MySQL 账号具有作为 MySQL slave 的权限, 如果已有账户可直接 grant</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">USER</span> canal <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'canal'</span>;  </span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span>, <span class="keyword">REPLICATION</span> <span class="keyword">SLAVE</span>, <span class="keyword">REPLICATION</span> <span class="keyword">CLIENT</span> <span class="keyword">ON</span> *.* <span class="keyword">TO</span> <span class="string">'canal'</span>@<span class="string">'%'</span>;</span><br><span class="line"><span class="comment">-- GRANT ALL PRIVILEGES ON *.* TO 'canal'@'%' ;</span></span><br><span class="line"><span class="keyword">FLUSH</span> <span class="keyword">PRIVILEGES</span>;</span><br></pre></td></tr></table></figure><h2 id="部署canal"><a href="#部署canal" class="headerlink" title="部署canal"></a>部署canal</h2><h3 id="安装canal"><a href="#安装canal" class="headerlink" title="安装canal"></a>安装canal</h3><ul><li>下载：<a href="https://github.com/alibaba/canal/releases" target="_blank" rel="noopener">点此下载</a></li><li>解压缩</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[kms@kms-1 softwares]$ tar -xzvf canal.deployer-1.1.4.tar.gz  -C /opt/modules/canal/</span><br></pre></td></tr></table></figure><ul><li>目录结构</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">drwxr-xr-x 2 root root 4096 Mar  5 14:19 bin</span><br><span class="line">drwxr-xr-x 5 root root 4096 Mar  5 13:54 conf</span><br><span class="line">drwxr-xr-x 2 root root 4096 Mar  5 13:04 lib</span><br><span class="line">drwxrwxrwx 4 root root 4096 Mar  5 14:19 logs</span><br></pre></td></tr></table></figure><h3 id="配置修改"><a href="#配置修改" class="headerlink" title="配置修改"></a>配置修改</h3><ul><li>修改conf/example/instance.properties，修改内容如下：</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## mysql serverId</span></span><br><span class="line">canal.instance.mysql.slaveId = 1234</span><br><span class="line"><span class="comment">#position info，需要改成自己的数据库信息</span></span><br><span class="line">canal.instance.master.address = kms-1.apache.com:3306 </span><br><span class="line"><span class="comment">#username/password，需要改成自己的数据库信息</span></span><br><span class="line">canal.instance.dbUsername = canal  </span><br><span class="line">canal.instance.dbPassword = canal</span><br><span class="line"><span class="comment"># mq config，kafka topic名称</span></span><br><span class="line">canal.mq.topic=<span class="built_in">test</span></span><br></pre></td></tr></table></figure><ul><li>修改conf/canal.properties，修改内容如下：</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 配置zookeeper地址</span></span><br><span class="line">canal.zkServers =kms-2:2181,kms-3:2181,kms-4:2181</span><br><span class="line"><span class="comment"># 可选项: tcp(默认), kafka, RocketMQ，</span></span><br><span class="line">canal.serverMode = kafka</span><br><span class="line"><span class="comment"># 配置kafka地址</span></span><br><span class="line">canal.mq.servers = kms-2:9092,kms-3:9092,kms-4:9092</span><br></pre></td></tr></table></figure><h3 id="启动canal"><a href="#启动canal" class="headerlink" title="启动canal"></a>启动canal</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh bin/startup.sh</span><br></pre></td></tr></table></figure><h3 id="关闭canal"><a href="#关闭canal" class="headerlink" title="关闭canal"></a>关闭canal</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh bin/stop.sh</span><br></pre></td></tr></table></figure><h2 id="部署Canal-Admin-可选"><a href="#部署Canal-Admin-可选" class="headerlink" title="部署Canal Admin(可选)"></a>部署Canal Admin(可选)</h2><p>canal-admin设计上是为canal提供整体配置管理、节点运维等面向运维的功能，提供相对友好的WebUI操作界面，方便更多用户快速和安全的操作。</p><h3 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h3><p>canal-admin的限定依赖：</p><ul><li>MySQL，用于存储配置和节点等相关数据</li><li>canal版本，要求&gt;=1.1.4 (需要依赖canal-server提供面向admin的动态运维管理接口)</li></ul><h3 id="安装canal-admin"><a href="#安装canal-admin" class="headerlink" title="安装canal-admin"></a>安装canal-admin</h3><ul><li><p>下载</p><p><a href="https://github.com/alibaba/canal/releases" target="_blank" rel="noopener">点此下载</a></p></li><li><p>解压缩</p></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[kms@kms-1 softwares]$ tar -xzvf canal.admin-1.1.4.tar.gz  -C /opt/modules/canal-admin/</span><br></pre></td></tr></table></figure><ul><li>目录结构</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">drwxrwxr-x 2 kms kms 4096 Mar  6 11:25 bin</span><br><span class="line">drwxrwxr-x 3 kms kms 4096 Mar  6 11:25 conf</span><br><span class="line">drwxrwxr-x 2 kms kms 4096 Mar  6 11:25 lib</span><br><span class="line">drwxrwxr-x 2 kms kms 4096 Sep  2  2019 logs</span><br></pre></td></tr></table></figure><ul><li>配置修改</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi conf/application.yml</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">server:</span><br><span class="line">  port: 8089</span><br><span class="line">spring:</span><br><span class="line">  jackson:</span><br><span class="line">    date-format: yyyy-MM-dd HH:mm:ss</span><br><span class="line">    time-zone: GMT+8</span><br><span class="line"></span><br><span class="line">spring.datasource:</span><br><span class="line">  address: kms-1:3306</span><br><span class="line">  database: canal_manager</span><br><span class="line">  username: canal</span><br><span class="line">  password: canal</span><br><span class="line">  driver-class-name: com.mysql.jdbc.Driver</span><br><span class="line">  url: jdbc:mysql://<span class="variable">$&#123;spring.datasource.address&#125;</span>/<span class="variable">$&#123;spring.datasource.database&#125;</span>?useUnicode=<span class="literal">true</span>&amp;characterEncoding=UTF-8&amp;useSSL=<span class="literal">false</span></span><br><span class="line">  hikari:</span><br><span class="line">    maximum-pool-size: 30</span><br><span class="line">    minimum-idle: 1</span><br><span class="line"></span><br><span class="line">canal:</span><br><span class="line">  adminUser: admin</span><br><span class="line">  adminPasswd: admin</span><br></pre></td></tr></table></figure><ul><li>初始化原数据库</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql -uroot -p</span><br><span class="line"><span class="comment"># 导入初始化SQL</span></span><br><span class="line"><span class="comment">#注：(1)初始化SQL脚本里会默认创建canal_manager的数据库，建议使用root等有超级权限的账号进行初始化 </span></span><br><span class="line"><span class="comment">#    (2)canal_manager.sql默认会在conf目录下</span></span><br><span class="line">&gt; mysql&gt; <span class="built_in">source</span> /opt/modules/canal-admin/conf/canal_manager.sql</span><br></pre></td></tr></table></figure><ul><li>启动canal-admin</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh bin/startup.sh</span><br></pre></td></tr></table></figure><ul><li>访问</li></ul><p>可以通过 <a href="http://kms-1:8089/" target="_blank" rel="noopener">http://kms-1:8089/</a> 访问，默认密码：admin/123456 </p><ul><li>canal-server端配置</li></ul><p>使用canal_local.properties的配置覆盖canal.properties,将下面配置内容配置在canal_local.properties文件里面，就可以了。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># register ip</span></span><br><span class="line">canal.register.ip =</span><br><span class="line"><span class="comment"># canal admin config</span></span><br><span class="line">canal.admin.manager = 127.0.0.1:8089</span><br><span class="line">canal.admin.port = 11110</span><br><span class="line">canal.admin.user = admin</span><br><span class="line">canal.admin.passwd = 4ACFE3202A5FF5CF467898FC58AAB1D615029441</span><br><span class="line"><span class="comment"># admin auto register</span></span><br><span class="line">canal.admin.register.auto = <span class="literal">true</span></span><br><span class="line">canal.admin.register.cluster =</span><br></pre></td></tr></table></figure><ul><li>启动canal-serve</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh bin/startup.sh  <span class="built_in">local</span></span><br></pre></td></tr></table></figure><p>注意：先启canal-server,然后再启动canal-admin，之后登陆canal-admin就可以添加serve和instance了。</p><h2 id="启动kafka控制台消费者测试"><a href="#启动kafka控制台消费者测试" class="headerlink" title="启动kafka控制台消费者测试"></a>启动kafka控制台消费者测试</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server kms-2:9092,kms-3:9092,kms-4:9092  --topic <span class="built_in">test</span> --from-beginning</span><br></pre></td></tr></table></figure><p>此时MySQL数据表若有变化，会将row类型的log写进Kakfa，具体格式为JSON：</p><ul><li>insert操作</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"data"</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"id"</span>:<span class="string">"338"</span>,</span><br><span class="line">            <span class="string">"city"</span>:<span class="string">"成都"</span>,</span><br><span class="line">            <span class="string">"province"</span>:<span class="string">"四川省"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"database"</span>:<span class="string">"qfbap_ods"</span>,</span><br><span class="line">    <span class="string">"es"</span>:1583394964000,</span><br><span class="line">    <span class="string">"id"</span>:2,</span><br><span class="line">    <span class="string">"isDdl"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="string">"mysqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:<span class="string">"int(11)"</span>,</span><br><span class="line">        <span class="string">"city"</span>:<span class="string">"varchar(256)"</span>,</span><br><span class="line">        <span class="string">"province"</span>:<span class="string">"varchar(256)"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"old"</span>:null,</span><br><span class="line">    <span class="string">"pkNames"</span>:[</span><br><span class="line">        <span class="string">"id"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"sql"</span>:<span class="string">""</span>,</span><br><span class="line">    <span class="string">"sqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:4,</span><br><span class="line">        <span class="string">"city"</span>:12,</span><br><span class="line">        <span class="string">"province"</span>:12</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"table"</span>:<span class="string">"code_city"</span>,</span><br><span class="line">    <span class="string">"ts"</span>:1583394964361,</span><br><span class="line">    <span class="string">"type"</span>:<span class="string">"INSERT"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>update操作</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"data"</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"id"</span>:<span class="string">"338"</span>,</span><br><span class="line">            <span class="string">"city"</span>:<span class="string">"绵阳市"</span>,</span><br><span class="line">            <span class="string">"province"</span>:<span class="string">"四川省"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"database"</span>:<span class="string">"qfbap_ods"</span>,</span><br><span class="line">    <span class="string">"es"</span>:1583395177000,</span><br><span class="line">    <span class="string">"id"</span>:3,</span><br><span class="line">    <span class="string">"isDdl"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="string">"mysqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:<span class="string">"int(11)"</span>,</span><br><span class="line">        <span class="string">"city"</span>:<span class="string">"varchar(256)"</span>,</span><br><span class="line">        <span class="string">"province"</span>:<span class="string">"varchar(256)"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"old"</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"city"</span>:<span class="string">"成都"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"pkNames"</span>:[</span><br><span class="line">        <span class="string">"id"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"sql"</span>:<span class="string">""</span>,</span><br><span class="line">    <span class="string">"sqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:4,</span><br><span class="line">        <span class="string">"city"</span>:12,</span><br><span class="line">        <span class="string">"province"</span>:12</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"table"</span>:<span class="string">"code_city"</span>,</span><br><span class="line">    <span class="string">"ts"</span>:1583395177408,</span><br><span class="line">    <span class="string">"type"</span>:<span class="string">"UPDATE"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>delete操作</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"data"</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"id"</span>:<span class="string">"338"</span>,</span><br><span class="line">            <span class="string">"city"</span>:<span class="string">"绵阳市"</span>,</span><br><span class="line">            <span class="string">"province"</span>:<span class="string">"四川省"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"database"</span>:<span class="string">"qfbap_ods"</span>,</span><br><span class="line">    <span class="string">"es"</span>:1583395333000,</span><br><span class="line">    <span class="string">"id"</span>:4,</span><br><span class="line">    <span class="string">"isDdl"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="string">"mysqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:<span class="string">"int(11)"</span>,</span><br><span class="line">        <span class="string">"city"</span>:<span class="string">"varchar(256)"</span>,</span><br><span class="line">        <span class="string">"province"</span>:<span class="string">"varchar(256)"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"old"</span>:null,</span><br><span class="line">    <span class="string">"pkNames"</span>:[</span><br><span class="line">        <span class="string">"id"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"sql"</span>:<span class="string">""</span>,</span><br><span class="line">    <span class="string">"sqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:4,</span><br><span class="line">        <span class="string">"city"</span>:12,</span><br><span class="line">        <span class="string">"province"</span>:12</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"table"</span>:<span class="string">"code_city"</span>,</span><br><span class="line">    <span class="string">"ts"</span>:1583395333208,</span><br><span class="line">    <span class="string">"type"</span>:<span class="string">"DELETE"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="JSON日志格式解释"><a href="#JSON日志格式解释" class="headerlink" title="JSON日志格式解释"></a>JSON日志格式解释</h3><ul><li>data：最新的数据，为JSON数组，如果是插入则表示最新插入的数据，如果是更新，则表示更新后的最新数据，如果是删除，则表示被删除的数据</li><li>database：数据库名称</li><li>es：事件时间，13位的时间戳</li><li>id：事件操作的序列号，1,2,3…</li><li>isDdl：是否是DDL操作</li><li>mysqlType：字段类型</li><li>old：旧数据</li><li>pkNames：主键名称</li><li>sql：SQL语句</li><li>sqlType：是经过canal转换处理的，比如unsigned int会被转化为Long，unsigned long会被转换为BigDecimal</li><li>table：表名</li><li>ts：日志时间</li><li>type：操作类型，比如DELETE，UPDATE，INSERT</li></ul><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>本文首先介绍了MySQL binlog日志的配置以及Canal的搭建，然后描述了通过canal数据传输到Kafka的配置，最后对canal解析之后的JSON数据进行了详细解释。本文是基于Canal与Flink实现数据实时增量同步的第一篇，在下一篇介绍如何使用Flink实现实时增量数据同步。</p><hr>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQL中的相关子查询解析</title>
      <link href="/2019/12/10/SQL%E4%B8%AD%E7%9A%84%E7%9B%B8%E5%85%B3%E5%AD%90%E6%9F%A5%E8%AF%A2%E8%A7%A3%E6%9E%90/"/>
      <url>/2019/12/10/SQL%E4%B8%AD%E7%9A%84%E7%9B%B8%E5%85%B3%E5%AD%90%E6%9F%A5%E8%AF%A2%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>分步骤解析SQL的相关子查询</p><a id="more"></a><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>在SQL语言中，一个SELECT-FROM-WHERE语句称为一个查询块。讲一个查询块嵌套在另一个查询块的WHERE子句或者HAVing短语的条件中的查询称为嵌套查询。其中上层查询块称为外层查询或者父查询，下层查询称为内查询或者子查询。</p><p>根据子查询是否依赖于父查询，可以分为不相关子查询和相关子查询。其中子查询的查询条件不依赖于父查询，称为不相关子查询，如果子查询的查询条件依赖于父查询，则这类子查询称之为相关子查询。</p><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><table><thead><tr><th align="center">学号</th><th align="center">课程号</th><th align="center">成绩</th></tr></thead><tbody><tr><td align="center">201215121</td><td align="center">1</td><td align="center">92</td></tr><tr><td align="center">201215121</td><td align="center">2</td><td align="center">85</td></tr><tr><td align="center">201215121</td><td align="center">3</td><td align="center">88</td></tr><tr><td align="center">201215122</td><td align="center">2</td><td align="center">90</td></tr><tr><td align="center">201215122</td><td align="center">3</td><td align="center">80</td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> SC (Sno <span class="built_in">char</span>(<span class="number">9</span>), Cno <span class="built_in">char</span>(<span class="number">4</span>),Grade <span class="built_in">SMALLINT</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> SC;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> SC <span class="keyword">values</span> (<span class="string">'201215121'</span>,<span class="string">'1'</span>, <span class="number">92</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> SC <span class="keyword">values</span> (<span class="string">'201215121'</span>,<span class="string">'2'</span>, <span class="number">85</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> SC <span class="keyword">values</span> (<span class="string">'201215121'</span>,<span class="string">'3'</span>, <span class="number">88</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> SC <span class="keyword">values</span> (<span class="string">'201215122'</span>,<span class="string">'2'</span>,<span class="number">90</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> SC <span class="keyword">values</span> (<span class="string">'201215122'</span>,<span class="string">'3'</span>,<span class="number">80</span>);</span><br></pre></td></tr></table></figure><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><ul><li>找出每个学生超过他自己选修课程平均分的课程号</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> Sno,Cno </span><br><span class="line"><span class="keyword">FROM</span> SC x </span><br><span class="line"><span class="keyword">WHERE</span> Grade &gt;= ( <span class="keyword">SELECT</span> <span class="keyword">avg</span>( Grade )</span><br><span class="line">                 <span class="keyword">FROM</span> SC y </span><br><span class="line"><span class="keyword">WHERE</span> y.Sno = x.Sno )</span><br></pre></td></tr></table></figure><p>x是SC的别名，又称为元祖变量，可以用来表示SC的一个元祖。内层查询时求一个学生所有选修课程的平均成绩的，至于是哪一个学生的平均成绩要看参数<code>x.Sno</code>的值，而该值是与父查询相关的，因此这类查询称之为相关子查询。</p><p>这个语句的一种可能执行过程采用以下三个步骤。</p><p>1.从外层查询中取出SC的一个元祖x，将元祖x的Sno值(201215121)传送给内层查询。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">AVG</span>(Grade)</span><br><span class="line"><span class="keyword">FROM</span>  SC y</span><br><span class="line"><span class="keyword">WHERE</span> y.Sno=<span class="string">'201215121'</span>;</span><br></pre></td></tr></table></figure><p>2.执行内层查询，得到值88(近似值)，用该值代替内层查询，得到外层查询：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> Sno,Cno</span><br><span class="line"><span class="keyword">FROM</span>  SC x</span><br><span class="line"><span class="keyword">WHERE</span> Grade &gt; <span class="number">88</span>;</span><br></pre></td></tr></table></figure><p>3.执行这个查询，得到 (201215121,1)</p><p>然后外层查询取出下一个元祖重复上述1至2步骤的处理，直到外层的SC元祖全部处理完毕。</p><hr><ul><li><p><strong>reference</strong></p><p>[1]王珊, 萨师煊. 数据库系统概论(第5版)</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeeCode数据库部分题目汇总</title>
      <link href="/2019/12/09/LeeCode%E6%95%B0%E6%8D%AE%E5%BA%93%E9%83%A8%E5%88%86%E9%A2%98%E7%9B%AE%E6%B1%87%E6%80%BB/"/>
      <url>/2019/12/09/LeeCode%E6%95%B0%E6%8D%AE%E5%BA%93%E9%83%A8%E5%88%86%E9%A2%98%E7%9B%AE%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<p>LeeCode数据库部分SQL题目总结</p><a id="more"></a><h2 id="176-第二高的薪水"><a href="#176-第二高的薪水" class="headerlink" title="176. 第二高的薪水"></a>176. 第二高的薪水</h2><h3 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h3><p>编写一个 SQL 查询，获取 Employee 表中第二高的薪水（Salary）</p><table><thead><tr><th align="center">Id</th><th align="center">Salary</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">100</td></tr><tr><td align="center">2</td><td align="center">200</td></tr><tr><td align="center">3</td><td align="center">300</td></tr></tbody></table><p>例如上述 Employee 表，SQL查询应该返回 200 作为第二高的薪水。如果不存在第二高的薪水，那么查询应返回 null</p><table><thead><tr><th align="center">SecondHighestSalary</th></tr></thead><tbody><tr><td align="center">200</td></tr></tbody></table><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Employee (<span class="keyword">Id</span> <span class="built_in">int</span>, Salary <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Employee;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, Salary) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'100'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, Salary) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'200'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, Salary) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'300'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句"><a href="#SQL语句" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">MAX</span>(Salary) SecondHighestSalary</span><br><span class="line"><span class="keyword">FROM</span> Employee</span><br><span class="line"><span class="keyword">WHERE</span> Salary &lt;</span><br><span class="line">(<span class="keyword">SELECT</span> <span class="keyword">MAX</span>(Salary) <span class="keyword">FROM</span> Employee)</span><br></pre></td></tr></table></figure><h2 id="178-分数排名"><a href="#178-分数排名" class="headerlink" title="178.分数排名"></a>178.分数排名</h2><h3 id="描述-1"><a href="#描述-1" class="headerlink" title="描述"></a>描述</h3><p>编写一个 SQL 查询来实现分数排名。如果两个分数相同，则两个分数排名（Rank）相同。请注意，平分后的下一个名次应该是下一个连续的整数值。换句话说，名次之间不应该有“间隔”。</p><table><thead><tr><th align="center">Id</th><th align="center">Score</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">3.50</td></tr><tr><td align="center">2</td><td align="center">3.65</td></tr><tr><td align="center">3</td><td align="center">4.00</td></tr><tr><td align="center">4</td><td align="center">3.85</td></tr><tr><td align="center">5</td><td align="center">4.00</td></tr><tr><td align="center">6</td><td align="center">3.65</td></tr></tbody></table><p>例如，根据上述给定的 Scores 表，你的查询应该返回（按分数从高到低排列）：</p><table><thead><tr><th align="center">Score</th><th align="center">Rank</th></tr></thead><tbody><tr><td align="center">4.00</td><td align="center">1</td></tr><tr><td align="center">4.00</td><td align="center">1</td></tr><tr><td align="center">3.85</td><td align="center">2</td></tr><tr><td align="center">3.65</td><td align="center">3</td></tr><tr><td align="center">3.65</td><td align="center">3</td></tr><tr><td align="center">3.50</td><td align="center">4</td></tr></tbody></table><h3 id="数据准备-1"><a href="#数据准备-1" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Scores (<span class="keyword">Id</span> <span class="built_in">int</span>, Score <span class="built_in">DECIMAL</span>(<span class="number">3</span>,<span class="number">2</span>));</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Scores;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Scores (<span class="keyword">Id</span>, Score) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'3.5'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Scores (<span class="keyword">Id</span>, Score) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'3.65'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Scores (<span class="keyword">Id</span>, Score) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'4.0'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Scores (<span class="keyword">Id</span>, Score) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'3.85'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Scores (<span class="keyword">Id</span>, Score) <span class="keyword">values</span> (<span class="string">'5'</span>, <span class="string">'4.0'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Scores (<span class="keyword">Id</span>, Score) <span class="keyword">values</span> (<span class="string">'6'</span>, <span class="string">'3.65'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-1"><a href="#SQL语句-1" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">Score,</span><br><span class="line">@<span class="keyword">rank</span> := @<span class="keyword">rank</span> + (@prev &lt;&gt; (@prev := Score)) <span class="keyword">Rank</span></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Scores,</span><br><span class="line">(<span class="keyword">SELECT</span> @<span class="keyword">rank</span> := <span class="number">0</span>, @prev := <span class="number">-1</span>) init</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> Score <span class="keyword">desc</span></span><br></pre></td></tr></table></figure><h2 id="180-连续出现的数字"><a href="#180-连续出现的数字" class="headerlink" title="180. 连续出现的数字"></a>180. 连续出现的数字</h2><h3 id="描述-2"><a href="#描述-2" class="headerlink" title="描述"></a>描述</h3><p>编写一个 SQL 查询，查找所有至少连续出现三次的数字。</p><table><thead><tr><th align="center">Id</th><th align="center">Num</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">1</td></tr><tr><td align="center">2</td><td align="center">1</td></tr><tr><td align="center">3</td><td align="center">1</td></tr><tr><td align="center">4</td><td align="center">2</td></tr><tr><td align="center">5</td><td align="center">1</td></tr><tr><td align="center">6</td><td align="center">2</td></tr><tr><td align="center">7</td><td align="center">2</td></tr></tbody></table><p>例如，给定上面的 Logs 表， 1 是唯一连续出现至少三次的数字。</p><table><thead><tr><th align="center">ConsecutiveNums</th></tr></thead><tbody><tr><td align="center">1</td></tr></tbody></table><h3 id="数据准备-2"><a href="#数据准备-2" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Num</span> <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> <span class="keyword">Logs</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'5'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'6'</span>, <span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'7'</span>, <span class="string">'2'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-2"><a href="#SQL语句-2" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> l1.Num ConsecutiveNums</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">Logs</span> l1,</span><br><span class="line"><span class="keyword">Logs</span> l2,</span><br><span class="line"><span class="keyword">Logs</span> l3</span><br><span class="line"><span class="keyword">WHERE</span> l1.Id=l2.Id<span class="number">-1</span></span><br><span class="line"><span class="keyword">AND</span> l2.Id =l3.Id<span class="number">-1</span></span><br><span class="line"><span class="keyword">AND</span> l1.Num =l2.Num</span><br><span class="line"><span class="keyword">AND</span> l2.Num =l3.Num</span><br></pre></td></tr></table></figure><h2 id="181-超过经理收入的员工"><a href="#181-超过经理收入的员工" class="headerlink" title="181. 超过经理收入的员工"></a>181. 超过经理收入的员工</h2><h3 id="描述-3"><a href="#描述-3" class="headerlink" title="描述"></a>描述</h3><p>Employee 表包含所有员工，他们的经理也属于员工。每个员工都有一个 Id，此外还有一列对应员工的经理的 Id。</p><table><thead><tr><th align="center">Id</th><th align="center">Name</th><th align="center">Salary</th><th align="center">ManagerId</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">Joe</td><td align="center">70000</td><td align="center">3</td></tr><tr><td align="center">2</td><td align="center">Henry</td><td align="center">80000</td><td align="center">4</td></tr><tr><td align="center">3</td><td align="center">Sam</td><td align="center">60000</td><td align="center">null</td></tr><tr><td align="center">4</td><td align="center">Max</td><td align="center">90000</td><td align="center">null</td></tr></tbody></table><p>给定 Employee 表，编写一个 SQL 查询，该查询可以获取收入超过他们经理的员工的姓名。在上面的表格中，Joe 是唯一一个收入超过他的经理的员工</p><table><thead><tr><th align="center">Employee</th></tr></thead><tbody><tr><td align="center">Joe</td></tr></tbody></table><h3 id="数据准备-3"><a href="#数据准备-3" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Employee (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), Salary <span class="built_in">int</span>, ManagerId <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Employee;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, ManagerId) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'Joe'</span>, <span class="string">'70000'</span>, <span class="string">'3'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, ManagerId) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Henry'</span>, <span class="string">'80000'</span>, <span class="string">'4'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, ManagerId) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'Sam'</span>, <span class="string">'60000'</span>, <span class="string">'None'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, ManagerId) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'Max'</span>, <span class="string">'90000'</span>, <span class="string">'None'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-3"><a href="#SQL语句-3" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">     a.NAME <span class="keyword">AS</span> Employee</span><br><span class="line"><span class="keyword">FROM</span> Employee <span class="keyword">AS</span> a <span class="keyword">JOIN</span> Employee <span class="keyword">AS</span> b</span><br><span class="line">     <span class="keyword">ON</span> a.ManagerId = b.Id</span><br><span class="line">     <span class="keyword">AND</span> a.Salary &gt; b.Salary</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="182-查找重复的电子邮箱"><a href="#182-查找重复的电子邮箱" class="headerlink" title="182. 查找重复的电子邮箱"></a>182. 查找重复的电子邮箱</h2><h3 id="描述-4"><a href="#描述-4" class="headerlink" title="描述"></a>描述</h3><p>编写一个 SQL 查询，查找 Person 表中所有重复的电子邮箱。示例：</p><table><thead><tr><th align="center">Id</th><th align="center">Email</th></tr></thead><tbody><tr><td align="center">1</td><td align="center"><a href="mailto:a@b.com" target="_blank" rel="noopener">a@b.com</a></td></tr><tr><td align="center">2</td><td align="center"><a href="mailto:c@d.com" target="_blank" rel="noopener">c@d.com</a></td></tr><tr><td align="center">3</td><td align="center"><a href="mailto:a@b.com" target="_blank" rel="noopener">a@b.com</a></td></tr></tbody></table><p>根据以上输入，你的查询应返回以下结果：</p><table><thead><tr><th align="center">Email</th></tr></thead><tbody><tr><td align="center"><a href="mailto:a@b.com" target="_blank" rel="noopener">a@b.com</a></td></tr></tbody></table><p>说明：所有电子邮箱都是小写字母。</p><h3 id="数据准备-4"><a href="#数据准备-4" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Person (<span class="keyword">Id</span> <span class="built_in">int</span>, Email <span class="built_in">varchar</span>(<span class="number">255</span>));</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Person;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Person (<span class="keyword">Id</span>, Email) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'a@b.com'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Person (<span class="keyword">Id</span>, Email) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'c@d.com'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Person (<span class="keyword">Id</span>, Email) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'a@b.com'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-4"><a href="#SQL语句-4" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 方法1：</span></span><br><span class="line"><span class="keyword">select</span> Email <span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">  <span class="keyword">select</span> Email, <span class="keyword">count</span>(Email) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line">  <span class="keyword">from</span> Person</span><br><span class="line">  <span class="keyword">group</span> <span class="keyword">by</span> Email</span><br><span class="line">) <span class="keyword">as</span> statistic</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">num</span> &gt; <span class="number">1</span></span><br><span class="line">;</span><br><span class="line"><span class="comment">-- 方法2</span></span><br><span class="line"><span class="keyword">select</span> Email</span><br><span class="line"><span class="keyword">from</span> Person</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> Email</span><br><span class="line"><span class="keyword">having</span> <span class="keyword">count</span>(Email) &gt; <span class="number">1</span>;</span><br><span class="line"><span class="comment">-- 方法3</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">     <span class="keyword">distinct</span>(P1.Email) <span class="string">'Email'</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">      Person P1,</span><br><span class="line">      Person P2</span><br><span class="line"><span class="keyword">where</span> P1.Id &lt;&gt; P2.Id <span class="keyword">and</span> P1.Email = P2.Email</span><br></pre></td></tr></table></figure><h2 id="183-从不订购的客户"><a href="#183-从不订购的客户" class="headerlink" title="183. 从不订购的客户"></a>183. 从不订购的客户</h2><h3 id="描述-5"><a href="#描述-5" class="headerlink" title="描述"></a>描述</h3><p>某网站包含两个表，Customers 表和 Orders 表。编写一个 SQL 查询，找出所有从不订购任何东西的客户。</p><p>Customers 表：</p><table><thead><tr><th align="center">Id</th><th align="center">Name</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">Joe</td></tr><tr><td align="center">2</td><td align="center">Henry</td></tr><tr><td align="center">3</td><td align="center">Sam</td></tr><tr><td align="center">4</td><td align="center">Max</td></tr></tbody></table><p>Orders 表：</p><table><thead><tr><th align="center">Id</th><th align="center">CustomerId</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">3</td></tr><tr><td align="center">2</td><td align="center">1</td></tr></tbody></table><p>例如给定上述表格，你的查询应返回：</p><table><thead><tr><th align="center">Customers</th></tr></thead><tbody><tr><td align="center">Henry</td></tr><tr><td align="center">Max</td></tr></tbody></table><h3 id="数据准备-5"><a href="#数据准备-5" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Customers (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Name</span> <span class="built_in">varchar</span>(<span class="number">255</span>));</span><br><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Orders (<span class="keyword">Id</span> <span class="built_in">int</span>, CustomerId <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Customers;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Customers (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'Joe'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Customers (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Henry'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Customers (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'Sam'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Customers (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'Max'</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Orders;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Orders (<span class="keyword">Id</span>, CustomerId) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'3'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Orders (<span class="keyword">Id</span>, CustomerId) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'1'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-5"><a href="#SQL语句-5" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 方法1：</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">a.NAME <span class="string">'Customers'</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Customers a</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> Orders b <span class="keyword">ON</span> a.Id = b.CustomerId </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">b.Id <span class="keyword">IS</span> <span class="literal">NULL</span></span><br><span class="line"><span class="comment">-- 方法2：</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">NAME</span></span><br><span class="line"><span class="string">'Customers'</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Customers </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line"><span class="keyword">Id</span> <span class="keyword">NOT</span> <span class="keyword">IN</span> ( <span class="keyword">SELECT</span> CustomerId <span class="keyword">FROM</span> Orders )</span><br></pre></td></tr></table></figure><h2 id="184-部门工资最高的员工"><a href="#184-部门工资最高的员工" class="headerlink" title="184. 部门工资最高的员工"></a>184. 部门工资最高的员工</h2><h3 id="描述-6"><a href="#描述-6" class="headerlink" title="描述"></a>描述</h3><p>Employee 表包含所有员工信息，每个员工有其对应的 Id, salary 和 department Id。</p><table><thead><tr><th>Id</th><th>Name</th><th>Salary</th><th>DepartmentId</th></tr></thead><tbody><tr><td>1</td><td>Joe</td><td>70000</td><td>1</td></tr><tr><td>2</td><td>Henry</td><td>80000</td><td>2</td></tr><tr><td>3</td><td>Sam</td><td>60000</td><td>2</td></tr><tr><td>4</td><td>Max</td><td>90000</td><td>1</td></tr></tbody></table><p>Department 表包含公司所有部门的信息。</p><table><thead><tr><th>Id</th><th>Name</th></tr></thead><tbody><tr><td>1</td><td>IT</td></tr><tr><td>2</td><td>Sales</td></tr></tbody></table><p>编写一个 SQL 查询，找出每个部门工资最高的员工。例如，根据上述给定的表格，Max 在 IT 部门有最高工资，Henry 在 Sales 部门有最高工资。</p><table><thead><tr><th>Department</th><th>Employee</th><th>Salary</th></tr></thead><tbody><tr><td>IT</td><td>Max</td><td>90000</td></tr><tr><td>Sales</td><td>Henry</td><td>80000</td></tr></tbody></table><h3 id="数据准备-6"><a href="#数据准备-6" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Employee (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), Salary <span class="built_in">int</span>, DepartmentId <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Department (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Name</span> <span class="built_in">varchar</span>(<span class="number">255</span>));</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Employee;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'Joe'</span>, <span class="string">'70000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Henry'</span>, <span class="string">'80000'</span>, <span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'Sam'</span>, <span class="string">'60000'</span>, <span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'Max'</span>, <span class="string">'90000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Department;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Department (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'IT'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Department (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Sales'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-6"><a href="#SQL语句-6" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    Department.name <span class="keyword">AS</span> <span class="string">'Department'</span>,</span><br><span class="line">    Employee.name <span class="keyword">AS</span> <span class="string">'Employee'</span>,</span><br><span class="line">    Salary</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">    Employee</span><br><span class="line">        <span class="keyword">JOIN</span></span><br><span class="line">    Department <span class="keyword">ON</span> Employee.DepartmentId = Department.Id</span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">    (Employee.DepartmentId , Salary) <span class="keyword">IN</span></span><br><span class="line">    (   <span class="keyword">SELECT</span></span><br><span class="line">            DepartmentId, <span class="keyword">MAX</span>(Salary)</span><br><span class="line">        <span class="keyword">FROM</span></span><br><span class="line">            Employee</span><br><span class="line">        <span class="keyword">GROUP</span> <span class="keyword">BY</span> DepartmentId</span><br><span class="line">)</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="185-部门工资前三高的所有员工"><a href="#185-部门工资前三高的所有员工" class="headerlink" title="185.部门工资前三高的所有员工"></a>185.部门工资前三高的所有员工</h2><h3 id="描述-7"><a href="#描述-7" class="headerlink" title="描述"></a>描述</h3><p>Employee 表包含所有员工信息，每个员工有其对应的工号 Id，姓名 Name，工资 Salary 和部门编号 DepartmentId 。</p><table><thead><tr><th align="center">Id</th><th align="center">Name</th><th align="center">Salary</th><th align="center">DepartmentId</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">Joe</td><td align="center">85000</td><td align="center">1</td></tr><tr><td align="center">2</td><td align="center">Henry</td><td align="center">80000</td><td align="center">2</td></tr><tr><td align="center">3</td><td align="center">Sam</td><td align="center">60000</td><td align="center">2</td></tr><tr><td align="center">4</td><td align="center">Max</td><td align="center">90000</td><td align="center">1</td></tr><tr><td align="center">5</td><td align="center">Janet</td><td align="center">69000</td><td align="center">1</td></tr><tr><td align="center">6</td><td align="center">Randy</td><td align="center">85000</td><td align="center">1</td></tr><tr><td align="center">7</td><td align="center">Will</td><td align="center">70000</td><td align="center">1</td></tr></tbody></table><p>Department 表包含公司所有部门的信息。</p><table><thead><tr><th align="center">Id</th><th align="center">Name</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">IT</td></tr><tr><td align="center">2</td><td align="center">Sales</td></tr></tbody></table><p>编写一个 SQL 查询，找出每个部门获得前三高工资的所有员工。例如，根据上述给定的表，查询结果应返回：</p><table><thead><tr><th align="center">Department</th><th align="center">Employee</th><th align="center">Salary</th></tr></thead><tbody><tr><td align="center">IT</td><td align="center">Max</td><td align="center">90000</td></tr><tr><td align="center">IT</td><td align="center">Randy</td><td align="center">85000</td></tr><tr><td align="center">IT</td><td align="center">Joe</td><td align="center">85000</td></tr><tr><td align="center">IT</td><td align="center">Will</td><td align="center">70000</td></tr><tr><td align="center">Sales</td><td align="center">Henry</td><td align="center">80000</td></tr><tr><td align="center">Sales</td><td align="center">Sam</td><td align="center">60000</td></tr></tbody></table><p>解释：</p><p>IT 部门中，Max 获得了最高的工资，Randy 和 Joe 都拿到了第二高的工资，Will 的工资排第三。销售部门（Sales）只有两名员工，Henry 的工资最高，Sam 的工资排第二。</p><h3 id="数据准备-7"><a href="#数据准备-7" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Employee (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), Salary <span class="built_in">int</span>, DepartmentId <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Department (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Name</span> <span class="built_in">varchar</span>(<span class="number">255</span>));</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Employee;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'Joe'</span>, <span class="string">'85000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Henry'</span>, <span class="string">'80000'</span>, <span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'Sam'</span>, <span class="string">'60000'</span>, <span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'Max'</span>, <span class="string">'90000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'5'</span>, <span class="string">'Janet'</span>, <span class="string">'69000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'6'</span>, <span class="string">'Randy'</span>, <span class="string">'85000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'7'</span>, <span class="string">'Will'</span>, <span class="string">'70000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Department;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Department (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'IT'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Department (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Sales'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-7"><a href="#SQL语句-7" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">d.Name <span class="keyword">AS</span> <span class="string">'Department'</span>, e1.Name <span class="keyword">AS</span> <span class="string">'Employee'</span>, e1.Salary</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Employee e1</span><br><span class="line"><span class="keyword">JOIN</span></span><br><span class="line">Department d <span class="keyword">ON</span> e1.DepartmentId = d.Id</span><br><span class="line"><span class="keyword">WHERE</span> <span class="comment">-- 相关子查询，父查询传递一个元祖到子查询，遍历子查询的的数据，如果满足不超过3个人的工资大于传过来的工资，则保留该元祖的数据，否则就过滤掉</span></span><br><span class="line"><span class="number">3</span> &gt; (<span class="keyword">SELECT</span></span><br><span class="line"><span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> e2.Salary) <span class="comment">-- 对于重复的工资，计数一次，从而保证相同的工资的排名相同</span></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Employee e2</span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">e2.Salary &gt; e1.Salary</span><br><span class="line"><span class="keyword">AND</span> e1.DepartmentId = e2.DepartmentId</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="196-删除重复的邮箱"><a href="#196-删除重复的邮箱" class="headerlink" title="196.删除重复的邮箱"></a>196.删除重复的邮箱</h2><h3 id="描述-8"><a href="#描述-8" class="headerlink" title="描述"></a>描述</h3><p>编写一个 SQL 查询，来删除 Person 表中所有重复的电子邮箱，重复的邮箱里只保留 Id 最小 的那个。</p><table><thead><tr><th>Id</th><th>Email</th></tr></thead><tbody><tr><td>1</td><td><a href="mailto:john@example.com" target="_blank" rel="noopener">john@example.com</a></td></tr><tr><td>2</td><td><a href="mailto:bob@example.com" target="_blank" rel="noopener">bob@example.com</a></td></tr><tr><td>3</td><td><a href="mailto:john@example.com" target="_blank" rel="noopener">john@example.com</a></td></tr></tbody></table><p>Id 是这个表的主键。<br>例如，在运行你的查询语句之后，上面的 Person 表应返回以下几行:</p><table><thead><tr><th align="center">Id</th><th align="center">Email</th></tr></thead><tbody><tr><td align="center">1</td><td align="center"><a href="mailto:john@example.com" target="_blank" rel="noopener">john@example.com</a></td></tr><tr><td align="center">2</td><td align="center"><a href="mailto:bob@example.com" target="_blank" rel="noopener">bob@example.com</a></td></tr></tbody></table><h3 id="数据准备-8"><a href="#数据准备-8" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Person (<span class="keyword">Id</span> <span class="built_in">int</span>,Email <span class="built_in">varchar</span>(<span class="number">20</span>));</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Person;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Person <span class="keyword">values</span> (<span class="string">'1'</span>,  <span class="string">'john@example.com'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Person <span class="keyword">values</span> (<span class="string">'2'</span>,  <span class="string">'bob@example.com'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Person <span class="keyword">values</span> (<span class="string">'3'</span>,  <span class="string">'john@example.com'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-8"><a href="#SQL语句-8" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DELETE</span> p1.* </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Person p1,</span><br><span class="line">Person p2 </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">p1.Email = p2.Email </span><br><span class="line"><span class="keyword">AND</span> p1.Id &gt; p2.Id</span><br></pre></td></tr></table></figure><h2 id="197-上升的温度"><a href="#197-上升的温度" class="headerlink" title="197.上升的温度"></a>197.上升的温度</h2><h3 id="描述-9"><a href="#描述-9" class="headerlink" title="描述"></a>描述</h3><p>给定一个 Weather 表，编写一个 SQL 查询，来查找与之前（昨天的）日期相比温度更高的所有日期的 Id。</p><table><thead><tr><th align="center">Id(INT)</th><th align="center">RecordDate(DATE)</th><th align="center">Temperature(INT)</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">2015-01-01</td><td align="center">10</td></tr><tr><td align="center">2</td><td align="center">2015-01-02</td><td align="center">25</td></tr><tr><td align="center">3</td><td align="center">2015-01-03</td><td align="center">20</td></tr><tr><td align="center">4</td><td align="center">2015-01-04</td><td align="center">30</td></tr></tbody></table><p>例如，根据上述给定的 Weather 表格，返回如下 Id:</p><table><thead><tr><th align="center">id</th></tr></thead><tbody><tr><td align="center">2</td></tr><tr><td align="center">4</td></tr></tbody></table><h3 id="数据准备-9"><a href="#数据准备-9" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Weather (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="built_in">Date</span> <span class="built_in">date</span>, Temperature <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Weather;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Weather (<span class="keyword">Id</span>, <span class="built_in">Date</span>, Temperature) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'2015-01-01'</span>, <span class="string">'10'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Weather (<span class="keyword">Id</span>, <span class="built_in">Date</span>, Temperature) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'2015-01-02'</span>, <span class="string">'25'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Weather (<span class="keyword">Id</span>, <span class="built_in">Date</span>, Temperature) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'2015-01-03'</span>, <span class="string">'20'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Weather (<span class="keyword">Id</span>, <span class="built_in">Date</span>, Temperature) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'2015-01-04'</span>, <span class="string">'30'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-9"><a href="#SQL语句-9" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">a.Id </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Weather a</span><br><span class="line"><span class="keyword">JOIN</span> Weather b <span class="keyword">ON</span> <span class="keyword">DATEDIFF</span>(a.RecordDate,b.RecordDate) = <span class="number">1</span></span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">a.Temperature &gt; b.Temperature</span><br></pre></td></tr></table></figure><h2 id="262-行程与用户"><a href="#262-行程与用户" class="headerlink" title="262.行程与用户"></a>262.行程与用户</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>Trips 表中存所有出租车的行程信息。每段行程有唯一键 Id，Client_Id 和 Driver_Id 是 Users 表中 Users_Id 的外键。Status 是枚举类型，枚举成员为 (‘completed’, ‘cancelled_by_driver’, ‘cancelled_by_client’)。</p><table><thead><tr><th align="center">Id</th><th align="center">Client_Id</th><th align="center">Driver_Id</th><th align="center">City_Id</th><th align="center">Status</th><th align="center">Request_at</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">1</td><td align="center">10</td><td align="center">1</td><td align="center">completed</td><td align="center">2013-10-01</td></tr><tr><td align="center">2</td><td align="center">2</td><td align="center">11</td><td align="center">1</td><td align="center">cancelled_by_driver</td><td align="center">2013-10-01</td></tr><tr><td align="center">3</td><td align="center">3</td><td align="center">12</td><td align="center">6</td><td align="center">cancelled</td><td align="center">2013-10-01</td></tr><tr><td align="center">4</td><td align="center">4</td><td align="center">13</td><td align="center">6</td><td align="center">cancelled_by_client</td><td align="center">2013-10-01</td></tr><tr><td align="center">5</td><td align="center">1</td><td align="center">10</td><td align="center">1</td><td align="center">completed</td><td align="center">2013-10-02</td></tr><tr><td align="center">6</td><td align="center">2</td><td align="center">11</td><td align="center">6</td><td align="center">completed</td><td align="center">2013-10-02</td></tr><tr><td align="center">7</td><td align="center">3</td><td align="center">12</td><td align="center">6</td><td align="center">completed</td><td align="center">2013-10-02</td></tr><tr><td align="center">8</td><td align="center">2</td><td align="center">12</td><td align="center">12</td><td align="center">completed</td><td align="center">2013-10-03</td></tr><tr><td align="center">9</td><td align="center">3</td><td align="center">10</td><td align="center">12</td><td align="center">completed</td><td align="center">2013-10-03</td></tr><tr><td align="center">10</td><td align="center">4</td><td align="center">13</td><td align="center">12</td><td align="center">cancelled_by_driver</td><td align="center">2013-10-03</td></tr></tbody></table><p>Users 表存所有用户。每个用户有唯一键 Users_Id。Banned 表示这个用户是否被禁止，Role 则是一个表示（‘client’, ‘driver’, ‘partner’）的枚举类型。</p><table><thead><tr><th align="center">Users_Id</th><th align="center">Banned</th><th align="center">Role</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">No</td><td align="center">client</td></tr><tr><td align="center">2</td><td align="center">Yes</td><td align="center">client</td></tr><tr><td align="center">3</td><td align="center">No</td><td align="center">client</td></tr><tr><td align="center">4</td><td align="center">No</td><td align="center">client</td></tr><tr><td align="center">10</td><td align="center">No</td><td align="center">driver</td></tr><tr><td align="center">11</td><td align="center">No</td><td align="center">driver</td></tr><tr><td align="center">12</td><td align="center">No</td><td align="center">driver</td></tr><tr><td align="center">13</td><td align="center">No</td><td align="center">driver</td></tr></tbody></table><p>写一段 SQL 语句查出 2013年10月1日 至 2013年10月3日 期间非禁止用户的取消率。基于上表，你的 SQL 语句应返回如下结果，取消率（Cancellation Rate）保留两位小数。</p><p>取消率的计算方式如下：(被司机或乘客取消的非禁止用户生成的订单数量) / (非禁止用户生成的订单总数)</p><table><thead><tr><th align="center">Day</th><th align="center">Cancellation Rate</th></tr></thead><tbody><tr><td align="center">2013-10-01</td><td align="center">0.33</td></tr><tr><td align="center">2013-10-02</td><td align="center">0.00</td></tr><tr><td align="center">2013-10-03</td><td align="center">0.50</td></tr></tbody></table><h3 id="数据准备-10"><a href="#数据准备-10" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Trips (<span class="keyword">Id</span> <span class="built_in">int</span>, Client_Id <span class="built_in">int</span>, Driver_Id <span class="built_in">int</span>,</span><br><span class="line">City_Id <span class="built_in">int</span>, <span class="keyword">Status</span> ENUM(<span class="string">'completed'</span>, <span class="string">'cancelled_by_driver'</span>, <span class="string">'cancelled_by_client'</span>),</span><br><span class="line">Request_at <span class="built_in">varchar</span>(<span class="number">50</span>));</span><br><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> <span class="keyword">Users</span> (Users_Id <span class="built_in">int</span>,</span><br><span class="line">Banned <span class="built_in">varchar</span>(<span class="number">50</span>), <span class="keyword">Role</span> ENUM(<span class="string">'client'</span>, <span class="string">'driver'</span>, <span class="string">'partner'</span>));</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Trips;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'1'</span>, <span class="string">'10'</span>, <span class="string">'1'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-01'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'2'</span>, <span class="string">'11'</span>, <span class="string">'1'</span>, <span class="string">'cancelled_by_driver'</span>, <span class="string">'2013-10-01'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'3'</span>, <span class="string">'12'</span>, <span class="string">'6'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-01'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'4'</span>, <span class="string">'13'</span>, <span class="string">'6'</span>, <span class="string">'cancelled_by_client'</span>, <span class="string">'2013-10-01'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'5'</span>, <span class="string">'1'</span>, <span class="string">'10'</span>, <span class="string">'1'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-02'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'6'</span>, <span class="string">'2'</span>, <span class="string">'11'</span>, <span class="string">'6'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-02'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'7'</span>, <span class="string">'3'</span>, <span class="string">'12'</span>, <span class="string">'6'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-02'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'8'</span>, <span class="string">'2'</span>, <span class="string">'12'</span>, <span class="string">'12'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-03'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'9'</span>, <span class="string">'3'</span>, <span class="string">'10'</span>, <span class="string">'12'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-03'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'10'</span>, <span class="string">'4'</span>, <span class="string">'13'</span>, <span class="string">'12'</span>, <span class="string">'cancelled_by_driver'</span>, <span class="string">'2013-10-03'</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> <span class="keyword">Users</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'No'</span>, <span class="string">'client'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Yes'</span>, <span class="string">'client'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'No'</span>, <span class="string">'client'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'No'</span>, <span class="string">'client'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'10'</span>, <span class="string">'No'</span>, <span class="string">'driver'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'11'</span>, <span class="string">'No'</span>, <span class="string">'driver'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'12'</span>, <span class="string">'No'</span>, <span class="string">'driver'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'13'</span>, <span class="string">'No'</span>, <span class="string">'driver'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-10"><a href="#SQL语句-10" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 方法1：</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">temp1.Request_at <span class="keyword">AS</span> <span class="keyword">DAY</span>,</span><br><span class="line"><span class="keyword">IF</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">cast</span>( ( temp2.cancelled_order / temp1.total_order ) <span class="keyword">AS</span> <span class="built_in">DECIMAL</span> ( <span class="number">3</span>, <span class="number">2</span> ) ) <span class="keyword">IS</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="number">0.00</span>,</span><br><span class="line"><span class="keyword">cast</span>( ( temp2.cancelled_order / temp1.total_order ) <span class="keyword">AS</span> <span class="built_in">DECIMAL</span> ( <span class="number">3</span>, <span class="number">2</span> ) ) </span><br><span class="line">) <span class="keyword">AS</span> <span class="string">'Cancellation Rate'</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">t1.Request_at,</span><br><span class="line"><span class="keyword">count</span>( * ) <span class="keyword">AS</span> total_order </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">( <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Trips <span class="keyword">WHERE</span> Request_at &gt;= <span class="string">'2013-10-01'</span> <span class="keyword">AND</span> Request_at &lt;= <span class="string">'2013-10-03'</span> ) t1</span><br><span class="line"><span class="keyword">JOIN</span> ( <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> <span class="keyword">Users</span> <span class="keyword">WHERE</span> Banned = <span class="string">'NO'</span> ) t2 <span class="keyword">ON</span> t1.Client_Id = t2.Users_Id </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">t1.Request_at </span><br><span class="line">) temp1</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> (</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">t1.Request_at,</span><br><span class="line"><span class="keyword">count</span>( * ) <span class="keyword">AS</span> cancelled_order </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">( <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Trips <span class="keyword">WHERE</span> Request_at &gt;= <span class="string">'2013-10-01'</span> <span class="keyword">AND</span> Request_at &lt;= <span class="string">'2013-10-03'</span> <span class="keyword">AND</span> ( <span class="keyword">STATUS</span> = <span class="string">'cancelled_by_driver'</span> <span class="keyword">OR</span> <span class="keyword">STATUS</span> = <span class="string">'cancelled_by_client'</span> ) ) t1</span><br><span class="line"><span class="keyword">JOIN</span> ( <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> <span class="keyword">Users</span> <span class="keyword">WHERE</span> Banned = <span class="string">'NO'</span> ) t2 <span class="keyword">ON</span> t1.Client_Id = t2.Users_Id </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">t1.Request_at </span><br><span class="line">) temp2 <span class="keyword">ON</span> temp1.Request_at = temp2.Request_at</span><br><span class="line"><span class="comment">-- ---------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 方法2：</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">temp.request_at <span class="keyword">DAY</span>,</span><br><span class="line"><span class="keyword">round</span>( <span class="keyword">sum</span>( <span class="keyword">CASE</span> temp.STATUS <span class="keyword">WHEN</span> <span class="string">'completed'</span> <span class="keyword">THEN</span> <span class="number">0</span> <span class="keyword">ELSE</span> <span class="number">1</span> <span class="keyword">END</span> ) / <span class="keyword">count</span>( temp.STATUS ), <span class="number">2</span> ) <span class="string">'Cancellation Rate'</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">( <span class="keyword">SELECT</span> <span class="keyword">STATUS</span>, request_at <span class="keyword">FROM</span> trips t <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> <span class="keyword">users</span> u <span class="keyword">ON</span> t.client_id = u.users_id <span class="keyword">WHERE</span> u.banned = <span class="string">'no'</span> ) temp </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">request_at <span class="keyword">BETWEEN</span> <span class="string">'2013-10-01'</span> </span><br><span class="line"><span class="keyword">AND</span> <span class="string">'2013-10-03'</span> </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">temp.request_at</span><br></pre></td></tr></table></figure><h2 id="511-游戏玩家分析I"><a href="#511-游戏玩家分析I" class="headerlink" title="511.游戏玩家分析I"></a>511.游戏玩家分析I</h2><h3 id="描述-10"><a href="#描述-10" class="headerlink" title="描述"></a>描述</h3><p>找出每个玩家第一次登录的日期。Activity表如下：</p><table><thead><tr><th align="center">player_id</th><th align="center">device_id</th><th align="center">event_date</th><th align="center">games_played</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">2</td><td align="center">2016<strong>-</strong>03<strong>-</strong>01</td><td align="center">5</td></tr><tr><td align="center">1</td><td align="center">2</td><td align="center">2016-03-02</td><td align="center">6</td></tr><tr><td align="center">2</td><td align="center">3</td><td align="center"><strong>2017</strong>-<strong>06</strong>-25</td><td align="center">1</td></tr><tr><td align="center">3</td><td align="center">1</td><td align="center">2016-03-02</td><td align="center">0</td></tr><tr><td align="center">3</td><td align="center">4</td><td align="center">2018<strong>-</strong>07<strong>-</strong>03</td><td align="center">5</td></tr></tbody></table><p>结果Result表如下：</p><table><thead><tr><th align="center">player_id</th><th align="center">first_login</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">2016-03-01</td></tr><tr><td align="center">2</td><td align="center">2017<strong>-</strong>06<strong>-</strong>25</td></tr><tr><td align="center">3</td><td align="center">2016-03-02</td></tr></tbody></table><h3 id="数据准备-11"><a href="#数据准备-11" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> activity(player_id <span class="built_in">int</span>,device_id <span class="built_in">int</span>,event_date <span class="built_in">date</span>,games_played <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> activity;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> activity <span class="keyword">values</span> (<span class="number">1</span>,<span class="number">2</span>,<span class="string">'2016-03-01'</span>,<span class="number">5</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> activity <span class="keyword">values</span> (<span class="number">1</span>,<span class="number">2</span>,<span class="string">'2016-03-02'</span>,<span class="number">6</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> activity <span class="keyword">values</span> (<span class="number">2</span>,<span class="number">3</span>,<span class="string">'2017-06-25'</span>,<span class="number">1</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> activity <span class="keyword">values</span> (<span class="number">3</span>,<span class="number">1</span>,<span class="string">'2016-03-02'</span>,<span class="number">0</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> activity <span class="keyword">values</span> (<span class="number">3</span>,<span class="number">4</span>,<span class="string">'2018-07-03'</span>,<span class="number">5</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-11"><a href="#SQL语句-11" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> player_id,<span class="keyword">min</span>(event_date) first_login <span class="keyword">from</span> activity <span class="keyword">group</span> <span class="keyword">by</span> player_id ;</span><br></pre></td></tr></table></figure><h2 id="512-游戏玩家分析II"><a href="#512-游戏玩家分析II" class="headerlink" title="512.  游戏玩家分析II"></a>512.  游戏玩家分析II</h2><h3 id="描述-11"><a href="#描述-11" class="headerlink" title="描述"></a>描述</h3><p>显示每个玩家首次登录的设备号(同时显示玩家ID)。</p><h3 id="数据准备-12"><a href="#数据准备-12" class="headerlink" title="数据准备"></a>数据准备</h3><p>见511题</p><h3 id="SQL语句-12"><a href="#SQL语句-12" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">player_id,</span><br><span class="line">device_id </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">activity </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">( player_id, event_date ) <span class="keyword">IN</span> ( <span class="keyword">SELECT</span> player_id, <span class="keyword">min</span>( event_date ) first_login <span class="keyword">FROM</span> activity <span class="keyword">GROUP</span> <span class="keyword">BY</span> player_id )</span><br></pre></td></tr></table></figure><h2 id="534-游戏玩家分析III"><a href="#534-游戏玩家分析III" class="headerlink" title="534 游戏玩家分析III"></a>534 游戏玩家分析III</h2><h3 id="描述-12"><a href="#描述-12" class="headerlink" title="描述"></a>描述</h3><p>编写一个 SQL 查询，同时显示每组玩家、日期以及玩家到目前为止玩了多少游戏。也就是说，在此日期之前玩家所玩的游戏总数。详细情况请查看示例。</p><p>结果为：</p><table><thead><tr><th align="center">player_id</th><th align="center">event_date</th><th align="center">games_played_so_far</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">2016-03-01</td><td align="center">5</td></tr><tr><td align="center">1</td><td align="center">2016-03-02</td><td align="center">11</td></tr><tr><td align="center">1</td><td align="center">2017-06-25</td><td align="center">1</td></tr><tr><td align="center">3</td><td align="center">2016-03-02</td><td align="center">0</td></tr><tr><td align="center">3</td><td align="center">2018-07-03</td><td align="center">5</td></tr></tbody></table><h3 id="数据准备-13"><a href="#数据准备-13" class="headerlink" title="数据准备"></a>数据准备</h3><p>见511题</p><h3 id="SQL语句-13"><a href="#SQL语句-13" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 方法一</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">B.player_id,</span><br><span class="line">B.event_date,</span><br><span class="line"><span class="keyword">SUM</span>( A.games_played ) <span class="keyword">AS</span> <span class="string">`games_played_so_far`</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Activity <span class="keyword">AS</span> A</span><br><span class="line"><span class="keyword">JOIN</span> Activity <span class="keyword">AS</span> B <span class="keyword">ON</span> ( A.player_id = B.player_id <span class="keyword">AND</span> A.event_date &lt;= B.event_date ) </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">B.player_id,</span><br><span class="line">B.event_date</span><br><span class="line"><span class="comment">-- 方法二</span></span><br><span class="line"><span class="keyword">SELECT</span> C.player_id,C.event_date,C.games_played_so_far</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">A.player_id,</span><br><span class="line">A.event_date,</span><br><span class="line">@sum_cnt:=</span><br><span class="line"><span class="keyword">if</span>(A.player_id = @pre_id <span class="keyword">AND</span> A.event_date != @pre_date,</span><br><span class="line">@sum_cnt + A.games_played,</span><br><span class="line">A.games_played </span><br><span class="line">)</span><br><span class="line"><span class="keyword">AS</span> <span class="string">`games_played_so_far`</span>,</span><br><span class="line">@pre_id:=A.player_id <span class="keyword">AS</span> <span class="string">`player_ids`</span>,</span><br><span class="line">@pre_date:=A.event_date <span class="keyword">AS</span> <span class="string">`event_dates`</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">activity <span class="keyword">AS</span> A,(<span class="keyword">SELECT</span> @pre_id:=<span class="literal">NULL</span>,@pre_date:=<span class="literal">NULL</span>,@sum_cnt:=<span class="number">0</span>) <span class="keyword">AS</span> B</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">BY</span> A.player_id,A.event_date</span><br><span class="line">) <span class="keyword">AS</span> C</span><br></pre></td></tr></table></figure><h2 id="550-游戏玩家分析IV"><a href="#550-游戏玩家分析IV" class="headerlink" title="550 游戏玩家分析IV"></a>550 游戏玩家分析IV</h2><h3 id="描述-13"><a href="#描述-13" class="headerlink" title="描述"></a>描述</h3><p>列出首次登录后，紧接着第二天又登录的人数占总人数的比例。比如511题中的数据，只有玩家1连续两天登录了，而总玩家有3个，所以连着两天登录的用户比例为：1/3 ~0.33</p><h3 id="数据准备-14"><a href="#数据准备-14" class="headerlink" title="数据准备"></a>数据准备</h3><p>见511题</p><h3 id="SQL语句-14"><a href="#SQL语句-14" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line"><span class="keyword">ROUND</span>(</span><br><span class="line">(</span><br><span class="line"><span class="comment">-- 求第二天连续登陆的用户数</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line"><span class="keyword">count</span>( <span class="keyword">DISTINCT</span> player_id ) <span class="keyword">AS</span> con_cnt </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">a.player_id,</span><br><span class="line"><span class="keyword">DATEDIFF</span>( b.event_date, a.event_date ) <span class="keyword">AS</span> diff </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">activity a</span><br><span class="line"><span class="keyword">JOIN</span> activity b <span class="keyword">ON</span> ( a.player_id = b.player_id <span class="keyword">AND</span> a.event_date &lt; b.event_date ) </span><br><span class="line">) t1 </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">diff = <span class="number">1</span> </span><br><span class="line">) / ( <span class="keyword">SELECT</span> <span class="keyword">count</span>( <span class="keyword">DISTINCT</span> player_id ) total_cnt <span class="keyword">FROM</span> activity ),<span class="comment">-- 总用户数</span></span><br><span class="line"><span class="number">2</span> </span><br><span class="line">) fraction</span><br></pre></td></tr></table></figure><h2 id="569-员工薪水中位数"><a href="#569-员工薪水中位数" class="headerlink" title="569 员工薪水中位数"></a>569 员工薪水中位数</h2><h3 id="描述-14"><a href="#描述-14" class="headerlink" title="描述"></a>描述</h3><p>有一张员工表Employees，字段为Id，Name，Salary，其中Id为员工Id，Name为公司名称，Salary为员工工资。如下面数据所示：</p><table><thead><tr><th align="center">Id</th><th align="center">Company</th><th align="center">Salary</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">A</td><td align="center">2341</td></tr><tr><td align="center">2</td><td align="center">A</td><td align="center">341</td></tr><tr><td align="center">3</td><td align="center">A</td><td align="center">15</td></tr><tr><td align="center">4</td><td align="center">A</td><td align="center">15314</td></tr><tr><td align="center">5</td><td align="center">A</td><td align="center">451</td></tr><tr><td align="center">6</td><td align="center">A</td><td align="center">513</td></tr><tr><td align="center">7</td><td align="center">B</td><td align="center">15</td></tr><tr><td align="center">8</td><td align="center">B</td><td align="center">13</td></tr><tr><td align="center">9</td><td align="center">B</td><td align="center">1154</td></tr><tr><td align="center">10</td><td align="center">B</td><td align="center">1345</td></tr><tr><td align="center">11</td><td align="center">B</td><td align="center">1221</td></tr><tr><td align="center">12</td><td align="center">B</td><td align="center">234</td></tr><tr><td align="center">13</td><td align="center">C</td><td align="center">2345</td></tr><tr><td align="center">14</td><td align="center">C</td><td align="center">2645</td></tr><tr><td align="center">15</td><td align="center">C</td><td align="center">2645</td></tr><tr><td align="center">16</td><td align="center">C</td><td align="center">2652</td></tr><tr><td align="center">17</td><td align="center">C</td><td align="center">65</td></tr></tbody></table><p>请编写SQL查询来查找每个公司的薪水中位数。结果如下：</p><table><thead><tr><th align="center">Id</th><th align="center">Company</th><th align="center">Salary</th></tr></thead><tbody><tr><td align="center">5</td><td align="center">A</td><td align="center">451</td></tr><tr><td align="center">6</td><td align="center">A</td><td align="center">513</td></tr><tr><td align="center">12</td><td align="center">B</td><td align="center">234</td></tr><tr><td align="center">9</td><td align="center">B</td><td align="center">1154</td></tr><tr><td align="center">14</td><td align="center">C</td><td align="center">2645</td></tr></tbody></table><h3 id="数据准备-15"><a href="#数据准备-15" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span>  <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> employees;</span><br><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> employees(<span class="keyword">Id</span> <span class="built_in">int</span>,Company <span class="built_in">varchar</span>(<span class="number">2</span>),salary <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'A'</span>,<span class="number">2341</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">'A'</span>,<span class="number">341</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">3</span>,<span class="string">'A'</span>,<span class="number">15</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">4</span>,<span class="string">'A'</span>,<span class="number">15314</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">5</span>,<span class="string">'A'</span>,<span class="number">451</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">6</span>,<span class="string">'A'</span>,<span class="number">513</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">7</span>,<span class="string">'B'</span>,<span class="number">15</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">8</span>,<span class="string">'B'</span>,<span class="number">13</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">9</span>,<span class="string">'B'</span>,<span class="number">1154</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">10</span>,<span class="string">'B'</span>,<span class="number">1345</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">11</span>,<span class="string">'B'</span>,<span class="number">1221</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">12</span>,<span class="string">'B'</span>,<span class="number">234</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">13</span>,<span class="string">'C'</span>,<span class="number">2345</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">14</span>,<span class="string">'C'</span>,<span class="number">2645</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">15</span>,<span class="string">'C'</span>,<span class="number">2645</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">16</span>,<span class="string">'C'</span>,<span class="number">2652</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">17</span>,<span class="string">'C'</span>,<span class="number">65</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-15"><a href="#SQL语句-15" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">     t1.id,</span><br><span class="line">     t1.company,</span><br><span class="line">     t1.salary</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line"></span><br><span class="line">(</span><br><span class="line"><span class="comment">-- 查询每个公司员工薪水排名</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">     <span class="keyword">id</span>,</span><br><span class="line">     company,</span><br><span class="line">     salary,</span><br><span class="line">     @<span class="keyword">num</span> := <span class="keyword">if</span>( @company =company  ,@<span class="keyword">num</span> + <span class="number">1</span>,<span class="number">1</span>) <span class="keyword">as</span> <span class="keyword">rank</span>,</span><br><span class="line">     @company := company</span><br><span class="line"><span class="keyword">from</span> employees a ,(<span class="keyword">select</span> @<span class="keyword">num</span> := <span class="number">0</span>,@company:=<span class="string">""</span>) b</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> company,salary) t1 </span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">(</span><br><span class="line"><span class="comment">-- 查询每个公司有多少个员工</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">     company,</span><br><span class="line">     <span class="keyword">count</span>(*) <span class="keyword">as</span> cnt</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    employees</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> company</span><br><span class="line"></span><br><span class="line">) t2 <span class="keyword">on</span> t1.company= t2.company <span class="keyword">and</span> t1.rank = (t2.cnt + <span class="number">1</span>) <span class="keyword">div</span> <span class="number">2</span> <span class="comment">-- （员工总数+1）/2 为中位数</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> LeeCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeeCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>电商业务常用指标分析之SQL实现</title>
      <link href="/2019/12/05/%E7%94%B5%E5%95%86%E4%B8%9A%E5%8A%A1%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E5%88%86%E6%9E%90%E4%B9%8BSQL%E5%AE%9E%E7%8E%B0/"/>
      <url>/2019/12/05/%E7%94%B5%E5%95%86%E4%B8%9A%E5%8A%A1%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E5%88%86%E6%9E%90%E4%B9%8BSQL%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<p>当构建好电商业务数仓之后，需要对业务需要的指标进行计算，从而进一步进行报表的展示，那么，电商的业务知识大概涉及哪些？关于电商业务的常用指标计算都有哪些？这些常用的指标该如何通过Hive数仓进行分析？本文将进行一一梳理.</p><a id="more"></a><h2 id="电商业务领域知识梳理"><a href="#电商业务领域知识梳理" class="headerlink" title="电商业务领域知识梳理"></a>电商业务领域知识梳理</h2><ul><li><strong>用户</strong></li></ul><p>用户以设备为判断标准，在移动统计中，每个独立设备认为是一个独立用户。Android系统根据IMEI号，IOS系统根据OpenUDID来标识一个独立用户，每部手机一个用户。</p><ul><li><strong>新增用户</strong></li></ul><p>首次联网使用应用的用户。如果一个用户首次打开某APP，那这个用户定义为新增用户；卸载再安装的设备，不会被算作一次新增。新增用户包括日新增用户、周新增用户、月新增用户。</p><ul><li><strong>活跃用户</strong></li></ul><p>打开应用的用户即为活跃用户，不考虑用户的使用情况。每天一台设备打开多次会被计为一个活跃用户。</p><ul><li><strong>周（月）活跃用户</strong></li></ul><p>某个自然周（月）内启动过应用的用户，该周（月）内的多次启动只记一个活跃用户。</p><ul><li><strong>月活跃率</strong></li></ul><p>月活跃用户与截止到该月累计的用户总和之间的比例。</p><ul><li><strong>沉默用户</strong></li></ul><p>用户仅在安装当天（次日）启动一次，后续时间无再启动行为。该指标可以反映新增用户质量和用户与APP的匹配程度。</p><ul><li><strong>版本分布</strong></li></ul><p>不同版本的周内各天新增用户数，活跃用户数和启动次数。利于判断APP各个版本之间的优劣和用户行为习惯。</p><ul><li><strong>本周回流用户</strong></li></ul><p>上周未启动过应用，本周启动了应用的用户。</p><ul><li><strong>连续N周活跃用户</strong></li></ul><p>连续n周，每周至少启动一次。</p><ul><li><strong>忠诚用户</strong></li></ul><p>连续活跃5周以上的用户</p><ul><li><strong>连续活跃用户</strong></li></ul><p>连续2周及以上活跃的用户</p><ul><li><strong>近期流失用户</strong></li></ul><p>连续n(2&lt;= n &lt;= 4)周没有启动应用的用户。（第n+1周没有启动过）</p><ul><li><strong>留存用户</strong></li></ul><p>某段时间内的新增用户，经过一段时间后，仍然使用应用的被认作是留存用户；这部分用户占当时新增用户的比例即是留存率。<br>例如，5月份新增用户200，这200人在6月份启动过应用的有100人，7月份启动过应用的有80人，8月份启动过应用的有50人；则5月份新增用户一个月后的留存率是50%，二个月后的留存率是40%，三个月后的留存率是25%。</p><ul><li><strong>用户新鲜度</strong></li></ul><p>每天启动应用的新老用户比例，即新增用户数占活跃用户数的比例。</p><ul><li><strong>单次使用时长</strong></li></ul><p>每次启动使用的时间长度。</p><ul><li><strong>日使用时长</strong></li></ul><p>累计一天内的使用时间长度。</p><ul><li><strong>启动次数计算标准</strong></li></ul><p>IOS平台应用退到后台就算一次独立的启动；Android平台我们规定，两次启动之间的间隔小于30秒，被计算一次启动。用户在使用过程中，若因收发短信或接电话等退出应用30秒又再次返回应用中，那这两次行为应该是延续而非独立的，所以可以被算作一次使用行为，即一次启动。业内大多使用30秒这个标准，但用户还是可以自定义此时间间隔。</p><h2 id="常用的日期函数处理"><a href="#常用的日期函数处理" class="headerlink" title="常用的日期函数处理"></a>常用的日期函数处理</h2><ul><li><strong>date_format函数（根据格式整理日期）</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">select date_format(<span class="string">'2019-12-05'</span>,<span class="string">'yyyy-MM'</span>);</span><br></pre></td></tr></table></figure><p>输出：2019-12</p><ul><li><strong>date_add函数（加减日期）</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">select date_add(<span class="string">'2019-12-05'</span>,-1);</span><br></pre></td></tr></table></figure><p>输出：2019-12-04</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">select date_add(<span class="string">'2019-12-05'</span>,1);</span><br></pre></td></tr></table></figure><p>输出：2019-12-06</p><ul><li><strong>next_day函数(返回当前时间的下一个星期X所对应的日期)</strong></li></ul><p>1）取当前天的下一个周一</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">select next_day(<span class="string">'2019-12-05'</span>,<span class="string">'MO'</span>)</span><br></pre></td></tr></table></figure><p>输出：2019-12-09</p><p>说明：星期一到星期日的英文（Monday，Tuesday、Wednesday、Thursday、Friday、Saturday、Sunday）</p><p>2）取当前周的周一</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">select date_add(next_day(<span class="string">'2019-12-05'</span>,<span class="string">'MO'</span>),-7);</span><br></pre></td></tr></table></figure><p>输出：2019-12-02</p><ul><li><strong>last_day函数（返回这个月的最后一天的日期）</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">select last_day(<span class="string">'2019-12-05'</span>);</span><br></pre></td></tr></table></figure><p>输出：2019-12-31</p><h2 id="业务指标分析"><a href="#业务指标分析" class="headerlink" title="业务指标分析"></a>业务指标分析</h2><h3 id="用户活跃相关指标分析"><a href="#用户活跃相关指标分析" class="headerlink" title="用户活跃相关指标分析"></a>用户活跃相关指标分析</h3><p>数仓的DWS层会建立好每日的活跃用户表明细、每周的活跃用户表明细以及每月的活跃用户明细表。</p><ul><li><strong>每日活跃用户明细表结构</strong></li></ul><p>每天一个分区，存储当天的日活明细，该表根据mid_id进行去重。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table dws_uv_detail_day</span><br><span class="line">(</span><br><span class="line">    `mid_id` string COMMENT <span class="string">'设备唯一标识'</span>,</span><br><span class="line">    `user_id` string COMMENT <span class="string">'用户标识'</span>, </span><br><span class="line">    `version_code` string COMMENT <span class="string">'程序版本号'</span>, </span><br><span class="line">    `version_name` string COMMENT <span class="string">'程序版本名'</span>, </span><br><span class="line">    `lang` string COMMENT <span class="string">'系统语言'</span>, </span><br><span class="line">    `<span class="built_in">source</span>` string COMMENT <span class="string">'渠道号'</span>, </span><br><span class="line">    `os` string COMMENT <span class="string">'安卓系统版本'</span>, </span><br><span class="line">    `area` string COMMENT <span class="string">'区域'</span>, </span><br><span class="line">    `model` string COMMENT <span class="string">'手机型号'</span>, </span><br><span class="line">    `brand` string COMMENT <span class="string">'手机品牌'</span>, </span><br><span class="line">    `sdk_version` string COMMENT <span class="string">'sdkVersion'</span>, </span><br><span class="line">    `gmail` string COMMENT <span class="string">'gmail'</span>, </span><br><span class="line">    `height_width` string COMMENT <span class="string">'屏幕宽高'</span>,</span><br><span class="line">    `app_time` string COMMENT <span class="string">'客户端日志产生时的时间'</span>,</span><br><span class="line">    `network` string COMMENT <span class="string">'网络模式'</span>,</span><br><span class="line">    `lng` string COMMENT <span class="string">'经度'</span>,</span><br><span class="line">    `lat` string COMMENT <span class="string">'纬度'</span></span><br><span class="line">)</span><br><span class="line">partitioned by(dt string)</span><br><span class="line">stored as parquet</span><br><span class="line">location <span class="string">'/warehouse/gmall/dws/dws_uv_detail_day'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><ul><li><strong>每周活跃用户明细表</strong></li></ul><p>根据日用户访问明细，获得周用户访问明细,周明细表按周一日期和周末日期拼接字段进行分区。即每个分区存储的是本周内的活跃用户明细，该表按mid_id进行去重，即一周内获取多次，只记录一条记录。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table dws_uv_detail_wk( </span><br><span class="line">    `mid_id` string COMMENT <span class="string">'设备唯一标识'</span>,</span><br><span class="line">    `user_id` string COMMENT <span class="string">'用户标识'</span>, </span><br><span class="line">    `version_code` string COMMENT <span class="string">'程序版本号'</span>, </span><br><span class="line">    `version_name` string COMMENT <span class="string">'程序版本名'</span>, </span><br><span class="line">    `lang` string COMMENT <span class="string">'系统语言'</span>, </span><br><span class="line">    `<span class="built_in">source</span>` string COMMENT <span class="string">'渠道号'</span>, </span><br><span class="line">    `os` string COMMENT <span class="string">'安卓系统版本'</span>, </span><br><span class="line">    `area` string COMMENT <span class="string">'区域'</span>, </span><br><span class="line">    `model` string COMMENT <span class="string">'手机型号'</span>, </span><br><span class="line">    `brand` string COMMENT <span class="string">'手机品牌'</span>, </span><br><span class="line">    `sdk_version` string COMMENT <span class="string">'sdkVersion'</span>, </span><br><span class="line">    `gmail` string COMMENT <span class="string">'gmail'</span>, </span><br><span class="line">    `height_width` string COMMENT <span class="string">'屏幕宽高'</span>,</span><br><span class="line">    `app_time` string COMMENT <span class="string">'客户端日志产生时的时间'</span>,</span><br><span class="line">    `network` string COMMENT <span class="string">'网络模式'</span>,</span><br><span class="line">    `lng` string COMMENT <span class="string">'经度'</span>,</span><br><span class="line">    `lat` string COMMENT <span class="string">'纬度'</span>,</span><br><span class="line">    `monday_date` string COMMENT <span class="string">'周一日期'</span>,</span><br><span class="line">    `sunday_date` string COMMENT  <span class="string">'周日日期'</span> </span><br><span class="line">) COMMENT <span class="string">'活跃用户按周明细'</span></span><br><span class="line">PARTITIONED BY (`wk_dt` string)</span><br><span class="line">stored as parquet</span><br><span class="line">location <span class="string">'/warehouse/gmall/dws/dws_uv_detail_wk/'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><ul><li><strong>每月活跃用户明细</strong></li></ul><p>该表按月进行分区，并按mid_id去重，数据来源与日活明细表</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table dws_uv_detail_mn( </span><br><span class="line">    `mid_id` string COMMENT <span class="string">'设备唯一标识'</span>,</span><br><span class="line">    `user_id` string COMMENT <span class="string">'用户标识'</span>, </span><br><span class="line">    `version_code` string COMMENT <span class="string">'程序版本号'</span>, </span><br><span class="line">    `version_name` string COMMENT <span class="string">'程序版本名'</span>, </span><br><span class="line">    `lang` string COMMENT <span class="string">'系统语言'</span>, </span><br><span class="line">    `<span class="built_in">source</span>` string COMMENT <span class="string">'渠道号'</span>, </span><br><span class="line">    `os` string COMMENT <span class="string">'安卓系统版本'</span>, </span><br><span class="line">    `area` string COMMENT <span class="string">'区域'</span>, </span><br><span class="line">    `model` string COMMENT <span class="string">'手机型号'</span>, </span><br><span class="line">    `brand` string COMMENT <span class="string">'手机品牌'</span>, </span><br><span class="line">    `sdk_version` string COMMENT <span class="string">'sdkVersion'</span>, </span><br><span class="line">    `gmail` string COMMENT <span class="string">'gmail'</span>, </span><br><span class="line">    `height_width` string COMMENT <span class="string">'屏幕宽高'</span>,</span><br><span class="line">    `app_time` string COMMENT <span class="string">'客户端日志产生时的时间'</span>,</span><br><span class="line">    `network` string COMMENT <span class="string">'网络模式'</span>,</span><br><span class="line">    `lng` string COMMENT <span class="string">'经度'</span>,</span><br><span class="line">    `lat` string COMMENT <span class="string">'纬度'</span></span><br><span class="line">) COMMENT <span class="string">'活跃用户按月明细'</span></span><br><span class="line">PARTITIONED BY (`mn` string)</span><br><span class="line">stored as parquet</span><br><span class="line">location <span class="string">'/warehouse/gmall/dws/dws_uv_detail_mn/'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><ul><li><strong>建立ADS层的活跃用户指标表</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table ads_uv_count( </span><br><span class="line">    `dt` string COMMENT <span class="string">'统计日期'</span>,</span><br><span class="line">    `day_count` bigint COMMENT <span class="string">'当日用户数量'</span>,</span><br><span class="line">    `wk_count`  bigint COMMENT <span class="string">'当周用户数量'</span>,</span><br><span class="line">    `mn_count`  bigint COMMENT <span class="string">'当月用户数量'</span>,</span><br><span class="line">    `is_weekend` string COMMENT <span class="string">'Y,N是否是周末,用于得到本周最终结果'</span>,</span><br><span class="line">    `is_monthend` string COMMENT <span class="string">'Y,N是否是月末,用于得到本月最终结果'</span> </span><br><span class="line">) COMMENT <span class="string">'活跃设备数'</span></span><br><span class="line">row format delimited fields terminated by <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/warehouse/gmall/ads/ads_uv_count/'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><p><strong>SQL具体实现：</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">insert into table ads_uv_count </span><br><span class="line">select  </span><br><span class="line">  <span class="string">'2019-02-10'</span> dt,</span><br><span class="line">   daycount.ct,</span><br><span class="line">   wkcount.ct,</span><br><span class="line">   mncount.ct,</span><br><span class="line">   <span class="keyword">if</span>(date_add(next_day(<span class="string">'2019-02-10'</span>,<span class="string">'MO'</span>),-1)=<span class="string">'2019-02-10'</span>,<span class="string">'Y'</span>,<span class="string">'N'</span>) , -- 判断跑任务的当天是否是周末</span><br><span class="line">   <span class="keyword">if</span>(last_day(<span class="string">'2019-02-10'</span>)=<span class="string">'2019-02-10'</span>,<span class="string">'Y'</span>,<span class="string">'N'</span>)  -- 判断跑任务的当天是否是月末</span><br><span class="line">from </span><br><span class="line">(</span><br><span class="line">-- 计算当天的日活</span><br><span class="line">   select  </span><br><span class="line">      <span class="string">'2019-02-10'</span> dt,</span><br><span class="line">       count(*) ct</span><br><span class="line">   from dws_uv_detail_day</span><br><span class="line">   <span class="built_in">where</span> dt=<span class="string">'2019-02-10'</span>  </span><br><span class="line">)daycount join </span><br><span class="line">( </span><br><span class="line">-- 计算当天所属周的周活</span><br><span class="line">   select  </span><br><span class="line">     <span class="string">'2019-02-10'</span> dt,</span><br><span class="line">     count (*) ct</span><br><span class="line">   from dws_uv_detail_wk</span><br><span class="line">   <span class="built_in">where</span> wk_dt=concat(date_add(next_day(<span class="string">'2019-02-10'</span>,<span class="string">'MO'</span>),-7),<span class="string">'_'</span> ,date_add(next_day(<span class="string">'2019-02-10'</span>,<span class="string">'MO'</span>),-1) )</span><br><span class="line">) wkcount on daycount.dt=wkcount.dt</span><br><span class="line">join </span><br><span class="line">( </span><br><span class="line">-- 计算当天所属月的月活</span><br><span class="line">   select  </span><br><span class="line">     <span class="string">'2019-02-10'</span> dt,</span><br><span class="line">     count (*) ct</span><br><span class="line">   from dws_uv_detail_mn</span><br><span class="line">   <span class="built_in">where</span> mn=date_format(<span class="string">'2019-02-10'</span>,<span class="string">'yyyy-MM'</span>)  </span><br><span class="line">)mncount on daycount.dt=mncount.dt</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h3 id="新增用户指标分析"><a href="#新增用户指标分析" class="headerlink" title="新增用户指标分析"></a>新增用户指标分析</h3><p>首次联网使用应用的用户。如果一个用户首次打开某APP，那这个用户定义为新增用户；卸载再安装的设备，不会被算作一次新增。新增用户包括日新增用户、周新增用户、月新增用户。</p><ul><li><strong>每日新增用户明细表</strong></li></ul><p>每日新增用户明细表来源于每天的日活表，使用每天的日活表去LEFT JOIN 每天新增用户明细表，关联的条件是mid_id,筛选条件为，每日新增设备表中为空</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table dws_new_mid_day</span><br><span class="line">(</span><br><span class="line">    `mid_id` string COMMENT <span class="string">'设备唯一标识'</span>,</span><br><span class="line">    `user_id` string COMMENT <span class="string">'用户标识'</span>, </span><br><span class="line">    `version_code` string COMMENT <span class="string">'程序版本号'</span>, </span><br><span class="line">    `version_name` string COMMENT <span class="string">'程序版本名'</span>, </span><br><span class="line">    `lang` string COMMENT <span class="string">'系统语言'</span>, </span><br><span class="line">    `<span class="built_in">source</span>` string COMMENT <span class="string">'渠道号'</span>, </span><br><span class="line">    `os` string COMMENT <span class="string">'安卓系统版本'</span>, </span><br><span class="line">    `area` string COMMENT <span class="string">'区域'</span>, </span><br><span class="line">    `model` string COMMENT <span class="string">'手机型号'</span>, </span><br><span class="line">    `brand` string COMMENT <span class="string">'手机品牌'</span>, </span><br><span class="line">    `sdk_version` string COMMENT <span class="string">'sdkVersion'</span>, </span><br><span class="line">    `gmail` string COMMENT <span class="string">'gmail'</span>, </span><br><span class="line">    `height_width` string COMMENT <span class="string">'屏幕宽高'</span>,</span><br><span class="line">    `app_time` string COMMENT <span class="string">'客户端日志产生时的时间'</span>,</span><br><span class="line">    `network` string COMMENT <span class="string">'网络模式'</span>,</span><br><span class="line">    `lng` string COMMENT <span class="string">'经度'</span>,</span><br><span class="line">    `lat` string COMMENT <span class="string">'纬度'</span>,</span><br><span class="line">    `create_date`  string  comment <span class="string">'创建时间'</span> </span><br><span class="line">)  COMMENT <span class="string">'每日新增设备信息'</span></span><br><span class="line">stored as parquet</span><br><span class="line">location <span class="string">'/warehouse/gmall/dws/dws_new_mid_day/'</span>;</span><br></pre></td></tr></table></figure><ul><li><strong>每日新增用户表</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table ads_new_mid_count</span><br><span class="line">(</span><br><span class="line">    `create_date`     string comment <span class="string">'创建时间'</span> ,</span><br><span class="line">    `new_mid_count`   BIGINT comment <span class="string">'新增设备数量'</span> </span><br><span class="line">)  COMMENT <span class="string">'每日新增设备信息数量'</span></span><br><span class="line">row format delimited fields terminated by <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/warehouse/gmall/ads/ads_new_mid_count/'</span>;</span><br></pre></td></tr></table></figure><p><strong>每日新增用户表装载SQL实现</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">insert into table ads_new_mid_count </span><br><span class="line">select</span><br><span class="line">create_date,</span><br><span class="line">count(*)</span><br><span class="line">from dws_new_mid_day</span><br><span class="line"><span class="built_in">where</span> create_date=<span class="string">'2019-02-10'</span></span><br><span class="line">group by create_date;</span><br></pre></td></tr></table></figure><h3 id="用户留存指标分析"><a href="#用户留存指标分析" class="headerlink" title="用户留存指标分析"></a>用户留存指标分析</h3><p><img src="//jiamaoxiang.top/2019/12/05/电商业务常用指标分析之SQL实现/%E7%94%A8%E6%88%B7%E7%95%99%E5%AD%98.png" alt></p><ul><li><strong>每日用户留存明细</strong></li></ul><p>该表以天作为分区，每天计算前1天的新用户访问留存明细，</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table dws_user_retention_day </span><br><span class="line">(</span><br><span class="line">    `mid_id` string COMMENT <span class="string">'设备唯一标识'</span>,</span><br><span class="line">    `user_id` string COMMENT <span class="string">'用户标识'</span>, </span><br><span class="line">    `version_code` string COMMENT <span class="string">'程序版本号'</span>, </span><br><span class="line">    `version_name` string COMMENT <span class="string">'程序版本名'</span>, </span><br><span class="line">`lang` string COMMENT <span class="string">'系统语言'</span>, </span><br><span class="line">`<span class="built_in">source</span>` string COMMENT <span class="string">'渠道号'</span>, </span><br><span class="line">`os` string COMMENT <span class="string">'安卓系统版本'</span>, </span><br><span class="line">`area` string COMMENT <span class="string">'区域'</span>, </span><br><span class="line">`model` string COMMENT <span class="string">'手机型号'</span>, </span><br><span class="line">`brand` string COMMENT <span class="string">'手机品牌'</span>, </span><br><span class="line">`sdk_version` string COMMENT <span class="string">'sdkVersion'</span>, </span><br><span class="line">`gmail` string COMMENT <span class="string">'gmail'</span>, </span><br><span class="line">`height_width` string COMMENT <span class="string">'屏幕宽高'</span>,</span><br><span class="line">`app_time` string COMMENT <span class="string">'客户端日志产生时的时间'</span>,</span><br><span class="line">`network` string COMMENT <span class="string">'网络模式'</span>,</span><br><span class="line">`lng` string COMMENT <span class="string">'经度'</span>,</span><br><span class="line">`lat` string COMMENT <span class="string">'纬度'</span>,</span><br><span class="line">   `create_date`    string  comment <span class="string">'设备新增时间'</span>,</span><br><span class="line">   `retention_day`  int comment <span class="string">'截止当前日期留存天数'</span></span><br><span class="line">)  COMMENT <span class="string">'每日用户留存情况'</span></span><br><span class="line">PARTITIONED BY (`dt` string)</span><br><span class="line">stored as parquet</span><br><span class="line">location <span class="string">'/warehouse/gmall/dws/dws_user_retention_day/'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>每日用户留存明细装载语句</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">insert overwrite table dws_user_retention_day</span><br><span class="line">partition(dt=<span class="string">"2019-02-11"</span>)</span><br><span class="line">select  </span><br><span class="line">    nm.mid_id,</span><br><span class="line">    nm.user_id , </span><br><span class="line">    nm.version_code , </span><br><span class="line">    nm.version_name , </span><br><span class="line">    nm.lang , </span><br><span class="line">    nm.source, </span><br><span class="line">    nm.os, </span><br><span class="line">    nm.area, </span><br><span class="line">    nm.model, </span><br><span class="line">    nm.brand, </span><br><span class="line">    nm.sdk_version, </span><br><span class="line">    nm.gmail, </span><br><span class="line">    nm.height_width,</span><br><span class="line">    nm.app_time,</span><br><span class="line">    nm.network,</span><br><span class="line">    nm.lng,</span><br><span class="line">    nm.lat,</span><br><span class="line">nm.create_date,</span><br><span class="line">1 retention_day </span><br><span class="line">from dws_uv_detail_day ud join dws_new_mid_day nm   on ud.mid_id =nm.mid_id </span><br><span class="line"><span class="built_in">where</span> ud.dt=<span class="string">'2019-02-11'</span> and nm.create_date=date_add(<span class="string">'2019-02-11'</span>,-1);</span><br></pre></td></tr></table></figure><ul><li><strong>留存用户数</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table ads_user_retention_day_count </span><br><span class="line">(</span><br><span class="line">   `create_date`       string  comment <span class="string">'设备新增日期'</span>,</span><br><span class="line">   `retention_day`     int comment <span class="string">'截止当前日期留存天数'</span>,</span><br><span class="line">   `retention_count`    bigint comment  <span class="string">'留存数量'</span></span><br><span class="line">)  COMMENT <span class="string">'每日用户留存情况'</span></span><br><span class="line">row format delimited fields terminated by <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/warehouse/gmall/ads/ads_user_retention_day_count/'</span>;</span><br></pre></td></tr></table></figure><p>留存用户数装载SQL</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">insert into table ads_user_retention_day_count </span><br><span class="line">select</span><br><span class="line">    create_date,</span><br><span class="line">    retention_day,</span><br><span class="line">    count(*) retention_count</span><br><span class="line">from dws_user_retention_day</span><br><span class="line"><span class="built_in">where</span> dt=<span class="string">'2019-02-11'</span> </span><br><span class="line">group by create_date,retention_day;</span><br></pre></td></tr></table></figure><h3 id="流失用户数分析"><a href="#流失用户数分析" class="headerlink" title="流失用户数分析"></a>流失用户数分析</h3><p>流失用户：最近7天未登录我们称之为流失用户</p><ul><li><strong>流失用户数表</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table ads_wastage_count( </span><br><span class="line">    `dt` string COMMENT <span class="string">'统计日期'</span>,</span><br><span class="line">    `wastage_count` bigint COMMENT <span class="string">'流失设备数'</span></span><br><span class="line">) </span><br><span class="line">row format delimited fields terminated by <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/warehouse/gmall/ads/ads_wastage_count'</span>;</span><br></pre></td></tr></table></figure><p>装载SQL,如果统计日期为2019-02-20，则7天未登陆的用户数的计算逻辑为：</p><p>查询日活表，并按mid_id进行分组，并且设备的最近访问时间小于等于当前时间的一周前，即活跃的最大日期(最近一次访问日期)小于等于2019-02-20</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">insert into table ads_wastage_count</span><br><span class="line">select</span><br><span class="line">     <span class="string">'2019-02-20'</span>,</span><br><span class="line">     count(*)</span><br><span class="line">from </span><br><span class="line">(</span><br><span class="line">    select mid_id</span><br><span class="line">from dws_uv_detail_day</span><br><span class="line">    group by mid_id</span><br><span class="line">    having max(dt)&lt;=date_add(<span class="string">'2019-02-20'</span>,-7)</span><br><span class="line">)t1;</span><br></pre></td></tr></table></figure><h3 id="最近七天内连续三天活跃用户数指标分析"><a href="#最近七天内连续三天活跃用户数指标分析" class="headerlink" title="最近七天内连续三天活跃用户数指标分析"></a>最近七天内连续三天活跃用户数指标分析</h3><ul><li><strong>最近七天内连续三天活跃用户数表</strong></li></ul><p>需要使用日活表，来获取最近7天内连续3天活跃用户数</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table ads_continuity_uv_count( </span><br><span class="line">    `dt` string COMMENT <span class="string">'统计日期'</span>,</span><br><span class="line">    `wk_dt` string COMMENT <span class="string">'最近7天日期'</span>,</span><br><span class="line">    `continuity_count` bigint</span><br><span class="line">) COMMENT <span class="string">'连续活跃设备数'</span></span><br><span class="line">row format delimited fields terminated by <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/warehouse/gmall/ads/ads_continuity_uv_count'</span>;</span><br></pre></td></tr></table></figure><p>装载语句为：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert into table ads_continuity_uv_count</span><br><span class="line">select</span><br><span class="line">    <span class="string">'2019-02-12'</span>,</span><br><span class="line">    concat(date_add(<span class="string">'2019-02-12'</span>,-6),<span class="string">'_'</span>,<span class="string">'2019-02-12'</span>),</span><br><span class="line">    count(*)</span><br><span class="line">from</span><br><span class="line"></span><br><span class="line">(</span><br><span class="line">select</span><br><span class="line">mid_id</span><br><span class="line">from </span><br><span class="line">( -- 筛选出连续3的活跃用户，可能存在重复</span><br><span class="line">select</span><br><span class="line">mid_id</span><br><span class="line"></span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">-- 计算活跃用户的活跃日期与其排名的差值</span><br><span class="line">select </span><br><span class="line"></span><br><span class="line">mid_id,</span><br><span class="line">date_sub(dt,rank) date_dif</span><br><span class="line"></span><br><span class="line">from </span><br><span class="line">(-- 查询出最近7天的活跃用户，并对活跃日期进行排名</span><br><span class="line">select</span><br><span class="line">mid_id,</span><br><span class="line">dt,</span><br><span class="line">rank() over (partition by mid_id order by dt) rank</span><br><span class="line">from dws_uv_detail_day</span><br><span class="line"><span class="built_in">where</span> dt &gt;= date_add(<span class="string">'2019-02-12'</span>,-6) and dt &lt;= <span class="string">'2109-02-12'</span></span><br><span class="line">) t1</span><br><span class="line">) t2</span><br><span class="line">group by mid_id,date_dif -- 对用户设备id和差值进行分组</span><br><span class="line">having count(*) &gt;=3   -- 统计大于等于3的差值数据筛选出来</span><br><span class="line"></span><br><span class="line">) t3</span><br><span class="line">group by mid_id -- 对mid_id进行去重</span><br><span class="line">) t4</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CDH集群之YARN性能调优</title>
      <link href="/2019/12/03/CDH%E9%9B%86%E7%BE%A4%E4%B9%8BYARN%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
      <url>/2019/12/03/CDH%E9%9B%86%E7%BE%A4%E4%B9%8BYARN%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<p>本文主要讨论CDH集群的YARN调优配置，关于YARN的调优配置，主要关注CPU和内存的调优，其中CPU是指物理CPU个数乘以CPU核数，即Vcores = CPU数量*CPU核数。YARN是以container容器的形式封装资源的，task在container内部执行。</p><a id="more"></a><h2 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h2><p>集群的配置主要包括三步，第一是先规划集群的工作主机以及每台主机的配置，第二是规划每台主机的安装的组件及其资源分配，第三是规划集群的规模大小。</p><h3 id="工作主机的配置"><a href="#工作主机的配置" class="headerlink" title="工作主机的配置"></a>工作主机的配置</h3><p>如下表所示：主机的内存为256G，4个6核CPU，CPU支持超线程，网络带宽为2G</p><table>    <tr>        <td bgcolor="#6495ED">主机组件</td>        <td bgcolor="#6495ED">数量</td>         <td bgcolor="#6495ED">大小</td>        <td bgcolor="#6495ED">总计</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>RAM</td>        <td>256G</td>         <td></td>        <td>256G</td>          <td>内存大小</td>     </tr>    <tr>        <td>CPU</td>        <td>4</td>         <td>6</td>        <td>48</td>         <td>总CPU核数</td>      </tr>    <tr>        <td>HyperThreading CPU</td>        <td>YES</td>         <td></td>        <td></td>         <td>超线程CPU，使操作系统认为处理器的核心数是实际核心数的2倍，因此如果有24个核心的处理器，操作系统会认为处理器有48个核心</td>      </tr>    <tr>        <td>网络</td>        <td>2</td>         <td>1G</td>        <td>2G</td>         <td>网络带宽</td>      </tr>  </table><h3 id="工作主机安装组件配置"><a href="#工作主机安装组件配置" class="headerlink" title="工作主机安装组件配置"></a>工作主机安装组件配置</h3><p>第一步已经明确每台主机的内存和CPU配置，下面为每台节点的服务分配资源，主要分配CPU和内存</p><table>    <tr>        <td bgcolor="#6495ED">服务</td>        <td bgcolor="#6495ED">类别</td>         <td bgcolor="#6495ED">CPU核数</td>        <td bgcolor="#6495ED">内存(MB)</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>操作系统</td>        <td>Overhead</td>         <td>1</td>        <td>8192</td>          <td>为操作系统分配1核8G内存，一般4~8G</td>     </tr>    <tr>        <td>其它服务</td>        <td>Overhead</td>         <td>0</td>        <td>0</td>         <td>非CDH集群、非操作系统占用的资源</td>      </tr>    <tr>        <td>Cloudera Manager agent</td>        <td>Overhead</td>         <td>1</td>        <td>1024</td>         <td>分配1核1G</td>      </tr>    <tr>        <td>HDFS DataNode</td>        <td>CDH</td>         <td>1</td>        <td>1024</td>         <td>默认1核1G</td>      </tr>       <tr>        <td>YARN NodeManager</td>        <td>CDH</td>         <td>1</td>        <td>1024</td>         <td>默认1核1G</td>      </tr>           <tr>        <td>Impala daemon</td>        <td>CDH</td>         <td>0</td>        <td>0</td>         <td>可选的服务，建议至少为impala demon分配16G内存</td>      </tr>           <tr>        <td>Hbase RegionServer</td>        <td>CDH</td>         <td>0</td>        <td>0</td>         <td>可选的服务，建议12~16G内存</td>      </tr>           <tr>        <td>Solr Server</td>        <td>CDH</td>         <td>0</td>        <td>0</td>         <td>可选的服务，最低1G内存</td>      </tr>           <tr>        <td>Kudu Server</td>        <td>CDH</td>         <td>0</td>        <td>0</td>         <td>可选的服务，kudu Tablet server最低1G内存</td>      </tr>           <tr>        <td>Available Container  Resources</td>        <td></td>         <td>44</td>        <td>250880</td>         <td>剩余分配给yarn的container</td>      </tr></table><p>container资源分配<br>Physical Cores to Vcores Multiplier：每个container的cpu core的并发线程数，本文设置为1</p><p>YARN Available Vcores：YARN可用的CPU核数=Available Container  Resources * Physical Cores to Vcores Multiplier，即为44</p><p>YARN Available Memory：250880</p><h3 id="集群大小"><a href="#集群大小" class="headerlink" title="集群大小"></a>集群大小</h3><p>集群的工作节点个数：10</p><h2 id="YARN配置"><a href="#YARN配置" class="headerlink" title="YARN配置"></a>YARN配置</h2><h3 id="YARN-NodeManager配置属性"><a href="#YARN-NodeManager配置属性" class="headerlink" title="YARN NodeManager配置属性"></a>YARN NodeManager配置属性</h3><table>     <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>yarn.nodemanager.resource.cpu-vcores</td>        <td>44</td>          <td>yarn 的nodemanager分配44核，每台节点剩余的CPU</td>     </tr>    <tr>        <td>yarn.nodemanager.resource.memory-mb</td>        <td>250800</td>         <td>分配的内存大小，每台节点剩余的内存</td>      </tr></table><h3 id="验证YARN的配置"><a href="#验证YARN的配置" class="headerlink" title="验证YARN的配置"></a>验证YARN的配置</h3><p>登录YARN的resourcemanager的WEBUI：http://<resourcemanagerip>:8088/，验证’Memory Total’与’Vcores Total’，如果节点都正常，那么Vcores Total应该为440，Memory应该为2450G，即250800/1024*10</resourcemanagerip></p><h3 id="YARN的container配置"><a href="#YARN的container配置" class="headerlink" title="YARN的container配置"></a>YARN的container配置</h3><p>YARN的container的Vcore配置</p><table>    <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>yarn.scheduler.minimum-allocation-vcores</td>        <td>1</td>          <td>分配给container的最小vcore个数</td>     </tr>    <tr>        <td>yarn.scheduler.maximum-allocation-vcores</td>        <td>44</td>         <td>分配给container的最大vcore数</td>      </tr>    <tr>        <td>yarn.scheduler.increment-allocation-vcores</td>        <td>1</td>         <td>容器虚拟CPU内核增量</td>      </tr></table><p>YARN的container内存配置</p><table>    <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>yarn.scheduler.minimum-allocation-mb</td>        <td>1024</td>          <td>分配给container的最小内存大小，为1G</td>     </tr>    <tr>        <td>yarn.scheduler.maximum-allocation-mb</td>        <td>250880</td>         <td>分配给container的最大内存，等于245G，即为每台节点剩余的最大内存</td>      </tr>    <tr>        <td>yarn.scheduler.increment-allocation-mb</td>        <td>512</td>         <td>容器内存增量，默认512M</td>      </tr></table><h3 id="集群资源分配估计"><a href="#集群资源分配估计" class="headerlink" title="集群资源分配估计"></a>集群资源分配估计</h3><table>    <tr>        <td bgcolor="#6495ED">描述</td>        <td bgcolor="#6495ED">最小值</td>         <td bgcolor="#6495ED">最大值</td>      </tr>    <tr>        <td>根据每个container的最小内存分配，集群最大的container数量为</td>        <td></td>          <td>2450</td>     </tr>    <tr>        <td>根据每个container的最小Vcore分配，集群最大的container数量为</td>        <td></td>         <td>440</td>      </tr>    <tr>        <td>根据每个container的最大内存分配，集群的最小container数为</td>        <td>10</td>         <td></td>      </tr>    <tr>        <td>根据每个container的最大Vcores分配，集群的最小container数为</td>        <td>10</td>         <td></td>      </tr></table><h3 id="container合理配置检查"><a href="#container合理配置检查" class="headerlink" title="container合理配置检查"></a>container合理配置检查</h3><table>    <tr>        <td bgcolor="#6495ED">配置约束</td>        <td bgcolor="#6495ED">描述</td>     </tr>    <tr>        <td>最大的Vcore数量必须大于等于分配的最小Vcore数</td>        <td>yarn.scheduler.maximum-allocation-vcores >= yarn.scheduler.minimum-allocation-vcores</td>      </tr>    <tr>        <td>分配的最大内存数必须大于等于分配的最小内存数</td>        <td>yarn.scheduler.maximum-allocation-mb >= yarn.scheduler.minimum-allocation-mb</td>     </tr>    <tr>        <td>分配的最小核数必须大于等于0</td>        <td>yarn.scheduler.minimum-allocation-vcores >= 0</td>      </tr>    <tr>        <td>分配的最大Vcore数必须大于等于1</td>        <td>yarn.scheduler.maximum-allocation-vcores >= 1</td>      </tr>    <tr>        <td>每台主机分配给nodemanaer的vcore总数必须大于分配的最小vcore数</td>        <td> yarn.nodemanager.resource.cpu-vcores >= yarn.scheduler.minimum-allocation-vcores</td>      </tr>    <tr>        <td>每台主机分配给nodemanaer的vcore总数必须大于分配的最大vcore数</td>        <td>yarn.nodemanager.resource.cpu-vcores >= yarn.scheduler.maximum-allocation-vcores </td>      </tr>    <tr>        <td>每台主机分配给nodemanaer的内存必须大于调度分配的最小内存</td>        <td>yarn.nodemanager.resource.memory-mb >= yarn.scheduler.maximum-allocation-mb</td>      </tr>    <tr>        <td>每台主机分配给nodemanaer的内存必须大于调度分配的最大内存</td>        <td>yarn.nodemanager.resource.memory-mb >= yarn.scheduler.minimum-allocation-mb</td>      </tr>    <tr>        <td>container最小配置</td>        <td>如果yarn.scheduler.minimum-allocation-mb小于1GB，container可能会被YARN杀死，因为会出现OutOfMemory内存溢出的现象</td>      </tr></table><h2 id="MapReduce配置"><a href="#MapReduce配置" class="headerlink" title="MapReduce配置"></a>MapReduce配置</h2><h3 id="ApplicationMaster配置"><a href="#ApplicationMaster配置" class="headerlink" title="ApplicationMaster配置"></a>ApplicationMaster配置</h3><table>     <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>yarn.app.mapreduce.am.resource.cpu-vcores</td>        <td>1</td>          <td>ApplicationMaster 的虚拟CPU内核数</td>     </tr>    <tr>        <td>yarn.app.mapreduce.am.resource.mb</td>        <td>1024</td>         <td>ApplicationMaster的物理内存要求(MiB)</td>      </tr>    <tr>        <td>yarn.app.mapreduce.am.command-opts</td>        <td>800</td>         <td>传递到 MapReduce ApplicationMaster 的 Java 命令行参数，AM Java heap 大小，为800M</td>      </tr></table><h3 id="堆与容器大小之比"><a href="#堆与容器大小之比" class="headerlink" title="堆与容器大小之比"></a>堆与容器大小之比</h3><table>    <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>task自动堆大小</td>        <td>yes</td>          <td></td>     </tr>    <tr>        <td>mapreduce.job.heap.memory-mb.ratio</td>        <td>0.8</td>         <td>Map 和 Reduce 任务的堆大小与容器大小之比。堆大小应小于容器大小，以允许 JVM 的某些开销，默认为0.8</td>      </tr></table><h3 id="map-task配置"><a href="#map-task配置" class="headerlink" title="map task配置"></a>map task配置</h3><table>    <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>mapreduce.map.cpu.vcores</td>        <td>1</td>          <td>分配给map task的vcore数</td>     </tr>    <tr>        <td>mapreduce.map.memory.mb</td>        <td>1024</td>         <td>分配给map task的内存数，1G</td>      </tr>    <tr>        <td>mapreduce.task.io.sort.mb</td>        <td>400</td>         <td>I/O 排序内存缓冲 (MiB),默认256M，一般不用修改</td>      </tr></table><h3 id="reduce-task配置"><a href="#reduce-task配置" class="headerlink" title="reduce task配置"></a>reduce task配置</h3><table>    <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>mapreduce.reduce.cpu.vcores</td>        <td>1</td>          <td>分配给reduce task的vcore数</td>     </tr>    <tr>        <td>mapreduce.reduce.memory.mb</td>        <td>1024</td>         <td>分配给reduce task的内存数，1G</td>      </tr></table><h3 id="MapReduce配置合理性检查"><a href="#MapReduce配置合理性检查" class="headerlink" title="MapReduce配置合理性检查"></a>MapReduce配置合理性检查</h3><ul><li>Application Master配置的合理性检查</li></ul><p>yarn.scheduler.minimum-allocation-vcores &lt;=  <strong>yarn.app.mapreduce.am.resource.cpu-vcores</strong>&lt;= yarn-scheduler.maximum-allocation-vcores</p><p>yarn.scheduler.minimum-allocation-mb &lt;= <strong>yarn.app.mapreduce.am.resource.cpu-vcores</strong> &lt;= yarn.scheduler.maximum-allocation-mb  </p><p>Java Heap大小是container大小的75%~90%: 太低会造成资源浪费, 太高会造成OOM<br>Map Task配置的合理性检查</p><ul><li>Reduce Task配置的合理性检查</li></ul><p>yarn.scheduler.minimum-allocation-vcores &lt;= <strong>mapreduce.map.cpu.vcores</strong> &lt;= yarn-scheduler.maximum-allocation-vcores</p><p>yarn.scheduler.minimum-allocation-mb &lt;= <strong>mapreduce.map.memory.mb</strong> &lt;= yarn.scheduler.maximum-allocation-mb</p><p>Spill/Sort内存为每个task堆内存的40%~60%</p><ul><li>Reduce Task配置的合理性检查</li></ul><p>yarn.scheduler.minimum-allocation-vcores &lt;= <strong>mapreduce.reduce.cpu.vcores</strong> &lt;= yarn-scheduler.maximum-allocation-vcores   </p><p>yarn.scheduler.minimum-allocation-mb &lt;= <strong>mapreduce.reduce.memory.mb</strong> &lt;= yarn.scheduler.maximum-allocation-mb</p><h2 id="YARN和MapReduce配置参数总结"><a href="#YARN和MapReduce配置参数总结" class="headerlink" title="YARN和MapReduce配置参数总结"></a>YARN和MapReduce配置参数总结</h2><table>    <tr>        <td bgcolor="#6495ED">YARN/MapReduce参数配置</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>yarn.nodemanager.resource.cpu-vcores</td>        <td>分配给container的虚拟cpu数</td>     </tr>    <tr>        <td>yarn.nodemanager.resource.memory-mb</td>        <td>分配给container的内存大小</td>     </tr>    <tr>        <td>yarn.scheduler.minimum-allocation-vcores</td>        <td>分配给container的最小虚拟cpu数</td>     </tr>    <tr>        <td>    yarn.scheduler.maximum-allocation-vcores</td>        <td>分配给container的最大虚拟cpu数</td>     </tr>    <tr>        <td>yarn.scheduler.increment-allocation-vcores</td>        <td>分配给container的递增虚拟cpu数</td>     </tr>    <tr>        <td>yarn.scheduler.minimum-allocation-mb</td>        <td>分配给container的最小内存大小</td>     </tr>    <tr>        <td>yarn.scheduler.maximum-allocation-mb</td>        <td>分配给container的最大内存</td>     </tr>    <tr>        <td>yarn.scheduler.increment-allocation-mb</td>        <td>分配给container的递增内存大小</td>     </tr>    <tr>        <td>yarn.app.mapreduce.am.resource.cpu-vcores</td>        <td>ApplicationMaste的虚拟cpu数</td>     </tr>    <tr>        <td>yarn.app.mapreduce.am.resource.mb</td>        <td>ApplicationMaste的内存大小</td>     </tr>    <tr>        <td>mapreduce.map.cpu.vcores</td>        <td>map task的虚拟CPU数</td>     </tr>    <tr>        <td>mapreduce.map.memory.mb</td>        <td>map task的内存大小</td>     </tr>    <tr>        <td>mapreduce.reduce.cpu.vcores</td>        <td>reduce task的虚拟cpu数</td>     </tr>    <tr>        <td>mapreduce.reduce.memory.mb</td>        <td>reduce task的内存大小</td>     </tr>    <tr>        <td>mapreduce.task.io.sort.mb</td>        <td>I/O排序内存大小</td>     </tr></table><p>note：在CDH5.5或者更高版本中，参数<strong>mapreduce.map.java.opts</strong>, <strong>mapreduce.reduce.java.opts</strong>, <strong>yarn.app.mapreduce.am.command-opts</strong>会基于container堆内存的比例进行自动配置 </p>]]></content>
      
      
      <categories>
          
          <category> CDH,YARN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH,YARN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>历史拉链表实战</title>
      <link href="/2019/11/08/%E5%8E%86%E5%8F%B2%E6%8B%89%E9%93%BE%E8%A1%A8%E5%AE%9E%E6%88%98/"/>
      <url>/2019/11/08/%E5%8E%86%E5%8F%B2%E6%8B%89%E9%93%BE%E8%A1%A8%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[<p>历史拉链表是一种数据模型，主要是针对数据仓库设计中表存储数据的方式而定义的。所谓历史拉链表，就是指记录一个事物从开始一直到当前状态的所有变化信息。拉所有记录链表可以避免按每一天存储造成的海量存储问题，同时也是处理缓慢变化数据的一种常见方式。</p><a id="more"></a><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>现假设有如下场景：一个企业拥有5000万会员信息，每天有20万会员资料变更，需要在数仓中记录会员表的历史变化以备分析使用，即每天都要保留一个快照供查询，反映历史数据的情况。在此场景中，需要反映5000万会员的历史变化，如果保留快照，存储两年就需要2X365X5000W条数据存储空间，数据量为365亿，如果存储更长时间，则无法估计需要的存储空间。而利用拉链算法存储，每日只向历史表中添加新增和变化的数据，每日不过20万条，存储4年也只需要3亿存储空间。</p><h2 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h2><p>在拉链表中，每一条数据都有一个生效日期(effective_date)和失效日期(expire_date)。假设在一个用户表中，在2019年11月8日新增了两个用户，如下表所示，则这两条记录的生效时间为当天，由于到2019年11月8日为止,这两条就还没有被修改过，所以失效时间为一个给定的比较大的值，比如：3000-12-31  </p><table>    <tr>        <td> member_id</td>         <td>phoneno</td>        <td>create_time</td>        <td>update_time</td>     </tr>    <tr>        <td>10001</td>        <td>13300000001</td>          <td>2019-11-08</td>         <td>3000-12-31</td>     </tr>    <tr>        <td>10002</td>        <td>13500000002</td>         <td>2019-11-08</td>        <td>3000-12-31</td>       </tr></table><p>第二天(2019-11-09)，用户10001被删除了，用户10002的电话号码被修改成13600000002.为了保留历史状态，用户10001的失效时间被修改为2019-11-09，用户10002则变成了两条记录，如下表所示：  </p><table>    <tr>        <td> member_id</td>         <td>phoneno</td>        <td>create_time</td>        <td>update_time</td>     </tr>    <tr>        <td>10001</td>        <td>13300000001</td>          <td>2019-11-08</td>         <td>2019-11-09</td>     </tr>    <tr>        <td>10002</td>        <td>13500000002</td>         <td>2019-11-08</td>        <td>2019-11-09</td>       </tr>    <tr>        <td>10002</td>        <td>13600000002</td>         <td>2019-11-09</td>        <td>3000-12-31</td>       </tr></table><p>第三天(2019-11-10),又新增了用户10003，则用户表数据如小表所示：  </p><table>    <tr>        <td> member_id</td>         <td>phoneno</td>        <td>create_time</td>        <td>update_time</td>     </tr>    <tr>        <td>10001</td>        <td>13300000001</td>          <td>2019-11-08</td>         <td>2019-11-09</td>     </tr>    <tr>        <td>10002</td>        <td>13500000002</td>         <td>2019-11-08</td>        <td>2019-11-09</td>       </tr>    <tr>        <td>10002</td>        <td>13600000002</td>         <td>2019-11-09</td>        <td>3000-12-31</td>       </tr>    <tr>        <td>10003</td>        <td>13300000006</td>         <td>2019-11-10</td>        <td>3000-12-31</td>       </tr></table><p>如果要查询最新的数据，那么只要查询失效时间为3000-12-31的数据即可，如果要查11月8号的历史数据，则筛选生效时间&lt;= 2019-11-08并且失效时间&gt;2019-11-08的数据即可。如果查询11月9号的数据，那么筛选条件则是生效时间&lt;=2019-11-09并且失效时间&gt;2019-11-09</p><h2 id="表结构"><a href="#表结构" class="headerlink" title="表结构"></a>表结构</h2><ul><li>MySQL源member表</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE member(</span><br><span class="line">             member_id VARCHAR ( 64 ),</span><br><span class="line">             phoneno VARCHAR ( 20 ), </span><br><span class="line">             create_time datetime,</span><br><span class="line">             update_time datetime );</span><br></pre></td></tr></table></figure><ul><li>ODS层增量表member_delta,每天一个分区</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">CREATE TABLE member_delta</span><br><span class="line">            (member_id string,</span><br><span class="line">             phoneno string,</span><br><span class="line">             create_time string,</span><br><span class="line">             update_time string) </span><br><span class="line">PARTITIONED BY (DAY string);</span><br></pre></td></tr></table></figure><ul><li>临时表</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">CREATE TABLE member_his_tmp</span><br><span class="line">            (member_id string,</span><br><span class="line">             phoneno string,</span><br><span class="line">             effective_date date,</span><br><span class="line">             expire_date date</span><br><span class="line">             );</span><br></pre></td></tr></table></figure><ul><li>DW层历史拉链表</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">CREATE TABLE member_his</span><br><span class="line">            (member_id string,</span><br><span class="line">             phoneno string,</span><br><span class="line">             effective_date date,</span><br><span class="line">             expire_date date);</span><br></pre></td></tr></table></figure><h2 id="Demo数据准备"><a href="#Demo数据准备" class="headerlink" title="Demo数据准备"></a>Demo数据准备</h2><p>2019-11-08的数据为：  </p><table>    <tr>        <td> member_id</td>         <td>phoneno</td>        <td>create_time</td>        <td>update_time</td>     </tr>    <tr>        <td>10001</td>        <td>13500000001</td>          <td>2019-11-08 14:47:55</td>         <td>2019-11-08 14:47:55</td>     </tr>    <tr>        <td>10002</td>        <td>13500000002</td>         <td>2019-11-08 14:48:33</td>        <td>2019-11-08 14:48:33</td>       </tr>    <tr>        <td>10003</td>        <td>13500000003</td>         <td>2019-11-08 14:48:53</td>        <td>2019-11-08 14:48:53</td>       </tr>    <tr>        <td>10004</td>        <td>13500000004</td>         <td>2019-11-08 14:49:02</td>        <td>2019-11-08 14:49:02</td>       </tr></table><p>2019-11-09的数据为：其中蓝色代表新增数据，红色代表修改的数据</p><table>    <tr>        <td> member_id</td>         <td>phoneno</td>        <td>create_time</td>        <td>update_time</td>     </tr>    <tr>        <td>10001</td>        <td>13500000001</td>          <td>2019-11-08 14:47:55</td>         <td>2019-11-08 14:47:55</td>     </tr>    <tr>        <td bgcolor="#DC143C">10002</td>        <td bgcolor="#DC143C">13600000002</td>         <td bgcolor="#DC143C">2019-11-08 14:48:33</td>        <td bgcolor="#DC143C">2019-11-09 14:48:33</td>       </tr>    <tr>        <td>10003</td>        <td>13500000003</td>         <td>2019-11-08 14:48:53</td>        <td>2019-11-08 14:48:53</td>       </tr>    <tr>        <td>10004</td>        <td>13500000004</td>         <td>2019-11-08 14:49:02</td>        <td>2019-11-08 14:49:02</td>       </tr>    <tr>        <td bgcolor="#6495ED">10005</td>         <td bgcolor="#6495ED">13500000005</td>        <td bgcolor="#6495ED">2019-11-09 08:54:03 </td>        <td bgcolor="#6495ED">2019-11-09 08:54:03</td>    </tr>    <tr>        <td bgcolor="#6495ED">10006</td>         <td bgcolor="#6495ED">13500000006</td>        <td bgcolor="#6495ED">2019-11-09 09:54:25 </td>        <td bgcolor="#6495ED">2019-11-09 09:54:25</td>    </tr></table><p>2019-11-10的数据：其中蓝色代表新增数据，红色代表修改的数据  </p><table>    <tr>        <td> member_id</td>         <td>phoneno</td>        <td>create_time</td>        <td>update_time</td>     </tr>    <tr>        <td>10001</td>        <td>13500000001</td>          <td>2019-11-08 14:47:55</td>         <td>2019-11-08 14:47:55</td>     </tr>    <tr>        <td>10002</td>        <td>13600000002</td>         <td>2019-11-08 14:48:33</td>        <td>2019-11-09 14:48:33</td>       </tr>    <tr>        <td>10003</td>        <td>13500000003</td>         <td>2019-11-08 14:48:53</td>        <td>2019-11-08 14:48:53</td>       </tr>    <tr>        <td bgcolor="#DC143C">10004</td>        <td bgcolor="#DC143C">13600000004</td>         <td bgcolor="#DC143C">2019-11-08 14:49:02</td>        <td bgcolor="#DC143C">2019-11-10 14:49:02</td>       </tr>    <tr>        <td>10005</td>         <td>13500000005</td>        <td>2019-11-09 08:54:03 </td>        <td>2019-11-09 08:54:03</td>    </tr>    <tr>        <td>10006</td>         <td>13500000006</td>        <td>2019-11-09 09:54:25 </td>        <td>2019-11-09 09:54:25</td>    </tr>    <tr>        <td bgcolor="#6495ED">10007</td>         <td bgcolor="#6495ED">13500000007</td>        <td bgcolor="#6495ED">2019-11-10 17:41:49 </td>        <td bgcolor="#6495ED">2019-11-10 17:41:49</td>    </tr></table><h2 id="全量初始装载"><a href="#全量初始装载" class="headerlink" title="全量初始装载"></a>全量初始装载</h2><p>在启用拉链表时，先对其进行初始装载，比如以2019-11-08为开始时间，<br>那么将MySQL源表全量抽取到ODS层member_delta表的2018-11-08的分区中，<br>然后初始装载DW层的拉链表member_his</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">INSERT overwrite TABLE member_his </span><br><span class="line">SELECT</span><br><span class="line">   member_id,</span><br><span class="line">   phoneno,</span><br><span class="line">   to_date ( create_time ) AS effective_date,</span><br><span class="line">  <span class="string">'3000-12-31'</span> </span><br><span class="line">FROM</span><br><span class="line">member_delta </span><br><span class="line">WHERE</span><br><span class="line">DAY = <span class="string">'2019-11-08'</span></span><br></pre></td></tr></table></figure><p>查询初始的历史拉链表数据</p><p><img src="//jiamaoxiang.top/2019/11/08/历史拉链表实战/1.png" alt></p><h2 id="增量抽取数据"><a href="#增量抽取数据" class="headerlink" title="增量抽取数据"></a>增量抽取数据</h2><p>每天，从源系统member表中，将前一天的增量数据抽取到ODS层的增量数据表member_delta对应的分区中。这里的增量需要通过member表中的创建时间和修改时间来确定，或者使用sqoop job监控update时间来进行增联抽取。<br>比如，本案例中2019-11-09和2019-11-10为两个分区，分别存储了2019-11-09和2019-11-10日的增量数据。<br>2019-11-09分区的数据为:</p><p><img src="//jiamaoxiang.top/2019/11/08/历史拉链表实战/2.png" alt></p><p>2019-11-10分区的数据为：</p><p><img src="//jiamaoxiang.top/2019/11/08/历史拉链表实战/3.png" alt></p><h2 id="增量刷新历史拉链数据"><a href="#增量刷新历史拉链数据" class="headerlink" title="增量刷新历史拉链数据"></a>增量刷新历史拉链数据</h2><ul><li>2019-11-09增量刷新历史拉链表<br>将数据放进临时表</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">INSERT overwrite TABLE member_his_tmp</span><br><span class="line">SELECT *</span><br><span class="line">FROM</span><br><span class="line">  (</span><br><span class="line">-- 2019-11-09增量数据，代表最新的状态，该数据的生效时间是2019-11-09，过期时间为3000-12-31</span><br><span class="line">-- 这些增量的数据需要被全部加载到历史拉链表中</span><br><span class="line">SELECT member_id,</span><br><span class="line">       phoneno,</span><br><span class="line">       <span class="string">'2019-11-09'</span> effective_date,</span><br><span class="line">                    <span class="string">'3000-12-31'</span> expire_date</span><br><span class="line">   FROM member_delta</span><br><span class="line">   WHERE DAY=<span class="string">'2019-11-09'</span></span><br><span class="line">   UNION ALL </span><br><span class="line">-- 用当前为生效状态的拉链数据，去left join 增量数据，</span><br><span class="line">-- 如果匹配得上，则表示该数据已发生了更新，</span><br><span class="line">-- 此时，需要将发生更新的数据的过期时间更改为当前时间.</span><br><span class="line">-- 如果匹配不上，则表明该数据没有发生更新，此时过期时间不变</span><br><span class="line">SELECT a.member_id,</span><br><span class="line">       a.phoneno,</span><br><span class="line">       a.effective_date,</span><br><span class="line">       <span class="keyword">if</span>(b.member_id IS NULL, to_date(a.expire_date), to_date(b.day)) expire_date</span><br><span class="line">   FROM</span><br><span class="line">     (SELECT *</span><br><span class="line">      FROM member_his</span><br><span class="line">    ) a</span><br><span class="line">   LEFT JOIN</span><br><span class="line">     (SELECT *</span><br><span class="line">      FROM member_delta</span><br><span class="line">      WHERE DAY=<span class="string">'2019-11-09'</span>) b ON a.member_id=b.member_id)his</span><br></pre></td></tr></table></figure><p>将数据覆盖到历史拉链表</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">INSERT overwrite TABLE member_his</span><br><span class="line">SELECT *</span><br><span class="line">FROM member_his_tmp</span><br></pre></td></tr></table></figure><p>查看历史拉链表</p><p><img src="//jiamaoxiang.top/2019/11/08/历史拉链表实战/4.png" alt></p><ul><li>2019-11-10增量刷新历史拉链表</li></ul><p>将数据放进临时表</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">INSERT overwrite TABLE member_his_tmp</span><br><span class="line">SELECT *</span><br><span class="line">FROM</span><br><span class="line">  (</span><br><span class="line">-- 2019-11-10增量数据，代表最新的状态，该数据的生效时间是2019-11-10，过期时间为3000-12-31</span><br><span class="line">-- 这些增量的数据需要被全部加载到历史拉链表中</span><br><span class="line">SELECT member_id,</span><br><span class="line">       phoneno,</span><br><span class="line">       <span class="string">'2019-11-10'</span> effective_date,</span><br><span class="line">                    <span class="string">'3000-12-31'</span> expire_date</span><br><span class="line">   FROM member_delta</span><br><span class="line">   WHERE DAY=<span class="string">'2019-11-10'</span></span><br><span class="line">   UNION ALL </span><br><span class="line">-- 用当前为生效状态的拉链数据，去left join 增量数据，</span><br><span class="line">-- 如果匹配得上，则表示该数据已发生了更新，</span><br><span class="line">-- 此时，需要将发生更新的数据的过期时间更改为当前时间.</span><br><span class="line">-- 如果匹配不上，则表明该数据没有发生更新，此时过期时间不变</span><br><span class="line">SELECT a.member_id,</span><br><span class="line">       a.phoneno,</span><br><span class="line">       a.effective_date,</span><br><span class="line">       <span class="keyword">if</span>(b.member_id IS NULL, to_date(a.expire_date), to_date(b.day)) expire_date</span><br><span class="line">   FROM</span><br><span class="line">     (SELECT *</span><br><span class="line">      FROM member_his</span><br><span class="line">    ) a</span><br><span class="line">   LEFT JOIN</span><br><span class="line">     (SELECT *</span><br><span class="line">      FROM member_delta</span><br><span class="line">      WHERE DAY=<span class="string">'2019-11-10'</span>) b ON a.member_id=b.member_id)his</span><br></pre></td></tr></table></figure><p>查看历史拉链表</p><p><img src="//jiamaoxiang.top/2019/11/08/历史拉链表实战/5.png" alt></p><p>将以上脚本封装成shell调度的脚本</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天</span></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$1</span>"</span> ] ;<span class="keyword">then</span></span><br><span class="line">do_date=<span class="variable">$1</span></span><br><span class="line"><span class="keyword">else</span> </span><br><span class="line">do_date=`date -d <span class="string">"-1 day"</span> +%F`  </span><br><span class="line"><span class="keyword">fi</span> </span><br><span class="line"></span><br><span class="line">sql=<span class="string">"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">INSERT overwrite TABLE member_his_tmp</span></span><br><span class="line"><span class="string">SELECT *</span></span><br><span class="line"><span class="string">FROM</span></span><br><span class="line"><span class="string">  (</span></span><br><span class="line"><span class="string">-- 2019-11-10增量数据，代表最新的状态，该数据的生效时间是2019-11-10，过期时间为3000-12-31</span></span><br><span class="line"><span class="string">-- 这些增量的数据需要被全部加载到历史拉链表中</span></span><br><span class="line"><span class="string">SELECT member_id,</span></span><br><span class="line"><span class="string">       phoneno,</span></span><br><span class="line"><span class="string">       '<span class="variable">$do_date</span>' effective_date,</span></span><br><span class="line"><span class="string">       '3000-12-31' expire_date</span></span><br><span class="line"><span class="string">   FROM member_delta</span></span><br><span class="line"><span class="string">   WHERE DAY='<span class="variable">$do_date</span>'</span></span><br><span class="line"><span class="string">   UNION ALL </span></span><br><span class="line"><span class="string">-- 用当前为生效状态的拉链数据，去left join 增量数据，</span></span><br><span class="line"><span class="string">-- 如果匹配得上，则表示该数据已发生了更新，</span></span><br><span class="line"><span class="string">-- 此时，需要将发生更新的数据的过期时间更改为当前时间.</span></span><br><span class="line"><span class="string">-- 如果匹配不上，则表明该数据没有发生更新，此时过期时间不变</span></span><br><span class="line"><span class="string">SELECT a.member_id,</span></span><br><span class="line"><span class="string">       a.phoneno,</span></span><br><span class="line"><span class="string">       a.effective_date,</span></span><br><span class="line"><span class="string">       if(b.member_id IS NULL, to_date(a.expire_date), to_date(b.day)) expire_date</span></span><br><span class="line"><span class="string">   FROM</span></span><br><span class="line"><span class="string">     (SELECT *</span></span><br><span class="line"><span class="string">      FROM member_his</span></span><br><span class="line"><span class="string"> ) a</span></span><br><span class="line"><span class="string">   LEFT JOIN</span></span><br><span class="line"><span class="string">     (SELECT *</span></span><br><span class="line"><span class="string">      FROM member_delta</span></span><br><span class="line"><span class="string">      WHERE DAY='<span class="variable">$do_date</span>') b ON a.member_id=b.member_id)his;</span></span><br><span class="line"><span class="string">"</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$hive</span> -e <span class="string">"<span class="variable">$sql</span>"</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数据仓库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink运行架构剖析</title>
      <link href="/2019/10/23/Flink%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84%E5%89%96%E6%9E%90/"/>
      <url>/2019/10/23/Flink%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84%E5%89%96%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>本文主要介绍 Flink Runtime 的作业执行的核心机制。首先介绍 Flink Runtime 的整体架构以及 Job 的基本执行流程，然后介绍Flink 的Standalone运行架构，最后对Flink on YARN的两种模式进行了详细剖析。</p><a id="more"></a><h2 id="Flink-Runtime作业执行流程分析"><a href="#Flink-Runtime作业执行流程分析" class="headerlink" title="Flink Runtime作业执行流程分析"></a>Flink Runtime作业执行流程分析</h2><h3 id="整体架构图"><a href="#整体架构图" class="headerlink" title="整体架构图"></a>整体架构图</h3><p>Flink Runtime 层的主要架构如下图所示，它展示了一个 Flink 集群的基本结构。整体来说，它采用了标准 master-slave 的结构，master负责管理整个集群中的资源和作业；TaskExecutor 则是 Slave，负责提供具体的资源并实际执行作业。  </p><p><img src="//jiamaoxiang.top/2019/10/23/Flink运行架构剖析/Runtime.png" alt></p><h3 id="执行流程分析"><a href="#执行流程分析" class="headerlink" title="执行流程分析"></a>执行流程分析</h3><h4 id="组件介绍"><a href="#组件介绍" class="headerlink" title="组件介绍"></a>组件介绍</h4><p>Application Master 部分包含了三个组件，即 Dispatcher、ResourceManager 和 JobManager。其中，Dispatcher 负责接收用户提供的作业，并且负责为这个新提交的作业拉起一个新的 JobManager 组件。ResourceManager 负责资源的管理，在整个 Flink 集群中只有一个 ResourceManager。JobManager 负责管理作业的执行，在一个 Flink 集群中可能有多个作业同时执行，每个作业都有自己的 JobManager 组件。这三个组件都包含在 AppMaster 进程。  </p><p>TaskManager主要负责执行具体的task任务，<a href="https://jiamaoxiang.top/2019/08/23/Flink%E7%9A%84%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF-State-Backends/">StateBackend</a> 主要应用于状态的checkpoint。  </p><p>Cluster Manager是集群管理器，比如Standalone、YARN、K8s等。  </p><h4 id="流程分析"><a href="#流程分析" class="headerlink" title="流程分析"></a>流程分析</h4><p>1.当用户提交作业的时候，提交脚本会首先启动一个 Client进程负责作业的编译与提交。它首先将用户编写的代码编译为一个 JobGraph，在这个过程，它还会进行一些检查或优化等工作，例如判断哪些 Operator 可以 Chain 到同一个 Task 中。然后，Client 将产生的 JobGraph 提交到集群中执行。此时有两种情况，一种是类似于 Standalone 这种 Session 模式，AM 会预先启动，此时 Client 直接与 Dispatcher 建立连接并提交作业即可。另一种是 Per-Job 模式，AM 不会预先启动，此时 Client 将首先向资源管理系统 （如Yarn、K8S）申请资源来启动 AM，然后再向 AM 中的 Dispatcher 提交作业。  </p><p>2.当作业到 Dispatcher 后，Dispatcher 会首先启动一个 JobManager 组件，然后 JobManager 会向 ResourceManager 申请资源来启动作业中具体的任务。如果是Session模式，则TaskManager已经启动了，就可以直接分配资源。如果是per-Job模式，ResourceManager 也需要首先向外部资源管理系统申请资源来启动 TaskExecutor，然后等待 TaskExecutor 注册相应资源后再继续选择空闲资源进程分配，JobManager 收到 TaskExecutor 注册上来的 Slot 后，就可以实际提交 Task 了。  </p><p>3.TaskExecutor 收到 JobManager 提交的 Task 之后，会启动一个新的线程来执行该 Task。Task 启动后就会开始进行预先指定的计算，并通过数据 Shuffle 模块互相交换数据。</p><h2 id="Flink-Standalone运行架构"><a href="#Flink-Standalone运行架构" class="headerlink" title="Flink Standalone运行架构"></a>Flink Standalone运行架构</h2><p>Flink Standalone运行架构如下图所示：</p><p><img src="//jiamaoxiang.top/2019/10/23/Flink运行架构剖析/standalone.png" alt><br>Standalone模式需要先启动Jobmanager和TaskManager进程，每一个作业都是自己的JobManager。<br>Client：任务提交，生成JobGraph  </p><p>JobManager：调度Job，协调Task，通信，申请资源  </p><p>TaskManager：具体任务执行，请求资源</p><h2 id="Flink-On-YARN运行架构"><a href="#Flink-On-YARN运行架构" class="headerlink" title="Flink On YARN运行架构"></a>Flink On YARN运行架构</h2><p>关于YARN的基本架构原理，详见另一篇我的博客<a href="https://blog.csdn.net/jmx_bigdata/article/details/84320188" target="_blank" rel="noopener">YARN架构原理</a></p><h3 id="Per-Job模式"><a href="#Per-Job模式" class="headerlink" title="Per-Job模式"></a>Per-Job模式</h3><p>Per-job 模式下整个 Flink 集群只执行单个作业，即每个作业会独享 Dispatcher 和 ResourceManager 组件。此外，Per-job 模式下 AppMaster 和 TaskExecutor 都是按需申请的。因此，Per-job 模式更适合运行执行时间较长的大作业，这些作业对稳定性要求较高，并且对申请资源的时间不敏感。<br>1.独享Dispatcher与ResourceManager  </p><p>2.按需申请资源(TaskExecutor)  </p><p>3.适合执行时间较长的大作业  </p><p><img src="//jiamaoxiang.top/2019/10/23/Flink运行架构剖析/perjob.png" alt></p><h3 id="Session模式"><a href="#Session模式" class="headerlink" title="Session模式"></a>Session模式</h3><p>在 Session 模式下，Flink 预先启动 AppMaster 以及一组 TaskExecutor，然后在整个集群的生命周期中会执行多个作业。可以看出，Session 模式更适合规模小，执行时间短的作业。<br>1.共享Dispatcher与ResourceManager  </p><p>2.共享资源  </p><p>3.适合小规模，执行时间较短的作业  </p><p><img src="//jiamaoxiang.top/2019/10/23/Flink运行架构剖析/session.png" alt></p><p>Reference:<br>[1]<a href="https://ververica.cn/developers/advanced-tutorial-1-analysis-of-the-core-mechanism-of-runtime/" target="_blank" rel="noopener">https://ververica.cn/developers/advanced-tutorial-1-analysis-of-the-core-mechanism-of-runtime/</a><br>[2]<a href="https://ververica.cn/developers/flink-training-course2/" target="_blank" rel="noopener">https://ververica.cn/developers/flink-training-course2/</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>经典Hive-SQL面试题</title>
      <link href="/2019/10/15/%E7%BB%8F%E5%85%B8Hive-SQL%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
      <url>/2019/10/15/%E7%BB%8F%E5%85%B8Hive-SQL%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>HQL练习</p><a id="more"></a><h2 id="第一题"><a href="#第一题" class="headerlink" title="第一题"></a>第一题</h2><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">我们有如下的用户访问数据</span><br><span class="line">userId  visitDate   visitCount</span><br><span class="line">u01 2017/1/21   5</span><br><span class="line">u02 2017/1/23   6</span><br><span class="line">u03 2017/1/22   8</span><br><span class="line">u04 2017/1/20   3</span><br><span class="line">u01 2017/1/23   6</span><br><span class="line">u01 2017/2/21   8</span><br><span class="line">U02 2017/1/23   6</span><br><span class="line">U01 2017/2/22   4</span><br><span class="line">要求使用SQL统计出每个用户的累积访问次数，如下表所示：</span><br><span class="line">用户id    月份  小计  累积</span><br><span class="line">u01 2017-01 11  11</span><br><span class="line">u01 2017-02 12  23</span><br><span class="line">u02 2017-01 12  12</span><br><span class="line">u03 2017-01 8   8</span><br><span class="line">u04 2017-01 3   3</span><br></pre></td></tr></table></figure><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test1 ( </span><br><span class="line">userId string, </span><br><span class="line">visitDate string,</span><br><span class="line">visitCount INT )</span><br><span class="line">ROW format delimited FIELDS TERMINATED BY <span class="string">"\t"</span>;</span><br><span class="line">INSERT INTO TABLE test_sql.test1</span><br><span class="line">VALUES</span><br><span class="line">( <span class="string">'u01'</span>, <span class="string">'2017/1/21'</span>, 5 ),</span><br><span class="line">( <span class="string">'u02'</span>, <span class="string">'2017/1/23'</span>, 6 ),</span><br><span class="line">( <span class="string">'u03'</span>, <span class="string">'2017/1/22'</span>, 8 ),</span><br><span class="line">( <span class="string">'u04'</span>, <span class="string">'2017/1/20'</span>, 3 ),</span><br><span class="line">( <span class="string">'u01'</span>, <span class="string">'2017/1/23'</span>, 6 ),</span><br><span class="line">( <span class="string">'u01'</span>, <span class="string">'2017/2/21'</span>, 8 ),</span><br><span class="line">( <span class="string">'u02'</span>, <span class="string">'2017/1/23'</span>, 6 ),</span><br><span class="line">( <span class="string">'u01'</span>, <span class="string">'2017/2/22'</span>, 4 );</span><br></pre></td></tr></table></figure><h4 id="查询SQL"><a href="#查询SQL" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT t2.userid,</span><br><span class="line">   t2.visitmonth,</span><br><span class="line">   subtotal_visit_cnt,</span><br><span class="line">   sum(subtotal_visit_cnt) over (partition BY userid</span><br><span class="line"> ORDER BY visitmonth) AS total_visit_cnt</span><br><span class="line">FROM</span><br><span class="line">  (SELECT userid,</span><br><span class="line">  visitmonth,</span><br><span class="line">  sum(visitcount) AS subtotal_visit_cnt</span><br><span class="line">   FROM</span><br><span class="line"> (SELECT userid,</span><br><span class="line"> date_format(regexp_replace(visitdate,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM'</span>) AS visitmonth,</span><br><span class="line"> visitcount</span><br><span class="line">  FROM test_sql.test1) t1</span><br><span class="line">   GROUP BY userid,</span><br><span class="line">visitmonth)t2</span><br><span class="line">ORDER BY t2.userid,</span><br><span class="line"> t2.visitmonth</span><br></pre></td></tr></table></figure><h2 id="第二题"><a href="#第二题" class="headerlink" title="第二题"></a>第二题</h2><h3 id="需求-1"><a href="#需求-1" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">有50W个京东店铺，每个顾客访客访问任何一个店铺的任何一个商品时都会产生一条访问日志，</span><br><span class="line">访问日志存储的表名为Visit，访客的用户id为user_id，被访问的店铺名称为shop，数据如下：</span><br><span class="line"></span><br><span class="line">u1a</span><br><span class="line">u2b</span><br><span class="line">u1b</span><br><span class="line">u1a</span><br><span class="line">u3c</span><br><span class="line">u4b</span><br><span class="line">u1a</span><br><span class="line">u2c</span><br><span class="line">u5b</span><br><span class="line">u4b</span><br><span class="line">u6c</span><br><span class="line">u2c</span><br><span class="line">u1b</span><br><span class="line">u2a</span><br><span class="line">u2a</span><br><span class="line">u3a</span><br><span class="line">u5a</span><br><span class="line">u5a</span><br><span class="line">u5a</span><br><span class="line">请统计：</span><br><span class="line">(1)每个店铺的UV（访客数）</span><br><span class="line">(2)每个店铺访问次数top3的访客信息。输出店铺名称、访客id、访问次数</span><br></pre></td></tr></table></figure><h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-1"><a href="#数据准备-1" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test2 ( </span><br><span class="line"> user_id string, </span><br><span class="line"> shop string )</span><br><span class="line">ROW format delimited FIELDS TERMINATED BY <span class="string">'\t'</span>;</span><br><span class="line">INSERT INTO TABLE test_sql.test2 VALUES</span><br><span class="line">( <span class="string">'u1'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u2'</span>, <span class="string">'b'</span> ),</span><br><span class="line">( <span class="string">'u1'</span>, <span class="string">'b'</span> ),</span><br><span class="line">( <span class="string">'u1'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u3'</span>, <span class="string">'c'</span> ),</span><br><span class="line">( <span class="string">'u4'</span>, <span class="string">'b'</span> ),</span><br><span class="line">( <span class="string">'u1'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u2'</span>, <span class="string">'c'</span> ),</span><br><span class="line">( <span class="string">'u5'</span>, <span class="string">'b'</span> ),</span><br><span class="line">( <span class="string">'u4'</span>, <span class="string">'b'</span> ),</span><br><span class="line">( <span class="string">'u6'</span>, <span class="string">'c'</span> ),</span><br><span class="line">( <span class="string">'u2'</span>, <span class="string">'c'</span> ),</span><br><span class="line">( <span class="string">'u1'</span>, <span class="string">'b'</span> ),</span><br><span class="line">( <span class="string">'u2'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u2'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u3'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u5'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u5'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u5'</span>, <span class="string">'a'</span> );</span><br></pre></td></tr></table></figure><h4 id="查询SQL实现"><a href="#查询SQL实现" class="headerlink" title="查询SQL实现"></a>查询SQL实现</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(1)方式1：</span><br><span class="line">SELECT shop,</span><br><span class="line">   count(DISTINCT user_id)</span><br><span class="line">FROM test_sql.test2</span><br><span class="line">GROUP BY shop</span><br><span class="line">方式2：</span><br><span class="line">SELECT t.shop,</span><br><span class="line">   count(*)</span><br><span class="line">FROM</span><br><span class="line">  (SELECT user_id,</span><br><span class="line">  shop</span><br><span class="line">   FROM test_sql.test2</span><br><span class="line">   GROUP BY user_id,</span><br><span class="line">shop) t</span><br><span class="line">GROUP BY t.shop</span><br><span class="line">(2)</span><br><span class="line">SELECT t2.shop,</span><br><span class="line">   t2.user_id,</span><br><span class="line">   t2.cnt</span><br><span class="line">FROM</span><br><span class="line">  (SELECT t1.*,</span><br><span class="line">  row_number() over(partition BY t1.shop</span><br><span class="line">ORDER BY t1.cnt DESC) rank</span><br><span class="line">   FROM</span><br><span class="line"> (SELECT user_id,</span><br><span class="line"> shop,</span><br><span class="line"> count(*) AS cnt</span><br><span class="line">  FROM test_sql.test2</span><br><span class="line">  GROUP BY user_id,</span><br><span class="line">   shop) t1)t2</span><br><span class="line">WHERE rank &lt;= 3</span><br></pre></td></tr></table></figure><h2 id="第三题"><a href="#第三题" class="headerlink" title="第三题"></a>第三题</h2><h3 id="需求-2"><a href="#需求-2" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">已知一个表STG.ORDER，有如下字段:Date，Order_id，User_id，amount。</span><br><span class="line">数据样例:2017-01-01,10029028,1000003251,33.57。</span><br><span class="line">请给出sql进行统计:</span><br><span class="line">(1)给出 2017年每个月的订单数、用户数、总成交金额。</span><br><span class="line">(2)给出2017年11月的新客数(指在11月才有第一笔订单)</span><br></pre></td></tr></table></figure><h3 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-2"><a href="#数据准备-2" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test3 ( </span><br><span class="line">dt string,</span><br><span class="line">order_id string, </span><br><span class="line">user_id string, </span><br><span class="line">amount DECIMAL ( 10, 2 ) )</span><br><span class="line">ROW format delimited FIELDS TERMINATED BY <span class="string">'\t'</span>;</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-01-01'</span>,<span class="string">'10029028'</span>,<span class="string">'1000003251'</span>,33.57);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-01-01'</span>,<span class="string">'10029029'</span>,<span class="string">'1000003251'</span>,33.57);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-01-01'</span>,<span class="string">'100290288'</span>,<span class="string">'1000003252'</span>,33.57);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-02-02'</span>,<span class="string">'10029088'</span>,<span class="string">'1000003251'</span>,33.57);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-02-02'</span>,<span class="string">'100290281'</span>,<span class="string">'1000003251'</span>,33.57);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-02-02'</span>,<span class="string">'100290282'</span>,<span class="string">'1000003253'</span>,33.57);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-11-02'</span>,<span class="string">'10290282'</span>,<span class="string">'100003253'</span>,234);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2018-11-02'</span>,<span class="string">'10290284'</span>,<span class="string">'100003243'</span>,234);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-1"><a href="#查询SQL-1" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(1)</span><br><span class="line">SELECT t1.mon,</span><br><span class="line">   count(t1.order_id) AS order_cnt,</span><br><span class="line">   count(DISTINCT t1.user_id) AS user_cnt,</span><br><span class="line">   sum(amount) AS total_amount</span><br><span class="line">FROM</span><br><span class="line">  (SELECT order_id,</span><br><span class="line">  user_id,</span><br><span class="line">  amount,</span><br><span class="line">  date_format(dt,<span class="string">'yyyy-MM'</span>) mon</span><br><span class="line">   FROM test_sql.test3</span><br><span class="line">   WHERE date_format(dt,<span class="string">'yyyy'</span>) = <span class="string">'2017'</span>) t1</span><br><span class="line">GROUP BY t1.mon</span><br><span class="line">(2)</span><br><span class="line">SELECT count(user_id)</span><br><span class="line">FROM test_sql.test3</span><br><span class="line">GROUP BY user_id</span><br><span class="line">HAVING date_format(min(dt),<span class="string">'yyyy-MM'</span>)=<span class="string">'2017-11'</span>;</span><br></pre></td></tr></table></figure><h2 id="第四题"><a href="#第四题" class="headerlink" title="第四题"></a>第四题</h2><h3 id="需求-3"><a href="#需求-3" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">有一个5000万的用户文件(user_id，name，age)，一个2亿记录的用户看电影的记录文件(user_id，url)，根据年龄段观看电影的次数进行排序？</span><br></pre></td></tr></table></figure><h3 id="实现-3"><a href="#实现-3" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-3"><a href="#数据准备-3" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test4user</span><br><span class="line">   (user_id string,</span><br><span class="line">name string,</span><br><span class="line">age int);</span><br><span class="line"></span><br><span class="line">CREATE TABLE test_sql.test4log</span><br><span class="line">(user_id string,</span><br><span class="line">url string);</span><br><span class="line"></span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'001'</span>,<span class="string">'u1'</span>,10);</span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'002'</span>,<span class="string">'u2'</span>,15);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'003'</span>,<span class="string">'u3'</span>,15);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'004'</span>,<span class="string">'u4'</span>,20);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'005'</span>,<span class="string">'u5'</span>,25);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'006'</span>,<span class="string">'u6'</span>,35);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'007'</span>,<span class="string">'u7'</span>,40);</span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'008'</span>,<span class="string">'u8'</span>,45);  </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'009'</span>,<span class="string">'u9'</span>,50);  </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'0010'</span>,<span class="string">'u10'</span>,65);  </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'001'</span>,<span class="string">'url1'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'002'</span>,<span class="string">'url1'</span>);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'003'</span>,<span class="string">'url2'</span>);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'004'</span>,<span class="string">'url3'</span>);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'005'</span>,<span class="string">'url3'</span>);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'006'</span>,<span class="string">'url1'</span>);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'007'</span>,<span class="string">'url5'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'008'</span>,<span class="string">'url7'</span>);  </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'009'</span>,<span class="string">'url5'</span>);  </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'0010'</span>,<span class="string">'url1'</span>);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-2"><a href="#查询SQL-2" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT </span><br><span class="line">t2.age_phase,</span><br><span class="line">sum(t1.cnt) as view_cnt</span><br><span class="line">FROM</span><br><span class="line"></span><br><span class="line">(SELECT user_id,</span><br><span class="line">  count(*) cnt</span><br><span class="line">FROM test_sql.test4log</span><br><span class="line">GROUP BY user_id) t1</span><br><span class="line">JOIN</span><br><span class="line">(SELECT user_id,</span><br><span class="line">  CASE WHEN age &lt;= 10 AND age &gt; 0 THEN <span class="string">'0-10'</span> </span><br><span class="line">  WHEN age &lt;= 20 AND age &gt; 10 THEN <span class="string">'10-20'</span></span><br><span class="line">  WHEN age &gt;20 AND age &lt;=30 THEN <span class="string">'20-30'</span></span><br><span class="line">  WHEN age &gt;30 AND age &lt;=40 THEN <span class="string">'30-40'</span></span><br><span class="line">  WHEN age &gt;40 AND age &lt;=50 THEN <span class="string">'40-50'</span></span><br><span class="line">  WHEN age &gt;50 AND age &lt;=60 THEN <span class="string">'50-60'</span></span><br><span class="line">  WHEN age &gt;60 AND age &lt;=70 THEN <span class="string">'60-70'</span></span><br><span class="line">  ELSE <span class="string">'70以上'</span> END as age_phase</span><br><span class="line">FROM test_sql.test4user) t2 ON t1.user_id = t2.user_id </span><br><span class="line">GROUP BY t2.age_phase</span><br></pre></td></tr></table></figure><h2 id="第五题"><a href="#第五题" class="headerlink" title="第五题"></a>第五题</h2><h3 id="需求-4"><a href="#需求-4" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">有日志如下，请写出代码求得所有用户和活跃用户的总数及平均年龄。（活跃用户指连续两天都有访问记录的用户）</span><br><span class="line">日期 用户 年龄</span><br><span class="line">2019-02-11,test_1,23</span><br><span class="line">2019-02-11,test_2,19</span><br><span class="line">2019-02-11,test_3,39</span><br><span class="line">2019-02-11,test_1,23</span><br><span class="line">2019-02-11,test_3,39</span><br><span class="line">2019-02-11,test_1,23</span><br><span class="line">2019-02-12,test_2,19</span><br><span class="line">2019-02-13,test_1,23</span><br><span class="line">2019-02-15,test_2,19</span><br><span class="line">2019-02-16,test_2,19</span><br></pre></td></tr></table></figure><h3 id="实现-4"><a href="#实现-4" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-4"><a href="#数据准备-4" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test5(</span><br><span class="line">dt string,</span><br><span class="line">user_id string,</span><br><span class="line">age int)</span><br><span class="line">ROW format delimited fields terminated BY <span class="string">','</span>;</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-11'</span>,<span class="string">'test_1'</span>,23);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-11'</span>,<span class="string">'test_2'</span>,19);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-11'</span>,<span class="string">'test_3'</span>,39);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-11'</span>,<span class="string">'test_1'</span>,23);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-11'</span>,<span class="string">'test_3'</span>,39);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-11'</span>,<span class="string">'test_1'</span>,23);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-12'</span>,<span class="string">'test_2'</span>,19);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-13'</span>,<span class="string">'test_1'</span>,23);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-15'</span>,<span class="string">'test_2'</span>,19);                                        </span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-16'</span>,<span class="string">'test_2'</span>,19);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-3"><a href="#查询SQL-3" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT sum(total_user_cnt) total_user_cnt,</span><br><span class="line">   sum(total_user_avg_age) total_user_avg_age,</span><br><span class="line">   sum(two_days_cnt) two_days_cnt,</span><br><span class="line">   sum(avg_age) avg_age</span><br><span class="line">FROM</span><br><span class="line">  (SELECT 0 total_user_cnt,</span><br><span class="line">  0 total_user_avg_age,</span><br><span class="line">  count(*) AS two_days_cnt,</span><br><span class="line">  cast(sum(age) / count(*) AS decimal(5,2)) AS avg_age</span><br><span class="line">   FROM</span><br><span class="line"> (SELECT user_id,</span><br><span class="line"> max(age) age</span><br><span class="line">  FROM</span><br><span class="line">(SELECT user_id,</span><br><span class="line">max(age) age</span><br><span class="line"> FROM</span><br><span class="line">   (SELECT user_id,</span><br><span class="line">   age,</span><br><span class="line">   date_sub(dt,rank) flag</span><br><span class="line">FROM</span><br><span class="line">  (SELECT dt,</span><br><span class="line">  user_id,</span><br><span class="line">  max(age) age,</span><br><span class="line">  row_number() over(PARTITION BY user_id</span><br><span class="line">ORDER BY dt) rank</span><br><span class="line">   FROM test_sql.test5</span><br><span class="line">   GROUP BY dt,</span><br><span class="line">user_id) t1) t2</span><br><span class="line"> GROUP BY user_id,</span><br><span class="line">  flag</span><br><span class="line"> HAVING count(*) &gt;=2) t3</span><br><span class="line">  GROUP BY user_id) t4</span><br><span class="line">   UNION ALL SELECT count(*) total_user_cnt,</span><br><span class="line">cast(sum(age) /count(*) AS decimal(5,2)) total_user_avg_age,</span><br><span class="line">0 two_days_cnt,</span><br><span class="line">0 avg_age</span><br><span class="line">   FROM</span><br><span class="line"> (SELECT user_id,</span><br><span class="line"> max(age) age</span><br><span class="line">  FROM test_sql.test5</span><br><span class="line">  GROUP BY user_id) t5) t6</span><br></pre></td></tr></table></figure><h2 id="第六题"><a href="#第六题" class="headerlink" title="第六题"></a>第六题</h2><h3 id="需求-5"><a href="#需求-5" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">请用sql写出所有用户中在今年10月份第一次购买商品的金额，</span><br><span class="line">表ordertable字段:</span><br><span class="line">(购买用户：userid，金额：money，购买时间：paymenttime(格式：2017-10-01)，订单id：orderid</span><br></pre></td></tr></table></figure><h3 id="实现-5"><a href="#实现-5" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-5"><a href="#数据准备-5" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test6 (</span><br><span class="line">userid string,</span><br><span class="line">money decimal(10,2),</span><br><span class="line">paymenttime string,</span><br><span class="line">orderid string);</span><br><span class="line"></span><br><span class="line">INSERT INTO TABLE test_sql.test6 VALUES(<span class="string">'001'</span>,100,<span class="string">'2017-10-01'</span>,<span class="string">'123'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test6 VALUES(<span class="string">'001'</span>,200,<span class="string">'2017-10-02'</span>,<span class="string">'124'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test6 VALUES(<span class="string">'002'</span>,500,<span class="string">'2017-10-01'</span>,<span class="string">'125'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test6 VALUES(<span class="string">'001'</span>,100,<span class="string">'2017-11-01'</span>,<span class="string">'126'</span>);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-4"><a href="#查询SQL-4" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT</span><br><span class="line">userid,</span><br><span class="line">paymenttime,</span><br><span class="line">money,</span><br><span class="line">orderid</span><br><span class="line">from</span><br><span class="line">(SELECT userid,</span><br><span class="line">   money,</span><br><span class="line">   paymenttime,</span><br><span class="line">   orderid,</span><br><span class="line">   row_number() over (PARTITION BY userid</span><br><span class="line">  ORDER BY paymenttime) rank</span><br><span class="line">FROM test_sql.test6</span><br><span class="line">WHERE date_format(paymenttime,<span class="string">'yyyy-MM'</span>) = <span class="string">'2017-10'</span>) t</span><br><span class="line">WHERE rank = 1</span><br></pre></td></tr></table></figure><h2 id="第七题"><a href="#第七题" class="headerlink" title="第七题"></a>第七题</h2><h3 id="需求-6"><a href="#需求-6" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">现有图书管理数据库的三个数据模型如下：</span><br><span class="line">图书（数据表名：BOOK）</span><br><span class="line">序号  字段名称    字段描述    字段类型</span><br><span class="line">1   BOOK_ID 总编号 文本</span><br><span class="line">2   SORT    分类号 文本</span><br><span class="line">3   BOOK_NAME   书名  文本</span><br><span class="line">4   WRITER  作者  文本</span><br><span class="line">5   OUTPUT  出版单位    文本</span><br><span class="line">6   PRICE   单价  数值（保留小数点后2位）</span><br><span class="line">读者（数据表名：READER）</span><br><span class="line">序号  字段名称    字段描述    字段类型</span><br><span class="line">1   READER_ID   借书证号    文本</span><br><span class="line">2   COMPANY 单位  文本</span><br><span class="line">3   NAME    姓名  文本</span><br><span class="line">4   SEX 性别  文本</span><br><span class="line">5   GRADE   职称  文本</span><br><span class="line">6   ADDR    地址  文本</span><br><span class="line">借阅记录（数据表名：BORROW LOG）</span><br><span class="line">序号  字段名称    字段描述    字段类型</span><br><span class="line">1   READER_ID   借书证号    文本</span><br><span class="line">2   BOOK_ID  总编号 文本</span><br><span class="line">3   BORROW_DATE  借书日期    日期</span><br><span class="line">（1）创建图书管理库的图书、读者和借阅三个基本表的表结构。请写出建表语句。</span><br><span class="line">（2）找出姓李的读者姓名（NAME）和所在单位（COMPANY）。</span><br><span class="line">（3）查找“高等教育出版社”的所有图书名称（BOOK_NAME）及单价（PRICE），结果按单价降序排序。</span><br><span class="line">（4）查找价格介于10元和20元之间的图书种类(SORT）出版单位（OUTPUT）和单价（PRICE），结果按出版单位（OUTPUT）和单价（PRICE）升序排序。</span><br><span class="line">（5）查找所有借了书的读者的姓名（NAME）及所在单位（COMPANY）。</span><br><span class="line">（6）求”科学出版社”图书的最高单价、最低单价、平均单价。</span><br><span class="line">（7）找出当前至少借阅了2本图书（大于等于2本）的读者姓名及其所在单位。</span><br><span class="line">（8）考虑到数据安全的需要，需定时将“借阅记录”中数据进行备份，请使用一条SQL语句，在备份用户bak下创建与“借阅记录”表结构完全一致的数据表BORROW_LOG_BAK.井且将“借阅记录”中现有数据全部复制到BORROW_L0G_ BAK中。</span><br><span class="line">（9）现在需要将原Oracle数据库中数据迁移至Hive仓库，请写出“图书”在Hive中的建表语句（Hive实现，提示：列分隔符|；数据表数据需要外部导入：分区分别以month＿part、day＿part 命名）</span><br><span class="line">（10）Hive中有表A，现在需要将表A的月分区　201505　中　user＿id为20000的user＿dinner字段更新为bonc8920，其他用户user＿dinner字段数据不变，请列出更新的方法步骤。（Hive实现，提示：Hlive中无update语法，请通过其他办法进行数据更新）</span><br></pre></td></tr></table></figure><h3 id="实现-6"><a href="#实现-6" class="headerlink" title="实现"></a>实现</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(1)</span><br><span class="line"></span><br><span class="line">-- 创建图书表book</span><br><span class="line"></span><br><span class="line">CREATE TABLE test_sql.book(book_id string,</span><br><span class="line">   `SORT` string,</span><br><span class="line">   book_name string,</span><br><span class="line">   writer string,</span><br><span class="line">   OUTPUT string,</span><br><span class="line">   price decimal(10,2));</span><br><span class="line">INSERT INTO TABLE test_sql.book VALUES (<span class="string">'001'</span>,<span class="string">'TP391'</span>,<span class="string">'信息处理'</span>,<span class="string">'author1'</span>,<span class="string">'机械工业出版社'</span>,<span class="string">'20'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.book VALUES (<span class="string">'002'</span>,<span class="string">'TP392'</span>,<span class="string">'数据库'</span>,<span class="string">'author12'</span>,<span class="string">'科学出版社'</span>,<span class="string">'15'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.book VALUES (<span class="string">'003'</span>,<span class="string">'TP393'</span>,<span class="string">'计算机网络'</span>,<span class="string">'author3'</span>,<span class="string">'机械工业出版社'</span>,<span class="string">'29'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.book VALUES (<span class="string">'004'</span>,<span class="string">'TP399'</span>,<span class="string">'微机原理'</span>,<span class="string">'author4'</span>,<span class="string">'科学出版社'</span>,<span class="string">'39'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.book VALUES (<span class="string">'005'</span>,<span class="string">'C931'</span>,<span class="string">'管理信息系统'</span>,<span class="string">'author5'</span>,<span class="string">'机械工业出版社'</span>,<span class="string">'40'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.book VALUES (<span class="string">'006'</span>,<span class="string">'C932'</span>,<span class="string">'运筹学'</span>,<span class="string">'author6'</span>,<span class="string">'科学出版社'</span>,<span class="string">'55'</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-- 创建读者表reader</span><br><span class="line"></span><br><span class="line">CREATE TABLE test_sql.reader (reader_id string,</span><br><span class="line">  company string,</span><br><span class="line">  name string,</span><br><span class="line">  sex string,</span><br><span class="line">  grade string,</span><br><span class="line">  addr string);</span><br><span class="line">INSERT INTO TABLE test_sql.reader VALUES (<span class="string">'0001'</span>,<span class="string">'阿里巴巴'</span>,<span class="string">'jack'</span>,<span class="string">'男'</span>,<span class="string">'vp'</span>,<span class="string">'addr1'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.reader VALUES (<span class="string">'0002'</span>,<span class="string">'百度'</span>,<span class="string">'robin'</span>,<span class="string">'男'</span>,<span class="string">'vp'</span>,<span class="string">'addr2'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.reader VALUES (<span class="string">'0003'</span>,<span class="string">'腾讯'</span>,<span class="string">'tony'</span>,<span class="string">'男'</span>,<span class="string">'vp'</span>,<span class="string">'addr3'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.reader VALUES (<span class="string">'0004'</span>,<span class="string">'京东'</span>,<span class="string">'jasper'</span>,<span class="string">'男'</span>,<span class="string">'cfo'</span>,<span class="string">'addr4'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.reader VALUES (<span class="string">'0005'</span>,<span class="string">'网易'</span>,<span class="string">'zhangsan'</span>,<span class="string">'女'</span>,<span class="string">'ceo'</span>,<span class="string">'addr5'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.reader VALUES (<span class="string">'0006'</span>,<span class="string">'搜狐'</span>,<span class="string">'lisi'</span>,<span class="string">'女'</span>,<span class="string">'ceo'</span>,<span class="string">'addr6'</span>);</span><br><span class="line"></span><br><span class="line">-- 创建借阅记录表borrow_log</span><br><span class="line"></span><br><span class="line">CREATE TABLE test_sql.borrow_log(reader_id string,</span><br><span class="line"> book_id string,</span><br><span class="line"> borrow_date string);</span><br><span class="line"> </span><br><span class="line">INSERT INTO TABLE test_sql.borrow_log VALUES (<span class="string">'0001'</span>,<span class="string">'002'</span>,<span class="string">'2019-10-14'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.borrow_log VALUES (<span class="string">'0002'</span>,<span class="string">'001'</span>,<span class="string">'2019-10-13'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.borrow_log VALUES (<span class="string">'0003'</span>,<span class="string">'005'</span>,<span class="string">'2019-09-14'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.borrow_log VALUES (<span class="string">'0004'</span>,<span class="string">'006'</span>,<span class="string">'2019-08-15'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.borrow_log VALUES (<span class="string">'0005'</span>,<span class="string">'003'</span>,<span class="string">'2019-10-10'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.borrow_log VALUES (<span class="string">'0006'</span>,<span class="string">'004'</span>,<span class="string">'2019-17-13'</span>);</span><br><span class="line"></span><br><span class="line">(2)</span><br><span class="line">SELECT name,</span><br><span class="line">   company</span><br><span class="line">FROM test_sql.reader</span><br><span class="line">WHERE name LIKE <span class="string">'李%'</span>;</span><br><span class="line">(3)</span><br><span class="line">SELECT book_name,</span><br><span class="line">   price</span><br><span class="line">FROM test_sql.book</span><br><span class="line">WHERE OUTPUT = <span class="string">"高等教育出版社"</span></span><br><span class="line">ORDER BY price DESC;</span><br><span class="line">(4)</span><br><span class="line">SELECT sort,</span><br><span class="line">   output,</span><br><span class="line">   price</span><br><span class="line">FROM test_sql.book</span><br><span class="line">WHERE price &gt;= 10 and price &lt;= 20</span><br><span class="line">ORDER BY output,price ;</span><br><span class="line">(5)</span><br><span class="line">SELECT b.name,</span><br><span class="line">   b.company</span><br><span class="line">FROM test_sql.borrow_log a</span><br><span class="line">JOIN test_sql.reader b ON a.reader_id = b.reader_id;</span><br><span class="line">(6)</span><br><span class="line">SELECT max(price),</span><br><span class="line">   min(price),</span><br><span class="line">   avg(price)</span><br><span class="line">FROM test_sql.book</span><br><span class="line">WHERE OUTPUT = <span class="string">'科学出版社'</span>;</span><br><span class="line">(7)</span><br><span class="line">SELECT b.name,</span><br><span class="line">   b.company</span><br><span class="line">FROM</span><br><span class="line">  (SELECT reader_id</span><br><span class="line">   FROM test_sql.borrow_log</span><br><span class="line">   GROUP BY reader_id</span><br><span class="line">   HAVING count(*) &gt;= 2) a</span><br><span class="line">JOIN test_sql.reader b ON a.reader_id = b.reader_id;</span><br><span class="line"></span><br><span class="line">(8)</span><br><span class="line">CREATE TABLE test_sql.borrow_log_bak AS</span><br><span class="line">SELECT *</span><br><span class="line">FROM test_sql.borrow_log;</span><br><span class="line">(9)</span><br><span class="line">CREATE TABLE book_hive ( </span><br><span class="line">book_id string,</span><br><span class="line">SORT string, </span><br><span class="line">book_name string,</span><br><span class="line">writer string, </span><br><span class="line">OUTPUT string, </span><br><span class="line">price DECIMAL ( 10, 2 ) )</span><br><span class="line">partitioned BY ( month_part string, day_part string )</span><br><span class="line">ROW format delimited FIELDS TERMINATED BY <span class="string">'\\|'</span> stored AS textfile;</span><br><span class="line">(10)</span><br><span class="line">方式1：配置hive支持事务操作，分桶表，orc存储格式</span><br><span class="line">方式2：第一步找到要更新的数据，将要更改的字段替换为新的值，第二步找到不需要更新的数据，第三步将上两步的数据插入一张新表中。</span><br></pre></td></tr></table></figure><h2 id="第八题"><a href="#第八题" class="headerlink" title="第八题"></a>第八题</h2><h3 id="需求-7"><a href="#需求-7" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">有一个线上服务器访问日志格式如下（用sql答题）</span><br><span class="line">时间                    接口                         ip地址</span><br><span class="line">2016-11-09 14:22:05/api/user/login110.23.5.33</span><br><span class="line">2016-11-09 14:23:10/api/user/detail57.3.2.16</span><br><span class="line">2016-11-09 15:59:40/api/user/login200.6.5.166</span><br><span class="line">… …</span><br><span class="line">求11月9号下午14点（14-15点），访问/api/user/login接口的top10的ip地址</span><br></pre></td></tr></table></figure><h3 id="实现-7"><a href="#实现-7" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-6"><a href="#数据准备-6" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test8(`date` string,</span><br><span class="line">interface string,</span><br><span class="line">ip string);</span><br><span class="line"></span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES (<span class="string">'2016-11-09 11:22:05'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'110.23.5.23'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES (<span class="string">'2016-11-09 11:23:10'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'57.3.2.16'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES (<span class="string">'2016-11-09 23:59:40'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'200.6.5.166'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 11:14:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'136.79.47.70'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 11:15:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'94.144.143.141'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 11:16:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'197.161.8.206'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 12:14:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'240.227.107.145'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 13:14:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'79.130.122.205'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:14:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'65.228.251.189'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:15:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'245.23.122.44'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:17:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'22.74.142.137'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:19:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'54.93.212.87'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:20:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'218.15.167.248'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:24:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'20.117.19.75'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 15:14:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'183.162.66.97'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 16:14:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'108.181.245.147'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:17:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'22.74.142.137'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:19:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'22.74.142.137'</span>);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-5"><a href="#查询SQL-5" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT ip,</span><br><span class="line">   count(*) AS cnt</span><br><span class="line">FROM test_sql.test8</span><br><span class="line">WHERE date_format(date,<span class="string">'yyyy-MM-dd HH'</span>) &gt;= <span class="string">'2016-11-09 14'</span></span><br><span class="line">  AND date_format(date,<span class="string">'yyyy-MM-dd HH'</span>) &lt; <span class="string">'2016-11-09 15'</span></span><br><span class="line">  AND interface=<span class="string">'/api/user/login'</span></span><br><span class="line">GROUP BY ip</span><br><span class="line">ORDER BY cnt desc</span><br><span class="line">LIMIT 10;</span><br></pre></td></tr></table></figure><h2 id="第九题"><a href="#第九题" class="headerlink" title="第九题"></a>第九题</h2><h3 id="需求-8"><a href="#需求-8" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">有一个充值日志表credit_log，字段如下：</span><br><span class="line"></span><br><span class="line">`dist_id` int  <span class="string">'区组id'</span>,</span><br><span class="line">`account` string  <span class="string">'账号'</span>,</span><br><span class="line">`money` int   <span class="string">'充值金额'</span>,</span><br><span class="line">`create_time` string  <span class="string">'订单时间'</span></span><br><span class="line"></span><br><span class="line">请写出SQL语句，查询充值日志表2019年01月02号每个区组下充值额最大的账号，要求结果：</span><br><span class="line">区组id，账号，金额，充值时间</span><br></pre></td></tr></table></figure><h3 id="实现-8"><a href="#实现-8" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-7"><a href="#数据准备-7" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test9(</span><br><span class="line">dist_id string COMMENT <span class="string">'区组id'</span>,</span><br><span class="line">account string COMMENT <span class="string">'账号'</span>,</span><br><span class="line">   `money` decimal(10,2) COMMENT <span class="string">'充值金额'</span>,</span><br><span class="line">create_time string COMMENT <span class="string">'订单时间'</span>);</span><br><span class="line"></span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'11'</span>,100006,<span class="string">'2019-01-02 13:00:01'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'22'</span>,110000,<span class="string">'2019-01-02 13:00:02'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'33'</span>,102000,<span class="string">'2019-01-02 13:00:03'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'44'</span>,100300,<span class="string">'2019-01-02 13:00:04'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'55'</span>,100040,<span class="string">'2019-01-02 13:00:05'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'66'</span>,100005,<span class="string">'2019-01-02 13:00:06'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'77'</span>,180000,<span class="string">'2019-01-03 13:00:07'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'88'</span>,106000,<span class="string">'2019-01-02 13:00:08'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'99'</span>,100400,<span class="string">'2019-01-02 13:00:09'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'12'</span>,100030,<span class="string">'2019-01-02 13:00:10'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'13'</span>,100003,<span class="string">'2019-01-02 13:00:20'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'14'</span>,100020,<span class="string">'2019-01-02 13:00:30'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'15'</span>,100500,<span class="string">'2019-01-02 13:00:40'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'16'</span>,106000,<span class="string">'2019-01-02 13:00:50'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'17'</span>,100800,<span class="string">'2019-01-02 13:00:59'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'2'</span>,<span class="string">'18'</span>,100800,<span class="string">'2019-01-02 13:00:11'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'2'</span>,<span class="string">'19'</span>,100030,<span class="string">'2019-01-02 13:00:12'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'2'</span>,<span class="string">'10'</span>,100000,<span class="string">'2019-01-02 13:00:13'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'2'</span>,<span class="string">'45'</span>,100010,<span class="string">'2019-01-02 13:00:14'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'2'</span>,<span class="string">'78'</span>,100070,<span class="string">'2019-01-02 13:00:15'</span>);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-6"><a href="#查询SQL-6" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">WITH TEMP AS</span><br><span class="line">  (SELECT dist_id,</span><br><span class="line">  account,</span><br><span class="line">  sum(`money`) sum_money</span><br><span class="line">   FROM test_sql.test9</span><br><span class="line">   WHERE date_format(create_time,<span class="string">'yyyy-MM-dd'</span>) = <span class="string">'2019-01-02'</span></span><br><span class="line">   GROUP BY dist_id,</span><br><span class="line">account)</span><br><span class="line">SELECT t1.dist_id,</span><br><span class="line">   t1.account,</span><br><span class="line">   t1.sum_money</span><br><span class="line">FROM</span><br><span class="line">  (SELECT temp.dist_id,</span><br><span class="line">  temp.account,</span><br><span class="line">  temp.sum_money,</span><br><span class="line">  rank() over(partition BY temp.dist_id</span><br><span class="line">  ORDER BY temp.sum_money DESC) ranks</span><br><span class="line">   FROM TEMP) t1</span><br><span class="line">WHERE ranks = 1</span><br></pre></td></tr></table></figure><h2 id="第十题"><a href="#第十题" class="headerlink" title="第十题"></a>第十题</h2><h3 id="需求-9"><a href="#需求-9" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">有一个账号表如下，请写出SQL语句，查询各自区组的money排名前十的账号（分组取前10）</span><br><span class="line">dist_id string  <span class="string">'区组id'</span>,</span><br><span class="line">account string  <span class="string">'账号'</span>,</span><br><span class="line">gold     int    <span class="string">'金币'</span></span><br></pre></td></tr></table></figure><h3 id="实现-9"><a href="#实现-9" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-8"><a href="#数据准备-8" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test10(</span><br><span class="line">`dist_id` string COMMENT <span class="string">'区组id'</span>,</span><br><span class="line">`account` string COMMENT <span class="string">'账号'</span>,</span><br><span class="line">`gold` int COMMENT <span class="string">'金币'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'77'</span>,18);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'88'</span>,106);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'99'</span>,10);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'12'</span>,13);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'13'</span>,14);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'14'</span>,25);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'15'</span>,36);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'16'</span>,12);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'17'</span>,158);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'2'</span>,<span class="string">'18'</span>,12);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'2'</span>,<span class="string">'19'</span>,44);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'2'</span>,<span class="string">'10'</span>,66);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'2'</span>,<span class="string">'45'</span>,80);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'2'</span>,<span class="string">'78'</span>,98);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-7"><a href="#查询SQL-7" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT dist_id,</span><br><span class="line">   account,</span><br><span class="line">   gold</span><br><span class="line">FROM</span><br><span class="line">(SELECT dist_id,</span><br><span class="line">  account,</span><br><span class="line">  gold,</span><br><span class="line">  row_number () over (PARTITION BY dist_id</span><br><span class="line">  ORDER BY gold DESC) rank</span><br><span class="line">FROM test_sql.test10) t</span><br><span class="line">WHERE rank &lt;= 10</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Impala使用的端口</title>
      <link href="/2019/08/29/Impala%E4%BD%BF%E7%94%A8%E7%9A%84%E7%AB%AF%E5%8F%A3/"/>
      <url>/2019/08/29/Impala%E4%BD%BF%E7%94%A8%E7%9A%84%E7%AB%AF%E5%8F%A3/</url>
      
        <content type="html"><![CDATA[<p>本文主要介绍了Impala所使用的端口号，在部署Impala的时候，确保下面列出的端口是开启的。</p><a id="more"></a><table><thead><tr><th align="center">组件</th><th>服务</th><th>端口</th><th align="center"><span style="white-space:nowrap;">访问需求&emsp;&emsp;</span></th><th>备注</th></tr></thead><tbody><tr><td align="center">Impala Daemon</td><td>Impala Daemon Frontend Port</td><td>21000</td><td align="center">外部</td><td>被 impala-shell, Beeswax, Cloudera ODBC 1.2 驱动 用于传递命令和接收结果</td></tr><tr><td align="center">Impala Daemon</td><td>Impala Daemon Frontend Port</td><td>21050</td><td align="center">外部</td><td>被使用 JDBC 或 Cloudera ODBC 2.0 及以上驱动的诸如 BI 工具之类的应用用来传递命令和接收结果</td></tr><tr><td align="center">Impala Daemon</td><td>Impala Daemon Backend Port</td><td>22000</td><td align="center">内部</td><td>仅内部使用。Impala</td></tr><tr><td align="center">Impala Daemon</td><td>StateStoreSubscriber Service Port</td><td>23000</td><td align="center">内部</td><td>仅内部使用。Impala 守护进程监听该端口接收来源于 state store 的更新</td></tr><tr><td align="center">Catalog Daemon</td><td>StateStoreSubscriber Service Port</td><td>23020</td><td align="center">内部</td><td>仅内部使用，catalog daemon监听该端口接收来源于 state store 的更新</td></tr><tr><td align="center">Impala Daemon</td><td>Impala Daemon HTTP Server Port</td><td>25000</td><td align="center">外部</td><td>Impala Daemon的Web端口，用于管理员监控和线上故障排查</td></tr><tr><td align="center">Impala StateStore Daemon</td><td>StateStore HTTP Server Port</td><td>25010</td><td align="center">外部</td><td>StateStore的Web端口，用于管理员监控和线上故障排查</td></tr><tr><td align="center">Impala Catalog Daemon</td><td>Catalog HTTP Server Port</td><td>25020</td><td align="center">外部</td><td>Catalog的Web端口，用于管理员监控和线上故障排查，从Impala1.2开始加入</td></tr><tr><td align="center">Impala StateStore Daemon</td><td>StateStore Service Port</td><td>24000</td><td align="center">内部</td><td>仅内部使用，statestore daemon监听的端口，用于registration/unregistration请求</td></tr><tr><td align="center">Impala Catalog Daemon</td><td>Catalog Service Port</td><td>26000</td><td align="center">内部</td><td>仅内部使用，catalog服务使用此端口与Impala Daemon进行通信，从Impala1.2开始加入</td></tr><tr><td align="center">Impala Daemon</td><td>KRPC Port</td><td>27000</td><td align="center">内部</td><td>仅内部使用，Impala daemon使用此端口进行基于krpc的相互通信。</td></tr></tbody></table><hr><p>Refrence:<a href="https://www.cloudera.com/documentation/enterprise/6/latest/topics/impala_ports.html#ports" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/6/latest/topics/impala_ports.html#ports</a></p>]]></content>
      
      
      <categories>
          
          <category> Impala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Impala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban安装部署</title>
      <link href="/2019/08/28/Azkaban%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
      <url>/2019/08/28/Azkaban%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<p>Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。</p><a id="more"></a><h1 id="安装前准备"><a href="#安装前准备" class="headerlink" title="安装前准备"></a>安装前准备</h1><p>(1)将Azkaban Web服务器、Azkaban执行服务器要安装机器的/opt/software目录下：  </p><ul><li>azkaban-web-server-2.5.0.tar.gz  </li><li>azkaban-executor-server-2.5.0.tar.gz  </li><li>azkaban-sql-script-2.5.0.tar.gz  </li><li>mysql-libs.zip  </li></ul><p>(2)目前azkaban只支持 mysql作为元数据库，需安装mysql，本文档中默认已安装好mysql服务器</p><h1 id="安装Azkaban"><a href="#安装Azkaban" class="headerlink" title="安装Azkaban"></a>安装Azkaban</h1><p>(1)在/opt/module/目录下创建azkaban目录<br>(2)解压azkaban-web-server-2.5.0.tar.gz、azkaban-executor-server-2.5.0.tar.gz、azkaban-sql-script-2.5.0.tar.gz到/opt/module/azkaban目录下<br>解压完成后的文件夹如下图所示：<br><img src="//jiamaoxiang.top/2019/08/28/Azkaban安装部署/1.png" alt><br>(3)初始化Azkaban的元数据库<br>登录mysql，创建azkaban的数据库，并执行脚本create-all-sql-2.5.0.sql，如下所示：  </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; create database azkaban;  </span><br><span class="line">Query OK, 1 row affected (0.00 sec)  </span><br><span class="line">mysql&gt; use azkaban;  </span><br><span class="line">Database changed  </span><br><span class="line">mysql&gt; <span class="built_in">source</span> /opt/module/azkaban/azkaban-2.5.0/create-all-sql-2.5.0.sql</span><br></pre></td></tr></table></figure><h2 id="创建SSL配置"><a href="#创建SSL配置" class="headerlink" title="创建SSL配置"></a>创建SSL配置</h2><p>(1)生成 keystore的密码及相应信息  </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban]$ keytool -keystore keystore -<span class="built_in">alias</span> jetty -genkey -keyalg RSA  </span><br><span class="line">输入keystore密码：   </span><br><span class="line">再次输入新密码:    </span><br><span class="line">您的名字与姓氏是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您的组织单位名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您的组织名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您所在的城市或区域名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您所在的州或省份名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">该单位的两字母国家代码是什么    </span><br><span class="line">[Unknown]：  CN    </span><br><span class="line">CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN 正确吗？  </span><br><span class="line">[否]：  y    </span><br><span class="line"> </span><br><span class="line">输入&lt;jetty&gt;的主密码    </span><br><span class="line">（如果和 keystore 密码相同，按回车）  ：</span><br></pre></td></tr></table></figure><p>(2)将keystore 考贝到 azkaban web服务器根目录中</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban]<span class="comment"># mv keystore  azkaban-web-2.5.0/</span></span><br></pre></td></tr></table></figure><h2 id="Web服务器配置"><a href="#Web服务器配置" class="headerlink" title="Web服务器配置"></a>Web服务器配置</h2><p>(1)进入azkaban web服务器安装目录 conf目录，修改azkaban.properties文件<br>(2)按照如下配置修改azkaban.properties文件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Azkaban Personalization Settings      </span></span><br><span class="line"><span class="comment">#服务器UI名称,用于服务器上方显示的名字      </span></span><br><span class="line">azkaban.name=Test      </span><br><span class="line"><span class="comment">#描述                            </span></span><br><span class="line">azkaban.label=My Local Azkaban     </span><br><span class="line"><span class="comment">#UI颜色                         </span></span><br><span class="line">azkaban.color=<span class="comment">#FF3601                                         </span></span><br><span class="line">azkaban.default.servlet.path=/index  </span><br><span class="line"><span class="comment">#根web目录,配置绝对路径，即web的目录  </span></span><br><span class="line">web.resource.dir=/opt/module/azkaban/azkaban-web-2.5.0/web   </span><br><span class="line"><span class="comment">#默认时区,已改为亚洲/上海 默认为美国                                            </span></span><br><span class="line">default.timezone.id=Asia/Shanghai                           </span><br><span class="line"><span class="comment">#用户权限管理默认类  </span></span><br><span class="line">user.manager.class=azkaban.user.XmlUserManager    </span><br><span class="line"><span class="comment">#用户配置,配置绝对路径，即azkaban-users.xml的路径     </span></span><br><span class="line">user.manager.xml.file=/opt/module/azkaban/azkaban-web-2.5.0/conf/azkaban-users.xml             </span><br><span class="line"><span class="comment">#Loader for projects .global配置文件所在位置,即global.properties绝对路径    </span></span><br><span class="line">executor.global.properties=/opt/module/azkaban/azkaban-executor-2.5.0/conf/global.properties    </span><br><span class="line">azkaban.project.dir=projects                                                 </span><br><span class="line"><span class="comment">#数据库类型  </span></span><br><span class="line">database.type=mysql                                                            </span><br><span class="line">mysql.port=3306                                                                  </span><br><span class="line">mysql.host=cdh01                                                    </span><br><span class="line">mysql.database=azkaban                                                      </span><br><span class="line">mysql.user=root  </span><br><span class="line"><span class="comment">#数据库密码                                                               </span></span><br><span class="line">mysql.password=123qwe                                                     </span><br><span class="line">mysql.numconnections=100                                                </span><br><span class="line"><span class="comment"># Velocity dev mode   </span></span><br><span class="line">velocity.dev.mode=<span class="literal">false</span>  </span><br><span class="line"><span class="comment"># Jetty服务器属性.  </span></span><br><span class="line"><span class="comment">#最大线程数     </span></span><br><span class="line">jetty.maxThreads=25   </span><br><span class="line"><span class="comment">#Jetty SSL端口                                                                 </span></span><br><span class="line">jetty.ssl.port=8443  </span><br><span class="line"><span class="comment">#Jetty端口                                                                      </span></span><br><span class="line">jetty.port=8081    </span><br><span class="line"><span class="comment">#SSL文件名,即keystore绝对路径                                                                              </span></span><br><span class="line">jetty.keystore=/opt/module/azkaban/azkaban-web-2.5.0/keystore    </span><br><span class="line"><span class="comment">#SSL文件密码,本配置与keystore密码相同                                                           </span></span><br><span class="line">jetty.password=123qwe  </span><br><span class="line"><span class="comment">#Jetty主密码 与 keystore文件相同                                                          </span></span><br><span class="line">jetty.keypassword=123qwe  </span><br><span class="line"><span class="comment">#SSL文件名,即keystore绝对路径                                                            </span></span><br><span class="line">jetty.truststore=/opt/module/azkaban/azkaban-web-2.5.0/keystore    </span><br><span class="line"><span class="comment"># SSL文件密码                                                             </span></span><br><span class="line">jetty.trustpassword=123qwe                                                    </span><br><span class="line"><span class="comment"># 执行服务器属性, 执行服务器端口  </span></span><br><span class="line">executor.port=12321                                                                </span><br><span class="line"><span class="comment"># 邮件设置,发送邮箱    </span></span><br><span class="line">mail.sender=xxxxxxxx@163.com    </span><br><span class="line"><span class="comment">#发送邮箱smtp地址                                           </span></span><br><span class="line">mail.host=smtp.163.com     </span><br><span class="line"><span class="comment">#发送邮件时显示的名称                                                            </span></span><br><span class="line">mail.user=xxxxxxxx  </span><br><span class="line"><span class="comment">#邮箱密码                                            </span></span><br><span class="line">mail.password=**********   </span><br><span class="line"><span class="comment">#任务失败时发送邮件的地址                                                        </span></span><br><span class="line">job.failure.email=xxxxxxxx@163.com   </span><br><span class="line"><span class="comment">#任务成功时发送邮件的地址                                 </span></span><br><span class="line">job.success.email=xxxxxxxx@163.com                            </span><br><span class="line">lockdown.create.projects=<span class="literal">false</span>    </span><br><span class="line"><span class="comment">#缓存目录                                            </span></span><br><span class="line">cache.directory=cache</span><br></pre></td></tr></table></figure><p>(3)web服务器用户配置<br>在azkaban web服务器安装目录 conf目录，按照如下配置修改azkaban-users.xml 文件，增加管理员用户。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;azkaban-users&gt;  </span><br><span class="line">       &lt;user username=<span class="string">"azkaban"</span> password=<span class="string">"azkaban"</span> roles=<span class="string">"admin"</span> groups=<span class="string">"azkaban"</span> /&gt;    </span><br><span class="line">       &lt;user username=<span class="string">"metrics"</span> password=<span class="string">"metrics"</span> roles=<span class="string">"metrics"</span>/&gt;  </span><br><span class="line">       &lt;user username=<span class="string">"admin"</span> password=<span class="string">"admin"</span> roles=<span class="string">"admin,metrics"</span> /&gt;  </span><br><span class="line">       &lt;role name=<span class="string">"admin"</span> permissions=<span class="string">"ADMIN"</span> /&gt;  </span><br><span class="line">       &lt;role name=<span class="string">"metrics"</span> permissions=<span class="string">"METRICS"</span>/&gt;    </span><br><span class="line">&lt;/azkaban-users&gt;</span><br></pre></td></tr></table></figure><h2 id="executor服务器配置"><a href="#executor服务器配置" class="headerlink" title="executor服务器配置"></a>executor服务器配置</h2><p>(1)进入executor安装目录，修改azkaban.properties</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Azkaban  </span></span><br><span class="line">default.timezone.id=Asia/Shanghai  </span><br><span class="line"><span class="comment"># Azkaban JobTypes Plugins  </span></span><br><span class="line">azkaban.jobtype.plugin.dir=./../plugins/jobtypes  </span><br><span class="line"><span class="comment">#Loader for projects  </span></span><br><span class="line">executor.global.properties=/opt/module/azkaban/azkaban-executor-2.5.0/conf/global.properties  </span><br><span class="line">azkaban.project.dir=projects  </span><br><span class="line">database.type=mysql  </span><br><span class="line">mysql.port=3306  </span><br><span class="line">mysql.host=cdh01  </span><br><span class="line">mysql.database=azkaban  </span><br><span class="line">mysql.user=root  </span><br><span class="line">mysql.password=123qwe    </span><br><span class="line">mysql.numconnections=100    </span><br><span class="line"><span class="comment"># Azkaban Executor settings  </span></span><br><span class="line">executor.maxThreads=50  </span><br><span class="line">executor.port=12321  </span><br><span class="line">executor.flow.threads=30</span><br></pre></td></tr></table></figure><h1 id="启动web服务器"><a href="#启动web服务器" class="headerlink" title="启动web服务器"></a>启动web服务器</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban-web-2.5.0]<span class="comment"># bin/azkaban-web-start.sh  &amp;</span></span><br></pre></td></tr></table></figure><h1 id="启动executor服务器"><a href="#启动executor服务器" class="headerlink" title="启动executor服务器"></a>启动executor服务器</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban-executor-2.5.0]<span class="comment"># bin/azkaban-executor-start.sh  &amp;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的数据类型</title>
      <link href="/2019/08/27/Flink%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
      <url>/2019/08/27/Flink%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>Flink使用type information来代表数据类型，Flink还具有一个类型提取系统，该系统分析函数的输入和返回类型，以自动获取类型信息(type information)，从而获得序列化程序和反序列化程序。但是，在某些情况下，例如lambda函数或泛型类型，需要显式地提供类型信息((type information)),从而提高其性能。本文主要讨论包括：(1)Flink支持的数据类型,(2)如何为数据类型创建type information，（3）如果无法自动推断函数的返回类型，如何使用提示(hints)来帮助Flink的类型系统识别类型信息。</p><a id="more"></a><h2 id="支持的数据类型"><a href="#支持的数据类型" class="headerlink" title="支持的数据类型"></a>支持的数据类型</h2><p>Flink支持Java和Scala中所有常见的数据类型，使用比较广泛的类型主要包括以下五种：</p><ul><li>原始类型  </li><li>Java和Scala的tuple类型  </li><li>Scala样例类  </li><li>POJO类型  </li><li>一些特殊的类型  </li></ul><p><strong>NOTE：</strong>不能被处理的类型将会被视为普通的数据类型，通过Kyro序列化框架进行序列化。</p><h3 id="原始类型"><a href="#原始类型" class="headerlink" title="原始类型"></a>原始类型</h3><p>Flink支持所有Java和Scala的原始类型，比如Int(Java中的Integer)，String、Double等。下面的例子是处理一个Long类型的数据流，处理每个元素+1  </p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> numbers: <span class="type">DataStream</span>[<span class="type">Long</span>] = env.fromElements(<span class="number">1</span>L, <span class="number">2</span>L,<span class="number">3</span>L, <span class="number">4</span>L)  </span><br><span class="line">numbers.map( n =&gt; n + <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="Java和Scala的tuple类型"><a href="#Java和Scala的tuple类型" class="headerlink" title="Java和Scala的tuple类型"></a>Java和Scala的tuple类型</h3><p>基于Scala的DataStream API使用的Scala的tuple。下面的例子是过滤一个具有两个字段的tuple类型的数据流.  </p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// DataStream of Tuple2[String, Integer] for Person(name,age)  </span></span><br><span class="line"><span class="keyword">val</span> persons: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Integer</span>)] = env.fromElements((<span class="string">"Adam"</span>, <span class="number">17</span>),(<span class="string">"Sarah"</span>, <span class="number">23</span>))  </span><br><span class="line"><span class="comment">// filter for persons of age &gt; 18  </span></span><br><span class="line">persons.filter(p =&gt; p._2 &gt; <span class="number">18</span>)</span><br></pre></td></tr></table></figure><p>Flink提供了有效的Java tuple实现，Flink的Java tuple最多包括25个字段，分别为tuple1，tuple2，直到tuple25，tuple类型是强类型的。使用Java DataStream API重写上面的例子:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// DataStream of Tuple2&lt;String, Integer&gt; for Person(name,age)  </span></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; persons =env.fromElements(Tuple2.of(<span class="string">"Adam"</span>, <span class="number">17</span>),Tuple2.of(<span class="string">"Sarah"</span>,<span class="number">23</span>));  </span><br><span class="line"><span class="comment">// filter for persons of age &gt; 18  </span></span><br><span class="line">persons.filter(p -&gt; p.f1 &gt; <span class="number">18</span>);</span><br></pre></td></tr></table></figure><p>Tuple字段可以通过使用f0，f1，f2的形式访问，也可以通过getField(int pos)方法访问，参数的索引起始值为0，比如:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Tuple2&lt;String, Integer&gt; personTuple = Tuple2.of(<span class="string">"Alex"</span>,<span class="string">"42"</span>);  </span><br><span class="line">Integer age = personTuple.getField(<span class="number">1</span>); <span class="comment">// age = 42</span></span><br></pre></td></tr></table></figure><p>与Scala相比，Flink的Java tuple是可变的，所以tuple的元素值是可以被重新复制的。Function可以重用Java tuple,从而减小垃圾回收的压力。下面的例子展示了如何更新一个tuple字段值</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">personTuple.f1 = <span class="number">42</span>; <span class="comment">// set the 2nd field to 42     </span></span><br><span class="line">personTuple.setField(<span class="number">43</span>, <span class="number">1</span>); <span class="comment">// set the 2nd field to 43</span></span><br></pre></td></tr></table></figure><h3 id="Scala的样例类"><a href="#Scala的样例类" class="headerlink" title="Scala的样例类"></a>Scala的样例类</h3><p>Flink支持Scala的样例类，可以通过字段名称来访问样例类的字段，下面的例子定义了一个<code>Person</code>样例类，该样例类有两个字段：<code>name</code>和<code>age</code>,按<code>age</code>过滤DataStream，如下所示</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)  </span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">persons</span></span>: <span class="type">DataStream</span>[<span class="type">Person</span>] = env.fromElements(<span class="type">Person</span>(<span class="string">"Adam"</span>, <span class="number">17</span>),<span class="type">Person</span>(<span class="string">"Sarah"</span>, <span class="number">23</span>))  </span><br><span class="line"><span class="comment">// filter for persons with age &gt; 18  </span></span><br><span class="line">persons.filter(p =&gt; p.age &gt; <span class="number">18</span>)</span><br></pre></td></tr></table></figure><h3 id="POJO"><a href="#POJO" class="headerlink" title="POJO"></a>POJO</h3><p>Flink接受的POJO类型需满足以下条件：</p><ul><li>public 类  </li><li>无参的共有构造方法  </li><li>所有字段都是public的，可以通过getter和setter方法访问  </li><li>所有字段类型必须是Flink能够支持的<br>下面的例子定义一个<code>Person</code>POJO</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;  </span><br><span class="line"><span class="comment">// both fields are public  </span></span><br><span class="line"><span class="keyword">public</span> String name;  </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">int</span> age;  </span><br><span class="line"><span class="comment">// default constructor is present  </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">()</span> </span>&#123;&#125;  </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(String name, <span class="keyword">int</span> age)</span> </span>&#123;  </span><br><span class="line"><span class="keyword">this</span>.name = name;  </span><br><span class="line"><span class="keyword">this</span>.age = age;  </span><br><span class="line">&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">DataStream&lt;Person&gt; persons = env.fromElements(   </span><br><span class="line"><span class="keyword">new</span> Person(<span class="string">"Alex"</span>, <span class="number">42</span>),  </span><br><span class="line"><span class="keyword">new</span> Person(<span class="string">"Wendy"</span>, <span class="number">23</span>));</span><br></pre></td></tr></table></figure><h3 id="一些特殊的类型"><a href="#一些特殊的类型" class="headerlink" title="一些特殊的类型"></a>一些特殊的类型</h3><p>Flink支持一些有特殊作用的数据类型，比如Array，Java中的ArrayList、HashMap和Enum等，也支持Hadoop的Writable类型。  </p><h2 id="为数据类型创建类型信息-type-information"><a href="#为数据类型创建类型信息-type-information" class="headerlink" title="为数据类型创建类型信息(type information)"></a>为数据类型创建类型信息(type information)</h2><h2 id="显示地指定类型信息-type-information"><a href="#显示地指定类型信息-type-information" class="headerlink" title="显示地指定类型信息(type information)"></a>显示地指定类型信息(type information)</h2>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于SparkStreaming的日志分析项目</title>
      <link href="/2019/08/26/%E5%9F%BA%E4%BA%8ESparkStreaming%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E9%A1%B9%E7%9B%AE/"/>
      <url>/2019/08/26/%E5%9F%BA%E4%BA%8ESparkStreaming%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E9%A1%B9%E7%9B%AE/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp;基于SparkStreaming实现实时的日志分析，首先基于discuz搭建一个论坛平台，然后将该论坛的日志写入到指定文件，最后通过SparkStreaming实时对日志进行分析。</p><a id="more"></a><h1 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h1><ul><li>统计指定时间段的热门文章</li></ul><ul><li>统计指定时间段内的最受欢迎的用户（以 ip 为单位）</li></ul><ul><li>统计指定时间段内的不同模块的访问量  </li></ul><h1 id="项目架构"><a href="#项目架构" class="headerlink" title="项目架构"></a>项目架构</h1><p><img src="//jiamaoxiang.top/2019/08/26/基于SparkStreaming的日志分析项目/%E9%A1%B9%E7%9B%AE%E6%9E%B6%E6%9E%84.png" alt></p><h1 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h1><p>resources<br>&emsp;&emsp;&emsp;&emsp;access_log.txt:日志样例<br>&emsp;&emsp;&emsp;&emsp;log_sta.conf ：配置文件<br>scala.com.jmx.analysis<br>&emsp;&emsp;&emsp;&emsp;AccessLogParser.scala :日志解析<br>&emsp;&emsp;&emsp;&emsp;logAnalysis：日志分析<br>scala.com.jmx.util<br>&emsp;&emsp;&emsp;&emsp;Utility.scala:工具类<br>scala<br>&emsp;&emsp;&emsp;&emsp;Run：驱动程序(main)<br>具体代码详见<a href="https://github.com/jiamx/log_analysis" target="_blank" rel="noopener">github</a></p><h1 id="搭建discuz论坛"><a href="#搭建discuz论坛" class="headerlink" title="搭建discuz论坛"></a>搭建discuz论坛</h1><h2 id="安装XAMPP"><a href="#安装XAMPP" class="headerlink" title="安装XAMPP"></a>安装XAMPP</h2><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><p><code>wget https://www.apachefriends.org/xampp-files/5.6.33/xampp-linux-x64-5.6.33-0-installer.run</code></p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><code># 赋予文件执行权限</code><br><code>chmod u+x xampp-linux-x64-5.6.33-0-installer.run</code><br><code># 运行安装文件</code><br>`./xampp-linux-x64-5.6.33-0-installer.run``</p><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><p>将以下内容加入到 ~/.bash_profile<br><code>export XAMPP=/opt/lampp/</code><br><code>export PATH=$PATH:$XAMPP:$XAMPP/bin</code>   </p><h3 id="刷新环境变量"><a href="#刷新环境变量" class="headerlink" title="刷新环境变量"></a>刷新环境变量</h3><p><code>source ~/.bash_profile</code></p><h3 id="启动XAMPP"><a href="#启动XAMPP" class="headerlink" title="启动XAMPP"></a>启动XAMPP</h3><p><code>xampp restart</code></p><h2 id="root用户密码和权限修改"><a href="#root用户密码和权限修改" class="headerlink" title="root用户密码和权限修改"></a>root用户密码和权限修改</h2><p><code>#修改root用户密码为123</code><br><code>update mysql.user set password=PASSWORD(&#39;123&#39;) where user=&#39;root&#39;;</code><br><code>flush privileges;</code><br><code>#赋予root用户远程登录权限</code><br><code>grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;123&#39; with grant option;</code><br><code>flush privileges;</code>  </p><h2 id="安装Discuz"><a href="#安装Discuz" class="headerlink" title="安装Discuz"></a>安装Discuz</h2><h3 id="下载discuz"><a href="#下载discuz" class="headerlink" title="下载discuz"></a>下载discuz</h3><p><code>wget http://download.comsenz.com/DiscuzX/3.2/Discuz_X3.2_SC_UTF8.zip</code>  </p><h3 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h3><p><code>#删除原有的web应用</code><br><code>rm -rf /opt/lampp/htdocs/*</code><br><code>unzip Discuz_X3.2_SC_UTF8.zip –d /opt/lampp/htdocs/</code><br><code>cd /opt/lampp/htdocs/</code><br><code>mv upload/*</code><br><code>#修改目录权限</code><br><code>chmod 777 -R /opt/lampp/htdocs/config/</code><br><code>chmod 777 -R /opt/lampp/htdocs/data/</code><br><code>chmod 777 -R /opt/lampp/htdocs/uc_client/</code><br><code>chmod 777 -R /opt/lampp/htdocs/uc_server/</code>  </p><h2 id="Discuz基本操作"><a href="#Discuz基本操作" class="headerlink" title="Discuz基本操作"></a>Discuz基本操作</h2><h3 id="自定义版块"><a href="#自定义版块" class="headerlink" title="自定义版块"></a>自定义版块</h3><ul><li>进入discuz后台：<a href="http://slave1/admin.php" target="_blank" rel="noopener">http://slave1/admin.php</a>  </li><li>点击顶部的“论坛”菜单  </li><li>按照页面提示创建所需版本，可以创建父子版块  </li></ul><h3 id="查看访问日志"><a href="#查看访问日志" class="headerlink" title="查看访问日志"></a>查看访问日志</h3><p>日志默认地址<br><code>/opt/lampp/logs/access_log</code><br>实时查看日志命令<br><code>tail –f /opt/lampp/logs/access_log</code>  </p><h2 id="Discuz帖子-版块存储简介"><a href="#Discuz帖子-版块存储简介" class="headerlink" title="Discuz帖子/版块存储简介"></a>Discuz帖子/版块存储简介</h2><p><code>mysql -uroot -p123 ultrax # 登录ultrax数据库</code><br><code>查看包含帖子id及标题对应关系的表</code><br><code>#tid, subject（文章id、标题）</code><br><code>select tid, subject from pre_forum_post limit 10;</code><br><code>#fid, name（版块id、标题）</code><br><code>select fid, name from pre_forum_forum limit 40;</code>  </p><h2 id="修改日志格式"><a href="#修改日志格式" class="headerlink" title="修改日志格式"></a>修改日志格式</h2><h3 id="找到Apache配置文件"><a href="#找到Apache配置文件" class="headerlink" title="找到Apache配置文件"></a>找到Apache配置文件</h3><p>Apache配置文件名称为httpd.conf，所在目录为 /opt/lampp/etc/ ，完整路径为 /opt/lampp/etc/httpd.conf</p><h3 id="修改日志格式-1"><a href="#修改日志格式-1" class="headerlink" title="修改日志格式"></a>修改日志格式</h3><p>关闭通用日志文件的使用<br><code>CustomLog &quot;logs/access_log&quot; common</code><br>启用组合日志文件<br><code>CustomLog &quot;logs/access_log&quot; combined</code><br>重新加载配置文件<br><code>xampp reload</code><br>检查访问日志<br><code>tail -f /opt/lampp/logs/access_log</code>  </p><h3 id="Flume与Kafka配置"><a href="#Flume与Kafka配置" class="headerlink" title="Flume与Kafka配置"></a>Flume与Kafka配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#agent的名称为a1  </span><br><span class="line">a1.sources = source1  </span><br><span class="line">a1.channels = channel1  </span><br><span class="line">a1.sinks = sink1</span><br><span class="line">#set source</span><br><span class="line">a1.sources.source1.type = TAILDIR  </span><br><span class="line">a1.sources.source1.filegroups = f1  </span><br><span class="line">a1.sources.source1.filegroups.f1 = /opt/lampp/logs/access_log  </span><br><span class="line">a1sources.source1.fileHeader = flase  </span><br><span class="line">#set sink</span><br><span class="line">a1.sinks.sink1.type = org.apache.flume.sink.kafka.KafkaSink  </span><br><span class="line">a1.sinks.sink1.brokerList=kms-2.apache.com:9092,kms-3.apache.com:9092,kms-4.apache.com:9092    </span><br><span class="line">​a1.sinks.sink1.topic= discuzlog  </span><br><span class="line">​a1.sinks.sink1.kafka.flumeBatchSize = 20  </span><br><span class="line">​a1.sinks.sink1.kafka.producer.acks = 1  </span><br><span class="line">​a1.sinks.sink1.kafka.producer.linger.ms = 1  </span><br><span class="line">​a1.sinks.sink1.kafka.producer.compression.type = snappy  </span><br><span class="line">#set channel</span><br><span class="line">​a1.channels.channel1.type = file  </span><br><span class="line">​a1.channels.channel1.checkpointDir = /home/kms/data/flume_data/checkpoint  </span><br><span class="line">​a1.channels.channel1.dataDirs= /home/kms/data/flume_data/data  </span><br><span class="line">#bind</span><br><span class="line">​a1.sources.source1.channels = channel1  </span><br><span class="line">​a1.sinks.sink1.channel = channel1</span><br></pre></td></tr></table></figure><h2 id="创建MySQL数据库和所需要的表"><a href="#创建MySQL数据库和所需要的表" class="headerlink" title="创建MySQL数据库和所需要的表"></a>创建MySQL数据库和所需要的表</h2><p><strong>创建数据库</strong>  </p><p><code>CREATE DATABASE</code>statistics<code>CHARACTER SET &#39;utf8&#39; COLLATE &#39;utf8_general_ci&#39;;</code>  </p><p><strong>创建表:</strong><br>&emsp;&emsp;特定时间段内不同ip的访问次数：client_ip_access<br>CREATE TABLE client_ip_access (<br>&emsp;&emsp;&emsp;&emsp;client_ip text COMMENT ‘客户端ip’,<br>&emsp;&emsp;&emsp;&emsp;sum BIGINT ( 20 ) NOT NULL COMMENT ‘访问次数’,<br>&emsp;&emsp;&emsp;&emsp;time text NOT NULL COMMENT ‘统计时间’<br>) ENGINE = INNODB DEFAULT CHARSET = utf8;  </p><p>&emsp;&emsp;特定时间段内不同文章的访问次数：hot_article<br>CREATE TABLE hot_article (<br>&emsp;&emsp;&emsp;&emsp;article_id text COMMENT ‘文章id’,<br>&emsp;&emsp;&emsp;&emsp;subject text NOT NULL COMMENT ‘文章标题’,<br>&emsp;&emsp;&emsp;&emsp;sum BIGINT ( 20 ) NOT NULL COMMENT ‘访问次数’,<br>&emsp;&emsp;&emsp;&emsp;time text NOT NULL COMMENT ‘统计时间’<br>) ENGINE = INNODB DEFAULT CHARSET = utf8;  </p><p>&emsp;&emsp;特定时间段内不同版块的访问次数：hot_section<br>CREATE TABLE hot_section (<br>&emsp;&emsp;&emsp;&emsp;section_id text COMMENT ‘版块id’,<br>&emsp;&emsp;&emsp;&emsp;name text NOT NULL COMMENT ‘版块标题’,<br>&emsp;&emsp;&emsp;&emsp;sum BIGINT ( 20 ) NOT NULL COMMENT ‘访问次数’,<br>&emsp;&emsp;&emsp;&emsp;time text NOT NULL COMMENT ‘统计时间’<br>) ENGINE = INNODB DEFAULT CHARSET = utf8; </p><h2 id="打包部署存在的问题"><a href="#打包部署存在的问题" class="headerlink" title="打包部署存在的问题"></a>打包部署存在的问题</h2><p><strong>问题1</strong> </p><pre><code>Exception in thread &quot;main&quot; java.lang.SecurityException: Invalid signature file digest for Manifest main attributes</code></pre><p><strong>解决方式</strong>  </p><p>原因:使用sbt打包的时候导致某些包的重复引用，所以打包之后的META-INF的目录下多出了一些<em>.SF,</em>.DSA,*.RSA文件  </p><p>解决办法：删除掉多于的<em>.SF,</em>.DSA,*.RSA文件  </p><p><code>zip -d log_analysis-1.0-SNAPSHOT.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF</code>  </p><p><strong>问题2</strong>    </p><pre><code>Exception in thread &quot;main&quot; java.io.FileNotFoundException: File file:/data/spark_data/history/event-log does not exist</code></pre><p><strong>解决方式</strong>  </p><p>原因:由于spark的spark-defaults.conf配置文件中配置 eventLog 时指定的路径在本机不存在。  </p><p>解决办法：创建对应的文件夹，并赋予对应权限<br><code>sudo mkdir -p /data/spark_data/history/spark-events</code><br><code>sudo mkdir -p /data/spark_data/history/event-log</code><br><code>sudo chmod 777 -R /data</code>  </p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>首先，基于discuz搭建了论坛，针对论坛产生的日志，对其进行分析。主要的处理流程为log—&gt;flume—&gt;kafka—&gt;sparkstreaming—&gt;MySQL,最后将处理的结果写入MySQL共报表查询。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的状态后端(State Backends)</title>
      <link href="/2019/08/23/Flink%E7%9A%84%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF-State-Backends/"/>
      <url>/2019/08/23/Flink%E7%9A%84%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF-State-Backends/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp;当使用checkpoint时，状态(state)会被持久化到checkpoint上，以防止数据的丢失并确保发生故障时能够完全恢复。状态是通过什么方式在哪里持久化，取决于使用的状态后端。</p><a id="more"></a><h2 id="可用的状态后端"><a href="#可用的状态后端" class="headerlink" title="可用的状态后端"></a>可用的状态后端</h2><p><strong>MemoryStateBackend</strong><br><strong>FsStateBackend</strong><br><strong>FsStateBackend</strong>  </p><p>注意：如果什么都不配置，系统默认的是MemoryStateBackend</p><h2 id="MemoryStateBackend"><a href="#MemoryStateBackend" class="headerlink" title="MemoryStateBackend"></a>MemoryStateBackend</h2><p><img src="//jiamaoxiang.top/2019/08/23/Flink的状态后端-State-Backends/memorystatebackend.png" alt><br>&emsp;&emsp;<code>MemoryStateBackend</code> 是将状态维护在 Java 堆上的一个内部状态后端。键值状态和窗口算子使用哈希表来存储数据（values）和定时器（timers）。当应用程序 checkpoint 时，此后端会在将状态发给 JobManager 之前快照下状态，JobManager 也将状态存储在 Java 堆上。默认情况下，<code>MemoryStateBackend</code> 配置成支持异步快照。异步快照可以避免阻塞数据流的处理，从而避免反压的发生。当然，使用 <code>new MemoryStateBackend(MAX_MEM_STATE_SIZE, false)</code>也可以禁用该特点。</p><p><strong>缺点</strong>：</p><ul><li>默认情况下，每一个状态的大小限制为 5 MB。可以通过 <code>MemoryStateBackend</code> 的构造函数增加这个大小。状态大小受到 akka 帧大小的限制(maxStateSize &lt;= akka.framesize 默认 10 M)，所以无论怎么调整状态大小配置，都不能大于 akka 的帧大小。也可以通过 akka.framesize 调整 akka 帧大小。</li><li>状态的总大小不能超过 JobManager 的内存。</li></ul><p><strong>推荐使用的场景</strong>：</p><ul><li>本地测试、几乎无状态的作业，比如 ETL、JobManager 不容易挂，或挂掉影响不大的情况。</li><li>不推荐在生产场景使用。</li></ul><h2 id="FsStateBackend"><a href="#FsStateBackend" class="headerlink" title="FsStateBackend"></a>FsStateBackend</h2><p><img src="//jiamaoxiang.top/2019/08/23/Flink的状态后端-State-Backends/fsstatebackend.png" alt><br>&emsp;&emsp;<code>FsStateBackend</code>需要配置的主要是文件系统，如 URL（类型，地址，路径）。比如可以是：<br><code>“hdfs://namenode:40010/flink/checkpoints”</code> 或<code>“s3://flink/checkpoints”</code></p><p>&emsp;&emsp;当选择使用 <code>FsStateBackend</code>时，正在进行的数据会被存在TaskManager的内存中。在checkpoint时，此后端会将状态快照写入配置的文件系统和目录的文件中，同时会在JobManager的内存中（在高可用场景下会存在 Zookeeper 中）存储极少的元数据。容量限制上，单 TaskManager 上 State 总量不超过它的内存，总大小不超过配置的文件系统容量。</p><p>&emsp;&emsp;默认情况下，<code>FsStateBackend</code> 配置成提供异步快照，以避免在状态 checkpoint 时阻塞数据流的处理。该特性可以实例化 <code>FsStateBackend</code> 时传入false的布尔标志来禁用掉，例如：<code>new FsStateBackend(path, false)</code></p><p><strong>推荐使用的场景</strong>：</p><ul><li>处理大状态，长窗口，或大键值状态的有状态处理任务， 例如分钟级窗口聚合或 join。</li><li>适合用于高可用方案（需要开启HA的作业）。</li><li>可以在生产环境中使用</li></ul><h2 id="RocksDBStateBackend"><a href="#RocksDBStateBackend" class="headerlink" title="RocksDBStateBackend"></a>RocksDBStateBackend</h2><p><img src="//jiamaoxiang.top/2019/08/23/Flink的状态后端-State-Backends/rocksdbstatebackend.png" alt><br>&emsp;&emsp;<code>RocksDBStateBackend</code> 的配置也需要一个文件系统（类型，地址，路径），如下所示：<br>“hdfs://namenode:40010/flink/checkpoints” 或“s3://flink/checkpoints”<br>RocksDB 是一种嵌入式的本地数据库。RocksDBStateBackend 将处理中的数据使用 <a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a> 存储在本地磁盘上。在 checkpoint 时，整个 RocksDB 数据库会被存储到配置的文件系统中，或者在超大状态作业时可以将增量的数据存储到配置的文件系统中。同时 Flink 会将极少的元数据存储在 JobManager 的内存中，或者在 Zookeeper 中（对于高可用的情况）。<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a> 默认也是配置成异步快照的模式。</p><p>&emsp;&emsp;<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>是一个 key/value 的内存存储系统，和其他的 key/value 一样，先将状态放到内存中，如果内存快满时，则写入到磁盘中，但需要注意<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>不支持同步的 Checkpoint，构造方法中没有同步快照这个选项。不过<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>支持增量的 Checkpoint，也是目前唯一增量 Checkpoint 的 Backend，意味着并不需要把所有 sst 文件上传到 Checkpoint 目录，仅需要上传新生成的 sst 文件即可。它的 Checkpoint 存储在外部文件系统（本地或HDFS），其容量限制只要单个 TaskManager 上 State 总量不超过它的内存+磁盘，单Key最大2G，总大小不超过配置的文件系统容量即可。</p><p><strong>缺点</strong>：</p><ul><li><a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>支持的单key和单value的大小最大为每个 2^31 字节。这是因为 RocksDB 的 JNI API 是基于byte[]的。<br></li><li>对于使用具有合并操作的状态的应用程序，例如 ListState，随着时间可能会累积到超过 2^31 字节大小，这将会导致在接下来的查询中失败。</li></ul><p><strong>推荐使用的场景</strong>：</p><ul><li>最适合用于处理大状态，长窗口，或大键值状态的有状态处理任务。</li><li>非常适合用于高可用方案。</li><li>最好是对状态读写性能要求不高的作业</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;那如何选择状态的类型和存储方式？结合前面的内容，可以看到，首先是要分析清楚业务场景；比如想要做什么，状态到底大不大。比较各个方案的利弊，选择根据需求合适的状态类型和存储方式即可。</p><hr><p><strong>Reference</strong></p><p>[1]<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/state/state_backends.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/state/state_backends.html</a><br>[2]<a href="https://ververica.cn/developers/state-management/" target="_blank" rel="noopener">https://ververica.cn/developers/state-management/</a><br>[3]<a href="https://www.ververica.com/blog/stateful-stream-processing-apache-flink-state-backends" target="_blank" rel="noopener">https://www.ververica.com/blog/stateful-stream-processing-apache-flink-state-backends</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浅析数据库缓冲池与SQL查询成本</title>
      <link href="/2019/08/14/%E6%B5%85%E6%9E%90%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%86%B2%E6%B1%A0%E4%B8%8ESQL%E6%9F%A5%E8%AF%A2%E6%88%90%E6%9C%AC/"/>
      <url>/2019/08/14/%E6%B5%85%E6%9E%90%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%86%B2%E6%B1%A0%E4%B8%8ESQL%E6%9F%A5%E8%AF%A2%E6%88%90%E6%9C%AC/</url>
      
        <content type="html"><![CDATA[<p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/background.jpg" alt><br>&emsp;&emsp;如果我们想要查找多行记录，查询时间是否会成倍地提升呢？其实数据库会采用缓冲池的方式提升页(page)的查找效率。数据库的缓冲池在数据库中起到了怎样的作用？如何查看一条 SQL 语句需要在缓冲池中进行加载的页的数量呢？</p><hr><h2 id="数据库缓冲池"><a href="#数据库缓冲池" class="headerlink" title="数据库缓冲池"></a>数据库缓冲池</h2><p>​        &emsp;&emsp;磁盘 I/O 需要消耗的时间很多，而在内存中进行操作，效率则会高很多，为了能让数据表或者索引中的数据随时被我们所用，DBMS 会申请占用内存来作为数据缓冲池，这样做的好处是可以让磁盘活动最小化，从而减少与磁盘直接进行 I/O 的时间。要知道，这种策略对提升 SQL 语句的查询性能来说至关重要。如果索引的数据在缓冲池里，那么访问的成本就会降低很多。<br>​       &emsp;&emsp;那么缓冲池如何读取数据呢？<br>​        &emsp;&emsp;缓冲池管理器会尽量将经常使用的数据保存起来，在数据库进行页面读操作的时候，首先会判断该页面是否在缓冲池中，如果存在就直接读取，如果不存在，就会通过内存或磁盘将页面存放到缓冲池中再进行读取。</p><h2 id="查看缓冲池大小"><a href="#查看缓冲池大小" class="headerlink" title="查看缓冲池大小"></a>查看缓冲池大小</h2><p>​         &emsp;&emsp;如果使用的是 MyISAM 存储引擎(只缓存索引，不缓存数据)，对应的键缓存参数为 key_buffer_size，可以用它进行查看。<br>​        &emsp;&emsp;如果使用的是 InnoDB 存储引擎，可以通过查看 innodb_buffer_pool_size 变量来查看缓冲池的大小，命令如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show variables like <span class="string">'innodb_buffer_pool_size'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/query_innodb_buffer_size.png" alt><br>​        &emsp;&emsp;此时 InnoDB 的缓冲池大小只有 8388608/1024/1024=8MB，我们可以修改缓冲池大小为 128MB，方法如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; <span class="built_in">set</span> global innodb_buffer_pool_size = 1073741824;</span><br></pre></td></tr></table></figure><p>​      &emsp;&emsp; 在 InnoDB 存储引擎中，可以同时开启多个缓冲池，查看缓冲池的个数，使用命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show variables like <span class="string">'innodb_buffer_pool_instances'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/innodb_buffer_pool_instance.png" alt><br>​        &emsp;&emsp;只有一个缓冲池。实际上innodb_buffer_pool_instances默认情况下为 8，为什么只显示只有一个呢？这里需要说明的是，如果想要开启多个缓冲池，你首先需要将innodb_buffer_pool_size参数设置为大于等于 1GB，这时innodb_buffer_pool_instances才会大于 1。你可以在 MySQL 的配置文件中对innodb_buffer_pool_size进行设置，大于等于 1GB，然后再针对innodb_buffer_pool_instances参数进行修改。</p><h2 id="查看SQL语句的查询成本"><a href="#查看SQL语句的查询成本" class="headerlink" title="查看SQL语句的查询成本"></a>查看SQL语句的查询成本</h2><p>​        &emsp;&emsp; 一条 SQL 查询语句在执行前需要确定查询计划，如果存在多种查询计划的话，MySQL 会计算每个查询计划所需要的成本，从中选择成本最小的一个作为最终执行的查询计划。</p><p>​          &emsp;&emsp;如果查看某条 SQL 语句的查询成本，可以在执行完这条 SQL 语句之后，通过查看当前会话中的 last_query_cost 变量值来得到当前查询的成本。这个查询成本对应的是 SQL 语句所需要读取的页(page)的数量。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show status like <span class="string">'last_query_cost'</span></span><br></pre></td></tr></table></figure><p><strong>example</strong>  </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; select userid,rating from movierating <span class="built_in">where</span> userid = 4169;</span><br></pre></td></tr></table></figure><p>结果：2313 rows in set (0.05 sec) </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show status like <span class="string">'last_query_cost'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/test1.png" alt></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; select userid,rating from movierating <span class="built_in">where</span> userid between 4168 and 4175;</span><br></pre></td></tr></table></figure><p>结果：2643 rows in set (0.01 sec) </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show status like <span class="string">'last_query_cost'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/test2.png" alt></p><p>&emsp;&emsp;你能看到页的数量是刚才的 1.4 倍，但是查询的效率并没有明显的变化，实际上这两个 SQL 查询的时间基本上一样，就是因为采用了顺序读取的方式将页面一次性加载到缓冲池中，然后再进行查找。虽然页数量（last_query_cost）增加了不少，但是通过缓冲池的机制，并没有增加多少查询时间。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程</title>
      <link href="/2019/08/13/Flink%E8%87%AA%E5%AD%A6%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B/"/>
      <url>/2019/08/13/Flink%E8%87%AA%E5%AD%A6%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p><img src="//jiamaoxiang.top/2019/08/13/Flink自学系列教程/logo.png" alt><br>&emsp;&emsp;Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p><hr><h4 id="1-Flink的几个重要概念"><a href="#1-Flink的几个重要概念" class="headerlink" title="1.Flink的几个重要概念"></a><a href="https://jiamaoxiang.top/2019/08/19/Flink%E7%9A%84%E5%87%A0%E4%B8%AA%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5/">1.Flink的几个重要概念</a></h4><h4 id="2-DataFrame-API"><a href="#2-DataFrame-API" class="headerlink" title="2. DataFrame API"></a><a href="https://jiamaoxiang.top/2019/08/19/DataFrame-API/">2. DataFrame API</a></h4><h4 id="3-基于时间的算子"><a href="#3-基于时间的算子" class="headerlink" title="3. 基于时间的算子"></a><a href="https://jiamaoxiang.top/2019/08/19/%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E7%9A%84%E7%AE%97%E5%AD%90/">3. 基于时间的算子</a></h4><h4 id="4-状态与容错"><a href="#4-状态与容错" class="headerlink" title="4. 状态与容错"></a><a href="https://jiamaoxiang.top/2019/08/19/%E7%8A%B6%E6%80%81%E4%B8%8E%E5%AE%B9%E9%94%99/">4. 状态与容错</a></h4><h4 id="5-source-sink-Connectors"><a href="#5-source-sink-Connectors" class="headerlink" title="5. source/sink Connectors"></a><a href="https://jiamaoxiang.top/2019/08/19/source-sink-Connectors/">5. source/sink Connectors</a></h4><h4 id="6-集群与部署"><a href="#6-集群与部署" class="headerlink" title="6. 集群与部署"></a><a href="https://jiamaoxiang.top/2019/08/19/%E9%9B%86%E7%BE%A4%E4%B8%8E%E9%83%A8%E7%BD%B2/">6. 集群与部署</a></h4><h4 id="7-运维与监控"><a href="#7-运维与监控" class="headerlink" title="7.运维与监控"></a><a href="https://jiamaoxiang.top/2019/08/19/%E8%BF%90%E7%BB%B4%E4%B8%8E%E7%9B%91%E6%8E%A7/">7.运维与监控</a></h4>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/08/12/hello-world/"/>
      <url>/2019/08/12/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p><p><img src="//jiamaoxiang.top/2019/08/12/hello-world/logo.png" alt></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
