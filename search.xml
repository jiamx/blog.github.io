<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>数仓|Hive性能调优指北</title>
      <link href="/2020/06/06/%E6%95%B0%E4%BB%93-Hive%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E6%8C%87%E5%8C%97/"/>
      <url>/2020/06/06/%E6%95%B0%E4%BB%93-Hive%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E6%8C%87%E5%8C%97/</url>
      
        <content type="html"><![CDATA[<p>在企业中使用Hive构建离线数仓是一种十分普遍的方案。尽管Hive的使用场景是通过批处理的方式处理大数据，通常对处理时间不敏感。但是在资源有限的情况下，我们需要关注Hive的性能调优，从而方便数据的快速产出。同时，关于Hive的性能调优，也是面试中比较常见的问题，因此掌握Hive性能调优的一些方法，不仅能够在工作中提升效率而且还可以在面试中脱颖而出。本文会通过四个方面介绍Hive性能调优，主要包括：</p><ul><li>性能调优的工具</li><li>设计优化</li><li>数据存储优化</li><li>作业优化</li></ul><h2 id="性能调优的工具"><a href="#性能调优的工具" class="headerlink" title="性能调优的工具"></a>性能调优的工具</h2><p>HQL提供了两个查看查询性能的工具：<strong>explain</strong>与<strong>analyze</strong>，除此之外Hive的日志也提供了非常详细的信息，方便查看执行性能和报错排查。</p><h3 id="善用explain语句"><a href="#善用explain语句" class="headerlink" title="善用explain语句"></a>善用explain语句</h3><p>explain语句是查看执行计划经常使用的一个工具，可以使用该语句分析查询执行计划，具体使用语法如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span> [FORMATTED|<span class="keyword">EXTENDED</span>|DEPENDENCY|AUTHORIZATION] hql_query</span><br></pre></td></tr></table></figure><p>上面的执行语句中，有4个可选的关键字，其具体含义如下：</p><ul><li>FORMATTED：对执行计划进行格式化，返回JSON格式的执行计划</li><li>EXTENDED：提供一些额外的信息，比如文件的路径信息</li><li>DEPENDENCY：以JSON格式返回查询所依赖的表和分区的列表，从Hive0.10开始使用，如下图</li></ul><p><img src="//jiamaoxiang.top/2020/06/06/数仓-Hive性能调优指北/%E4%BE%9D%E8%B5%96.png" alt></p><ul><li>AUTHORIZATION：列出需要被授权的条目，包括输入与输出，从Hive0.14开始使用,如下图</li></ul><p><img src="//jiamaoxiang.top/2020/06/06/数仓-Hive性能调优指北/explain%E6%8E%88%E6%9D%83.png" alt></p><p>一个典型的查询执行计划主要包括三部分，具体如下：</p><ul><li><strong>Abstract Syntax Tree (AST)</strong>：抽象语法树，Hive使用一个称之为antlr的解析生成器，可以自动地将HQL生成为抽象语法树</li><li><strong>Stage Dependencies</strong>：会列出运行查询所有的依赖以及stage的数量</li><li><strong>Stage Plans</strong>：包含了非常重要的信息，比如运行作业时的operator 和sort orders</li></ul><h4 id="举个栗子"><a href="#举个栗子" class="headerlink" title="举个栗子"></a>举个栗子</h4><p>假设有一张表：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> employee_partitioned</span><br><span class="line">(</span><br><span class="line">  <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">  work_place <span class="built_in">ARRAY</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">  gender_age <span class="keyword">STRUCT</span>&lt;gender:<span class="keyword">string</span>,age:<span class="built_in">int</span>&gt;,</span><br><span class="line">  skills_score <span class="keyword">MAP</span>&lt;<span class="keyword">string</span>,<span class="built_in">int</span>&gt;,</span><br><span class="line">  depart_title <span class="keyword">MAP</span>&lt;<span class="keyword">STRING</span>,<span class="built_in">ARRAY</span>&lt;<span class="keyword">STRING</span>&gt;&gt;</span><br><span class="line">)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (<span class="keyword">Year</span> <span class="built_in">INT</span>, <span class="keyword">Month</span> <span class="built_in">INT</span>)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line"><span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'|'</span></span><br><span class="line">COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span></span><br><span class="line"><span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">':'</span>;</span><br></pre></td></tr></table></figure><p>查看执行计划：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span></span><br><span class="line"><span class="keyword">SELECT</span> gender_age.gender,</span><br><span class="line">       <span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> employee_partitioned</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">YEAR</span>=<span class="number">2020</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> gender_age.gender</span><br><span class="line"><span class="keyword">LIMIT</span> <span class="number">2</span>;</span><br></pre></td></tr></table></figure><p>执行计划概览：</p><p><img src="//jiamaoxiang.top/2020/06/06/数仓-Hive性能调优指北/%E6%9F%A5%E8%AF%A2%E8%AE%A1%E5%88%921.png" alt></p><p>如上图：<em>Map/Reduce operator tree</em>是抽象语法树<strong>AST</strong>部分；<strong>STAGE<br>DEPENDENCIES</strong>包括三个阶段：Stage-0 、Stage-1及Stage-2，其中Stage-0 是root stage，即Stage-1与Stage-2依赖于Stage-0；<strong>STAGE PLANS</strong>部分，Stage-1与Stage2都包含一个Map Operator Tree和一个Reduce Operator Tree，Stage-0不包含map和reduce，仅仅是一个fetch数据的操作。</p><p>执行计划详细信息：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage-1 is a root stage</span><br><span class="line">  Stage-2 depends on stages: Stage-1</span><br><span class="line">  Stage-0 depends on stages: Stage-2</span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage-1</span><br><span class="line">    Map Reduce</span><br><span class="line">      Map Operator Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            <span class="built_in">alias</span>: employee_partitioned</span><br><span class="line">            filterExpr: (year = 2020) (<span class="built_in">type</span>: boolean)</span><br><span class="line">            Statistics: Num rows: 1 Data size: 227 Basic stats: PARTIAL Column stats: NONE</span><br><span class="line">            Select Operator</span><br><span class="line">              expressions: gender_age (<span class="built_in">type</span>: struct&lt;gender:string,age:int&gt;)</span><br><span class="line">              outputColumnNames: gender_age</span><br><span class="line">              Statistics: Num rows: 1 Data size: 227 Basic stats: PARTIAL Column stats: NONE</span><br><span class="line">              Reduce Output Operator</span><br><span class="line">                key expressions: gender_age.gender (<span class="built_in">type</span>: string)</span><br><span class="line">                sort order: +</span><br><span class="line">                Map-reduce partition columns: rand() (<span class="built_in">type</span>: double)</span><br><span class="line">                Statistics: Num rows: 1 Data size: 227 Basic stats: PARTIAL Column stats: NONE</span><br><span class="line">      Reduce Operator Tree:</span><br><span class="line">        Group By Operator</span><br><span class="line">          aggregations: count()</span><br><span class="line">          keys: KEY._col0 (<span class="built_in">type</span>: string)</span><br><span class="line">          mode: partial1</span><br><span class="line">          outputColumnNames: _col0, _col1</span><br><span class="line">          Statistics: Num rows: 1 Data size: 227 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">          File Output Operator</span><br><span class="line">            compressed: <span class="literal">false</span></span><br><span class="line">            table:</span><br><span class="line">                input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe</span><br><span class="line"></span><br><span class="line">  Stage: Stage-2</span><br><span class="line">    Map Reduce</span><br><span class="line">      Map Operator Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            Reduce Output Operator</span><br><span class="line">              key expressions: _col0 (<span class="built_in">type</span>: string)</span><br><span class="line">              sort order: +</span><br><span class="line">              Map-reduce partition columns: _col0 (<span class="built_in">type</span>: string)</span><br><span class="line">              Statistics: Num rows: 1 Data size: 227 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">              value expressions: _col1 (<span class="built_in">type</span>: bigint)</span><br><span class="line">      Reduce Operator Tree:</span><br><span class="line">        Group By Operator</span><br><span class="line">          aggregations: count(VALUE._col0)</span><br><span class="line">          keys: KEY._col0 (<span class="built_in">type</span>: string)</span><br><span class="line">          mode: final</span><br><span class="line">          outputColumnNames: _col0, _col1</span><br><span class="line">          Statistics: Num rows: 1 Data size: 227 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">          Limit</span><br><span class="line">            Number of rows: 2</span><br><span class="line">            Statistics: Num rows: 1 Data size: 227 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">            File Output Operator</span><br><span class="line">              compressed: <span class="literal">false</span></span><br><span class="line">              Statistics: Num rows: 1 Data size: 227 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">              table:</span><br><span class="line">                  input format: org.apache.hadoop.mapred.TextInputFormat</span><br><span class="line">                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat</span><br><span class="line">                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><br><span class="line"></span><br><span class="line">  Stage: Stage-0</span><br><span class="line">    Fetch Operator</span><br><span class="line">      <span class="built_in">limit</span>: 2</span><br><span class="line">      Processor Tree:</span><br><span class="line">        ListSink</span><br></pre></td></tr></table></figure><h3 id="巧用analyze语句"><a href="#巧用analyze语句" class="headerlink" title="巧用analyze语句"></a>巧用analyze语句</h3><p>analyze语句可以收集一些详细的统计信息，比如表的行数、文件数、数据的大小等信息。这些统计信息作为元数据存储在hive的元数据库中。Hive支持表、分区和列级别的统计(与Impala类似)，这些信息作为Hive基于成本优化策略(Cost-Based Optimizer (CBO))的输入,该优化器的主要作用是选择耗费最小系统资源的查询计划。其实，在Hive3.2.0版本中，可以自动收集这些统计信息，当然也可以通过analyze语句进行手动统计表、分区或者字段的信息。具体的使用方式如下：</p><ul><li>1.收集表的统计信息(非分区表)，当指定NOSCAN关键字时，会忽略扫描文件内容，仅仅统计文件的数量与大小，速度会比较快</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 不使用NOSCAN关键字</span></span><br><span class="line">hive&gt; ANALYZE TABLE user_behavior  COMPUTE STATISTICS;</span><br><span class="line">...</span><br><span class="line">Table default.user_behavior stats: [numFiles=1, numRows=10, totalSize=229, rawDataSize=219]</span><br><span class="line">Time taken: 23.504 seconds</span><br><span class="line"><span class="comment">-- 使用NOSCAN关键字</span></span><br><span class="line">hive&gt; ANALYZE TABLE user_behavior  COMPUTE STATISTICS NOSCAN;</span><br><span class="line">Table default.user_behavior stats: [numFiles=1, numRows=10, totalSize=229, rawDataSize=219]</span><br><span class="line">Time taken: 0.309 seconds</span><br></pre></td></tr></table></figure><ul><li>2.收集分区表的统计信息</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 收集具体分区的统计信息</span></span><br><span class="line">hive&gt; ANALYZE TABLE employee_partitioned PARTITION(year=2020, month=06) COMPUTE STATISTICS;</span><br><span class="line">...</span><br><span class="line">Partition default.employee_partitioned&#123;year=2020, month=06&#125; stats: [numFiles=1, numRows=0, totalSize=227, rawDataSize=0]</span><br><span class="line">Time taken: 19.283 seconds</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 收集所有分区的统计信息</span></span><br><span class="line">hive&gt; ANALYZE TABLE employee_partitioned PARTITION(year, month) COMPUTE STATISTICS;</span><br><span class="line">...</span><br><span class="line">Partition default.employee_partitioned&#123;year=2020, month=06&#125; stats: [numFiles=1, numRows=0, totalSize=227, rawDataSize=0]</span><br><span class="line">Time taken: 17.528 seconds</span><br></pre></td></tr></table></figure><ul><li>3.收集表的某个字段的统计信息</li></ul><figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; ANALYZE TABLE user_behavior COMPUTE STATISTICS FOR COLUMNS user_id ;</span><br></pre></td></tr></table></figure><blockquote><p><strong>尖叫提示</strong>：</p><p>可以通过设置：<em>SET hive.stats.autogather=true</em>，进行自动收集统计信息，对于INSERT OVERWRITE/INTO操作的表或者分区，可以自动收集统计信息。值得注意的是，LOAD操作不能够自动收集统计信息</p></blockquote><p>一旦这些统计信息收集完毕，可以通过DESCRIBE EXTENDED/FORMATTED语句查询统计信息，具体使用如下：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查看一个分区的统计信息</span></span><br><span class="line">hive&gt; DESCRIBE FORMATTED employee_partitioned PARTITION(year=2020, month=06);</span><br><span class="line">...</span><br><span class="line">Partition Parameters:            </span><br><span class="line">        COLUMN_STATS_ACCURATE   true                </span><br><span class="line">        numFiles                1                   </span><br><span class="line">        numRows                 0                   </span><br><span class="line">        rawDataSize             0                   </span><br><span class="line">        totalSize               227                 </span><br><span class="line">        transient_lastDdlTime   1591437967 </span><br><span class="line">...</span><br><span class="line"><span class="comment">-- 查看一张表的统计信息</span></span><br><span class="line">hive&gt; DESCRIBE FORMATTED employee_partitioned;</span><br><span class="line">...</span><br><span class="line">Table Parameters:                </span><br><span class="line">        numPartitions           1                   </span><br><span class="line">        transient_lastDdlTime   1591431482 </span><br><span class="line">...</span><br><span class="line"><span class="comment">-- 查看某列的统计信息</span></span><br><span class="line">hive&gt; DESCRIBE FORMATTED  user_behavior.user_id;</span><br></pre></td></tr></table></figure><h3 id="常用日志分析"><a href="#常用日志分析" class="headerlink" title="常用日志分析"></a>常用日志分析</h3><p>日志提供了job运行的详细信息，通过查看日志信息，可以分析出导致作业执行瓶颈的问题，主要包括两种类型的日志：系统日志和作业日志。</p><p>系统日志包含了Hive运行时的状态等信息，可以通过{HIVE_HOME}/conf/hive-log4j.properties文件进行配置，主要的配置选项有：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive.root.logger=WARN,DRFA <span class="comment">## 日志级别</span></span><br><span class="line">hive.log.dir=/tmp/<span class="variable">$&#123;user.name&#125;</span> <span class="comment">## 日志路径</span></span><br><span class="line">hive.log.file=hive.log <span class="comment">## 日志名称</span></span><br></pre></td></tr></table></figure><p>也可以通过Hive cli命令行设置日志级别：<code>$hive --hiveconf hive.root.logger=DEBUG,console</code>这种方式只能在当前会话生效。</p><p>作业日志所包含的作业信息通常是由YARN管理的，可以通过<code>yarn logs -applicationId &lt;application_id&gt;</code>命令查看作业日志。</p><h2 id="设计优化"><a href="#设计优化" class="headerlink" title="设计优化"></a>设计优化</h2><h3 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h3><p>对于一张比较大的表，将其设计成分区表可以提升查询的性能，对于一个特定分区的查询，只会加载对应分区路径的文件数据，所以执行速度会比较快。值得注意的是，分区字段的选择是影响查询性能的重要因素，尽量避免层级较深的分区，这样会造成太多的子文件夹。一些常见的分区字段可以是：</p><ul><li>日期或者时间</li></ul><p>比如year、month、day或者hour，当表中存在时间或者日期字段时，可以使用些字段。</p><ul><li>地理位置</li></ul><p>比如国家、省份、城市等</p><ul><li>业务逻辑</li></ul><p>比如部门、销售区域、客户等等</p><h3 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h3><p>与分区表类似，分桶表的组织方式是将HDFS上的文件分割成多个文件。分桶可以加快数据采样，也可以提升join的性能(join的字段是分桶字段)，因为分桶可以确保某个key对应的数据在一个特定的桶内(文件)，所以巧妙地选择分桶字段可以大幅度提升join的性能。通常情况下，分桶字段可以选择经常用在过滤操作或者join操作的字段。</p><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><p>创建索引是关系型数据库性能调优的常见手段，在Hive中也不例外。Hive从0.7版本开始支持索引，使用索引相比全表扫描而言，是一种比较廉价的操作，Hive中创建索引的方式如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">INDEX</span> idx_user_id_user_behavior</span><br><span class="line"><span class="keyword">ON</span> <span class="keyword">TABLE</span> user_behavior (user_id)</span><br><span class="line"><span class="keyword">AS</span> <span class="string">'COMPACT'</span></span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">DEFERRED</span> <span class="keyword">REBUILD</span>;</span><br></pre></td></tr></table></figure><p>上面创建的是COMPACT索引，存储的是索引列与其对应的block id的pair对。除了此种索引外，Hive还支持位图索引(BITMAP),使用方式如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">INDEX</span> idx_behavior_user_behavior</span><br><span class="line"><span class="keyword">ON</span> <span class="keyword">TABLE</span> user_behavior (behavior)</span><br><span class="line"><span class="keyword">AS</span> <span class="string">'BITMAP'</span></span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">DEFERRED</span> <span class="keyword">REBUILD</span>;</span><br></pre></td></tr></table></figure><p>上面创建的索引时，使用了<code>WITH DEFERRED REBUILD</code>选项，该选项可以避免索引立即被创建，当建立索引时，可以使用<code>LTER...REBUILD</code>命令(见下面的示例)，值得注意的是：当基表(被创建索引的表)发生变化时，该命令需要被再次执行以便更新索引到最新的状态。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">INDEX</span> idx_user_id_user_behavior <span class="keyword">ON</span> user_behavior <span class="keyword">REBUILD</span>;</span><br></pre></td></tr></table></figure><p>一旦索引创建成功，会生成一张索引表，表的名称格式为：<code>数据库名__表名_索引名__</code>，可以使用下面的命令查看索引：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive&gt; SHOW TABLES '*idx*';</span><br><span class="line">OK</span><br><span class="line">default__user_behavior_idx_user_id_user_behavior__</span><br><span class="line">Time taken: 0.044 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p>索引表包含索引列、HDFS的文件URI以及每行的偏移量，可以通过下面命令查看：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查看索引表结构</span></span><br><span class="line">hive&gt; DESC default__user_behavior_idx_user_id_user_behavior__;</span><br><span class="line">OK</span><br><span class="line">user_id                 int                                         </span><br><span class="line">_bucketname             string                                      </span><br><span class="line">_offsets                array&lt;bigint&gt;                               </span><br><span class="line">Time taken: 0.109 seconds, Fetched: 3 row(s)</span><br><span class="line"><span class="comment">-- 查看索引表内容</span></span><br><span class="line">hive&gt; SELECT * FROM default__user_behavior_idx_user_id_user_behavior__;</span><br><span class="line">OK</span><br><span class="line">9       hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [181]</span><br><span class="line">7       hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [136]</span><br><span class="line">1       hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [0]</span><br><span class="line">6       hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [113]</span><br><span class="line">5       hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [90]</span><br><span class="line">10      hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [205]</span><br><span class="line">4       hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [66]</span><br><span class="line">8       hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [158]</span><br><span class="line">3       hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [44]</span><br><span class="line">2       hdfs://cdh03:8020/user/hive/warehouse/user_behavior/userbehavior.csv    [22]</span><br><span class="line">Time taken: 0.28 seconds, Fetched: 10 row(s)</span><br></pre></td></tr></table></figure><p>如果要删除索引，可以使用DROP INDEX命令，如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">INDEX</span> idx_user_id_user_behavior <span class="keyword">ON</span> user_behavior;</span><br></pre></td></tr></table></figure><h3 id="使用skewed-temporary表"><a href="#使用skewed-temporary表" class="headerlink" title="使用skewed/temporary表"></a>使用skewed/temporary表</h3><p>Hive除了可以使用内部表、外部表、分区表、分桶表之外，也可以使用skewed/temporary表，也可以在一定程度上提升性能。</p><p>Hive从0.10版本之后开始支持skewed表，该表可以缓解数据倾斜。这种表之所以能够提升性能，是因为可以自动将造成数据倾斜的数据分割成不同的文件或者路径。使用示例如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sample_skewed_table (</span><br><span class="line">dept_no <span class="built_in">int</span>, </span><br><span class="line">dept_name <span class="keyword">string</span></span><br><span class="line">) </span><br><span class="line">SKEWED <span class="keyword">BY</span> (dept_no) <span class="keyword">ON</span> (<span class="number">1000</span>, <span class="number">2000</span>);<span class="comment">-- 指定数据倾斜字段</span></span><br></pre></td></tr></table></figure><p>另外，还可以使用temporary临时表，将公共使用部分的数据集建成临时表，同时临时表支持SSD或memory的数据存储，从而可以提升性能。</p><h2 id="数据存储优化"><a href="#数据存储优化" class="headerlink" title="数据存储优化"></a>数据存储优化</h2><h3 id="文件格式"><a href="#文件格式" class="headerlink" title="文件格式"></a>文件格式</h3><p>Hive支持TEXTFILE, SEQUENCEFILE, AVRO, RCFILE, ORC,以及PARQUET文件格式，可以通过两种方式指定表的文件格式：</p><ul><li>CREATE TABLE … STORE AS <file_format>:即在建表时指定文件格式，默认是TEXTFILE</file_format></li><li>ALTER TABLE … [PARTITION partition_spec] SET FILEFORMAT <file_format>:修改具体表的文件格式</file_format></li></ul><p>一旦存储文件格式为TEXT的表被创建，可以直接通过load命令装载一个text类型的文件。我们可以先使用此命令将数据装载到一张TEXT格式的表中，然后在通过<code>INSERT OVERWRITE/INTO TABLE ... SELECT</code>命令将数据装载到其他文件格式的表中。</p><blockquote><p><strong>尖叫提示</strong>：</p><p>如果要改变创建表的默认文件格式，可以使用hive.default.fileformat=<file_format>进行配置，改配置可以针对所有表。同时也可以使用hive.default.fileformat.managed =<br><file_format>进行配置，改配置仅适用于内部表或外部表</file_format></file_format></p></blockquote><p>TEXT, SEQUENCE和 AVRO文件是面向行的文件存储格式，不是最佳的文件格式，因为即便是只查询一列数据，使用这些存储格式的表也需要读取完整的一行数据。另一方面，面向列的存储格式(RCFILE, ORC, PARQUET)可以很好地解决上面的问题。关于每种文件格式的说明，如下：</p><ul><li>TEXTFILE</li></ul><p>创建表时的默认文件格式，数据被存储成文本格式。文本文件可以被分割和并行处理，也可以使用压缩，比如GZip、LZO或者Snappy。然而大部分的压缩文件不支持分割和并行处理，会造成一个作业只有一个mapper去处理数据，使用压缩的文本文件要确保文件的不要过大，一般接近两个HDFS块的大小。</p><ul><li>SEQUENCEFILE</li></ul><p>key/value对的二进制存储格式，sequence文件的优势是比文本格式更好压缩，sequence文件可以被压缩成块级别的记录，块级别的压缩是一个很好的压缩比例。如果使用块压缩，需要使用下面的配置：set hive.exec.compress.output=true; set io.seqfile.compression.type=BLOCK</p><ul><li>AVRO</li></ul><p>二进制格式文件，除此之外，avro也是一个序列化和反序列化的框架。avro提供了具体的数据schema。</p><ul><li>RCFILE</li></ul><p>全称是Record Columnar File，首先将表分为几个行组，对每个行组内的数据进行按列存储，每一列的数据都是分开存储，即先水平划分，再垂直划分。</p><ul><li>ORC</li></ul><p>全称是Optimized Row Columnar，从hive0.11版本开始支持，ORC格式是RCFILE格式的一种优化的格式，提供了更大的默认块(256M)</p><ul><li>PARQUET</li></ul><p>另外一种列式存储的文件格式，与ORC非常类似，与ORC相比，Parquet格式支持的生态更广，比如低版本的impala不支持orc格式</p><h3 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h3><p>压缩技术可以减少map与reduce之间的数据传输，从而可以提升查询性能，关于压缩的配置可以在hive的命令行中或者hive-site.xml文件中进行配置</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.exec.compress.intermediate=<span class="literal">true</span></span><br></pre></td></tr></table></figure><p>开启压缩之后，可以选择下面的压缩格式：</p><table><thead><tr><th>压缩格式</th><th>codec</th><th>扩展名</th><th>支持分割</th></tr></thead><tbody><tr><td>Deflate</td><td>org.apache.hadoop.io.compress.DefaultCodec</td><td>.deflate</td><td>N</td></tr><tr><td>Gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td><td>.gz</td><td>N</td></tr><tr><td>Bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td><td>.gz</td><td>Y</td></tr><tr><td>LZO</td><td>com.apache.compression.lzo.LzopCodec</td><td>.lzo</td><td>N</td></tr><tr><td>LZ4</td><td>org.apache.hadoop.io.compress.Lz4Codec</td><td>.lz4</td><td>N</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td><td>.snappy</td><td>N</td></tr></tbody></table><p>关于压缩的编码器可以通过mapred-site.xml, hive-site.xml进行配置，也可以通过命令行进行配置,比如：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-- 中间结果压缩</span><br><span class="line">SET hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec</span><br><span class="line">-- 输出结果压缩</span><br><span class="line">SET hive.exec.compress.output=true;</span><br><span class="line">SET mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodc</span><br></pre></td></tr></table></figure><h3 id="存储优化"><a href="#存储优化" class="headerlink" title="存储优化"></a>存储优化</h3><p>经常被访问的数据称之为热数据，可以针对热数据提升查询的性能。比如通过增加热数据的副本数，可以增加数据本地性命中的可能性，从而提升查询性能，当然这要与存储容量之间做出权衡。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ hdfs dfs -setrep -R -w 4 /user/hive/warehouse/employee</span><br></pre></td></tr></table></figure><p>注意，大量的小文件或者冗余副本会造成namenode节点内存耗费，尤其是大量小于HDFS块大小的文件。HDSF本身提供了应对小文件的解决方案：</p><ul><li>Hadoop Archive/HAR:将小文件打包成大文件</li><li>SEQUENCEFILE格式：将小文件压缩成大文件</li><li>CombineFileInputFormat:在map和reduce处理之前组合小文件</li><li>HDFS Federation:HDFS联盟，使用多个namenode节点管理文件</li></ul><p>对于Hive而言，可以使用下面的配置将查询结果的文件进行合并，从而避免产生小文件：</p><ul><li>hive.merge.mapfiles: 在一个仅有map的作业中，合并最后的结果文件，默认为true</li><li>hive.merge.mapredfiles:合并mapreduce作业的结果小文件 默认false，可以设置true</li><li>hive.merge.size.per.task:定义合并文件的大小，默认 256,000,000，即256MB</li><li>hive.merge.smallfiles.avgsize: T触发文件合并的文件大小阈值，默认值是16,000,000</li></ul><p>当一个作业的输出结果文件的大小小于hive.merge.smallfiles.avgsize设定的阈值，并且hive.merge.mapfiles与hive.merge.mapredfiles设置为true，Hive会额外启动一个mr作业将输出小文件合并成大文件。</p><h2 id="作业优化"><a href="#作业优化" class="headerlink" title="作业优化"></a>作业优化</h2><h3 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h3><p>当Hive处理的数据量较小时，启动分布式去处理数据会有点浪费，因为可能启动的时间比数据处理的时间还要长，从Hive0.7版本之后，Hive支持将作业动态地转为本地模式，需要使用下面的配置：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.exec.mode.local.auto=<span class="literal">true</span>; <span class="comment">-- 默认 false</span></span><br><span class="line"><span class="keyword">SET</span> hive.exec.mode.local.auto.inputbytes.max=<span class="number">50000000</span>;</span><br><span class="line"><span class="keyword">SET</span> hive.exec.mode.local.auto.input.files.max=<span class="number">5</span>; <span class="comment">-- 默认 4</span></span><br></pre></td></tr></table></figure><p>一个作业只要满足下面的条件，会启用本地模式</p><ul><li>输入文件的大小小于<code>hive.exec.mode.local.auto.inputbytes.max</code>配置的大小</li><li>map任务的数量小于<code>hive.exec.mode.local.auto.input.files.max</code>配置的大小</li><li>reduce任务的数量是1或者0</li></ul><h3 id="JVM重用"><a href="#JVM重用" class="headerlink" title="JVM重用"></a>JVM重用</h3><p>默认情况下，Hadoop会为为一个map或者reduce启动一个JVM，这样可以并行执行map和reduce。当map或者reduce是那种仅运行几秒钟的轻量级作业时，JVM启动进程所耗费的时间会比作业执行的时间还要长。Hadoop可以重用JVM，通过共享JVM以串行而非并行的方式运行map或者reduce。JVM的重用适用于同一个作业的map和reduce，对于不同作业的task不能够共享JVM。如果要开启JVM重用，需要配置一个作业最大task数量，默认值为1，如果设置为-1，则表示不限制：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SET mapreduce.job.jvm.numtasks=5;</span><br></pre></td></tr></table></figure><p>这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。</p><h3 id="并行执行"><a href="#并行执行" class="headerlink" title="并行执行"></a>并行执行</h3><p>Hive的查询通常会被转换成一系列的stage，这些stage之间并不是一直相互依赖的，所以可以并行执行这些stage，可以通过下面的方式进行配置：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SET hive.exec.parallel=<span class="literal">true</span>; -- 默认<span class="literal">false</span></span><br><span class="line">SET hive.exec.parallel.thread.number=16; -- 默认8</span><br></pre></td></tr></table></figure><p>并行执行可以增加集群资源的利用率，如果集群的资源使用率已经很高了，那么并行执行的效果不会很明显。</p><h3 id="Fetch模式"><a href="#Fetch模式" class="headerlink" title="Fetch模式"></a>Fetch模式</h3><p>Fetch模式是指Hive中对某些情况的查询可以不必使用MapReduce计算。可以简单地读取表对应的存储目录下的文件，然后输出查询结果到控制台。在开启fetch模式之后，在全局查找、字段查找、limit查找等都启动mapreduce，通过下面方式进行配置：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive.fetch.task.conversion=more</span><br></pre></td></tr></table></figure><h3 id="JOIN优化"><a href="#JOIN优化" class="headerlink" title="JOIN优化"></a>JOIN优化</h3><h4 id="普通join"><a href="#普通join" class="headerlink" title="普通join"></a>普通join</h4><p>普通join又称之为reduce端join，是一种最基本的join，并且耗时较长。对于大表join小表，需要将大表放在右侧，即小表join大表。新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。</p><h4 id="map端join"><a href="#map端join" class="headerlink" title="map端join"></a>map端join</h4><p>map端join适用于当一张表很小(可以存在内存中)的情况，即可以将小表加载至内存。Hive从0.7开始支持自动转为map端join，具体配置如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SET hive.auto.convert.join=<span class="literal">true</span>; --  hivev0.11.0之后默认<span class="literal">true</span></span><br><span class="line">SET hive.mapjoin.smalltable.filesize=600000000; -- 默认 25m</span><br><span class="line">SET hive.auto.convert.join.noconditionaltask=<span class="literal">true</span>; -- 默认<span class="literal">true</span>，所以不需要指定map join hint</span><br><span class="line">SET hive.auto.convert.join.noconditionaltask.size=10000000; -- 控制加载到内存的表的大小</span><br></pre></td></tr></table></figure><p>一旦开启map端join配置，Hive会自动检查小表是否大于<code>hive.mapjoin.smalltable.filesize</code>配置的大小，如果大于则转为普通的join，如果小于则转为map端join。</p><p>关于map端join的原理，如下图所示：</p><p><img src="//jiamaoxiang.top/2020/06/06/数仓-Hive性能调优指北/map%E7%AB%AFjoin.png" alt></p><p>首先，Task A(客户端本地执行的task)负责读取小表a，并将其转成一个HashTable的数据结构，写入到本地文件，之后将其加载至分布式缓存。</p><p>然后，Task B任务会启动map任务读取大表b，在Map阶段，根据每条记录与分布式缓存中的a表对应的hashtable关联，并输出结果</p><p>注意：map端join没有reduce任务，所以map直接输出结果，即有多少个map任务就会产生多少个结果文件。</p><h4 id="Bucket-map-join"><a href="#Bucket-map-join" class="headerlink" title="Bucket map join"></a>Bucket map join</h4><p>bucket map join是一种特殊的map端join，主要区别是其应用在分桶表上。如果要开启分桶的map端join，需要开启一下配置：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.auto.convert.join=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">SET</span> hive.optimize.bucketmapjoin=<span class="literal">true</span>; <span class="comment">-- 默认false</span></span><br></pre></td></tr></table></figure><p>在一个分桶的map端join中，所有参与join的表必须是分桶表，并且join的字段是分桶字段(通过CLUSTERED BY指定)，另外，对于大表的分桶数量必须是小表分桶数量的倍数。</p><p>与普通的join相比，分桶join仅仅只读取所需要的桶数据，不需要全表扫描。</p><h4 id="Sort-merge-bucket-SMB-join"><a href="#Sort-merge-bucket-SMB-join" class="headerlink" title="Sort merge bucket (SMB) join"></a>Sort merge bucket (SMB) join</h4><p>SMBjoin应用与分桶表，如果两张参与join的表是排序的，并且分桶字段相同，这样可以使用sort-merge join，其优势在于不用把小表完全加载至内存中，会读取两张分桶表对应的桶，执行普通join(包括map与reduce)配置如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.input.format=</span><br><span class="line">org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;</span><br><span class="line"><span class="keyword">SET</span> hive.auto.convert.sortmerge.join=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">SET</span> hive.optimize.bucketmapjoin=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">SET</span> hive.optimize.bucketmapjoin.sortedmerge=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">SET</span> hive.auto.convert.sortmerge.join.noconditionaltask=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure><h4 id="Sort-merge-bucket-map-SMBM-join"><a href="#Sort-merge-bucket-map-SMBM-join" class="headerlink" title="Sort merge bucket map (SMBM) join"></a>Sort merge bucket map (SMBM) join</h4><p>SMBM join是一种特殊的bucket map join，与map端join不同的是，不用将小表的所有数据行都加载至内存中。使用SMBM join，参与join的表必须是排序的，有着相同的分桶字段，并且join字段与分桶字段相同。配置如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SET hive.auto.convert.join=<span class="literal">true</span>;</span><br><span class="line">SET hive.auto.convert.sortmerge.join=<span class="literal">true</span></span><br><span class="line">SET hive.optimize.bucketmapjoin=<span class="literal">true</span>;</span><br><span class="line">SET hive.optimize.bucketmapjoin.sortedmerge=<span class="literal">true</span>;</span><br><span class="line">SET hive.auto.convert.sortmerge.join.noconditionaltask=<span class="literal">true</span>;</span><br><span class="line">SET hive.auto.convert.sortmerge.join.bigtable.selection.policy=</span><br><span class="line">org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ;</span><br></pre></td></tr></table></figure><h4 id="Skew-join"><a href="#Skew-join" class="headerlink" title="Skew join"></a>Skew join</h4><p>当被处理的数据分布极其不均匀时，会造成数据倾斜的现象。Hive可以通过如下的配置优化数据倾斜的情况：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 默认false，如果数据倾斜，可以将其设置为true</span></span><br><span class="line"><span class="keyword">SET</span> hive.optimize.skewjoin=<span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 默认为100000，如果key的数量大于配置的值，则超过的数量的key对应的数据会被发送到其他的reduce任务</span></span><br><span class="line"><span class="keyword">SET</span> hive.skewjoin.key=<span class="number">100000</span>;</span><br></pre></td></tr></table></figure><blockquote><p><strong>尖叫提示</strong>：</p><p>数据倾斜在group by的情况下也会发生，所以可以开启一个配置：set hive.groupby.skewindata=true，优化group by出现的数据倾斜，一旦开启之后，执行作业时会首先额外触发一个mr作业，该作业的map任务的输出会被随机地分配到reduce任务上，从而避免数据倾斜</p></blockquote><h3 id="执行引擎"><a href="#执行引擎" class="headerlink" title="执行引擎"></a>执行引擎</h3><p>Hive支持多种执行引擎，比如spark、tez。对于执行引擎的选择，会影响整体的查询性能。使用的配置如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SET hive.execution.engine=&lt;engine&gt;; -- &lt;engine&gt; = mr|tez|spark</span><br></pre></td></tr></table></figure><ul><li>mr:默认的执行引擎，在Hive2.0版本版本中被标记过时</li><li>tez:可以将多个有依赖的作业转换为一个作业，这样只需写一次HDFS，且中间节点较少，从而大大提升作业的计算性能。</li><li>spark:一个通用的大数据计算框架，基于内存计算，速度较快</li></ul><h3 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h3><p>与关系型数据库类似，Hive会在真正执行计算之前，生成和优化逻辑执行计划与物理执行计划。Hive有两种优化器：<strong>Vectorize(向量化优化器)</strong>与<strong>Cost-Based Optimization (CBO,成本优化器)</strong>。</p><h4 id="向量化优化器"><a href="#向量化优化器" class="headerlink" title="向量化优化器"></a>向量化优化器</h4><p>向量化优化器会同时处理大批量的数据，而不是一行一行地处理。要使用这种向量化的操作，要求表的文件格式为ORC，配置如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SET hive.vectorized.execution.enabled=<span class="literal">true</span>; -- 默认 <span class="literal">false</span></span><br></pre></td></tr></table></figure><h4 id="成本优化器"><a href="#成本优化器" class="headerlink" title="成本优化器"></a>成本优化器</h4><p>Hive的CBO是基于apache Calcite的，Hive的CBO通过查询成本(有analyze收集的统计信息)会生成有效率的执行计划，最终会减少执行的时间和资源的利用，使用CBO的配置如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SET hive.cbo.enable=<span class="literal">true</span>; --从 v0.14.0默认<span class="literal">true</span></span><br><span class="line">SET hive.compute.query.using.stats=<span class="literal">true</span>; -- 默认<span class="literal">false</span></span><br><span class="line">SET hive.stats.fetch.column.stats=<span class="literal">true</span>; -- 默认<span class="literal">false</span></span><br><span class="line">SET hive.stats.fetch.partition.stats=<span class="literal">true</span>; -- 默认<span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要介绍了Hive调优的基本思路。总共分为四部分，首先介绍了调优的基本工具使用(explain、analyze);接着从表设计层面介绍了一些优化策略(分区、分桶、索引)；然后介绍了数据存储方面的优化(文件格式、压缩、存储优化)；最后从作业层面介绍了优化的技巧(开启本地模式、JVM重用、并行执行、fetch模式、Join优化、执行引擎与优化器)。本文主要为Hive性能调优提供一些思路，在实际的操作过程中需要具体问题具体分析。总之一句话，重剑无锋，为作业分配合理的资源基本上可以满足大部分的情况，适合的就是最好的，没有必要追求狂拽酷炫的技巧，应该把更多的精力放在业务问题上，因为工具的存在的价值是为了解决业务问题的，切不可本末倒置。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>‘实时数仓|基于Flink SQL构建实时数仓探索实践</title>
      <link href="/2020/06/05/%E2%80%98%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-%E5%9F%BA%E4%BA%8EFlink-SQL%E6%9E%84%E5%BB%BA%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E6%8E%A2%E7%B4%A2%E5%AE%9E%E8%B7%B5/"/>
      <url>/2020/06/05/%E2%80%98%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-%E5%9F%BA%E4%BA%8EFlink-SQL%E6%9E%84%E5%BB%BA%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E6%8E%A2%E7%B4%A2%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>实时数仓|Flink SQL之维表join</title>
      <link href="/2020/06/05/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-Flink-SQL%E4%B9%8B%E7%BB%B4%E8%A1%A8join/"/>
      <url>/2020/06/05/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93-Flink-SQL%E4%B9%8B%E7%BB%B4%E8%A1%A8join/</url>
      
        <content type="html"><![CDATA[<p>对于每条流式数据，可以关联一个外部维表数据源，为实时计算提供数据关联查询。</p><p><strong>说明</strong> 维表是一张不断变化的表，在维表JOIN时，需指明这条记录关联维表快照的时刻。维表JOIN仅支持对当前时刻维表快照的关联，未来会支持关联左表rowtime所对应的维表快照</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">column</span>-<span class="keyword">names</span></span><br><span class="line"><span class="keyword">FROM</span> table1  [<span class="keyword">AS</span> &lt;alias1&gt;]</span><br><span class="line">[<span class="keyword">LEFT</span>] <span class="keyword">JOIN</span> table2 <span class="keyword">FOR</span> SYSTEM_TIME <span class="keyword">AS</span> <span class="keyword">OF</span> PROCTIME() [<span class="keyword">AS</span> &lt;alias2&gt;]</span><br><span class="line"><span class="keyword">ON</span> table1.column-name1 = table2.key-name1</span><br></pre></td></tr></table></figure><p><strong>说明</strong>维表支持INNER JOIN和LEFT JOIN，不支持RIGHT JOIN或FULL JOIN。必须加上FOR SYSTEM_TIME AS OF PROCTIME()，表示JOIN维表当前时刻所看到的每条数据。源表后面进来的数据只会关联当时维表的最新信息，即JOIN行为只发生在处理时间（Processing Time），如果JOIN行为发生后，维表中的数据发生了变化（新增、更新或删除），则已关联的维表数据不会被同步变化。ON条件中必须包含维表所有的PRIMARY KEY的等值条件（且要求与真实表定义一致）。此外，ON条件中也可以有其他等值条件。维表和维表不能进行JOIN。</p><p><strong>说明</strong>维表必须指定主键。维表JOIN时，ON的条件必须包含所有主键的等值条件。目前仅支持源表INNER JOIN或LEFT JOIN维表。维表的唯一键（UK）必须为数据库表中的唯一键。如果维表声明的唯一键不是数据库表的唯一键会产生以下影响：维表的读取速度变慢。在维表JOIN时，会从第一条数据进行JOIN，在加入Job的过程中，相同KEY的多条记录在数据库中按顺序发生变化，可能导致JOIN结果错误。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink Table API&amp;SQL编程指南之时间属性(3)</title>
      <link href="/2020/06/02/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97%E4%B9%8B%E6%97%B6%E9%97%B4%E5%B1%9E%E6%80%A7-3/"/>
      <url>/2020/06/02/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97%E4%B9%8B%E6%97%B6%E9%97%B4%E5%B1%9E%E6%80%A7-3/</url>
      
        <content type="html"><![CDATA[<p>Flink总共有三种时间语义：<em>Processing time</em>(处理时间)、<em>Event time</em>(事件时间)以及<em>Ingestion time</em>(摄入时间)。关于这些时间语义的具体解释，可以参考另一篇文章<a href="https://mp.weixin.qq.com/s/ycz_N5m6RjsW9ZBhNMvACw" target="_blank" rel="noopener">Flink的时间与watermarks详解</a>。本文主要讲解Flink Table API &amp; SQL中基于时间的算子如何定义时间语义。通过本文你可以了解到：</p><ul><li>时间属性的简介</li><li>处理时间</li><li>事件时间</li></ul><h2 id="时间属性简介"><a href="#时间属性简介" class="headerlink" title="时间属性简介"></a>时间属性简介</h2><p>Flink TableAPI&amp;SQL中的基于时间的操作(如window)，需要指定时间语义，表可以根据指定的时间戳提供一个逻辑时间属性。</p><p>时间属性是表schama的一部分，当使用DDL创建表时、DataStream转为表时或者使用TableSource时，会定义时间属性。一旦时间属性被定义完成，该时间属性可以看做是一个字段的引用，从而在基于时间的操作中使用该字段。</p><p>时间属性像一个时间戳，可以被访问并参与计算，如果一个时间属性参与计算，那么该时间属性会被雾化成一个常规的时间戳，常规的时间戳不能与Flink的时间与水位线兼容，不能被基于时间的操作所使用。</p><p>Flink TableAPI &amp; SQL所需要的时间属性可以通过Datastream程序中指定，如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime); <span class="comment">// 默认</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 可以选择:</span></span><br><span class="line"><span class="comment">// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);</span></span><br><span class="line"><span class="comment">// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span></span><br></pre></td></tr></table></figure><h2 id="处理时间"><a href="#处理时间" class="headerlink" title="处理时间"></a>处理时间</h2><p>基于本地的机器时间，是一种最简单的时间语义，但是不能保证结果一致性，使用该时间语义不需要提取时间戳和生成水位线。总共有三种方式定义处理时间属性，具体如下</p><h3 id="DDL语句创建表时定义处理时间"><a href="#DDL语句创建表时定义处理时间" class="headerlink" title="DDL语句创建表时定义处理时间"></a>DDL语句创建表时定义处理时间</h3><p>处理时间的属性可以在DDL语句中被定义为一个计算列，需要使用PROCTIME()函数，如下所示：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_actions (</span><br><span class="line">  user_name <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="keyword">data</span> <span class="keyword">STRING</span>,</span><br><span class="line">  user_action_time <span class="keyword">AS</span> PROCTIME() <span class="comment">-- 声明一个额外字段，作为处理时间属性</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> TUMBLE_START(user_action_time, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>), <span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> user_name)</span><br><span class="line"><span class="keyword">FROM</span> user_actions</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> TUMBLE(user_action_time, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>); <span class="comment">-- 10分钟的滚动窗口</span></span><br></pre></td></tr></table></figure><h3 id="DataStream转为Table的过程中定义处理时间"><a href="#DataStream转为Table的过程中定义处理时间" class="headerlink" title="DataStream转为Table的过程中定义处理时间"></a>DataStream转为Table的过程中定义处理时间</h3><p>在将DataStream转为表时，在schema定义中可以通过.proctime属性指定时间属性，并将其放在其他schema字段的最后面，具体如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, String&gt;&gt; stream = ...;</span><br><span class="line"><span class="comment">// 声明一个额外逻辑字段作为处理时间属性</span></span><br><span class="line">Table table = tEnv.fromDataStream(stream, <span class="string">"user_name, data, user_action_time.proctime"</span>);</span><br><span class="line"></span><br><span class="line">WindowedTable windowedTable = table.window(Tumble.over(<span class="string">"10.minutes"</span>).on(<span class="string">"user_action_time"</span>).as(<span class="string">"userActionWindow"</span>));</span><br></pre></td></tr></table></figure><h3 id="使用TableSource"><a href="#使用TableSource" class="headerlink" title="使用TableSource"></a>使用TableSource</h3><p>自定义TableSource并实现<code>DefinedProctimeAttribute</code> 接口，如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义个带有处理时间属性的table source</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserActionSource</span> <span class="keyword">implements</span> <span class="title">StreamTableSource</span>&lt;<span class="title">Row</span>&gt;, <span class="title">DefinedProctimeAttribute</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> TypeInformation&lt;Row&gt; <span class="title">getReturnType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">String[] names = <span class="keyword">new</span> String[] &#123;<span class="string">"user_name"</span> , <span class="string">"data"</span>&#125;;</span><br><span class="line">TypeInformation[] types = <span class="keyword">new</span> TypeInformation[] &#123;Types.STRING(), Types.STRING()&#125;;</span><br><span class="line"><span class="keyword">return</span> Types.ROW(names, types);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> DataStream&lt;Row&gt; <span class="title">getDataStream</span><span class="params">(StreamExecutionEnvironment execEnv)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 创建stream</span></span><br><span class="line">DataStream&lt;Row&gt; stream = ...;</span><br><span class="line"><span class="keyword">return</span> stream;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getProctimeAttribute</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 该字段会追加到schema中，作为第三个字段</span></span><br><span class="line"><span class="keyword">return</span> <span class="string">"user_action_time"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册table source</span></span><br><span class="line">tEnv.registerTableSource(<span class="string">"user_actions"</span>, <span class="keyword">new</span> UserActionSource());</span><br><span class="line"></span><br><span class="line">WindowedTable windowedTable = tEnv</span><br><span class="line">.from(<span class="string">"user_actions"</span>)</span><br><span class="line">.window(Tumble.over(<span class="string">"10.minutes"</span>).on(<span class="string">"user_action_time"</span>).as(<span class="string">"userActionWindow"</span>));</span><br></pre></td></tr></table></figure><h2 id="事件时间"><a href="#事件时间" class="headerlink" title="事件时间"></a>事件时间</h2><p>基于记录的具体时间戳，即便是存在乱序或者迟到数据也会保证结果的一致性。总共有三种方式定义处理时间属性，具体如下</p><h3 id="DDL语句创建表时定事件时间"><a href="#DDL语句创建表时定事件时间" class="headerlink" title="DDL语句创建表时定事件时间"></a>DDL语句创建表时定事件时间</h3><p>事件时间属性可以通过 WATERMARK语句进行定义，如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_actions (</span><br><span class="line">  user_name <span class="keyword">STRING</span>,</span><br><span class="line">  <span class="keyword">data</span> <span class="keyword">STRING</span>,</span><br><span class="line">  user_action_time <span class="built_in">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  <span class="comment">-- 声明user_action_time作为事件时间属性，并允许5S的延迟  </span></span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> user_action_time <span class="keyword">AS</span> user_action_time - <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> TUMBLE_START(user_action_time, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>), <span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> user_name)</span><br><span class="line"><span class="keyword">FROM</span> user_actions</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> TUMBLE(user_action_time, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>);</span><br></pre></td></tr></table></figure><h3 id="DataStream转为Table的过程中定义事件时间"><a href="#DataStream转为Table的过程中定义事件时间" class="headerlink" title="DataStream转为Table的过程中定义事件时间"></a>DataStream转为Table的过程中定义事件时间</h3><p>当定义Schema时通过.rowtime属性指定事件时间属性，必须在DataStream中指定时间戳与水位线。例如在数据集中，事件时间属性为event_time，此时Table中的事件时间字段中可以通过’event_time. rowtime‘来指定。</p><p>目前Flink支持两种方式定义EventTime字段，如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 方式1:</span></span><br><span class="line"><span class="comment">// 提取timestamp并分配watermarks</span></span><br><span class="line">DataStream&lt;Tuple2&lt;String, String&gt;&gt; stream = inputStream.assignTimestampsAndWatermarks(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 声明一个额外逻辑字段作为事件时间属性</span></span><br><span class="line"><span class="comment">// 在table schema的末尾使用user_action_time.rowtime定义事件时间属性</span></span><br><span class="line"><span class="comment">// 系统会在TableEnvironment中获取事件时间属性</span></span><br><span class="line">Table table = tEnv.fromDataStream(stream, <span class="string">"user_name, data, user_action_time.rowtime"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方式2:</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 从第一个字段提取timestamp并分配watermarks</span></span><br><span class="line">DataStream&lt;Tuple3&lt;Long, String, String&gt;&gt; stream = inputStream.assignTimestampsAndWatermarks(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第一个字段已经用来提取时间戳，可以直接使用对应的字段作为事件时间属性</span></span><br><span class="line">Table table = tEnv.fromDataStream(stream, <span class="string">"user_action_time.rowtime, user_name, data"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用:</span></span><br><span class="line"></span><br><span class="line">WindowedTable windowedTable = table.window(Tumble.over(<span class="string">"10.minutes"</span>).on(<span class="string">"user_action_time"</span>).as(<span class="string">"userActionWindow"</span>));</span><br></pre></td></tr></table></figure><h3 id="使用TableSource-1"><a href="#使用TableSource-1" class="headerlink" title="使用TableSource"></a>使用TableSource</h3><p>另外也可以在创建TableSource的时候，实现DefinedRowtimeAttributes接口来定义EventTime字段，在接口中需要实现getRowtimeAttributeDescriptors方法，创建基于EventTime的时间属性信息。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义带有rowtime属性的table source</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserActionSource</span> <span class="keyword">implements</span> <span class="title">StreamTableSource</span>&lt;<span class="title">Row</span>&gt;, <span class="title">DefinedRowtimeAttributes</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> TypeInformation&lt;Row&gt; <span class="title">getReturnType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">String[] names = <span class="keyword">new</span> String[] &#123;<span class="string">"user_name"</span>, <span class="string">"data"</span>, <span class="string">"user_action_time"</span>&#125;;</span><br><span class="line">TypeInformation[] types =</span><br><span class="line">    <span class="keyword">new</span> TypeInformation[] &#123;Types.STRING(), Types.STRING(), Types.LONG()&#125;;</span><br><span class="line"><span class="keyword">return</span> Types.ROW(names, types);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> DataStream&lt;Row&gt; <span class="title">getDataStream</span><span class="params">(StreamExecutionEnvironment execEnv)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建流，基于user_action_time属性分配水位线</span></span><br><span class="line">DataStream&lt;Row&gt; stream = inputStream.assignTimestampsAndWatermarks(...);</span><br><span class="line"><span class="keyword">return</span> stream;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;RowtimeAttributeDescriptor&gt; <span class="title">getRowtimeAttributeDescriptors</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 标记user_action_time字段作为事件时间属性</span></span><br><span class="line">        <span class="comment">// 创建user_action_time描述符，用来标识时间属性字段</span></span><br><span class="line">RowtimeAttributeDescriptor rowtimeAttrDescr = <span class="keyword">new</span> RowtimeAttributeDescriptor(</span><br><span class="line"><span class="string">"user_action_time"</span>,</span><br><span class="line"><span class="keyword">new</span> ExistingField(<span class="string">"user_action_time"</span>),</span><br><span class="line"><span class="keyword">new</span> AscendingTimestamps());</span><br><span class="line">List&lt;RowtimeAttributeDescriptor&gt; listRowtimeAttrDescr = Collections.singletonList(rowtimeAttrDescr);</span><br><span class="line"><span class="keyword">return</span> listRowtimeAttrDescr;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// register表</span></span><br><span class="line">tEnv.registerTableSource(<span class="string">"user_actions"</span>, <span class="keyword">new</span> UserActionSource());</span><br><span class="line"></span><br><span class="line">WindowedTable windowedTable = tEnv</span><br><span class="line">.from(<span class="string">"user_actions"</span>)</span><br><span class="line">.window(Tumble.over(<span class="string">"10.minutes"</span>).on(<span class="string">"user_action_time"</span>).as(<span class="string">"userActionWindow"</span>));</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了如何在Flink Table API和SQL中使用时间语义，可以使用两种时间语义：处理时间和事件时间。分别对每种的时间语义的使用方式进行了详细解释。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数仓开发需要了解的BI数据分析方法</title>
      <link href="/2020/05/28/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91%E9%9C%80%E8%A6%81%E4%BA%86%E8%A7%A3%E7%9A%84BI%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/"/>
      <url>/2020/05/28/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91%E9%9C%80%E8%A6%81%E4%BA%86%E8%A7%A3%E7%9A%84BI%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>数仓开发经常需要与数据表打交道，那么数仓表开发完成之后就万事大吉了吗？显然不是，还需要思考一下如何分析数据以及如何呈现数据，因为这是发挥数据价值很重要的一个方面。通过数据的分析与可视化呈现可以更加直观的提供数据背后的秘密，从而辅助业务决策，实现真正的数据赋能业务。通过本文你可以了解到：</p><ul><li><p>帕累托分析方法与数据可视化</p></li><li><p>RFM分析与数据可视化</p></li><li><p>波士顿矩阵与数据可视化</p></li></ul><h2 id="帕累托分析与数据可视化"><a href="#帕累托分析与数据可视化" class="headerlink" title="帕累托分析与数据可视化"></a>帕累托分析与数据可视化</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>帕累托(Pareto)分析法，又称ABC分析法，即我们平时所提到的80／20法则。关于帕累托(Pareto)分析法，在不同的行业都有不同的应用。</p><ul><li><strong>举个栗子</strong></li></ul><p>在企业的库存管理中，可以发现少数品种在总需用量(或是总供给额、库存总量、储备金总额)中，占了很大的比重，但在相应的量值中所占的比重很少。因此可以运用帕累托分析法，将企业所需的各种物品，按其需用量的大小、物品的重要程度、资源短缺和采购的难易程度、单价的高低、占用储备资金的多少等因素分为若干类，实施分类管理。</p><p>商品销售额分析中，某些商品的销售额占了总销售额的很大部分，某些商品的销售额仅占很小的比例，这样就可以将其分为A、B、C几大类，对销售额占比较多的分类进行投入，以获得更多的销售额。</p><p>在质量分析中，对某种原因导致产品质量不合格的产品数量进行分析，使用帕累托(Pareto)分析法，可以很直观的看出哪些原因造成了产品质量不合格以及哪些原因比较严重。这样就可以着重解决重要的问题，明确目标，更易于操作。</p><ul><li><strong>另一种表述方式</strong></li></ul><p>根据事物在技术或经济方面的主要特征，进行分类，分清重点与非重点。对每一种分类进行区别对待管理，把被分析的对象分成 A、B、C 三类，三类物品没有明确的划分数值界限。</p><table><thead><tr><th>分类与重要程度</th><th>描述</th></tr></thead><tbody><tr><td>A类(非常重要)</td><td>数量占比少，价值占比大</td></tr><tr><td>B类(比较重要)</td><td>没有A类那么重要，介于 A、C 之间</td></tr><tr><td>C类(一般重要)</td><td>数量占比大但价值占比很小</td></tr></tbody></table><p>分类的核心思想：少数贡献了大部分价值。以商品品类和销售额为例：A 品类数量占总体 10% ，却贡献了 80% 的销售额。</p><p><img src="//jiamaoxiang.top/2020/05/28/数仓开发需要了解的BI数据分析方法/%E5%B8%95%E7%B4%AF%E6%89%98%E4%B8%BE%E4%BE%8B.png" alt></p><h3 id="数据分析案例"><a href="#数据分析案例" class="headerlink" title="数据分析案例"></a>数据分析案例</h3><ul><li>效果图</li></ul><p><img src="//jiamaoxiang.top/2020/05/28/数仓开发需要了解的BI数据分析方法/%E5%B8%95%E7%B4%AF%E6%89%98%E5%9B%BE.png" alt></p><ul><li>实现步骤</li></ul><p>假设有如下数据集格式：</p><table><thead><tr><th>品牌</th><th>销售额</th></tr></thead><tbody><tr><td>NEW BALANCE(新百伦)</td><td>8750</td></tr><tr><td>ZIPPO(之宝)</td><td>9760</td></tr><tr><td>OCTMAMI(十月妈咪)</td><td>5800</td></tr></tbody></table><p>需要将数据加工成下面的格式：</p><table><thead><tr><th>品牌</th><th>销售额</th><th>销售总额</th><th>累计销售额</th><th>累计销售额占比</th></tr></thead><tbody><tr><td></td><td></td><td>=∑所有品牌销售额</td><td>=当前品牌销售额 +上一个品牌销售额</td><td>累计销售额/销售总额</td></tr></tbody></table><p>具体的SQL实现如下：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">     brand, <span class="comment">-- 品牌</span></span><br><span class="line">     total_money, <span class="comment">-- 销售额</span></span><br><span class="line">     <span class="keyword">sum</span>(total_money) <span class="keyword">over</span>() <span class="keyword">AS</span> sum_total_money,<span class="comment">-- 销售总额</span></span><br><span class="line">     <span class="keyword">sum</span>(total_money) <span class="keyword">over</span>(<span class="keyword">ORDER</span> <span class="keyword">BY</span> total_money <span class="keyword">DESC</span> <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) <span class="keyword">AS</span> acc_sum_total_money <span class="comment">-- 累计销售额</span></span><br><span class="line"><span class="keyword">FROM</span> sales_money</span><br></pre></td></tr></table></figure><p>上面给出了具体的SQL实现，其实BI工具已经内置了许多的处理函数和拖拽式的数据处理，不需要写SQL也可以将一份明细数据加工成上面的形式。</p><ul><li>结论分析</li></ul><p>从上面的帕累托图中可以看出：A类的(绿色部分)占了总销售额的80%左右，B类(黄色部分)占总销售额的10%,C类(红色部分)占总销售额的10%。接下来可以进行长尾分析，制定营销策略等等。</p><h2 id="RFM分析与数据可视化"><a href="#RFM分析与数据可视化" class="headerlink" title="RFM分析与数据可视化"></a>RFM分析与数据可视化</h2><h3 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h3><p>RFM模型是在客户关系管理(CRM)中常用到的一个模型,RFM模型是衡量客户价值和客户创利能力的重要工具和手段。该模型通过一个客户的近期购买行为、购买的总体频率以及花了多少钱三项指标来描述该客户的价值状况。</p><p>RFM模型较为动态地层示了一个客户的全部轮廓，这对个性化的沟通和服务提供了依据，同时，如果与该客户打交道的时间足够长，也能够较为精确地判断该客户的长期价值(甚至是终身价值)，通过改善三项指标的状况，从而为更多的营销决策提供支持。</p><p>　　在RFM模式中，包括三个关键的因素，分别为：</p><ul><li>R(Recency)：表示客户最近一次购买的时间有多远，即最近的一次消费，消费时间越近的客户价值越大</li><li>F(Frequency)：表示客户在最近一段时间内购买的次数，即消费频率，经常购买的用户也就是熟客，价值肯定比偶尔来一次的客户价值大</li><li>M (Monetary)：表示客户在最近一段时间内购买的金额，即客户的消费能力，通常以客户单次的平均消费金额作为衡量指标，消费越多的用户价值越大。</li></ul><p>最近一次消费、消费频率、消费金额是测算消费者价值最重要也是最容易的方法，这充分的表现了这三个指标对营销活动的指导意义。而其中，最近一次消费是最有力的预测指标。</p><p>通过上面分析可以对客户群体进行分类：</p><table><thead><tr><th>客户类型与等级</th><th>R</th><th>F</th><th>M</th><th>客户特征</th></tr></thead><tbody><tr><td>重要价值客户(A级/111)</td><td>高(1)</td><td>高(1)</td><td>高(1)</td><td>最近消费时间近、消费频次和消费金额都很高</td></tr><tr><td>重要发展客户(A级/101)</td><td>高(1)</td><td>低(0)</td><td>高(1)</td><td>最近消费时间较近、消费金额高，但频次不高，忠诚度不高，很有潜力的用户，必须重点发展</td></tr><tr><td>重要保持客户(B级/011)</td><td>低(0)</td><td>高(1)</td><td>高(1)</td><td>最近消费时间交远，消费金额和频次都很高。</td></tr><tr><td>重要挽留客户(B级/001)</td><td>低(0)</td><td>低(0)</td><td>高(1)</td><td>最近消费时间较远、消费频次不高，但消费金额高的用户，可能是将要流失或者已经要流失的用户，应当基于挽留措施。</td></tr><tr><td>一般价值客户(B级/110)</td><td>高(1)</td><td>高(1)</td><td>低(0)</td><td>最近消费时间近，频率高，但消费金额低，需要提高其客单价。</td></tr><tr><td>一般发展客户(B级/100)</td><td>高(1)</td><td>低(0)</td><td>低(0)</td><td>最近消费时间较近、消费金额，频次都不高。</td></tr><tr><td>一般保持客户(C级/010)</td><td>低(0)</td><td>高(1)</td><td>低(0)</td><td>最近消费时间较远、消费频次高，但金额不高。</td></tr><tr><td>一般挽留客户(C级/000)</td><td>低(0)</td><td>低(0)</td><td>低(0)</td><td>都很低</td></tr></tbody></table><h3 id="数据分析案例-1"><a href="#数据分析案例-1" class="headerlink" title="数据分析案例"></a>数据分析案例</h3><ul><li>效果图</li></ul><p><img src="//jiamaoxiang.top/2020/05/28/数仓开发需要了解的BI数据分析方法/RFM%E6%95%88%E6%9E%9C%E5%9B%BE.png" alt></p><ul><li>实现步骤</li></ul><p>假设有如下的样例数据：</p><table><thead><tr><th>客户名称</th><th>日期</th><th>消费金额</th><th>消费数量</th></tr></thead><tbody><tr><td>上海****有限公司</td><td>2020-05-20</td><td>76802</td><td>2630</td></tr></tbody></table><p>需要将数据集加工成如下格式：</p><p><img src="//jiamaoxiang.top/2020/05/28/数仓开发需要了解的BI数据分析方法/RFM%E8%A1%A8%E7%BB%93%E6%9E%84.png" alt></p><p>具体SQL实现</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> customer_name,<span class="comment">-- 客户名称</span></span><br><span class="line">customer_avg_money,<span class="comment">-- 当前客户的平均消费金额</span></span><br><span class="line">customer_frequency, <span class="comment">-- 当前客户的消费频次</span></span><br><span class="line">total_frequency,<span class="comment">-- 所有客户的总消费频次</span></span><br><span class="line">total_avg_frequency, <span class="comment">-- 所有客户平均消费频次</span></span><br><span class="line">customer_recency_diff, <span class="comment">-- 当前客户最近一次消费日期与当前日期差值</span></span><br><span class="line">total_recency, <span class="comment">-- 所有客户最近一次消费日期与当前日期差值的平均值</span></span><br><span class="line">monetary,<span class="comment">-- 消费金额向量化</span></span><br><span class="line">frequency, <span class="comment">-- 消费频次向量化</span></span><br><span class="line">recency, <span class="comment">-- 最近消费向量化</span></span><br><span class="line">rfm, <span class="comment">-- rfm</span></span><br><span class="line"><span class="keyword">CASE</span></span><br><span class="line">    <span class="keyword">WHEN</span> rfm = <span class="string">"111"</span> <span class="keyword">THEN</span> <span class="string">"重要价值客户"</span></span><br><span class="line">    <span class="keyword">WHEN</span> rfm = <span class="string">"101"</span> <span class="keyword">THEN</span> <span class="string">"重要发展客户"</span></span><br><span class="line">    <span class="keyword">WHEN</span> rfm = <span class="string">"011"</span> <span class="keyword">THEN</span> <span class="string">"重要保持客户"</span></span><br><span class="line">    <span class="keyword">WHEN</span> rfm = <span class="string">"001"</span> <span class="keyword">THEN</span> <span class="string">"重要挽留客户"</span></span><br><span class="line">    <span class="keyword">WHEN</span> rfm = <span class="string">"110"</span> <span class="keyword">THEN</span> <span class="string">"一般价值客户"</span></span><br><span class="line">    <span class="keyword">WHEN</span> rfm = <span class="string">"100"</span> <span class="keyword">THEN</span> <span class="string">"一般发展客户"</span></span><br><span class="line">    <span class="keyword">WHEN</span> rfm = <span class="string">"010"</span> <span class="keyword">THEN</span> <span class="string">"一般保持客户"</span></span><br><span class="line">    <span class="keyword">WHEN</span> rfm = <span class="string">"000"</span> <span class="keyword">THEN</span> <span class="string">"一般挽留客户"</span></span><br><span class="line">           <span class="keyword">END</span> <span class="keyword">AS</span> rfm_text</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  (<span class="keyword">SELECT</span> customer_name,<span class="comment">-- 客户名称</span></span><br><span class="line">customer_avg_money,<span class="comment">-- 当前客户的平均消费金额</span></span><br><span class="line">customer_frequency, <span class="comment">-- 当前客户的消费频次</span></span><br><span class="line">total_avg_money ,<span class="comment">-- 所有客户的平均消费总额</span></span><br><span class="line">total_frequency,<span class="comment">-- 所有客户的总消费频次</span></span><br><span class="line">total_frequency / <span class="keyword">count</span>(*) <span class="keyword">over</span>() <span class="keyword">AS</span> total_avg_frequency, <span class="comment">-- 所有客户平均消费频次</span></span><br><span class="line">customer_recency_diff, <span class="comment">-- 当前客户最近一次消费日期与当前日期差值</span></span><br><span class="line"><span class="keyword">avg</span>(customer_recency_diff) <span class="keyword">over</span>() <span class="keyword">AS</span> total_recency, <span class="comment">-- 所有客户最近一次消费日期与当前日期差值的平均值</span></span><br><span class="line"><span class="keyword">if</span>(customer_avg_money &gt; total_avg_money,<span class="number">1</span>,<span class="number">0</span>) <span class="keyword">AS</span> monetary, <span class="comment">-- 消费金额向量化</span></span><br><span class="line"><span class="keyword">if</span>(customer_frequency &gt; total_frequency / <span class="keyword">count</span>(*) <span class="keyword">over</span>(),<span class="number">1</span>,<span class="number">0</span>) <span class="keyword">AS</span> frequency, <span class="comment">-- 消费频次向量化</span></span><br><span class="line"><span class="keyword">if</span>(customer_recency_diff &gt; <span class="keyword">avg</span>(customer_recency_diff) <span class="keyword">over</span>(),<span class="number">0</span>,<span class="number">1</span>) <span class="keyword">AS</span> recency, <span class="comment">-- 最近消费向量化</span></span><br><span class="line"><span class="keyword">concat</span>(<span class="keyword">if</span>(customer_recency_diff &gt; <span class="keyword">avg</span>(customer_recency_diff) <span class="keyword">over</span>(),<span class="number">0</span>,<span class="number">1</span>),<span class="keyword">if</span>(customer_frequency &gt; total_frequency / <span class="keyword">count</span>(*) <span class="keyword">over</span>(),<span class="number">1</span>,<span class="number">0</span>),<span class="keyword">if</span>(customer_avg_money &gt; total_avg_money,<span class="number">1</span>,<span class="number">0</span>)) <span class="keyword">AS</span> rfm</span><br><span class="line">   <span class="keyword">FROM</span></span><br><span class="line">     (<span class="keyword">SELECT</span> customer_name, <span class="comment">-- 客户名称</span></span><br><span class="line"><span class="keyword">max</span>(customer_avg_money) <span class="keyword">AS</span> customer_avg_money , <span class="comment">-- 当前客户的平均消费金额</span></span><br><span class="line"><span class="keyword">max</span>(customer_frequency) <span class="keyword">AS</span> customer_frequency, <span class="comment">-- 当前客户的消费频次</span></span><br><span class="line"><span class="keyword">max</span>(total_avg_money) <span class="keyword">AS</span> total_avg_money ,<span class="comment">-- 所有客户的平均消费总额</span></span><br><span class="line"><span class="keyword">max</span>(total_frequency) <span class="keyword">AS</span> total_frequency,<span class="comment">-- 所有客户的总消费频次</span></span><br><span class="line"><span class="keyword">datediff</span>(<span class="keyword">CURRENT_DATE</span>,<span class="keyword">max</span>(customer_recency)) <span class="keyword">AS</span> customer_recency_diff <span class="comment">-- 当前客户最近一次消费日期与当前日期差值</span></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">        (<span class="keyword">SELECT</span> customer_name, <span class="comment">-- 客户名称</span></span><br><span class="line"><span class="keyword">avg</span>(money) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">BY</span> customer_name) <span class="keyword">AS</span> customer_avg_money, <span class="comment">-- 当前客户的平均消费金额</span></span><br><span class="line"><span class="keyword">count</span>(amount) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">BY</span> customer_name) <span class="keyword">AS</span> customer_frequency, <span class="comment">-- 当前客户的消费频次</span></span><br><span class="line"><span class="keyword">avg</span>(money) <span class="keyword">over</span>() <span class="keyword">AS</span> total_avg_money,<span class="comment">-- 所有客户的平均消费总额</span></span><br><span class="line"><span class="keyword">count</span>(amount) <span class="keyword">over</span>() <span class="keyword">AS</span> total_frequency, <span class="comment">--所有客户的总消费频次</span></span><br><span class="line"><span class="keyword">max</span>(sale_date) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">BY</span> customer_name) <span class="keyword">AS</span> customer_recency <span class="comment">-- 当前客户最近一次消费日期</span></span><br><span class="line"></span><br><span class="line">         <span class="keyword">FROM</span> customer_sales) t1</span><br><span class="line">      <span class="keyword">GROUP</span> <span class="keyword">BY</span> customer_name)t2) t3</span><br></pre></td></tr></table></figure><p>通过上面的分析，可以为相对应的客户打上客户特征标签，这样就可以针对某类客户指定不同的营销策略。</p><h2 id="波士顿矩阵与数据可视化"><a href="#波士顿矩阵与数据可视化" class="headerlink" title="波士顿矩阵与数据可视化"></a>波士顿矩阵与数据可视化</h2><h3 id="基本概念-2"><a href="#基本概念-2" class="headerlink" title="基本概念"></a>基本概念</h3><p>波士顿矩阵<strong>(BCG Matrix)</strong>又称市场增长率-相对市场份额矩阵、波士顿咨询集团法、四象限分析法、产品系列结构管理法等。</p><p>　BCG矩阵区分出4种业务组合:</p><ul><li>1.明星型业务（Stars，指高增长、高市场份额）</li><li>2.问题型业务（Question Marks，指高增长、低市场份额）</li><li>3.现金牛业务（Cash cows，指低增长、高市场份额）</li><li>4.瘦狗型业务（Dogs，指低增长、低市场份额）</li></ul><p>波士顿矩阵通过销售增长率（反映市场引力的指标）和市场占有率（反映企业实力的指标）来分析决定企业的产品结构。</p><p><img src="//jiamaoxiang.top/2020/05/28/数仓开发需要了解的BI数据分析方法/%E6%B3%A2%E5%A3%AB%E9%A1%BF%E7%9F%A9%E9%98%B5.png" alt></p><h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><ul><li>效果图</li></ul><p><img src="//jiamaoxiang.top/2020/05/28/数仓开发需要了解的BI数据分析方法/%E6%B3%A2%E5%A3%AB%E9%A1%BF%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90.png" alt></p><ul><li>实现步骤</li></ul><p>本案例以分析客户为背景，将客户分类，找到明星客户、现金牛客户、问题客户以及瘦狗客户。</p><p>假设数据集的样式如下：</p><table><thead><tr><th>客户类型</th><th>客户名称</th><th>消费金额</th><th></th></tr></thead><tbody><tr><td>A类</td><td>上海****公司</td><td>20000</td><td>2020-05-30</td></tr></tbody></table><p>首先需要计算<strong>客单价</strong>：每个客户的平均消费金额，即客单价=某客户总消费金额)/某客户消费次数</p><p>其次需要计算<strong>记录数</strong>：每个客户的消费次数，即某个客户总共消费的次数</p><p>接着需要计算<strong>平均消费金额</strong>：所有客户的平均消费金额，即所有客户的总消费金额/所有客户消费次数</p><p>最后计算<strong>平均消费次数</strong>：所有客户的平均消费次数，即所有客户的总消费次数/总客户数</p><p>具体SQL实现：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    customer_name, <span class="comment">-- 客户名称</span></span><br><span class="line">    customer_avg_money, <span class="comment">-- 客单价</span></span><br><span class="line">    customer_frequency , <span class="comment">-- 当前客户的消费次数</span></span><br><span class="line">    total_avg_money,<span class="comment">-- 所有客户的平均消费金额</span></span><br><span class="line">    total_frequency / <span class="keyword">count</span>(*) <span class="keyword">over</span>() <span class="keyword">AS</span> total_avg_frequency <span class="comment">-- 平均消费次数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  (<span class="keyword">SELECT</span> </span><br><span class="line">        customer_name, <span class="comment">-- 客户名称</span></span><br><span class="line">        <span class="keyword">max</span>(customer_avg_money) <span class="keyword">AS</span> customer_avg_money, <span class="comment">-- 客单价</span></span><br><span class="line">        <span class="keyword">max</span>(customer_frequency) <span class="keyword">AS</span> customer_frequency , <span class="comment">-- 当前客户的消费次数</span></span><br><span class="line">        <span class="keyword">max</span>(total_avg_money) <span class="keyword">AS</span> total_avg_money,<span class="comment">-- 所有客户的平均消费金额</span></span><br><span class="line">        <span class="keyword">max</span>(total_frequency) <span class="keyword">AS</span> total_frequency <span class="comment">--所有客户的总消费频次</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">FROM</span></span><br><span class="line">     (</span><br><span class="line">        <span class="keyword">SELECT</span></span><br><span class="line">             customer_name, <span class="comment">-- 客户名称</span></span><br><span class="line">             <span class="keyword">avg</span>(money) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">BY</span> customer_name) <span class="keyword">AS</span> customer_avg_money, <span class="comment">-- 客单价</span></span><br><span class="line">             <span class="keyword">count</span>(*) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">BY</span> customer_name) <span class="keyword">AS</span> customer_frequency, <span class="comment">-- 当前客户的消费次数</span></span><br><span class="line">            <span class="keyword">avg</span>(money) <span class="keyword">over</span>() <span class="keyword">AS</span> total_avg_money,<span class="comment">-- 所有客户的平均消费金额</span></span><br><span class="line">            <span class="keyword">count</span>(*) <span class="keyword">over</span>() <span class="keyword">AS</span> total_frequency <span class="comment">--所有客户的总消费频次</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">FROM</span> customer_sales ) t1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> customer_name) t2</span><br></pre></td></tr></table></figure><p>经过上面的分析，大致可以看出客户画像：</p><ul><li>某客户的消费次数超过平均值，并且每次消费力度(客单价)也超过平均水平的客户：判定为明星客户，这类客户需要重点关注；</li><li>某客户的消费次数超过平均值，但每次消费力度未达到平均水平的客户：被判定为现金牛客户，这类客户通常消费频次比较频繁，能给企业带来较为稳定的现金流，这类客户是企业利润基石；</li><li>某客户的消费次数未达到平均值，但每次消费力度超过平均水平的客户：是问题客户，这类客户最有希望转化为明星客户，但是因为客户存在一定的潜在问题，导致消费频次不高，这类客户需要进行重点跟进和长期沟通；</li><li>消费次数未达到平均值，消费力度也未达到平均水平的客户：属于瘦狗客户，这类客户通常占企业客户的大多数，只需要一般性维护，如果企业资源有限，则可以不用投入太多的精力。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要介绍了数仓开发应该要了解的常见的数据分析方法，主要有三种：帕累托分析、RFM分析以及波士顿矩阵分析。本文分别介绍了三种分析方法的基本概念、操作步骤以及SQL实现，并给出了相应的可视化分析图表，每个案例都是企业的真实应用场景。希望给数仓开发的同学提供一些观察数据的分析角度，从而在实际的开发过程中能够多思考一下数据的应用价值以及数据如何赋能业务，进一步提升自己的综合能力。</p><p>​    </p>]]></content>
      
      
      <categories>
          
          <category> 数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数仓 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink Table API &amp; SQL编程指南之动态表(2)</title>
      <link href="/2020/05/28/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97-2/"/>
      <url>/2020/05/28/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97-2/</url>
      
        <content type="html"><![CDATA[<p>在<a href="https://mp.weixin.qq.com/s/NUuwqzsJmUAn8QEWrjjlyQ" target="_blank" rel="noopener">Flink Table API &amp; SQL编程指南(1)</a>一文中介绍了Flink Table API &amp;SQL的一些基本的概念和通用的API，在本文将会更加深入地讲解Flink Table API &amp;SQL的流处理的基本概念。Flink Table API &amp;SQL是实现了批流处理的统一，这也意味着无论是有界的批处理输入还是无界的流处理输入，使用Flink Table API &amp;SQL进行查询操作，都具有相同的语义。此外，由于SQL最初是为批处理而设计的，所有在无界流上使用关系查询与在有界流上使用关系查询是有所不同的，本文将着重介绍一下动态表。</p><h2 id="动态表"><a href="#动态表" class="headerlink" title="动态表"></a>动态表</h2><p>SQL与关系代数最初的设计不是为了流处理，所以SQL与流处理之间存在一定的差异，Flink实现了在无界的数据流上使用SQL操作。</p><h3 id="数据流上的关系查询"><a href="#数据流上的关系查询" class="headerlink" title="数据流上的关系查询"></a>数据流上的关系查询</h3><p>传统的关系代数(SQL)与流处理在数据的输入、执行以及结果的输出都有所差异，具体如下：</p><table><thead><tr><th>区别</th><th>关系代数/SQL</th><th>流处理</th></tr></thead><tbody><tr><td>数据输入</td><td>表、有界的元祖的集合</td><td>无界的数据流</td></tr><tr><td>执行</td><td>批处理，在整个输入数据上执行查询等操作</td><td>不能在所有的数据上执行查询，需要等待数据流的到来</td></tr><tr><td>结果输出</td><td>查询处理结束之后，输出固定大小的结果</td><td>需要连续不断地更新之前的结果，永远不会结束</td></tr></tbody></table><p>尽管存在这些差异，但并不意味着SQL与流处理不能融合。一些高级的关系型数据库都提供了物化视图的功能，一个物化视图由一个查询语句进行定义，与普通的视图相比，物化视图缓存了查询的结果，所以当访问物化视图时，不需要重复执行SQL查询操作。缓存的一个常见挑战是如何防止提供过时的结果，当定义物化视图的查询基表发生变化时，物化视图的结果就会过时。*<em>Eager View Maintenance *</em>是一种更新物化视图的技术，只要物化视图的查询基表被更新，那么物化视图就会被更新。</p><p>如果考虑以下因素，那么*<em>Eager View Maintenance *</em>与对流进行SQL查询之间的联系将变得显而易见：</p><ul><li>数据库表是在流上执行<code>INSERT</code>，<code>UPDATE</code>和<code>DELETE</code>DML操作语句的结果，通常被称为changelog stream(更新日志流)。</li><li>一个物化视图由一个查询语句进行定义。为了更新雾化视图，查询会连续处理雾化视图的变更日志流。</li><li>雾化视图是流式SQL查询的结果。</li></ul><h3 id="动态表与持续的查询"><a href="#动态表与持续的查询" class="headerlink" title="动态表与持续的查询"></a>动态表与持续的查询</h3><p>动态表是Flink TableAPI &amp;SQL支持流处理的核心概念，与批处理的静态表相比，动态表会随着时间的变化而变化。动态表查询会产生一个<em>Continuous Query</em>，Continuous Query不会终止并且会产生动态表 的结果。</p><p>关于流、动态表与Continuous Query之间的关系，如下图所示：</p><p><img src="//jiamaoxiang.top/2020/05/28/Flink-Table-API-SQL编程指南-2/%E5%8A%A8%E6%80%81%E8%A1%A8%E6%95%B0%E6%8D%AE%E6%B5%81.png" alt></p><ul><li>1.流被转换为动态表</li><li>2.Continuous Query在动态表上不停的执行，产生一个新的动态表</li><li>3.动态表被转换成流</li></ul><p><strong>尖叫提示：</strong>动态表是一个逻辑概念，在执行查询期间动态表不会被雾化。</p><h3 id="在流上定义表"><a href="#在流上定义表" class="headerlink" title="在流上定义表"></a>在流上定义表</h3><p>在数据流上使用SQL查询，需要将流转换成表。流中的记录会被解析并插入到表中(对于一个只有插入操作的流)，如下图所示：</p><p><img src="//jiamaoxiang.top/2020/05/28/Flink-Table-API-SQL编程指南-2/%E8%A1%A81.png" alt></p><h4 id="持续查询"><a href="#持续查询" class="headerlink" title="持续查询"></a>持续查询</h4><ul><li>分组聚合(group aggregation)</li></ul><p>在一个动态表上的持续查询，会产生一个新的动态表结果。与批处理的查询相比，持续的查询从不会结束并且会根据输入的数据更新之前的结果。下面的示例中，展示了点击事件流，并使用分组聚合计算，如下图：</p><p><img src="//jiamaoxiang.top/2020/05/28/Flink-Table-API-SQL编程指南-2/%E8%A1%A82.png" alt></p><p>上图中展示了一个用户点击行为的数据，计算操作使用的是分组聚合(group aggregation)。当第一条数据[Mary,./home]进来时，会立即进行计算操作，并输出计算结果：[Mary，1]。当[Bob,./cart]进来时，也会立即进行计算，并输出计算结果：[Mary，1],[Bob，1]。当[Mary,./prod?id=1]进来时，会立即进行计算，并输出结果：[Mary，2],[Bob，1]。由此可以看出：分组聚合会作用在所有的数据上，并且会更新之前输出的结果。</p><ul><li>窗口聚合(window aggregation)</li></ul><p>上面演示的是一个分组聚合的案例，下面再来看一个窗口聚合的案例。按照一个小时的滚动窗口(Tumble Window)计算该一小时内的用户点击情况，具体如下图所示：</p><p><img src="//jiamaoxiang.top/2020/05/28/Flink-Table-API-SQL编程指南-2/F:%5Cnpm%5Cmywebsite%5Csource_posts%5CFlink-Table-API-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97-2%5C%E5%9B%BE3.png" alt></p><p>如上图所示：Ctime表示事件发生的时间，可以看出在[12:00:00,12:59:59]的一小时内总共有四行数据，在[13:00:00,13:59:59]的一小时内有三行数据，在[14:00:00,14:59:59]一小时内总共有四行数据。</p><p>可以看出：在[12:00:00,12:59:59]时间段内，计算的结果为[Marry,13:00:00,3],[Bob,13:00:00,1],该结果会追加(Append)到该结果表中，在[13:00:00,13:59:59]时间段内，计算结果为[Bob,14:00:00,1],[Liz,14:00:00,2]，该结果同样是追加到结果表中，之前窗口的数据并不会更新。所以窗口聚合的特点就是只计算属于该窗口的数据，并以追加的方式将结果插入结果表中。</p><ul><li>分组聚合与窗口聚合的异同</li></ul><table><thead><tr><th>比较</th><th>分组聚合</th><th>窗口聚合</th></tr></thead><tbody><tr><td>输出模式</td><td>提前输出，每来一条数据计算一次</td><td>按窗口触发时间输出</td></tr><tr><td>输出量</td><td>一个窗口输出一次结果</td><td>每个key输出N个结果</td></tr><tr><td>输出流</td><td>追加流(Append Stream)</td><td>更新流(Update Stream)</td></tr><tr><td>状态清理</td><td>及时清理掉过时的数据</td><td>状态会无限增长</td></tr><tr><td></td><td>不要求输出端支持update操作</td><td>支持更新操作(kudu,HBase,MySQL等)</td></tr></tbody></table><h4 id="更新与追加查询"><a href="#更新与追加查询" class="headerlink" title="更新与追加查询"></a>更新与追加查询</h4><p>上面的两个例子分别演示了更新的查询与追加查询，第一个分组聚合的案例输出的结果会更新之前的结果，即结果表包含<strong>INSERT</strong>与<strong>UPDATE</strong>操作。</p><p>第二个窗口聚合的案例仅仅是追加计算结果的结果表中，即结果表仅包含<strong>INSERT</strong>操作。</p><p>当一个查询产生一个仅追加(append-only)的表或者更新表(updated table)时，区别如下：</p><ul><li>当查询产生的是一个更新表时(即会更新之前输出的结果)，需要维护一个更大的状态</li><li>append-only表与更新表(updated table)转为流(Stream)的方式有所不同</li></ul><h3 id="表到流的转换"><a href="#表到流的转换" class="headerlink" title="表到流的转换"></a>表到流的转换</h3><p>动态表会被INSER、UPDATE、DELETE操作持续地被更改。当将一个动态表转为流或者写出到外部的存储系统时，这些改变的值需要被编码，Flink Table API和SQL支持三种方式对这些改变的数据进行编码：</p><ul><li><strong>Append-only stream</strong></li></ul><p>动态表只会被INSERT操作进行修改，改变的数据(新增的数据)会被插入到动态表的行中</p><ul><li><strong>Retract stream</strong></li></ul><p>一个retract stream包含两种类型的消息，分别为添加消息(add messages)与撤回消息(retract message)。动态表被转为retract stream(撤回流)时，将INSERT操作的变化数据编码为添加消息(add messages)，将DELETE操作引起的变化数据编码为撤回消息(retract message)，将UPDATE操作引起的变化数据而言，会先将旧数据(需要被更新的)编码为retract message，将新的更新的数据编码为添加消息(add messages)，具体如下图所示：</p><p><img src="//jiamaoxiang.top/2020/05/28/Flink-Table-API-SQL编程指南-2/%E5%9B%BE4.png" alt></p><ul><li><strong>Upsert stream</strong></li></ul><p>upsert 流有两种类型的消息：<em>upsert messages</em>与<em>delete messages</em>。动态表被转换为upsert流需要一个唯一主键(可能是复合)key，附带唯一主键key的动态表在被转化为流的时候，会把INSERT与UPDATE操作引起的变化数据编码为upsert messages，把DELETE操作引起的变化数据编码为delete message。与retract 流相比，upsert 流对于UPADTE操作引起的变化数据的编码，使用的是单个消息，即upsert message。对于retract 流，需要先将旧数据编码为retract message，然后再将新数据编码为add message，即需要编码Delete与Insert两条消息，因此使用upsert流效率更高。具体如下图所示：</p><p><img src="//jiamaoxiang.top/2020/05/28/Flink-Table-API-SQL编程指南-2/%E5%9B%BE5.png" alt></p><p><strong>尖叫提示：</strong>将动态表转为datastream时，仅支持append 流与retract流。将动态表输出到外部系统时，支持Append、Retract以及Upsert模式。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了Flink TableAPI&amp;SQL中动态表的概念。首先介绍了动态表的基本概念，然后介绍了在流上定义表的方式，并指出了分组聚合与窗口聚合的异同，最后介绍了表到流的转换并输出到外部系统的三种模式。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink Table API &amp; SQL编程指南(1)</title>
      <link href="/2020/05/25/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/"/>
      <url>/2020/05/25/Flink-Table-API-SQL%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<p>Apache Flink提供了两种顶层的关系型API，分别为Table API和SQL，Flink通过Table API&amp;SQL实现了批流统一。其中Table API是用于Scala和Java的语言集成查询API，它允许以非常直观的方式组合关系运算符（例如select，where和join）的查询。Flink SQL基于<a href="https://calcite.apache.org/" target="_blank" rel="noopener">Apache Calcite</a> 实现了标准的SQL，用户可以使用标准的SQL处理数据集。Table API和SQL与Flink的DataStream和DataSet API紧密集成在一起，用户可以实现相互转化，比如可以将DataStream或者DataSet注册为table进行操作数据。值得注意的是，<strong>Table API and SQL</strong>目前尚未完全完善，还在积极的开发中，所以并不是所有的算子操作都可以通过其实现。</p><a id="more"></a><h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><p>从Flink1.9开始，Flink为Table &amp; SQL API提供了两种planner,分别为Blink planner和old planner，其中old planner是在Flink1.9之前的版本使用。主要区别如下：</p><p><strong>尖叫提示</strong>：对于生产环境，目前推荐使用old planner.</p><ul><li><code>flink-table-common</code>: 通用模块，包含 Flink Planner 和 Blink Planner 一些共用的代码</li><li><code>flink-table-api-java</code>: java语言的Table &amp; SQL API，仅针对table(处于早期的开发阶段，不推荐使用) </li><li><code>flink-table-api-scala</code>: scala语言的Table &amp; SQL API，仅针对table(处于早期的开发阶段，不推荐使用) </li><li><code>flink-table-api-java-bridge</code>: java语言的Table &amp; SQL API，支持DataStream/DataSet API(推荐使用) </li><li><code>flink-table-api-scala-bridge</code>: scala语言的Table &amp; SQL API，支持DataStream/DataSet API(推荐使用) </li><li><code>flink-table-planner</code>:planner 和runtime. planner为Flink1,9之前的old planner(推荐使用) </li><li><code>flink-table-planner-blink</code>: 新的Blink planner.</li><li><code>flink-table-runtime-blink</code>: 新的Blink runtime.</li><li><code>flink-table-uber</code>: 将上述的API模块及old planner打成一个jar包，形如flink-table-*.jar，位与/lib目录下</li><li><code>flink-table-uber-blink</code>:将上述的API模块及Blink 模块打成一个jar包，形如fflink-table-blink-*.jar，位与/lib目录下</li></ul><h2 id="Blink-planner-amp-old-planner"><a href="#Blink-planner-amp-old-planner" class="headerlink" title="Blink planner &amp; old planner"></a>Blink planner &amp; old planner</h2><p>Blink planner和old planner有许多不同的特点，具体列举如下：</p><ul><li>Blink planner将批处理作业看做是流处理作业的特例。所以，不支持Table 与DataSet之间的转换，批处理的作业也不会被转成DataSet程序，而是被转为DataStream程序。</li><li>Blink planner不支持 <code>BatchTableSource</code>，使用的是有界的StreamTableSource。</li><li>Blink planner仅支持新的 <code>Catalog</code>，不支持<code>ExternalCatalog</code> (已过时)。</li><li>对于FilterableTableSource的实现，两种Planner是不同的。old planner会谓词下推到<code>PlannerExpression</code>(未来会被移除)，而Blink planner 会谓词下推到 <code>Expression</code>(表示一个产生计算结果的逻辑树)。</li><li>仅仅Blink planner支持key-value形式的配置，即通过Configuration进行参数设置。</li><li>关于PlannerConfig的实现，两种planner有所不同。</li><li>Blink planner 会将多个sink优化成一个DAG(仅支持TableEnvironment，StreamTableEnvironment不支持)，old planner总是将每一个sink优化成一个新的DAG，每一个DAG都是相互独立的。</li><li>old planner不支持catalog统计，Blink planner支持catalog统计。</li></ul><h2 id="Flink-Table-amp-SQL程序的pom依赖"><a href="#Flink-Table-amp-SQL程序的pom依赖" class="headerlink" title="Flink Table &amp; SQL程序的pom依赖"></a>Flink Table &amp; SQL程序的pom依赖</h2><p>根据使用的语言不同，可以选择下面的依赖，包括scala版和java版，如下：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;!-- java版 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-table-api-java-bridge_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;!-- scala版 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-table-api-scala-bridge_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>除此之外，如果需要在本地的IDE中运行Table API &amp; SQL的程序，则需要添加下面的pom依赖：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;!-- Flink <span class="number">1.9</span>之前的old planner --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-table-planner_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;!-- 新的Blink planner --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-table-planner-blink_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>另外，如果需要实现自定义的格式(比如和kafka交互)或者用户自定义函数，需要添加如下依赖：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">  &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h2 id="Table-API-amp-SQL的编程模板"><a href="#Table-API-amp-SQL的编程模板" class="headerlink" title="Table API &amp; SQL的编程模板"></a>Table API &amp; SQL的编程模板</h2><p>所有的Table API&amp;SQL的程序(无论是批处理还是流处理)都有着相同的形式，下面将给出通用的编程结构形式：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建一个TableEnvironment对象，指定planner、处理模式(batch、streaming)</span></span><br><span class="line">TableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// 创建一个表</span></span><br><span class="line">tableEnv.connect(...).createTemporaryTable(<span class="string">"table1"</span>);</span><br><span class="line"><span class="comment">// 注册一个外部的表</span></span><br><span class="line">tableEnv.connect(...).createTemporaryTable(<span class="string">"outputTable"</span>);</span><br><span class="line"><span class="comment">// 通过Table API的查询创建一个Table 对象</span></span><br><span class="line">Table tapiResult = tableEnv.from(<span class="string">"table1"</span>).select(...);</span><br><span class="line"><span class="comment">// 通过SQL查询的查询创建一个Table 对象</span></span><br><span class="line">Table sqlResult  = tableEnv.sqlQuery(<span class="string">"SELECT ... FROM table1 ... "</span>);</span><br><span class="line"><span class="comment">// 将结果写入TableSink</span></span><br><span class="line">tapiResult.insertInto(<span class="string">"outputTable"</span>);</span><br><span class="line"><span class="comment">// 执行</span></span><br><span class="line">tableEnv.execute(<span class="string">"java_job"</span>);</span><br></pre></td></tr></table></figure><p>注意：Table API &amp; SQL的查询可以相互集成，另外还可以在DataStream或者DataSet中使用Table API &amp; SQL的API，实现DataStreams、 DataSet与Table之间的相互转换。</p><h2 id="创建TableEnvironment"><a href="#创建TableEnvironment" class="headerlink" title="创建TableEnvironment"></a>创建TableEnvironment</h2><p>TableEnvironment是Table API &amp; SQL程序的一个入口，主要包括如下的功能：</p><ul><li>在内部的catalog中注册Table</li><li>注册catalog</li><li>加载可插拔模块</li><li>执行SQL查询</li><li>注册用户定义函数</li><li><code>DataStream</code> 、<code>DataSet</code>与Table之间的相互转换</li><li>持有对<code>ExecutionEnvironment</code> 、<code>StreamExecutionEnvironment</code>的引用</li></ul><p>一个Table必定属于一个具体的TableEnvironment，不可以将不同TableEnvironment的表放在一起使用(比如join，union等操作)。</p><p>TableEnvironment是通过调用 <code>BatchTableEnvironment.create()</code> 或者StreamTableEnvironment.create()的静态方法进行创建的。另外，默认两个planner的jar包都存在与classpath下，所有需要明确指定使用的planner。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// **********************</span></span><br><span class="line"><span class="comment">// FLINK 流处理查询</span></span><br><span class="line"><span class="comment">// **********************</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line"></span><br><span class="line">EnvironmentSettings fsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build();</span><br><span class="line">StreamExecutionEnvironment fsEnv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment fsTableEnv = StreamTableEnvironment.create(fsEnv, fsSettings);</span><br><span class="line"><span class="comment">//或者TableEnvironment fsTableEnv = TableEnvironment.create(fsSettings);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line"><span class="comment">// FLINK 批处理查询</span></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.BatchTableEnvironment;</span><br><span class="line"></span><br><span class="line">ExecutionEnvironment fbEnv = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">BatchTableEnvironment fbTableEnv = BatchTableEnvironment.create(fbEnv);</span><br><span class="line"></span><br><span class="line"><span class="comment">// **********************</span></span><br><span class="line"><span class="comment">// BLINK 流处理查询</span></span><br><span class="line"><span class="comment">// **********************</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.java.StreamTableEnvironment;</span><br><span class="line"></span><br><span class="line">StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();</span><br><span class="line">StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);</span><br><span class="line"><span class="comment">// 或者 TableEnvironment bsTableEnv = TableEnvironment.create(bsSettings);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line"><span class="comment">// BLINK 批处理查询</span></span><br><span class="line"><span class="comment">// ******************</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"></span><br><span class="line">EnvironmentSettings bbSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build();</span><br><span class="line">TableEnvironment bbTableEnv = TableEnvironment.create(bbSettings);</span><br></pre></td></tr></table></figure><h2 id="在catalog中创建表"><a href="#在catalog中创建表" class="headerlink" title="在catalog中创建表"></a>在catalog中创建表</h2><h3 id="临时表与永久表"><a href="#临时表与永久表" class="headerlink" title="临时表与永久表"></a>临时表与永久表</h3><p>表可以分为临时表和永久表两种，其中永久表需要一个catalog(比如Hive的Metastore)俩维护表的元数据信息，一旦永久表被创建，只要连接到该catalog就可以访问该表，只有显示删除永久表，该表才可以被删除。临时表的生命周期是Flink Session，这些表不能够被其他的Flink Session访问，这些表不属于任何的catalog或者数据库，如果与临时表相对应的数据库被删除了，该临时表也不会被删除。</p><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><h4 id="虚表-Virtual-Tables"><a href="#虚表-Virtual-Tables" class="headerlink" title="虚表(Virtual Tables)"></a>虚表(Virtual Tables)</h4><p>一个Table对象相当于SQL中的视图(虚表)，它封装了一个逻辑执行计划，可以通过一个catalog创建，具体如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取一个TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// table对象，查询的结果集</span></span><br><span class="line">Table projTable = tableEnv.from(<span class="string">"X"</span>).select(...);</span><br><span class="line"><span class="comment">// 注册一个表，名称为 "projectedTable"</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"projectedTable"</span>, projTable);</span><br></pre></td></tr></table></figure><h4 id="外部数据源表-Connector-Tables"><a href="#外部数据源表-Connector-Tables" class="headerlink" title="外部数据源表(Connector Tables)"></a>外部数据源表(Connector Tables)</h4><p>可以把外部的数据源注册成表，比如可以读取MySQL数据库数据、Kafka数据等</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">tableEnvironment</span><br><span class="line">  .connect(...)</span><br><span class="line">  .withFormat(...)</span><br><span class="line">  .withSchema(...)</span><br><span class="line">  .inAppendMode()</span><br><span class="line">  .createTemporaryTable(<span class="string">"MyTable"</span>)</span><br></pre></td></tr></table></figure><h3 id="扩展创建表的标识属性"><a href="#扩展创建表的标识属性" class="headerlink" title="扩展创建表的标识属性"></a>扩展创建表的标识属性</h3><p>表的注册总是包含三部分标识属性：catalog、数据库、表名。用户可以在内部设置一个catalog和一个数据库作为当前的catalog和数据库，所以对于catalog和数据库这两个标识属性是可选的，即如果不指定，默认使用的是“current catalog”和 “current database”。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">TableEnvironment tEnv = ...;</span><br><span class="line">tEnv.useCatalog(<span class="string">"custom_catalog"</span>);<span class="comment">//设置catalog</span></span><br><span class="line">tEnv.useDatabase(<span class="string">"custom_database"</span>);<span class="comment">//设置数据库</span></span><br><span class="line">Table table = ...;</span><br><span class="line"><span class="comment">// 注册一个名为exampleView的视图，catalog名为custom_catalog</span></span><br><span class="line"><span class="comment">// 数据库的名为custom_database</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"exampleView"</span>, table);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个名为exampleView的视图，catalog的名为custom_catalog</span></span><br><span class="line"><span class="comment">// 数据库的名为other_database</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"other_database.exampleView"</span>, table);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 注册一个名为'View'的视图，catalog的名称为custom_catalog</span></span><br><span class="line"><span class="comment">// 数据库的名为custom_database，'View'是保留关键字，需要使用``(反引号)</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"`View`"</span>, table);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个名为example.View的视图，catalog的名为custom_catalog，</span></span><br><span class="line"><span class="comment">// 数据库名为custom_database</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"`example.View`"</span>, table);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个名为'exampleView'的视图， catalog的名为'other_catalog'</span></span><br><span class="line"><span class="comment">// 数据库名为other_database' </span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"other_catalog.other_database.exampleView"</span>, table);</span><br></pre></td></tr></table></figure><h2 id="查询表"><a href="#查询表" class="headerlink" title="查询表"></a>查询表</h2><h3 id="Table-API"><a href="#Table-API" class="headerlink" title="Table API"></a>Table API</h3><p>Table API是一个集成Scala与Java语言的查询API，与SQL相比，它的查询不是一个标准的SQL语句，而是由一步一步的操作组成的。如下展示了一个使用Table API实现一个简单的聚合查询。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"><span class="comment">//注册Orders表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 查询注册的表</span></span><br><span class="line">Table orders = tableEnv.from(<span class="string">"Orders"</span>);</span><br><span class="line"><span class="comment">// 计算操作</span></span><br><span class="line">Table revenue = orders</span><br><span class="line">  .filter(<span class="string">"cCountry === 'FRANCE'"</span>)</span><br><span class="line">  .groupBy(<span class="string">"cID, cName"</span>)</span><br><span class="line">  .select(<span class="string">"cID, cName, revenue.sum AS revSum"</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h3><p>Flink SQL依赖于<a href="https://calcite.apache.org/" target="_blank" rel="noopener">Apache Calcite</a>，其实现了标准的SQL语法，如下案例：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">//注册Orders表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算逻辑同上面的Table API</span></span><br><span class="line">Table revenue = tableEnv.sqlQuery(</span><br><span class="line">    <span class="string">"SELECT cID, cName, SUM(revenue) AS revSum "</span> +</span><br><span class="line">    <span class="string">"FROM Orders "</span> +</span><br><span class="line">    <span class="string">"WHERE cCountry = 'FRANCE' "</span> +</span><br><span class="line">    <span class="string">"GROUP BY cID, cName"</span></span><br><span class="line">  );</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册"RevenueFrance"外部输出表</span></span><br><span class="line"><span class="comment">// 计算结果插入"RevenueFrance"表</span></span><br><span class="line">tableEnv.sqlUpdate(</span><br><span class="line">    <span class="string">"INSERT INTO RevenueFrance "</span> +</span><br><span class="line">    <span class="string">"SELECT cID, cName, SUM(revenue) AS revSum "</span> +</span><br><span class="line">    <span class="string">"FROM Orders "</span> +</span><br><span class="line">    <span class="string">"WHERE cCountry = 'FRANCE' "</span> +</span><br><span class="line">    <span class="string">"GROUP BY cID, cName"</span></span><br><span class="line">  );</span><br></pre></td></tr></table></figure><h2 id="输出表"><a href="#输出表" class="headerlink" title="输出表"></a>输出表</h2><p>一个表通过将其写入到TableSink，然后进行输出。TableSink是一个通用的支持多种文件格式(CSV、Parquet, Avro)和多种外部存储系统(JDBC, Apache HBase, Apache Cassandra, Elasticsearch)以及多种消息对列(Apache Kafka, RabbitMQ)的接口。</p><p>批处理的表只能被写入到 <code>BatchTableSink</code>,流处理的表需要指明AppendStreamTableSink、RetractStreamTableSink或者 <code>UpsertStreamTableSink</code></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取TableEnvironment</span></span><br><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建输出表</span></span><br><span class="line"><span class="keyword">final</span> Schema schema = <span class="keyword">new</span> Schema()</span><br><span class="line">    .field(<span class="string">"a"</span>, DataTypes.INT())</span><br><span class="line">    .field(<span class="string">"b"</span>, DataTypes.STRING())</span><br><span class="line">    .field(<span class="string">"c"</span>, DataTypes.LONG());</span><br><span class="line"></span><br><span class="line">tableEnv.connect(<span class="keyword">new</span> FileSystem(<span class="string">"/path/to/file"</span>))</span><br><span class="line">    .withFormat(<span class="keyword">new</span> Csv().fieldDelimiter(<span class="string">'|'</span>).deriveSchema())</span><br><span class="line">    .withSchema(schema)</span><br><span class="line">    .createTemporaryTable(<span class="string">"CsvSinkTable"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算结果表</span></span><br><span class="line">Table result = ...</span><br><span class="line"><span class="comment">// 输出结果表到注册的TableSink</span></span><br><span class="line">result.insertInto(<span class="string">"CsvSinkTable"</span>);</span><br></pre></td></tr></table></figure><h2 id="Table-API-amp-SQL底层的转换与执行"><a href="#Table-API-amp-SQL底层的转换与执行" class="headerlink" title="Table API &amp; SQL底层的转换与执行"></a>Table API &amp; SQL底层的转换与执行</h2><p>上文提到了Flink提供了两种planner，分别为old planner和Blink planner，对于不同的planner而言，Table API &amp; SQL底层的执行与转换是有所不同的。</p><h4 id="Old-planner"><a href="#Old-planner" class="headerlink" title="Old planner"></a>Old planner</h4><p>根据是流处理作业还是批处理作业，Table API &amp;SQL会被转换成DataStream或者DataSet程序。一个查询在内部表示为一个逻辑查询计划，会被转换为两个阶段:</p><ul><li>1.逻辑查询计划优化</li><li>2.转换成DataStream或者DataSet程序</li></ul><p>上面的两个阶段只有下面的操作被执行时才会被执行：</p><ul><li>当一个表被输出到TableSink时，比如调用了Table.insertInto()方法</li><li>当执行更新查询时，比如调用TableEnvironment.sqlUpdate()方法</li><li>当一个表被转换为DataStream或者DataSet时</li></ul><p>一旦执行上述两个阶段，Table API &amp; SQL的操作会被看做是普通的DataStream或者DataSet程序，所以当<code>StreamExecutionEnvironment.execute()</code>或者<code>ExecutionEnvironment.execute()</code> 被调用时，会执行转换后的程序。</p><h4 id="Blink-planner"><a href="#Blink-planner" class="headerlink" title="Blink planner"></a>Blink planner</h4><p>无论是批处理作业还是流处理作业，如果使用的是Blink planner，底层都会被转换为DataStream程序。在一个查询在内部表示为一个逻辑查询计划，会被转换成两个阶段：</p><ul><li>1.逻辑查询计划优化</li><li>2.转换成DataStream程序</li></ul><p>对于<code>TableEnvironment</code> and <code>StreamTableEnvironment</code>而言，一个查询的转换是不同的</p><p>首先对于TableEnvironment，当TableEnvironment.execute()方法执行时，Table API &amp; SQL的查询才会被转换，因为TableEnvironment会将多个sink优化为一个DAG。</p><p>对于StreamTableEnvironment，转换发生的时间与old planner相同。</p><h2 id="与DataStream-amp-DataSet-API集成"><a href="#与DataStream-amp-DataSet-API集成" class="headerlink" title="与DataStream &amp; DataSet API集成"></a>与DataStream &amp; DataSet API集成</h2><p>对于Old planner与Blink planner而言，只要是流处理的操作，都可以与DataStream API集成，<strong>仅仅只有Old planner才可以与DataSet API集成</strong>，由于Blink planner的批处理作业会被转换成DataStream程序，所以不能够与DataSet API集成。值得注意的是，下面提到的table与DataSet之间的转换仅适用于Old planner。</p><p>Table API &amp; SQL的查询很容易与DataStream或者DataSet程序集成，并可以将Table API &amp; SQL的查询嵌入DataStream或者DataSet程序中。DataStream或者DataSet可以转换成表，反之，表也可以被转换成DataStream或者DataSet。</p><h3 id="从DataStream或者DataSet中注册临时表-视图"><a href="#从DataStream或者DataSet中注册临时表-视图" class="headerlink" title="从DataStream或者DataSet中注册临时表(视图)"></a>从DataStream或者DataSet中注册临时表(视图)</h3><p><strong>尖叫提示：</strong>只能将DataStream或者DataSet转换为临时表(视图)</p><p>下面演示DataStream的转换，对于DataSet的转换类似。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream注册为一个名为myTable的视图，其中字段分别为"f0", "f1"</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"myTable"</span>, stream);</span><br><span class="line"><span class="comment">// 将DataStream注册为一个名为myTable2的视图,其中字段分别为"myLong", "myString"</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">"myTable2"</span>, stream, <span class="string">"myLong, myString"</span>);</span><br></pre></td></tr></table></figure><h3 id="将DataStream或者DataSet转化为Table对象"><a href="#将DataStream或者DataSet转化为Table对象" class="headerlink" title="将DataStream或者DataSet转化为Table对象"></a>将DataStream或者DataSet转化为Table对象</h3><p>可以直接将DataStream或者DataSet转换为Table对象，之后可以使用Table API进行查询操作。下面演示DataStream的转换，对于DataSet的转换类似。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转换为Table对象，默认的字段为"f0", "f1"</span></span><br><span class="line">Table table1 = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">// 将DataStream转换为Table对象，默认的字段为"myLong", "myString"</span></span><br><span class="line">Table table2 = tableEnv.fromDataStream(stream, <span class="string">"myLong, myString"</span>);</span><br></pre></td></tr></table></figure><h3 id="将表转换为DataStream或者DataSet"><a href="#将表转换为DataStream或者DataSet" class="headerlink" title="将表转换为DataStream或者DataSet"></a>将表转换为DataStream或者DataSet</h3><p>当将Table转为DataStream或者DataSet时，需要指定DataStream或者DataSet的数据类型。通常最方便的数据类型是row类型，Flink提供了很多的数据类型供用户选择，具体包括Row、POJO、样例类、Tuple和原子类型。</p><h4 id="将表转换为DataStream"><a href="#将表转换为DataStream" class="headerlink" title="将表转换为DataStream"></a>将表转换为DataStream</h4><p>一个流处理查询的结果是动态变化的，所以将表转为DataStream时需要指定一个更新模式，共有两种模式：<strong>Append Mode</strong>和<strong>Retract Mode</strong>。</p><ul><li><strong>Append Mode</strong></li></ul><p>如果动态表仅只有Insert操作，即之前输出的结果不会被更新，则使用该模式。如果更新或删除操作使用追加模式会失败报错</p><ul><li><strong>Retract Mode</strong></li></ul><p>始终可以使用此模式。返回值是boolean类型。它用true或false来标记数据的插入和撤回，返回true代表数据插入，false代表数据的撤回。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment. </span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// 包含两个字段的表(String name, Integer age)</span></span><br><span class="line">Table table = ...</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用Append Mode追加模式，数据类型为Row</span></span><br><span class="line">DataStream&lt;Row&gt; dsRow = tableEnv.toAppendStream(table, Row.class);</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用Append Mode追加模式，数据类型为定义好的TypeInformation</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(</span><br><span class="line">  Types.STRING(),</span><br><span class="line">  Types.INT());</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = </span><br><span class="line">  tableEnv.toAppendStream(table, tupleType);</span><br><span class="line"><span class="comment">// 将表转为DataStream，使用的模式为Retract Mode撤回模式，类型为Row</span></span><br><span class="line"><span class="comment">// 对于转换后的DataStream&lt;Tuple2&lt;Boolean, X&gt;&gt;，X表示流的数据类型，</span></span><br><span class="line"><span class="comment">// boolean值表示数据改变的类型，其中INSERT返回true，DELETE返回的是false</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retractStream = </span><br><span class="line">  tableEnv.toRetractStream(table, Row.class);</span><br></pre></td></tr></table></figure><h4 id="将表转换为DataSet"><a href="#将表转换为DataSet" class="headerlink" title="将表转换为DataSet"></a>将表转换为DataSet</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取BatchTableEnvironment</span></span><br><span class="line">BatchTableEnvironment tableEnv = BatchTableEnvironment.create(env);</span><br><span class="line"><span class="comment">// 包含两个字段的表(String name, Integer age)</span></span><br><span class="line">Table table = ...</span><br><span class="line"><span class="comment">// 将表转为DataSet数据类型为Row</span></span><br><span class="line">DataSet&lt;Row&gt; dsRow = tableEnv.toDataSet(table, Row.class);</span><br><span class="line"><span class="comment">// 将表转为DataSet，通过TypeInformation定义Tuple2&lt;String, Integer&gt;数据类型</span></span><br><span class="line">TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = <span class="keyword">new</span> TupleTypeInfo&lt;&gt;(</span><br><span class="line">  Types.STRING(),</span><br><span class="line">  Types.INT());</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = </span><br><span class="line">  tableEnv.toDataSet(table, tupleType);</span><br></pre></td></tr></table></figure><h3 id="表的Schema与数据类型之间的映射"><a href="#表的Schema与数据类型之间的映射" class="headerlink" title="表的Schema与数据类型之间的映射"></a>表的Schema与数据类型之间的映射</h3><p>表的Schema与数据类型之间的映射有两种方式：分别是基于字段下标位置的映射和基于字段名称的映射。</p><h4 id="基于字段下标位置的映射"><a href="#基于字段下标位置的映射" class="headerlink" title="基于字段下标位置的映射"></a>基于字段下标位置的映射</h4><p>该方式是按照字段的顺序进行一一映射，使用方式如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Integer&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为"f0"和"f1"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">// 将DataStream转为表，选取tuple的第一个元素，指定一个名为"myLong"的字段名</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"myLong"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，为tuple的第一个元素指定名为"myLong"，为第二个元素指定myInt的字段名</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"myLong, myInt"</span>);</span><br></pre></td></tr></table></figure><h4 id="基于字段名称的映射"><a href="#基于字段名称的映射" class="headerlink" title="基于字段名称的映射"></a>基于字段名称的映射</h4><p>基于字段名称的映射方式支持任意的数据类型包括POJO类型，可以很灵活地定义表Schema映射，所有的字段被映射成一个具体的字段名称，同时也可以使用”as”为字段起一个别名。其中Tuple元素的第一个元素为f0,第二个元素为f1，以此类推。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Integer&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为"f0"和"f1"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">// 将DataStream转为表，选择tuple的第二个元素，指定一个名为"f1"的字段名</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"f1"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，交换字段的顺序</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"f1, f0"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，交换字段的顺序，并为f1起别名为"myInt"，为f0起别名为"myLong</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"f1 as myInt, f0 as myLong"</span>);</span><br></pre></td></tr></table></figure><h4 id="原子类型"><a href="#原子类型" class="headerlink" title="原子类型"></a>原子类型</h4><p>Flink将<code>Integer</code>, <code>Double</code>, <code>String</code>或者普通的类型称之为原子类型，一个数据类型为原子类型的DataStream或者DataSet可以被转成单个字段属性的表，这个字段的类型与DataStream或者DataSet的数据类型一致，这个字段的名称可以进行指定。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// 数据类型为原子类型Long</span></span><br><span class="line">DataStream&lt;Long&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为"f0"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">// 将DataStream转为表，指定字段名为myLong"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"myLong"</span>);</span><br></pre></td></tr></table></figure><h4 id="Tuple类型"><a href="#Tuple类型" class="headerlink" title="Tuple类型"></a>Tuple类型</h4><p>Tuple类型的DataStream或者DataSet都可以转为表，可以重新设定表的字段名(即根据tuple元素的位置进行一一映射，转为表之后，每个元素都有一个别名)，如果不为字段指定名称，则使用默认的名称(java语言默认的是f0,f1,scala默认的是_1),用户也可以重新排列字段的顺序，并为每个字段起一个别名。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">//Tuple2&lt;Long, String&gt;类型的DataStream</span></span><br><span class="line">DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为 "f0", "f1"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">// 将DataStream转为表，指定字段名为 "myLong", "myString"(按照Tuple元素的顺序位置)</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"myLong, myString"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，指定字段名为 "f0", "f1"，并且交换顺序</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"f1, f0"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，只选择Tuple的第二个元素，指定字段名为"f1"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"f1"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，为Tuple的第二个元素指定别名为myString，为第一个元素指定字段名为myLong</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"f1 as 'myString', f0 as 'myLong'"</span>);</span><br></pre></td></tr></table></figure><h4 id="POJO类型"><a href="#POJO类型" class="headerlink" title="POJO类型"></a>POJO类型</h4><p>当将POJO类型的DataStream或者DataSet转为表时，如果不指定表名，则默认使用的是POJO字段本身的名称，原始字段名称的映射需要指定原始字段的名称，可以为其起一个别名，也可以调换字段的顺序，也可以只选择部分的字段。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">//数据类型为Person的POJO类型，字段包括"name"和"age"</span></span><br><span class="line">DataStream&lt;Person&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名称为"age", "name"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">//  将DataStream转为表，为"age"字段指定别名myAge, 为"name"字段指定别名myName</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"age as myAge, name as myName"</span>);</span><br><span class="line"><span class="comment">//  将DataStream转为表，只选择一个name字段</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"name"</span>);</span><br><span class="line"><span class="comment">//  将DataStream转为表，只选择一个name字段，并起一个别名myName</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"name as myName"</span>);</span><br></pre></td></tr></table></figure><h4 id="Row类型"><a href="#Row类型" class="headerlink" title="Row类型"></a>Row类型</h4><p>Row类型的DataStream或者DataSet转为表的过程中，可以根据字段的位置或者字段名称进行映射，同时也可以为字段起一个别名，或者只选择部分字段。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取StreamTableEnvironment</span></span><br><span class="line">StreamTableEnvironment tableEnv = ...; </span><br><span class="line"><span class="comment">// Row类型的DataStream，通过RowTypeInfo指定两个字段"name"和"age"</span></span><br><span class="line">DataStream&lt;Row&gt; stream = ...</span><br><span class="line"><span class="comment">// 将DataStream转为表，默认的字段名为原始字段名"name"和"age"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据位置映射，为第一个字段指定myName别名，为第二个字段指定myAge别名</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"myName, myAge"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据字段名映射，为name字段起别名myName，为age字段起别名myAge</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"name as myName, age as myAge"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据字段名映射，只选择name字段</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"name"</span>);</span><br><span class="line"><span class="comment">// 将DataStream转为表，根据字段名映射，只选择name字段，并起一个别名"myName"</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, <span class="string">"name as myName"</span>);</span><br></pre></td></tr></table></figure><h2 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a>查询优化</h2><h3 id="Old-planner-1"><a href="#Old-planner-1" class="headerlink" title="Old planner"></a>Old planner</h3><p>Apache Flink利用Apache Calcite来优化和转换查询。当前执行的优化包括投影和过滤器下推，去相关子查询以及其他类型的查询重写。Old Planner目前不支持优化JOIN的顺序，而是按照查询中定义的顺序执行它们。</p><p>通过提供一个<code>CalciteConfig</code>对象，可以调整在不同阶段应用的优化规则集。这可通过调用<code>CalciteConfig.createBuilder()</code>方法来进行创建，并通过调用<code>tableEnv.getConfig.setPlannerConfig(calciteConfig)</code>方法将该对象传递给TableEnvironment。</p><h3 id="Blink-planner-1"><a href="#Blink-planner-1" class="headerlink" title="Blink planner"></a>Blink planner</h3><p>Apache Flink利用并扩展了Apache Calcite来执行复杂的查询优化。这包括一系列基于规则和基于成本的优化(cost_based)，例如：</p><ul><li>基于Apache Calcite的去相关子查询</li><li>投影裁剪</li><li>分区裁剪</li><li>过滤器谓词下推</li><li>过滤器下推</li><li>子计划重复数据删除以避免重复计算</li><li>特殊的子查询重写，包括两个部分：<ul><li>将IN和EXISTS转换为左半联接( left semi-join)</li><li>将NOT IN和NOT EXISTS转换为left anti-join</li></ul></li><li>调整join的顺序，需要启用 <code>table.optimizer.join-reorder-enabled</code></li></ul><p><strong>注意：</strong> IN / EXISTS / NOT IN / NOT EXISTS当前仅在子查询重写的结合条件下受支持。</p><p>查询优化器不仅基于计划，而且还可以基于数据源的统计信息以及每个操作的细粒度开销(例如io，cpu，网络和内存）,从而做出更加明智且合理的优化决策。</p><p>高级用户可以通过<code>CalciteConfig</code>对象提供自定义优化规则，通过调用tableEnv.getConfig.setPlannerConfig(calciteConfig)，将参数传递给TableEnvironment。</p><h3 id="查看执行计划"><a href="#查看执行计划" class="headerlink" title="查看执行计划"></a>查看执行计划</h3><p>SQL语言支持通过explain来查看某条SQL的执行计划，Flink Table API也可以通过调用explain()方法来查看具体的执行计划。该方法返回一个字符串用来描述三个部分计划，分别为：</p><ol><li>关系查询的抽象语法树，即未优化的逻辑查询计划，</li><li>优化的逻辑查询计划</li><li>实际执行计划</li></ol><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);</span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream1 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>, <span class="string">"hello"</span>));</span><br><span class="line">DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream2 = env.fromElements(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>, <span class="string">"hello"</span>));</span><br><span class="line">Table table1 = tEnv.fromDataStream(stream1, <span class="string">"count, word"</span>);</span><br><span class="line">Table table2 = tEnv.fromDataStream(stream2, <span class="string">"count, word"</span>);</span><br><span class="line">Table table = table1</span><br><span class="line">  .where(<span class="string">"LIKE(word, 'F%')"</span>)</span><br><span class="line">  .unionAll(table2);</span><br><span class="line"><span class="comment">// 查看执行计划</span></span><br><span class="line">String explanation = tEnv.explain(table);</span><br><span class="line">System.out.println(explanation);</span><br></pre></td></tr></table></figure><p>执行计划的结果为：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">== 抽象语法树 ==</span><br><span class="line">LogicalUnion(all=[<span class="keyword">true</span>])</span><br><span class="line">  LogicalFilter(condition=[LIKE($<span class="number">1</span>, _UTF-<span class="number">16L</span>E<span class="string">'F%'</span>)])</span><br><span class="line">    FlinkLogicalDataStreamScan(id=[<span class="number">1</span>], fields=[count, word])</span><br><span class="line">  FlinkLogicalDataStreamScan(id=[<span class="number">2</span>], fields=[count, word])</span><br><span class="line"></span><br><span class="line">== 优化的逻辑执行计划 ==</span><br><span class="line">DataStreamUnion(all=[<span class="keyword">true</span>], union all=[count, word])</span><br><span class="line">  DataStreamCalc(select=[count, word], where=[LIKE(word, _UTF-<span class="number">16L</span>E<span class="string">'F%'</span>)])</span><br><span class="line">    DataStreamScan(id=[<span class="number">1</span>], fields=[count, word])</span><br><span class="line">  DataStreamScan(id=[<span class="number">2</span>], fields=[count, word])</span><br><span class="line"></span><br><span class="line">== 物理执行计划 ==</span><br><span class="line">Stage <span class="number">1</span> : Data Source</span><br><span class="line">content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">Stage <span class="number">2</span> : Data Source</span><br><span class="line">content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">Stage <span class="number">3</span> : Operator</span><br><span class="line">content : from: (count, word)</span><br><span class="line">ship_strategy : REBALANCE</span><br><span class="line"></span><br><span class="line">Stage <span class="number">4</span> : Operator</span><br><span class="line">content : where: (LIKE(word, _UTF-<span class="number">16L</span>E<span class="string">'F%'</span>)), select: (count, word)</span><br><span class="line">ship_strategy : FORWARD</span><br><span class="line"></span><br><span class="line">Stage <span class="number">5</span> : Operator</span><br><span class="line">content : from: (count, word)</span><br><span class="line">ship_strategy : REBALANCE</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了Flink TableAPI &amp;SQL，首先介绍了Flink Table API &amp;SQL的基本概念 ，然后介绍了构建Flink Table API &amp; SQL程序所需要的依赖，接着介绍了Flink的两种planner，还介绍了如何注册表以及DataStream、DataSet与表的相互转换，最后介绍了Flink的两种planner对应的查询优化并给出了一个查看执行计划的案例。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一统江湖的数仓开发辅助神器--DBeaver</title>
      <link href="/2020/05/21/%E4%B8%80%E7%BB%9F%E6%B1%9F%E6%B9%96%E7%9A%84%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E7%A5%9E%E5%99%A8-DBeaver/"/>
      <url>/2020/05/21/%E4%B8%80%E7%BB%9F%E6%B1%9F%E6%B9%96%E7%9A%84%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E7%A5%9E%E5%99%A8-DBeaver/</url>
      
        <content type="html"><![CDATA[<p><strong>DBeaver</strong>是一个SQL客户端和数据库管理工具。对于关系数据库，它使用JDBC API通过JDBC驱动程序与数据库交互。对于其他数据库NoSQL，它使用专有数据库驱动程序。<strong>DBeaver</strong>支持非常丰富的数据库，可以说只有你想不到的，没有它做不到的，开箱即用的<strong>DBeaver</strong>支持80多种数据库产品，主要包括：</p><table><thead><tr><th>种类</th><th>名称</th></tr></thead><tbody><tr><td>关系型</td><td><a href="https://mysql.com/" target="_blank" rel="noopener">MySQL</a>、<a href="https://mariadb.org/" target="_blank" rel="noopener">MariaDB</a>、<a href="https://postgresql.org/" target="_blank" rel="noopener">PostgreSQL</a>、<a href="https://www.microsoft.com/en-us/sql-server/sql-server-2017" target="_blank" rel="noopener">Microsoft SQL Server</a>、<a href="https://dbeaver.com/databases/#" target="_blank" rel="noopener">Oracle</a>、<a href="https://www.ibm.com/analytics/us/en/db2/" target="_blank" rel="noopener">DB2</a>、<a href="https://www.ibm.com/analytics/informix" target="_blank" rel="noopener">Informix</a>等等</td></tr><tr><td>分析型</td><td><a href="https://greenplum.org/" target="_blank" rel="noopener">Greenplum</a>、<a href="http://www.teradata.com/products-and-services/teradata-database" target="_blank" rel="noopener">Teradata</a>、<a href="https://prestosql.io/" target="_blank" rel="noopener">PrestoDB</a>、<a href="https://clickhouse.yandex/" target="_blank" rel="noopener">ClickHouse</a>、<a href="https://www.vertica.com/" target="_blank" rel="noopener">Vertica</a>等等</td></tr><tr><td>文档型</td><td><a href="https://dbeaver.com/databases/mongo/" target="_blank" rel="noopener">MongoDB</a>、<a href="https://dbeaver.com/databases/#" target="_blank" rel="noopener">Couchbase</a></td></tr><tr><td>云数据库</td><td>AWS Athena、AWS Redshift、<a href="https://aws.amazon.com/dynamodb/" target="_blank" rel="noopener">Amazon DynamoDB</a>、<a href="https://azure.microsoft.com/services/sql-database/" target="_blank" rel="noopener">SQL Azure</a>、<a href="https://dbeaver.com/databases/#" target="_blank" rel="noopener">Snowflake</a>、<a href="https://cloud.google.com/bigtable/" target="_blank" rel="noopener">Google Bigtable</a>等等</td></tr><tr><td>大数据</td><td><a href="https://hive.apache.org/" target="_blank" rel="noopener">Apache Hive</a>、<a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-sql-thrift-server.html" target="_blank" rel="noopener">Spark Hive</a>、<a href="https://drill.apache.org/" target="_blank" rel="noopener">Apache Drill</a>、<a href="http://phoenix.apache.org/" target="_blank" rel="noopener">Apache Phoenix</a>、<a href="http://impala.apache.org/" target="_blank" rel="noopener">Apache Impala</a>、<a href="https://content.pivotal.io/pivotal-gemfire" target="_blank" rel="noopener">Gemfire XD</a>、<a href="https://snappydatainc.github.io/snappydata/" target="_blank" rel="noopener">SnappyData</a></td></tr><tr><td>键值型</td><td><a href="https://dbeaver.com/databases/cassandra/" target="_blank" rel="noopener">Apache Cassandra</a>、<a href="https://dbeaver.com/databases/redis/" target="_blank" rel="noopener">Redis</a></td></tr><tr><td>时间序列</td><td><a href="https://dbeaver.com/databases/#" target="_blank" rel="noopener">TimescaleDB</a>、<a href="https://dbeaver.com/databases/influxdb/" target="_blank" rel="noopener">InfluxDB</a></td></tr><tr><td>图数据库</td><td><a href="https://neo4j.com/" target="_blank" rel="noopener">Neo4j</a>、<a href="http://orientdb.com/orientdb/" target="_blank" rel="noopener">OrientDB</a></td></tr><tr><td>搜索引擎</td><td><a href="https://dbeaver.com/databases/#" target="_blank" rel="noopener">Elasticsearch</a>、<a href="http://lucene.apache.org/solr/" target="_blank" rel="noopener">Solr</a></td></tr><tr><td>内嵌型</td><td>SQLite、<a href="http://ucanaccess.sourceforge.net/site.html" target="_blank" rel="noopener">Microsoft Access</a>、<a href="https://db.apache.org/derby/" target="_blank" rel="noopener">Apache Derby</a>等等</td></tr></tbody></table><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>DBeaver支持在Window、MacOS和Linux上安装，本文主要演示在Window上安装，其他的操作系统可以参考官网。DBeaver有企业版和社区版两种，其中企业版支持所有的功能(两周的试用时间)，开源版仅支持部分功能，具体差异请参考[<a href="https://dbeaver.com/edition/]。" target="_blank" rel="noopener">https://dbeaver.com/edition/]。</a></p><p>商用版的收费是按时长计费的，具体可以参考官网，列举如下：</p><table><thead><tr><th>时长</th><th>服务</th><th>收费标准</th></tr></thead><tbody><tr><td>一个月</td><td>无</td><td>19美元</td></tr><tr><td>一年</td><td>升级和客户支持</td><td>199美元</td></tr><tr><td>两年</td><td>升级和客户支持</td><td>333美元</td></tr></tbody></table><p>在Window、MacOS上安装DBeaver的方式有两种，官方推荐的安装方式是使用installer安装(也可以使用 ZIP archive)，<a href="https://dbeaver.io/download/" target="_blank" rel="noopener">下载地址</a>。安装非常方便，下载dbeaver-ce-7.0.5-x86_64-setup.exe，直接双击安装即可。下面将会演示如何连接MySQL、Hive、Impala和 Phoenix。</p><h2 id="连接MySQL"><a href="#连接MySQL" class="headerlink" title="连接MySQL"></a>连接MySQL</h2><ul><li>第一步，新建连接，选择MySQL</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/mysql.png" alt></p><ul><li>第二步，下载驱动。点击之后，需要下载MySQL的驱动，可以点击驱动属性进行下载，填好服务器地址、用户名和密码之后测试连接：</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/%E6%B5%8B%E8%AF%95%E8%BF%9E%E6%8E%A5.png" alt></p><p>完成上面的步骤之后，就可以使用了，可以非常方便的查看表的元数据信息、数据以及ER图。连接之后的信息如下：</p><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/%E8%BF%9E%E6%8E%A5%E4%BF%A1%E6%81%AF.png" alt></p><h2 id="连接Hive"><a href="#连接Hive" class="headerlink" title="连接Hive"></a>连接Hive</h2><ul><li>第一步，新建连接，选择Apache Hive</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/Hive.png" alt></p><ul><li>第二步，点击驱动属性、下载对应的驱动，驱动下载完成后，填写连接的url信息。必须开启HiveServer2服务，HiveServer2的默认端口是10000</li></ul><p><strong>尖叫提示</strong>：如果选择自动下载驱动，会出现版本不兼容或者下载失败的情况，所以不建议使用这种方式。最简单的方式是将hive JDBC的jar包直接加载进去即可，本文使用的Hive是CDH5.16的hive1.1.0版本，在<code>/opt/cloudera/parcels/CDH/lib/hive/lib</code>目录下找到<code>hive-jdbc-1.1.0-cdh5.16.1-standalone.jar</code>文件，将其放在本地的一个文件夹下(可以放置在DBeaver的安装目录下)，然后选择编辑驱动设置：如下图</p><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/hive%E9%A9%B1%E5%8A%A8%E9%85%8D%E7%BD%AE.png" alt></p><p>在点击编辑驱动设置之后，会弹出一个窗口，让你选择驱动的位置，点击添加文件，选择相应的hive驱动即可。然后点击确定。</p><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/%E9%80%89%E6%8B%A9hive%E9%A9%B1%E5%8A%A8.png" alt></p><p>然后填写好url，点击测试链接进行测试，如下图：</p><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/hive%E6%B5%8B%E8%AF%95%E8%BF%9E%E6%8E%A5.png" alt></p><p>成功链接之后，就可以像Hue一样操作Hive了，如下:</p><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/hive%E8%BF%9E%E6%8E%A5%E6%88%90%E5%8A%9F.png" alt></p><h2 id="连接Impala"><a href="#连接Impala" class="headerlink" title="连接Impala"></a>连接Impala</h2><ul><li>第一步，点击工具栏的数据库，新建连接</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/impala%E6%96%B0%E5%BB%BA%E8%BF%9E%E6%8E%A51.png" alt></p><ul><li>第二步，选择Hadoop/Bigdata,选择Cloudera Impala，然后点击下一步，如下：</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/impala%E6%96%B0%E5%BB%BA%E8%BF%9E%E6%8E%A52.png" alt></p><ul><li>第三步，填写好url，端口号默认是21050，该端口被使用 JDBC 或 Cloudera ODBC 2.0 及以上驱动的诸如 BI 工具之类的应用用来传递命令和接收结果，关于Impala的各端口的解释说明，可以参考我的另一篇文章：<a href="https://mp.weixin.qq.com/s/7cIlpcXnm_j8LPYUtPgYiw" target="_blank" rel="noopener">Impala使用端口号汇总</a>。如下图：</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/impala%E6%96%B0%E5%BB%BA%E8%BF%9E%E6%8E%A53.png" alt></p><ul><li>第四步，编辑驱动配置，与Hive的配置一样，选择相对应的驱动jar包，并添加。关于jar包的下载，可以在Cloudera官网进行下载[<a href="https://www.cloudera.com/downloads/connectors/impala/jdbc/2-5-41.html],本文使用的是`ImpalaJDBC41.jar`,后台回复:impala驱动，即可获取。" target="_blank" rel="noopener">https://www.cloudera.com/downloads/connectors/impala/jdbc/2-5-41.html],本文使用的是`ImpalaJDBC41.jar`,后台回复:impala驱动，即可获取。</a></li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/impala%E6%96%B0%E5%BB%BA%E8%BF%9E%E6%8E%A54.png" alt></p><ul><li>第五步，测试连接成功，接下来就可以访问Hive中的表了</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/%E6%B5%8B%E8%AF%95%E8%BF%9E%E6%8E%A5%E6%88%90%E5%8A%9F.png" alt></p><h2 id="连接Phoenix"><a href="#连接Phoenix" class="headerlink" title="连接Phoenix"></a>连接Phoenix</h2><ul><li>第一步，选择Apache Phoenix连接</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/phoenix%E8%BF%9E%E6%8E%A5.png" alt></p><ul><li>第二步，填写连接的url，主机名为zookeeper的地址，端口号为zookeeper的端口号2181,填写完成之后，点击编辑驱动设置</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/Phoenix%E8%BF%9E%E6%8E%A5url.png" alt></p><ul><li>第三步，编辑驱动设置，把Phoenix安装目录下的<code>phoenix-4.14.3-HBase-1.3-client.jar</code>文件复制到本地的一个文件下，并且把hbase-site.xml文件添加到该jar包中，然后选择添加文件，选择该jar包。</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/phoenix%E6%B7%BB%E5%8A%A0%E9%A9%B1%E5%8A%A8.png" alt></p><ul><li>第四步，phoenix测试连接</li></ul><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/phoenix%E6%B5%8B%E8%AF%95%E9%93%BE%E6%8E%A5.png" alt></p><p><strong>尖叫提示:</strong>我使用的版本是DBeaverEE6.0，需要在快捷方式的属性中配置重新配置java路径，否则会报错。具体配置为:在属性后面添加java的目录，<code>-vm C:\mysoftwares\Java\jdk1.8.0_151\bin\javaw</code></p><p><img src="//jiamaoxiang.top/2020/05/21/一统江湖的数仓开发辅助神器-DBeaver/java%E9%85%8D%E7%BD%AE.png" alt></p><p>连接完成之后就可以通过SQL的语法访问HBase了。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了<strong>DBeaver</strong>数据库管理工具，该工具提供了非常丰富的数据库支持，在工作中只需要一个工具就可以花式连接各种各样的数据库。另外本文主要演示了如何连接MySQL、Hive、Impala以及Phoenix，对于其他的数据库而言，用户可以自行测试连接。</p>]]></content>
      
      
      <categories>
          
          <category> 数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DBeaver </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的条件函数与日期函数全面汇总解析</title>
      <link href="/2020/05/20/Hive%E7%9A%84%E6%9D%A1%E4%BB%B6%E5%87%BD%E6%95%B0%E4%B8%8E%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0%E5%85%A8%E9%9D%A2%E6%B1%87%E6%80%BB%E8%A7%A3%E6%9E%90/"/>
      <url>/2020/05/20/Hive%E7%9A%84%E6%9D%A1%E4%BB%B6%E5%87%BD%E6%95%B0%E4%B8%8E%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0%E5%85%A8%E9%9D%A2%E6%B1%87%E6%80%BB%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>在<a href="https://mp.weixin.qq.com/s/K2TA_PhNzGEkucYxBXqhLw" target="_blank" rel="noopener">Hive的开窗函数实战</a>的文章中，主要介绍了Hive的分析函数的基本使用。本文是这篇文章的延续，涵盖了Hive所有的条件函数和日期函数，对于每个函数，本文都给出了具体的解释和使用案例，方便在工作中查阅。</p><a id="more"></a><h2 id="条件函数"><a href="#条件函数" class="headerlink" title="条件函数"></a>条件函数</h2><h3 id="assert-true-BOOLEAN-condition"><a href="#assert-true-BOOLEAN-condition" class="headerlink" title="assert_true(BOOLEAN condition)"></a>assert_true(BOOLEAN condition)</h3><ul><li>解释</li></ul><p>如果condition不为true，则抛出异常，否则返回null</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> assert_true(<span class="number">1</span>&lt;<span class="number">2</span>) <span class="comment">-- 返回null</span></span><br><span class="line"><span class="keyword">select</span> assert_true(<span class="number">1</span>&gt;<span class="number">2</span>) <span class="comment">-- 抛出异常</span></span><br></pre></td></tr></table></figure><h3 id="coalesce-T-v1-T-v2-…"><a href="#coalesce-T-v1-T-v2-…" class="headerlink" title="coalesce(T v1, T v2, …)"></a>coalesce(T v1, T v2, …)</h3><ul><li>解释</li></ul><p>返回第一个不为null的值，如果都为null，则返回null</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">coalesce</span>(<span class="literal">null</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="literal">null</span>)  <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">coalesce</span>(<span class="number">1</span>,<span class="literal">null</span>) <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">coalesce</span>(<span class="literal">null</span>,<span class="literal">null</span>) <span class="comment">-- 返回null</span></span><br></pre></td></tr></table></figure><h3 id="if-BOOLEAN-testCondition-T-valueTrue-T-valueFalseOrNull"><a href="#if-BOOLEAN-testCondition-T-valueTrue-T-valueFalseOrNull" class="headerlink" title="if(BOOLEAN testCondition, T valueTrue, T valueFalseOrNull)"></a>if(BOOLEAN testCondition, T valueTrue, T valueFalseOrNull)</h3><ul><li>解释</li></ul><p>如果testCondition条件为true，则返回第一个值，否则返回第二个值</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">if</span>(<span class="number">1</span> <span class="keyword">is</span> <span class="literal">null</span>,<span class="number">0</span>,<span class="number">1</span>)  <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">if</span>(<span class="literal">null</span> <span class="keyword">is</span> <span class="literal">null</span>,<span class="number">0</span>,<span class="number">1</span>) <span class="comment">-- 返回0</span></span><br></pre></td></tr></table></figure><h3 id="isnotnull-a"><a href="#isnotnull-a" class="headerlink" title="isnotnull(a)"></a>isnotnull(a)</h3><ul><li>解释</li></ul><p>如果参数a不为null，则返回true，否则返回false</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> isnotnull(<span class="number">1</span>) <span class="comment">-- 返回true</span></span><br><span class="line"><span class="keyword">select</span> isnotnull(<span class="literal">null</span>) <span class="comment">-- 返回false</span></span><br></pre></td></tr></table></figure><h3 id="isnull-a"><a href="#isnull-a" class="headerlink" title="isnull(a)"></a>isnull(a)</h3><ul><li>解释</li></ul><p>与isnotnull相反，如果参数a为null，则返回true，否则返回false</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">isnull</span>(<span class="literal">null</span>) <span class="comment">-- 返回true</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">isnull</span>(<span class="number">1</span>) <span class="comment">-- 返回false</span></span><br></pre></td></tr></table></figure><h3 id="nullif-a-b"><a href="#nullif-a-b" class="headerlink" title="nullif(a, b)"></a>nullif(a, b)</h3><ul><li>解释</li></ul><p>如果参数a=b，返回null，否则返回a值(Hive2.2.0版本)</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">nullif</span>(<span class="number">1</span>,<span class="number">2</span>) <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">nullif</span>(<span class="number">1</span>,<span class="number">1</span>) <span class="comment">-- 返回null</span></span><br></pre></td></tr></table></figure><h3 id="nvl-T-value-T-default-value"><a href="#nvl-T-value-T-default-value" class="headerlink" title="nvl(T value, T default_value)"></a>nvl(T value, T default_value)</h3><ul><li>解释</li></ul><p>如果value的值为null，则返回default_value默认值，否则返回value的值。在null值判断时，可以使用if函数给定默认值，也可以使用此函数给定默认值，使用该函数sql特别简洁。</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> nvl(<span class="number">1</span>,<span class="number">0</span>) <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> nvl(<span class="literal">null</span>,<span class="number">0</span>) <span class="comment">-- 返回0</span></span><br></pre></td></tr></table></figure><h2 id="日期函数"><a href="#日期函数" class="headerlink" title="日期函数"></a>日期函数</h2><h3 id="add-months-DATE-STRING-TIMESTAMP-start-date-INT-num-months"><a href="#add-months-DATE-STRING-TIMESTAMP-start-date-INT-num-months" class="headerlink" title="add_months(DATE|STRING|TIMESTAMP start_date, INT num_months)"></a>add_months(DATE|STRING|TIMESTAMP start_date, INT num_months)</h3><ul><li>解释</li></ul><p>start_date参数可以是string, date 或者timestamp类型，num_months参数时int类型。返回一个日期，该日期是在start_date基础之上加上num_months个月，即start_date之后null_months个月的一个日期。如果start_date的时间部分的数据会被忽略。注意：如果start_date所在月份的天数大于结果日期月的天数，则返回结果月的最后一天的日期。</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> add_months(<span class="string">"2020-05-20"</span>,<span class="number">2</span>); <span class="comment">-- 返回2020-07-20</span></span><br><span class="line"><span class="keyword">select</span> add_months(<span class="string">"2020-05-20"</span>,<span class="number">8</span>); <span class="comment">-- 返回2021-01-20</span></span><br><span class="line"><span class="keyword">select</span> add_months(<span class="string">"2020-05-31"</span>,<span class="number">1</span>); <span class="comment">-- 返回2020-06-30,5月有31天，6月只有30天，所以返回下一个月的最后一天</span></span><br></pre></td></tr></table></figure><h3 id="current-date"><a href="#current-date" class="headerlink" title="current_date"></a>current_date</h3><ul><li>解释</li></ul><p>返回查询时刻的当前日期</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">current_date</span>() <span class="comment">-- 返回当前查询日期2020-05-20</span></span><br></pre></td></tr></table></figure><h3 id="current-timestamp"><a href="#current-timestamp" class="headerlink" title="current_timestamp()"></a>current_timestamp()</h3><ul><li>解释</li></ul><p>返回查询时刻的当前时间</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">current_timestamp</span>() <span class="comment">-- 2020-05-20 14:40:47.273</span></span><br></pre></td></tr></table></figure><h3 id="datediff-STRING-enddate-STRING-startdate"><a href="#datediff-STRING-enddate-STRING-startdate" class="headerlink" title="datediff(STRING enddate, STRING startdate)"></a>datediff(STRING enddate, STRING startdate)</h3><ul><li>解释</li></ul><p>返回开始日期startdate与结束日期enddate之前相差的天数</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">datediff</span>(<span class="string">"2020-05-20"</span>,<span class="string">"2020-05-21"</span>); <span class="comment">-- 返回-1</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">datediff</span>(<span class="string">"2020-05-21"</span>,<span class="string">"2020-05-20"</span>); <span class="comment">-- 返回1</span></span><br></pre></td></tr></table></figure><h3 id="date-add-DATE-startdate-INT-days"><a href="#date-add-DATE-startdate-INT-days" class="headerlink" title="date_add(DATE startdate, INT days)"></a>date_add(DATE startdate, INT days)</h3><ul><li>解释</li></ul><p>在startdate基础上加上几天，然后返回加上几天之后的一个日期</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_add</span>(<span class="string">"2020-05-20"</span>,<span class="number">1</span>); <span class="comment">-- 返回2020-05-21,1表示加1天</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">date_add</span>(<span class="string">"2020-05-20"</span>,<span class="number">-1</span>); <span class="comment">-- 返回2020-05-19，-1表示减一天</span></span><br></pre></td></tr></table></figure><h3 id="date-sub-DATE-startdate-INT-days"><a href="#date-sub-DATE-startdate-INT-days" class="headerlink" title="date_sub(DATE startdate, INT days)"></a>date_sub(DATE startdate, INT days)</h3><ul><li>解释</li></ul><p>在startdate基础上减去几天，然后返回减去几天之后的一个日期,功能与date_add很类似</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_sub</span>(<span class="string">"2020-05-20"</span>,<span class="number">1</span>); <span class="comment">-- 返回2020-05-19,1表示减1天</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">date_sub</span>(<span class="string">"2020-05-20"</span>,<span class="number">-1</span>); <span class="comment">-- 返回2020-05-21，-1表示加1天</span></span><br></pre></td></tr></table></figure><h3 id="date-format-DATE-TIMESTAMP-STRING-ts-STRING-fmt"><a href="#date-format-DATE-TIMESTAMP-STRING-ts-STRING-fmt" class="headerlink" title="date_format(DATE|TIMESTAMP|STRING ts, STRING fmt)"></a>date_format(DATE|TIMESTAMP|STRING ts, STRING fmt)</h3><ul><li>解释</li></ul><p>将date/timestamp/string类型的值转换为一个具体格式化的字符串。支持java的SimpleDateFormat格式，第二个参数fmt必须是一个常量</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">'2020-05-20'</span>, <span class="string">'yyyy'</span>); <span class="comment">-- 返回2020</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">'2020-05-20'</span>, <span class="string">'MM'</span>); <span class="comment">-- 返回05</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">'2020-05-20'</span>, <span class="string">'dd'</span>); <span class="comment">-- 返回20</span></span><br><span class="line"><span class="comment">-- 返回2020年05月20日 00时00分00秒</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">'2020-05-20'</span>, <span class="string">'yyyy年MM月dd日 HH时mm分ss秒'</span>) ;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">date_format</span>(<span class="string">'2020-05-20'</span>, <span class="string">'yy/MM/dd'</span>) <span class="comment">-- 返回 20/05/20</span></span><br></pre></td></tr></table></figure><h3 id="dayofmonth-STRING-date"><a href="#dayofmonth-STRING-date" class="headerlink" title="dayofmonth(STRING date)"></a>dayofmonth(STRING date)</h3><ul><li>解释</li></ul><p>返回一个日期或时间的天,与day()函数功能相同</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">dayofmonth</span>(<span class="string">'2020-05-20'</span>) <span class="comment">-- 返回20</span></span><br></pre></td></tr></table></figure><h3 id="extract-field-FROM-source"><a href="#extract-field-FROM-source" class="headerlink" title="extract(field FROM source)"></a>extract(field FROM source)</h3><ul><li>解释</li></ul><p>提取 day, dayofweek, hour, minute, month, quarter, second, week 或者year的值，field可以选择day, dayofweek, hour, minute, month, quarter, second, week 或者year，source必须是一个date、timestamp或者可以转为 date 、timestamp的字符串。注意：Hive 2.2.0版本之后支持该函数</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">extract</span>(<span class="keyword">year</span> <span class="keyword">from</span> <span class="string">'2020-05-20 15:21:34.467'</span>); <span class="comment">-- 返回2020，年</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">extract</span>(<span class="keyword">quarter</span> <span class="keyword">from</span> <span class="string">'2020-05-20 15:21:34.467'</span>); <span class="comment">-- 返回2，季度</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">extract</span>(<span class="keyword">month</span> <span class="keyword">from</span> <span class="string">'2020-05-20 15:21:34.467'</span>); <span class="comment">-- 返回05，月份</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">extract</span>(<span class="keyword">week</span> <span class="keyword">from</span> <span class="string">'2020-05-20 15:21:34.467'</span>); <span class="comment">-- 返回21，同weekofyear，一年中的第几周</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">extract</span>(<span class="keyword">dayofweek</span> <span class="keyword">from</span> <span class="string">'2020-05-20 15:21:34.467'</span>); <span class="comment">-- 返回4,代表星期三</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">extract</span>(<span class="keyword">day</span> <span class="keyword">from</span> <span class="string">'2020-05-20 15:21:34.467'</span>); <span class="comment">-- 返回20，天</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">extract</span>(<span class="keyword">hour</span> <span class="keyword">from</span> <span class="string">'2020-05-20 15:21:34.467'</span>); <span class="comment">-- 返回15，小时</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">extract</span>(<span class="keyword">minute</span> <span class="keyword">from</span> <span class="string">'2020-05-20 15:21:34.467'</span>); <span class="comment">-- 返回21，分钟</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">extract</span>(<span class="keyword">second</span> <span class="keyword">from</span> <span class="string">'2020-05-20 15:21:34.467'</span>); <span class="comment">-- 返回34，秒</span></span><br></pre></td></tr></table></figure><h3 id="year-STRING-date"><a href="#year-STRING-date" class="headerlink" title="year(STRING date)"></a>year(STRING date)</h3><ul><li>解释</li></ul><p>返回时间的年份,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">year</span>(<span class="string">'2020-05-20 15:21:34'</span>); <span class="comment">-- 返回2020</span></span><br></pre></td></tr></table></figure><h3 id="quarter-DATE-TIMESTAMP-STRING-a"><a href="#quarter-DATE-TIMESTAMP-STRING-a" class="headerlink" title="quarter(DATE|TIMESTAMP|STRING a)"></a>quarter(DATE|TIMESTAMP|STRING a)</h3><ul><li>解释</li></ul><p>返回给定时间或日期的季度，1至4个季度,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">quarter</span>(<span class="string">'2020-05-20 15:21:34'</span>); <span class="comment">-- 返回2，第2季度</span></span><br></pre></td></tr></table></figure><h3 id="month-STRING-date"><a href="#month-STRING-date" class="headerlink" title="month(STRING date)"></a>month(STRING date)</h3><ul><li>解释</li></ul><p>返回时间的月份,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">month</span>(<span class="string">'2020-05-20 15:21:34'</span>) <span class="comment">-- 返回5</span></span><br></pre></td></tr></table></figure><h3 id="day-STRING-date"><a href="#day-STRING-date" class="headerlink" title="day(STRING date),"></a>day(STRING date),</h3><ul><li>解释</li></ul><p>返回一个日期或者时间的天,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">day</span>(<span class="string">"2020-05-20"</span>); <span class="comment">-- 返回20</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">day</span>(<span class="string">"2020-05-20 15:05:27.5"</span>); <span class="comment">-- 返回20</span></span><br></pre></td></tr></table></figure><h3 id="hour-STRING-date"><a href="#hour-STRING-date" class="headerlink" title="hour(STRING date)"></a>hour(STRING date)</h3><ul><li>解释</li></ul><p>返回一个时间的小时,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">hour</span>(<span class="string">'2020-05-20 15:21:34'</span>);<span class="comment">-- 返回15</span></span><br></pre></td></tr></table></figure><h3 id="minute-STRING-date"><a href="#minute-STRING-date" class="headerlink" title="minute(STRING date)"></a>minute(STRING date)</h3><ul><li>解释</li></ul><p>返回一个时间的分钟值,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">minute</span>(<span class="string">'2020-05-20 15:21:34'</span>); <span class="comment">-- 返回21</span></span><br></pre></td></tr></table></figure><h3 id="second-STRING-date"><a href="#second-STRING-date" class="headerlink" title="second(STRING date)"></a>second(STRING date)</h3><ul><li>解释</li></ul><p>返回一个时间的秒,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">second</span>(<span class="string">'2020-05-20 15:21:34'</span>); <span class="comment">--返回34</span></span><br></pre></td></tr></table></figure><h3 id="from-unixtime-BIGINT-unixtime-STRING-format"><a href="#from-unixtime-BIGINT-unixtime-STRING-format" class="headerlink" title="from_unixtime(BIGINT unixtime [, STRING format])"></a>from_unixtime(BIGINT unixtime [, STRING format])</h3><ul><li>解释</li></ul><p>将将Unix时间戳转换为字符串格式的时间(比如yyyy-MM-dd HH:mm:ss格式)</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1589960708</span>); <span class="comment">-- 返回2020-05-20 15:45:08</span></span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1589960708</span>, <span class="string">'yyyy-MM-dd hh:mm:ss'</span>); <span class="comment">-- -- 返回2020-05-20 15:45:08</span></span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1589960708</span>, <span class="string">'yyyy-MM-dd'</span>); <span class="comment">-- 返回2020-05-20</span></span><br></pre></td></tr></table></figure><h3 id="from-utc-timestamp-T-a-STRING-timezone"><a href="#from-utc-timestamp-T-a-STRING-timezone" class="headerlink" title="from_utc_timestamp(T a, STRING timezone)"></a>from_utc_timestamp(T a, STRING timezone)</h3><ul><li>解释</li></ul><p>转换为特定时区的时间</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">'2020-05-20 15:21:34'</span>,<span class="string">'PST'</span>); <span class="comment">-- 返回2020-05-20 08:21:34.0</span></span><br><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">'2020-05-20 15:21:34'</span>,<span class="string">'GMT'</span>); <span class="comment">-- 返回2020-05-20 15:21:34.0</span></span><br><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">'2020-05-20 15:21:34'</span>,<span class="string">'UTC'</span>); <span class="comment">-- 返回2020-05-20 15:21:34.0</span></span><br><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">'2020-05-20 15:21:34'</span>,<span class="string">'DST'</span>); <span class="comment">-- 返回2020-05-20 15:21:34.0</span></span><br><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">'2020-05-20 15:21:34'</span>,<span class="string">'CST'</span>); <span class="comment">-- 返回2020-05-20 10:21:34.0</span></span><br></pre></td></tr></table></figure><h3 id="last-day-STRING-date"><a href="#last-day-STRING-date" class="headerlink" title="last_day(STRING date)"></a>last_day(STRING date)</h3><ul><li>解释</li></ul><p>返回给定时间或日期所在月的最后一天，参数可以是’yyyy-MM-dd HH:mm:ss’ 或者 ‘yyyy-MM-dd’类型，时间部分会被忽略</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">last_day</span>(<span class="string">'2020-05-20 15:21:34'</span>); <span class="comment">-- 返回2020-05-31</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">last_day</span>(<span class="string">'2020-05-20'</span>); <span class="comment">-- 返回2020-05-31</span></span><br></pre></td></tr></table></figure><h3 id="to-date-STRING-timestamp"><a href="#to-date-STRING-timestamp" class="headerlink" title="to_date(STRING timestamp)"></a>to_date(STRING timestamp)</h3><ul><li>解释</li></ul><p>返回一个字符串时间的日期部分，去掉时间部分，2.1.0之前版本返回的是string，2.1.0版本及之后返回的是date</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">to_date</span>(<span class="string">'2020-05-20 15:21:34'</span>); <span class="comment">-- 返回2020-05-20</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">to_date</span>(<span class="string">'2020-05-20'</span>); <span class="comment">-- 返回2020-05-20</span></span><br></pre></td></tr></table></figure><h3 id="to-utc-timestamp-T-a-STRING-timezone"><a href="#to-utc-timestamp-T-a-STRING-timezone" class="headerlink" title="to_utc_timestamp(T a, STRING timezone)"></a>to_utc_timestamp(T a, STRING timezone)</h3><ul><li>解释</li></ul><p>转换为世界标准时间UTC的时间戳,与from_utc_timestamp类似</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> to_utc_timestamp(<span class="string">'2020-05-20 15:21:34'</span>, <span class="string">'GMT'</span>); <span class="comment">-- 返回2020-05-20 15:21:34.0</span></span><br></pre></td></tr></table></figure><h3 id="trunc-STRING-date-STRING-format"><a href="#trunc-STRING-date-STRING-format" class="headerlink" title="trunc(STRING date, STRING format)"></a>trunc(STRING date, STRING format)</h3><ul><li>解释</li></ul><p>截断日期到指定的日期精度，仅支持月（MONTH/MON/MM）或者年（YEAR/YYYY/YY）</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> trunc(<span class="string">'2020-05-20'</span>, <span class="string">'YY'</span>);   <span class="comment">-- 返回2020-01-01，返回年的1月1日</span></span><br><span class="line"><span class="keyword">select</span> trunc(<span class="string">'2020-05-20'</span>, <span class="string">'MM'</span>);   <span class="comment">-- 返回2020-05-01，返回月的第一天</span></span><br><span class="line"><span class="keyword">select</span> trunc(<span class="string">'2020-05-20 15:21:34'</span>, <span class="string">'MM'</span>);   <span class="comment">-- 返回2020-05-01</span></span><br></pre></td></tr></table></figure><h3 id="unix-timestamp-STRING-date-STRING-pattern"><a href="#unix-timestamp-STRING-date-STRING-pattern" class="headerlink" title="unix_timestamp([STRING date [, STRING pattern]])"></a>unix_timestamp([STRING date [, STRING pattern]])</h3><ul><li>解释</li></ul><p>参数时可选的，当参数为空时，返回当前Unix是时间戳，精确到秒。可以指定一个具体的日期，转换为Unix时间戳格式</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 返回1589959294</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">unix_timestamp</span>(<span class="string">'2020-05-20 15:21:34'</span>,<span class="string">'yyyy-MM-dd hh:mm:ss'</span>);</span><br><span class="line"><span class="comment">-- 返回1589904000</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">unix_timestamp</span>(<span class="string">'2020-05-20'</span>,<span class="string">'yyyy-MM-dd'</span>);</span><br></pre></td></tr></table></figure><h3 id="weekofyear-STRING-date"><a href="#weekofyear-STRING-date" class="headerlink" title="weekofyear(STRING date)"></a>weekofyear(STRING date)</h3><ul><li>解释</li></ul><p>返回一个日期或时间在一年中的第几周，可以用extract替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">weekofyear</span>(<span class="string">'2020-05-20 15:21:34'</span>); <span class="comment">-- 返回21，第21周</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">weekofyear</span>(<span class="string">'2020-05-20'</span>); <span class="comment">-- 返回21，第21周</span></span><br></pre></td></tr></table></figure><h3 id="next-day-STRING-start-date-STRING-day-of-week"><a href="#next-day-STRING-start-date-STRING-day-of-week" class="headerlink" title="next_day(STRING start_date, STRING day_of_week)"></a>next_day(STRING start_date, STRING day_of_week)</h3><ul><li>解释</li></ul><p>参数start_date可以是一个时间或日期，day_of_week表示星期几，比如Mo表示星期一，Tu表示星期二，Wed表示星期三，Thur表示星期四，Fri表示星期五，Sat表示星期六，Sun表示星期日。如果指定的星期几在该日期所在的周且在该日期之后，则返回当周的星期几日期，如果指定的星期几不在该日期所在的周，则返回下一个星期几对应的日期</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> next_day(<span class="string">'2020-05-20'</span>,<span class="string">'Mon'</span>);<span class="comment">-- 返回当前日期的下一个周一日期:2020-05-25</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">'2020-05-20'</span>,<span class="string">'Tu'</span>);<span class="comment">-- 返回当前日期的下一个周二日期:2020-05-26</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">'2020-05-20'</span>,<span class="string">'Wed'</span>);<span class="comment">-- 返回当前日期的下一个周三日期:2020-05-27</span></span><br><span class="line"><span class="comment">-- 2020-05-20为周三，指定的参数为周四，所以返回当周的周四就是2020-05-21</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">'2020-05-20'</span>,<span class="string">'Th'</span>);</span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">'2020-05-20'</span>,<span class="string">'Fri'</span>);<span class="comment">-- 返回周五日期2020-05-22</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">'2020-05-20'</span>,<span class="string">'Sat'</span>); <span class="comment">-- 返回周六日期2020-05-23</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">'2020-05-20'</span>,<span class="string">'Sun'</span>); <span class="comment">-- 返回周六日期2020-05-24</span></span><br></pre></td></tr></table></figure><p>该函数比较重要：比如取当前日期所在的周一和周日，通过长用在按周进行汇总数据</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">date_add</span>(next_day(<span class="string">'2020-05-20'</span>,<span class="string">'MO'</span>),<span class="number">-7</span>); <span class="comment">-- 返回当前日期的周一日期2020-05-18</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">date_add</span>(next_day(<span class="string">'2020-05-20'</span>,<span class="string">'MO'</span>),<span class="number">-1</span>); <span class="comment">-- 返回当前日期的周日日期2020-05-24</span></span><br></pre></td></tr></table></figure><h3 id="months-between-DATE-TIMESTAMP-STRING-date1-DATE-TIMESTAMP-STRING-date2"><a href="#months-between-DATE-TIMESTAMP-STRING-date1-DATE-TIMESTAMP-STRING-date2" class="headerlink" title="months_between(DATE|TIMESTAMP|STRING date1, DATE|TIMESTAMP|STRING date2)"></a>months_between(DATE|TIMESTAMP|STRING date1, DATE|TIMESTAMP|STRING date2)</h3><ul><li>解释</li></ul><p>返回 date1 和 date2 的月份差。如果date1大于date2，返回正值，否则返回负值，如果是相减是整数月，则返回一个整数，否则会返回小数</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> months_between(<span class="string">'2020-05-20'</span>,<span class="string">'2020-05-20'</span>); <span class="comment">-- 返回0</span></span><br><span class="line"><span class="keyword">select</span> months_between(<span class="string">'2020-05-20'</span>,<span class="string">'2020-06-20'</span>); <span class="comment">-- 返回-1</span></span><br><span class="line"><span class="comment">-- 相差的整数月</span></span><br><span class="line"><span class="keyword">select</span> months_between(<span class="string">'2020-06-30'</span>,<span class="string">'2020-05-31'</span>); <span class="comment">-- 返回1</span></span><br><span class="line"><span class="comment">-- 非整数月，一个月差一天</span></span><br><span class="line"><span class="keyword">select</span> months_between(<span class="string">'2020-06-29'</span>,<span class="string">'2020-05-31'</span>); <span class="comment">-- 返回0.93548387</span></span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了Hive的条件函数和日期函数，并给出了每个函数的解释说明和使用案例，本文覆盖了所有Hive内置的条件函数和日期函数，可以作为一个函数字典，方便工作中使用。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Greenplum集群Master与Segment节点故障检测与恢复</title>
      <link href="/2020/05/18/Greenplum%E9%9B%86%E7%BE%A4Master%E4%B8%8ESegment%E8%8A%82%E7%82%B9%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B%E4%B8%8E%E6%81%A2%E5%A4%8D/"/>
      <url>/2020/05/18/Greenplum%E9%9B%86%E7%BE%A4Master%E4%B8%8ESegment%E8%8A%82%E7%82%B9%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B%E4%B8%8E%E6%81%A2%E5%A4%8D/</url>
      
        <content type="html"><![CDATA[<p>Greenplum集群主要包括Master节点和Segment节点，Master节点称之为主节点，Segment节点称之为数据节点。Master节点与Segment节点都是有备份的，其中Master节点的备节点为Standby Master(不能够自动故障转移)，Segment是通过Primary Segment与Mirror Segment进行容错的。通过本文你可以了解：</p><ul><li>Greenplum数据库的高可用(HA)原理</li><li>Greenplum生产集群中master节点故障恢复</li><li>greenplum生产集群中segment故障检测与恢复</li><li>Segment节点故障恢复原理与实践</li></ul><h2 id="Greenplum数据库的HA"><a href="#Greenplum数据库的HA" class="headerlink" title="Greenplum数据库的HA"></a>Greenplum数据库的HA</h2><h3 id="master-mirroring概述"><a href="#master-mirroring概述" class="headerlink" title="master mirroring概述"></a>master mirroring概述</h3><p>可以在单独的主机或同一主机上部署master实例的备份或镜像。如果primary master服务器宕机，则standby master服务器将用作热备用服务器。在primary master服务器在线时，可以从primary master服务器创建备用master服务器。</p><p>Primary master服务器持续为用户提供服务，同时获取Primary master实例的事务快照。在standby master服务器上部署事务快照时，还会记录对primary master服务器的更改。在standby master服务器上部署快照后，更新也会被部署，用于使standby master服务器与primary master服务器同步。</p><p>Primary master服务器和备用master服务器同步后，standbymaster服务器将通过walsender 和 walreceiver 的复制进程保持最新状态。该walreceiver是standby master上的进程， walsender流程是primary master上的流程。这两个进程使用基于预读日志（WAL）的流复制来保持primary master和standby master服务器同步。在WAL日志记录中，所有修改都会在应用生效之前写入日志，以确保任何进程内操作的数据完整性。</p><p>由于primary master不包含任何用户数据，因此只需要在主master和备份master之间同步系统目录表(catalog tables)。当这些表发生更新时，更改的结果会自动复制到备用master上，以确保与主master同步。</p><p>如果primary master发生故障，管理员需要使用gpactivatestandby工具激活standby master。可以为primary master和standby master配置一个虚拟IP地址，这样，在primary master出现故障时，客户端程序就不用切换到其他的网络地址，因为在master出现故障时，虚拟IP地址可以交换到充当primary master的主机上。</p><h3 id="mirror-segment概述"><a href="#mirror-segment概述" class="headerlink" title="mirror segment概述"></a>mirror segment概述</h3><p>当启用Greenplum数据库高可用性时，有两种类型的segment：primary和mirror。每个主segment具有一个对应的mirror segment。主segment接收来自master的请求以更改主segment的数据库，然后将这些更改复制到相应的mirror segment上。如果主segment不可用，则数据库查询将故障转移到mirror segment上。</p><p>Mirror segment采用物理文件复制的方案—primary segment中数据文件I / O被复制到mirror segment上，因此mirror segment的文件与primary segment上的文件相同。Greenplum数据库中的数据用元组(tuple)表示，元组被打包成块（block）。数据库的表存储在由一个或多个块组成的磁盘文件中。对元组进行更改操作，同时会更改保存的块，然后将其写入primary segment上的磁盘并通过网络复制到mirror segment。Mirror segment只更新其文件副本中的相应块。</p><p>对于堆表(heap)而言，块被保存在内存缓存区中，直到为新更改的块腾出空间时，才会将它们清除，这允许系统可以多次读取或更新内存中的块，而无需执行昂贵的磁盘I / O。 当块从缓存中清除时，它将被写入磁盘并复制到mirror segment主机的磁盘。当块保存在缓存中时，primary segment和mirror segment具有不同的块镜像，但是，数据库仍然是一致的，因为事务日志已经被复制了。</p><p>AO表(Append-optimized)不使用内存缓存机制。对AO表的块所做的更改会立即复制到mirror segment上。通常，文件写入操作是异步的，但是打开、创建和同步文件是“同步复制”的，这意味着primary segment的块需要从mirror segment上接收确认。</p><p>如果primary segment失败，则文件复制进程将会停止，mirror segment会自动作为活动segment实例，活动mirror segment的系统状态会变为“ 更改跟踪”(<em>Change Tracking</em>)，这意味着在primary segment不可用时，mirror segment维护着一个系统表和记录所有块的更改日志。当故障的primary segment被修复好，并准备重新上线时，管理员启动恢复过程，系统进入重新同步状态。恢复过程将记录的更改日志应用于已修复的primary segment上。恢复过程完成后，mirror segment的系统状态将更改为“已同步 ”。</p><p>如果mirror segment在primary segment处于活动状态时失败或无法访问，则primary segment的系统状态将更改为“ 更改跟踪”，并且它会记录更改的状态，用于mirror segment的恢复。</p><ul><li><strong>group mirroring方式</strong></li></ul><p>只要primary segment实例和mirror segment实例位于不同的主机上，mirror segment就可以以不同的配置方式放置在群集中的主机上。每个主机必须具有相同数量的mirror segment和primary segment。默认mirror segment配置方式是group mirroring，其中每个主机的primary segment的mirror segment位于另一个主机上。如果单个主机发生故障，则部署该主机的mirror segment主机上的活动primary segment数量会翻倍，从而会加大该主机的负载。下图说明了group mirroring配置。</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/grouping.png" alt></p><ul><li><strong>Spread mirroring方式</strong></li></ul><p><em>Spread mirroring</em>方式是指将每个主机的mirror segment分布在多个主机上，这样，如果任何单个主机发生故障，该主机的mirror segment会分散到其他多个主机上运行，从而达到负载均衡的效果。仅当主机数量多于每个主机的segment数时，才可以使用<em>Spread</em>方式。下图说明了<em>Spread mirroring</em>方式。</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/spread.png" alt></p><h2 id="Master节点故障恢复"><a href="#Master节点故障恢复" class="headerlink" title="Master节点故障恢复"></a>Master节点故障恢复</h2><p>如果primary master节点失败，日志复制进程就会停止。可以使用<code>gpstate -f</code>命令查看standby master的状态，使用<code>gpactivatestandby</code>命令激活standby master。</p><h3 id="激活Standby-master"><a href="#激活Standby-master" class="headerlink" title="激活Standby master"></a>激活Standby master</h3><ul><li><p>(1)确保原来的集群中配置了standby master</p></li><li><p>(2)在standby master主机上运行gpactivatestandby命令</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpactivatestandby -d /data/master/gpseg-1</span><br></pre></td></tr></table></figure><p><code>-d</code>参数是指standby master的数据目录，一旦激活成功，原来的standby master就成为了primary master。</p></li><li><p>(3)执行激活命令后，运行gpstate命令检查状态</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -f</span><br></pre></td></tr></table></figure></li></ul><p>  新激活的master的状态是active，如果已经为集群配置一个新的standby master节点，则其状态会是passive。如果还没有为集群配置一个新的standby master，则会看到下面的信息：No entries found，该信息表明尚未配置standby master。</p><ul><li><p>(4)在成功切换到了standbymaster之后，运行ANALYZE命令，收集该数据库的统计信息</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ psql postgres -c <span class="string">'ANALYZE;'</span></span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(5)可选：如果在成功激活standby master之后，尚未指定新的standby master，可以在active master上运行gpinitstandby命令，配置一个新的standby master</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpinitstandby -s new_standby_master_hostname</span><br></pre></td></tr></table></figure></li></ul><h3 id="恢复到原来的设置-可选的"><a href="#恢复到原来的设置-可选的" class="headerlink" title="恢复到原来的设置(可选的)"></a>恢复到原来的设置(可选的)</h3><ul><li><p>(1)确保之前的master节点能够正常使用</p></li><li><p>(2)在原来的master主机上，移除(备份)原来的数据目录gpseg-1,比如：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mv /data/master/gpseg-1  /data/master/backup_gpseg-1</span><br></pre></td></tr></table></figure></li><li><p>(3)在原来的master节点上，初始化standby master，在active master上运行如下命令</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpinitstandby -s mdw</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(4)初始化完成之后，检查standby master的状态</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -f</span><br></pre></td></tr></table></figure></li></ul><p>   显示的状态应该是–Sync state: sync</p><ul><li><p>(5)在active master节点上运行下面的命令，用于停止master</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstop -m</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(6)在原来的master节点(mdw)上运行gpactivatestandby命令</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpactivatestandby -d /data/master/gpseg-1</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(7)在上述命名运行结束之后，再运行gpstate命令查看状态</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -f</span><br></pre></td></tr></table></figure></li></ul><p>  确认原始的primary master状态是active。</p><ul><li><p>(8)在原来的standby master节点(smdw)上，移除(备份)数据目录gpseg-1</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mv /data/master/gpseg-1  /data/master/backup_gpseg-1</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(9)原来的master节点正常运行之后，在该节点上执行如下命令，用于激活standby master</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpinitstandby -s smdw</span><br></pre></td></tr></table></figure></li></ul><h3 id="检查standby-master的状态"><a href="#检查standby-master的状态" class="headerlink" title="检查standby master的状态"></a>检查standby master的状态</h3><p>可以通过查看视图pg_stat_replication，来获取更多的信息。该视图可以列出walsender进程的信息，下面的命令是查看walsender进程的进程id和状态信息。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ psql postgres -c <span class="string">'SELECT procpid, state FROM pg_stat_replication;'</span></span><br></pre></td></tr></table></figure><h2 id="segment节点故障检测与恢复"><a href="#segment节点故障检测与恢复" class="headerlink" title="segment节点故障检测与恢复"></a>segment节点故障检测与恢复</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>Greenplum数据库服务器（Postgres）有一个子进程，该子进程为ftsprobe，主要作用是处理故障检测。 ftsprobe 监视Greenplum数据库阵列，它以可以配置的间隔连接并扫描所有segment和数据库进程。<br>如果 ftsprobe无法连接到segment，它会在Greenplum数据库系统目录中将segment标记为”down”。在管理员启动恢复进程之前，该segment是不可以被操作的。</p><p>启用mirror备份后，如果primary segment不可用，Greenplum数据库会自动故障转移到mirror segment。如果segment实例或主机发生故障，系统仍可以运行，前提是所有在剩余的活动segment上数据都可用。</p><p>要恢复失败的segment，管理员需要执行 gprecoverseg 恢复工具。此工具可以找到失败的segment，验证它们是否有效，并将事务状态与当前活动的segment进行比较，以确定在segment脱机时所做的更改。gprecoverseg将更改的数据库文件与活动segment同步，并使该segment重新上线。管理员需要在在Greenplum数据库启动并运行时执行恢复操作。</p><p>禁用mirror备份时，如果segment实例失败，系统将会自动关闭。管理员需要手动恢复所有失败的segment。</p><h3 id="检测和管理失败的segment"><a href="#检测和管理失败的segment" class="headerlink" title="检测和管理失败的segment"></a>检测和管理失败的segment</h3><h4 id="使用工具命令查看"><a href="#使用工具命令查看" class="headerlink" title="使用工具命令查看"></a>使用工具命令查看</h4><p>启用mirror备份后，当primary segment发生故障时，Greenplum会自动故障转移到mirror segment。如果每个数据部分所在的segment实例都是在线的，则用户可能无法意识到segment已经出现故障。如果在发生故障时正在进行事务，则正在进行的事务将回滚并在重新配置的segment集上自动重新启动。</p><p>如果整个Greenplum数据库系统由于segment故障而变得不可访问（例如，如果未启用mirror备份或没有足够的segment在线），则用户在尝试连接数据库时将看到错误。返回到客户端程序的错误可能表示失败。例如：<br><code>ERROR: All segment databases are unavailable</code></p><ul><li><p>(1)在master节点上，运行gpstate命令，使用-e参数显示错误的segment</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -e</span><br></pre></td></tr></table></figure><p>   标记为<code>Change Tracking</code>的segment节点表明对应的mirror segment已经宕机。</p></li><li><p>(2)要获取有关故障segment的详细信息，可以查看 gp_segment_configuration目录表</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ psql -c <span class="string">"SELECT * FROM gp_segment_configuration WHERE status='d';"</span></span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(3) 对于失败的segment实例，记下主机，端口，初始化时的角色和数据目录。此信息将帮助确定要进行故障排除的主机和segment实例。</p></li><li><p>(4) 显示mirror segment详细信息，运行下面命名：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -m</span><br></pre></td></tr></table></figure></li></ul><h4 id="检查日志文件"><a href="#检查日志文件" class="headerlink" title="检查日志文件"></a>检查日志文件</h4><p>  日志文件可以提供信息以帮助确定错误的原因。Master实例和segment实例都有自己的日志文件，这些日志文件位于pg_log的目录下。Master的日志文件包含最多信息，应该首先检查它。</p><p>  使用 gplogfilter工具检查Greenplum数据库日志文件，可以获取额外信息。要检查segment日志文件，可以在master主机上使用gpssh命令运行 gplogfilter。</p><ul><li><p>(1)使用 gplogfilter 检查master日志文件的WARNING, ERROR, FATAL 或者 PANIC日志级别消息</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gplogfilter -t</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(2)使用 gpssh 检查每个segment实例上的日志级别为WARNING, ERROR, FATAL 或者 PANIC的消息。例如：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpssh -f seg_hosts_file -e <span class="string">'source</span></span><br><span class="line"><span class="string">/usr/local/greenplum-db/greenplum_path.sh ; gplogfilter -t</span></span><br><span class="line"><span class="string">/data1/primary/*/pg_log/gpdb*.log'</span> &gt; seglog.out</span><br></pre></td></tr></table></figure></li></ul><h3 id="恢复失败的segment"><a href="#恢复失败的segment" class="headerlink" title="恢复失败的segment"></a>恢复失败的segment</h3><p>  如果master服务器无法连接到segment实例，则会在Greenplum数据库系统目录中将该segment标记为“down”状态。在管理员采取措施使segment实例重新上线之前，segment实例将保持脱机离线状态。segment实例可能由于多种原因而不可用：</p><ul><li><p>(1)segment主机不可用; 例如，由于网络或硬件故障。</p></li><li><p>(2)segment实例未运行; 例如，没Postgres的数据库监听进程。</p></li><li><p>(3)segment实例的数据目录损坏或丢失; 例如，无法访问数据，文件系统已损坏或磁盘发生故障。</p><h4 id="在启用mirror-segment的情况下进行恢复"><a href="#在启用mirror-segment的情况下进行恢复" class="headerlink" title="在启用mirror segment的情况下进行恢复"></a>在启用mirror segment的情况下进行恢复</h4></li><li><p>(1)确保master主机能够ping通失败的segment主机</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ping failed_seg_host_address</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(2)如果是阻止master主机连接segment主机，则可以重启该segment主机。</p></li><li><p>(3)如果该segment主机上线之后，可以通过master连接，则在master主机上运行下面命令，重新激活失败的segment</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gprecoverseg</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(4)恢复进程会显示故障segment并标识需要同步的已更改文件。这个过程可能需要一些时间， 等待该过程完成。在此过程中，数据库不允许写入操作。</p></li><li><p>(5)在 gprecoverseg完成后，系统进入重新同步模式并开始复制已更改的文件。当系统处于联机状态并接受数据库请求时，此进程在后台运行。</p></li><li><p>(6)重新同步过程完成后，系统状态为“已同步”（ Synchronized）。运行gpstate 命令用于验证重新同步过程状态</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -m</span><br></pre></td></tr></table></figure></li></ul><h4 id="将所有的segment恢复到原来的角色设置"><a href="#将所有的segment恢复到原来的角色设置" class="headerlink" title="将所有的segment恢复到原来的角色设置"></a>将所有的segment恢复到原来的角色设置</h4><p>  当primary segment发生故障时，mirror segment会被激活为primary segment。运行gprecoverseg命令之后，当前活动的segment是primary segment，失败的primary segment成为了mirror segment。segment实例不会返回到在系统初始化时配置的首选角色。这意味着某些segment主机上可能运行多个primary segment实例，而某些segment主机上运行较少的segment，即系统可能处于潜在的不平衡状态。要检查不平衡的segment并重新平衡系统，可以使用如下命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -e</span><br></pre></td></tr></table></figure><p>  所有segment必须在线并完全同步以重新平衡系统，数据库会话在重新平衡期间保持连接，但正在进行的查询将被取消并回滚。</p><ul><li><p>(1)运行下面命令，查看mirror segment的角色和同步状态</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -m</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(2)如果有mirror segment处于非同步状态，等待他们同步完成</p></li><li><p>(3)运行gprecoverseg命令，使用-r参数将segment恢复到原来初始化时的角色设置</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gprecoverseg -r</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(4)运行gpstate -e命令，确认所有的segment是否恢复到初始化时的角色设置</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -e</span><br></pre></td></tr></table></figure></li></ul><h4 id="从双重故障中恢复"><a href="#从双重故障中恢复" class="headerlink" title="从双重故障中恢复"></a>从双重故障中恢复</h4><p>  在双重故障情况下，即primary segment和mirror segment都处于失败状态。如果不同segment的主机同时发生硬件故障，则会导致primary segment和mirror segment都处于失败状态，如果发生双重故障，Greenplum数据库将不可用。要从双重故障中恢复，执行如下步骤：</p><ul><li><p>(1)重启greenplum数据库</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstop -r</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(2)再重启系统之后，运行gprecoverseg命令</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gprecoverseg</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(3)在gprecoverseg执行结束后，运行gpstate命令查看mirror状态信息</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="variable">$gpstate</span> -m</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>(4)如果segment仍是“Change Tracking”状态，则运行下面命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gprecoverseg -F</span><br></pre></td></tr></table></figure></li></ul><h4 id="从segment主机故障中恢复"><a href="#从segment主机故障中恢复" class="headerlink" title="从segment主机故障中恢复"></a>从segment主机故障中恢复</h4><p>  如果主机处于不可操作状态（例如，由于硬件故障），可以将segment恢复到备用主机上。如果启用了mirror segment，则可以使用gprecoverseg命令将mirror segment恢复到备用主机。例如：</p> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gprecoverseg -i recover_config_file</span><br></pre></td></tr></table></figure><p>生成的recover_config_file文件的格式为：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">filespaceOrder=[filespace1_name[:filespace2_name:...]failed_host_address:</span><br><span class="line">port:fselocation [recovery_host_address:port:replication_port:fselocation</span><br><span class="line">[:fselocation:...]]</span><br></pre></td></tr></table></figure><p>​     例如，要在没有配置其他文件空间的情况下恢复到与故障主机不同的另一台主机（除了默认的pg_system文件空间）：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">filespaceOrder=sdw5-2:50002:/gpdata/gpseg2 sdw9-2:50002:53002:/gpdata/gpseg2</span><br></pre></td></tr></table></figure><p> 该gp_segment_configuration和pg_filespace_entry系统目录表可以帮助确定当前的段配置，这样可以计划mirror的恢复配置。例如，运行以下查询：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">=<span class="comment"># SELECT dbid, content, hostname, address, port,</span></span><br><span class="line">replication_port, fselocation as datadir</span><br><span class="line">FROM gp_segment_configuration, pg_filespace_entry</span><br><span class="line">WHERE dbid=fsedbid</span><br><span class="line">ORDER BY dbid;</span><br></pre></td></tr></table></figure><p>上述命令会输出:</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/%E7%BB%93%E6%9E%9C%E8%BE%93%E5%87%BA.png" alt></p><p>新恢复的segment主机必须预先安装Greenplum数据库软件，并且其配置要与现有的segment主机一致。</p><h2 id="Segment故障恢复原理与实践"><a href="#Segment故障恢复原理与实践" class="headerlink" title="Segment故障恢复原理与实践"></a>Segment故障恢复原理与实践</h2><h3 id="greenplum集群环境介绍"><a href="#greenplum集群环境介绍" class="headerlink" title="greenplum集群环境介绍"></a>greenplum集群环境介绍</h3><p>该生产环境集群由四台服务器构成，其中一台为primary master节点，一台为standby master节点，两外两台为segment节点，每个segment节点有四个segment(两个primary segment，两个mirror segment)，segment采用group方式进行备份(sdw1的备份都在sdw2上，sdw2的备份都在sdw1上)，其角色分配如下图所示：</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83.png" alt></p><h3 id="segment故障检查"><a href="#segment故障检查" class="headerlink" title="segment故障检查"></a>segment故障检查</h3><ul><li>gpstate -m日志信息</li></ul><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstatem.png" alt></p><ul><li>gpstate -c 日志信息</li></ul><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstatec.png" alt></p><ul><li>gpstate -e 日志信息</li></ul><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstatee.png" alt></p><ul><li><p>gpstate -s 日志信息</p><p>(1)sdw1节点的日志信息</p></li></ul><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstates1.png" alt></p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstates2.png" alt></p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstates3.png" alt></p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstates4.png" alt></p><p>  (1)sdw2节点的日志信息</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstates5.png" alt></p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstates6.png" alt></p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstates7.png" alt></p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstates8.png" alt></p><h3 id="故障说明"><a href="#故障说明" class="headerlink" title="故障说明"></a>故障说明</h3><p>Sdw1节点primary segment正常，mirror segment被激活，其mirror segment为sdw2节点上的primary segment备份。Sdw2节点primary segment失败，mirror segment失败。此时集群环境能够正常提供服务，全部负载到sdw1节点上。</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/%E6%95%85%E9%9A%9C%E8%AF%B4%E6%98%8E1.png" alt></p><p>使用<code>select * from gp_segment_configuration</code>查看segment角色信息，如下图所示：</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/%E6%95%85%E9%9A%9C%E8%AF%B4%E6%98%8E2.png" alt></p><h3 id="segment故障恢复"><a href="#segment故障恢复" class="headerlink" title="segment故障恢复"></a>segment故障恢复</h3><ul><li>在master主机上运行下面命令，重新激活失败的segment</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gprecoverseg</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/%E6%81%A2%E5%A4%8D1.png" alt></p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/%E6%81%A2%E5%A4%8D2.png" alt></p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/%E6%81%A2%E5%A4%8D3.png" alt></p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/%E6%81%A2%E5%A4%8D4.png" alt></p><ul><li>运行gpstate 命令用于验证重新同步过程状态</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpstate -m</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpstatem%E6%81%A2%E5%A4%8D.png" alt></p><p>当primary segment发生故障时，mirror segment会被激活为primary segment。运行gprecoverseg命令之后，失败的primary segment成为了mirror segment，而被激活的mirror segment成为了primary segment，segment实例不会返回到在系统初始化时配置的首选角色。这意味着某些segment主机上可能运行多个primary segment实例，而某些segment主机上运行较少的segment，即系统可能处于潜在的不平衡状态。如下图所示，sdw1上的mirror segment变为了primary segment，sdw2上的primary segment变为了mirror segment。即sdw2的primary segment运行在sdw1节点上，系统处于不平衡状态。</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/%E4%B8%8D%E5%B9%B3%E8%A1%A1.png" alt></p><p>此时GPCC的状态为：</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpcc1.png" alt></p><ul><li><p>运行gprecoverseg命令，使用-r参数将segment恢复到原来初始化时的角色设置</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gprecoverseg -r</span><br></pre></td></tr></table></figure><p>查看gpcc状态:</p><p><img src="//jiamaoxiang.top/2020/05/18/Greenplum集群Master与Segment节点故障检测与恢复/gpcc2.png" alt></p></li></ul><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了GP的高可用原理及实践。首先介绍了Master与Segment的容错策略，然后介绍了Master节点与Segment节点故障恢复的步骤，最后给出了一个完整的实践过程。</p>]]></content>
      
      
      <categories>
          
          <category> greenplum </category>
          
      </categories>
      
      
        <tags>
            
            <tag> greenplum </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>是时候换掉你的调度系统了</title>
      <link href="/2020/05/18/%E6%98%AF%E6%97%B6%E5%80%99%E6%8D%A2%E6%8E%89%E4%BD%A0%E7%9A%84%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F%E4%BA%86/"/>
      <url>/2020/05/18/%E6%98%AF%E6%97%B6%E5%80%99%E6%8D%A2%E6%8E%89%E4%BD%A0%E7%9A%84%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F%E4%BA%86/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h2 id="克隆源码"><a href="#克隆源码" class="headerlink" title="克隆源码"></a>克隆源码</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/scxwhite/hera.git</span><br></pre></td></tr></table></figure><h2 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; create database hera</span><br><span class="line">mysql&gt; use hera;</span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; <span class="built_in">source</span> /opt/hera.sql</span><br></pre></td></tr></table></figure><h3 id="修改application-yml"><a href="#修改application-yml" class="headerlink" title="修改application.yml"></a>修改application.yml</h3><h3 id="打包部署"><a href="#打包部署" class="headerlink" title="打包部署"></a>打包部署</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mvn clean package -Dmaven.test.skip -Pdev</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2020/05/18/是时候换掉你的调度系统了/mvn%E6%89%93%E5%8C%85.png" alt></p><p>打包后可以进入<code>hera-admin/target</code>目录下查看打包后的<code>hera-dev.jar</code> 。此时可以简单使用<code>java -server -Xms4G -Xmx4G -Xmn2G -jar hera.jar</code>启动项目，此时即可在浏览器中输入</p><p><img src="//jiamaoxiang.top/2020/05/18/是时候换掉你的调度系统了/%E6%89%93%E5%8C%85%E7%BB%93%E6%9E%9C.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> hera </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hera </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink DataSet API编程指南</title>
      <link href="/2020/05/09/Flink-DataSet-API%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/"/>
      <url>/2020/05/09/Flink-DataSet-API%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>Flink最大的亮点是实时处理部分，Flink认为批处理是流处理的特殊情况，可以通过一套引擎处理批量和流式数据，而Flink在未来也会重点投入更多的资源到批流融合中。我在<a href="https://mp.weixin.qq.com/s/rllW7XS9m-BH-2lxp_N8QA" target="_blank" rel="noopener">Flink DataStream API编程指南</a>中介绍了DataStream API的使用，在本文中将介绍Flink批处理计算的DataSet API的使用。通过本文你可以了解：</p><ul><li>DataSet转换操作(Transformation)</li><li>Source与Sink的使用</li><li>广播变量的基本概念与使用Demo</li><li>分布式缓存的概念及使用Demo</li><li>DataSet API的Transformation使用Demo案例</li></ul><h2 id="WordCount示例"><a href="#WordCount示例" class="headerlink" title="WordCount示例"></a>WordCount示例</h2><p>在开始讲解DataSet API之前，先看一个Word Count的简单示例，来直观感受一下DataSet API的编程模型，具体代码如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 用于批处理的执行环境</span></span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 数据源</span></span><br><span class="line">        DataSource&lt;String&gt; stringDataSource = env.fromElements(<span class="string">"hello Flink What is Apache Flink"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 转换</span></span><br><span class="line">        AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; wordCnt = stringDataSource</span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        String[] split = value.split(<span class="string">" "</span>);</span><br><span class="line">                        <span class="keyword">for</span> (String word : split) &#123;</span><br><span class="line">                            out.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .groupBy(<span class="number">0</span>)</span><br><span class="line">                .sum(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 输出</span></span><br><span class="line">        wordCnt.print();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上面的示例中可以看出，基本的编程模型是：</p><ul><li>获取批处理的执行环境ExecutionEnvironment</li><li>加载数据源</li><li>转换操作</li><li>数据输出</li></ul><p>下面会对数据源、转换操作、数据输出进行一一解读。</p><h2 id="Data-Source"><a href="#Data-Source" class="headerlink" title="Data Source"></a>Data Source</h2><p>DataSet API支持从多种数据源中将批量数据集读到Flink系统中，并转换成DataSet数据集。主要包括三种类型：分别是基于文件的、基于集合的及通用类数据源。同时在DataSet API中可以自定义实现InputFormat/RichInputFormat接口，以接入不同数据格式类型的数据源，比如CsvInputFormat、TextInputFormat等。从ExecutionEnvironment类提供的方法中可以看出支持的数据源方法，如下图所示：</p><p><img src="//jiamaoxiang.top/2020/05/09/Flink-DataSet-API编程指南/dataset%E6%95%B0%E6%8D%AE%E6%BA%90.png" alt></p><h3 id="基于文件的数据源"><a href="#基于文件的数据源" class="headerlink" title="基于文件的数据源"></a>基于文件的数据源</h3><h4 id="readTextFile-path-TextInputFormat"><a href="#readTextFile-path-TextInputFormat" class="headerlink" title="readTextFile(path) / TextInputFormat"></a>readTextFile(path) / TextInputFormat</h4><ul><li>解释</li></ul><p>读取文本文件，传递文件路径参数，并将文件内容转换成DataSet<string>类型数据集。</string></p><ul><li>使用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 读取本地文件</span></span><br><span class="line">DataSet&lt;String&gt; localLines = env.readTextFile(<span class="string">"file:///path/to/my/textfile"</span>);</span><br><span class="line"><span class="comment">// 读取HDSF文件</span></span><br><span class="line">DataSet&lt;String&gt; hdfsLines = env.readTextFile(<span class="string">"hdfs://nnHost:nnPort/path/to/my/textfile"</span>);</span><br></pre></td></tr></table></figure><h4 id="readTextFileWithValue-path-TextValueInputFormat"><a href="#readTextFileWithValue-path-TextValueInputFormat" class="headerlink" title="readTextFileWithValue(path)/ TextValueInputFormat"></a>readTextFileWithValue(path)/ TextValueInputFormat</h4><ul><li>解释</li></ul><p>读取文本文件内容，将文件内容转换成DataSet[StringValue]类型数据集。该方法与readTextFile(String)不同的是，其泛型是StringValue，是一种可变的String类型，通过StringValue存储文本数据可以有效降低String对象创建数量，减小垃圾回收的压力。</p><ul><li>使用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 读取本地文件</span></span><br><span class="line">DataSet&lt;StringValue&gt; localLines = env.readTextFileWithValue(<span class="string">"file:///some/local/file"</span>);</span><br><span class="line"><span class="comment">// 读取HDSF文件</span></span><br><span class="line">DataSet&lt;StringValue&gt; hdfsLines = env.readTextFileWithValue(<span class="string">"hdfs://host:port/file/path"</span>);</span><br></pre></td></tr></table></figure><h4 id="readCsvFile-path-CsvInputFormat"><a href="#readCsvFile-path-CsvInputFormat" class="headerlink" title="readCsvFile(path)/ CsvInputFormat"></a>readCsvFile(path)/ CsvInputFormat</h4><ul><li>解释</li></ul><p>创建一个CSV的reader，读取逗号分隔(或其他分隔符)的文件。可以直接转换成Tuple类型、POJOs类的DataSet。在方法中可以指定行切割符、列切割符、字段等信息。</p><ul><li>使用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// read a CSV file with five fields, taking only two of them</span></span><br><span class="line"><span class="comment">// 读取一个具有5个字段的CSV文件，只取第一个和第四个字段</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Double&gt;&gt; csvInput = env.readCsvFile(<span class="string">"hdfs:///the/CSV/file"</span>)</span><br><span class="line">                               .includeFields(<span class="string">"10010"</span>)  </span><br><span class="line">                          .types(String.class, Double.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取一个有三个字段的CSV文件，将其转为POJO类型</span></span><br><span class="line">DataSet&lt;Person&gt;&gt; csvInput = env.readCsvFile(<span class="string">"hdfs:///the/CSV/file"</span>)</span><br><span class="line">                         .pojoType(Person.class, <span class="string">"name"</span>, <span class="string">"age"</span>, <span class="string">"zipcode"</span>);</span><br></pre></td></tr></table></figure><h4 id="readFileOfPrimitives-path-Class-PrimitiveInputFormat"><a href="#readFileOfPrimitives-path-Class-PrimitiveInputFormat" class="headerlink" title="readFileOfPrimitives(path, Class) / PrimitiveInputFormat"></a>readFileOfPrimitives(path, Class) / PrimitiveInputFormat</h4><ul><li>解释</li></ul><p>读取一个原始数据类型(如String,Integer)的文件,返回一个对应的原始类型的DataSet集合</p><ul><li>使用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;String&gt; Data = env.readFileOfPrimitives(<span class="string">"file:///some/local/file"</span>, String.class);</span><br></pre></td></tr></table></figure><h3 id="基于集合的数据源"><a href="#基于集合的数据源" class="headerlink" title="基于集合的数据源"></a>基于集合的数据源</h3><h4 id="fromCollection-Collection"><a href="#fromCollection-Collection" class="headerlink" title="fromCollection(Collection)"></a>fromCollection(Collection)</h4><ul><li>解释</li></ul><p>从java的集合中创建DataSet数据集，集合中的元素数据类型相同</p><ul><li>使用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;String&gt; data= env.fromCollection(arrayList);</span><br></pre></td></tr></table></figure><h4 id="fromElements-T-…"><a href="#fromElements-T-…" class="headerlink" title="fromElements(T …)"></a>fromElements(T …)</h4><ul><li>解释</li></ul><p>从给定数据元素序列中创建DataSet数据集，且所有的数据对象类型必须一致</p><ul><li>使用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;String&gt; stringDataSource = env.fromElements(<span class="string">"hello Flink What is Apache Flink"</span>);</span><br></pre></td></tr></table></figure><h4 id="generateSequence-from-to"><a href="#generateSequence-from-to" class="headerlink" title="generateSequence(from, to)"></a>generateSequence(from, to)</h4><ul><li>解释</li></ul><p>指定from到to范围区间，然后在区间内部生成数字序列数据集,由于是并行处理的，所以最终的顺序不能保证一致。</p><ul><li>使用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Long&gt; longDataSource = env.generateSequence(<span class="number">1</span>, <span class="number">20</span>);</span><br></pre></td></tr></table></figure><h3 id="通用类型数据源"><a href="#通用类型数据源" class="headerlink" title="通用类型数据源"></a>通用类型数据源</h3><p>DataSet API中提供了Inputformat通用的数据接口，以接入不同数据源和格式类型的数据。InputFormat接口主要分为两种类型：一种是基于文件类型，在DataSet API对应readFile()方法；另外一种是基于通用数据类型的接口，例如读取RDBMS或NoSQL数据库中等，在DataSet API中对应createInput()方法。</p><h4 id="readFile-inputFormat-path-FileInputFormat"><a href="#readFile-inputFormat-path-FileInputFormat" class="headerlink" title="readFile(inputFormat, path) / FileInputFormat"></a>readFile(inputFormat, path) / FileInputFormat</h4><ul><li>解释</li></ul><p>自定义文件类型输入源，将指定格式文件读取并转成DataSet数据集</p><ul><li>使用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.readFile(<span class="keyword">new</span> MyInputFormat(), <span class="string">"file:///some/local/file"</span>);</span><br></pre></td></tr></table></figure><h4 id="createInput-inputFormat-InputFormat"><a href="#createInput-inputFormat-InputFormat" class="headerlink" title="createInput(inputFormat) / InputFormat"></a>createInput(inputFormat) / InputFormat</h4><ul><li>解释</li></ul><p>自定义通用型数据源，将读取的数据转换为DataSet数据集。如以下实例使用Flink内置的JDBCInputFormat，创建读取mysql数据源的JDBCInput Format，完成从mysql中读取Person表，并转换成DataSet [Row]数据集</p><ul><li>使用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt; dbData =</span><br><span class="line">    env.createInput(</span><br><span class="line">      JDBCInputFormat.buildJDBCInputFormat()</span><br><span class="line">                     .setDrivername(<span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">                     .setDBUrl(<span class="string">"jdbc:mysql://localhost/mydb"</span>)</span><br><span class="line">                     .setQuery(<span class="string">"select name, age from stu"</span>)</span><br><span class="line">                     .setRowTypeInfo(<span class="keyword">new</span> RowTypeInfo(BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.INT_TYPE_INFO))</span><br><span class="line">                     .finish()</span><br><span class="line">    );</span><br></pre></td></tr></table></figure><h2 id="Data-Sink"><a href="#Data-Sink" class="headerlink" title="Data Sink"></a>Data Sink</h2><p>Flink在DataSet API中的数据输出共分为三种类型。第一种是基于文件实现，对应DataSet的write()方法，实现将DataSet数据输出到文件系统中。第二种是基于通用存储介质实现，对应DataSet的output()方法，例如使用JDBCOutputFormat将数据输出到关系型数据库中。最后一种是客户端输出，直接将DataSet数据从不同的节点收集到Client，并在客户端中输出，例如DataSet的print()方法。</p><h3 id="标准的数据输出方法"><a href="#标准的数据输出方法" class="headerlink" title="标准的数据输出方法"></a>标准的数据输出方法</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 文本数据</span></span><br><span class="line">DataSet&lt;String&gt; textData = <span class="comment">// [...]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 将数据写入本地文件</span></span><br><span class="line">textData.writeAsText(<span class="string">"file:///my/result/on/localFS"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将数据写入HDFS文件</span></span><br><span class="line">textData.writeAsText(<span class="string">"hdfs://nnHost:nnPort/my/result/on/localFS"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 写数据到本地文件，如果文件存在则覆盖</span></span><br><span class="line">textData.writeAsText(<span class="string">"file:///my/result/on/localFS"</span>, WriteMode.OVERWRITE);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将数据输出到本地的CSV文件，指定分隔符为"|"</span></span><br><span class="line">DataSet&lt;Tuple3&lt;String, Integer, Double&gt;&gt; values = <span class="comment">// [...]</span></span><br><span class="line">values.writeAsCsv(<span class="string">"file:///path/to/the/result/file"</span>, <span class="string">"\n"</span>, <span class="string">"|"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用自定义的TextFormatter对象</span></span><br><span class="line">values.writeAsFormattedText(<span class="string">"file:///path/to/the/result/file"</span>,</span><br><span class="line">    <span class="keyword">new</span> TextFormatter&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">format</span> <span class="params">(Tuple2&lt;Integer, Integer&gt; value)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> value.f1 + <span class="string">" - "</span> + value.f0;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><h3 id="使用自定义的输出类型"><a href="#使用自定义的输出类型" class="headerlink" title="使用自定义的输出类型"></a>使用自定义的输出类型</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple3&lt;String, Integer, Double&gt;&gt; myResult = [...]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将tuple类型的数据写入关系型数据库</span></span><br><span class="line">myResult.output(</span><br><span class="line">    <span class="comment">// 创建并配置OutputFormat</span></span><br><span class="line">    JDBCOutputFormat.buildJDBCOutputFormat()</span><br><span class="line">                    .setDrivername(<span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">                    .setDBUrl(<span class="string">"jdbc:mysql://localhost/mydb"</span>)</span><br><span class="line">                    .setQuery(<span class="string">"insert into persons (name, age, height) values (?,?,?)"</span>)</span><br><span class="line">                    .finish()</span><br><span class="line">    );</span><br></pre></td></tr></table></figure><h2 id="DataSet转换"><a href="#DataSet转换" class="headerlink" title="DataSet转换"></a>DataSet转换</h2><p>转换(transformations)将一个DataSet转成另外一个DataSet，Flink提供了非常丰富的转换操作符。具体使用如下：</p><h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><p>一进一出</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;String&gt; source = env.fromElements(<span class="string">"I"</span>, <span class="string">"like"</span>, <span class="string">"flink"</span>);</span><br><span class="line">      source.map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">          <span class="meta">@Override</span></span><br><span class="line">          <span class="comment">// 将数据转为大写</span></span><br><span class="line">          <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">              <span class="keyword">return</span> value.toUpperCase();</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;).print();</span><br></pre></td></tr></table></figure><h3 id="FlatMap"><a href="#FlatMap" class="headerlink" title="FlatMap"></a>FlatMap</h3><p>输入一个元素，产生0个、1个或多个元素</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stringDataSource</span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        String[] split = value.split(<span class="string">" "</span>);</span><br><span class="line">                        <span class="keyword">for</span> (String word : split) &#123;</span><br><span class="line">                            out.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .groupBy(<span class="number">0</span>)</span><br><span class="line">                .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h3 id="MapPartition"><a href="#MapPartition" class="headerlink" title="MapPartition"></a>MapPartition</h3><p>功能和Map函数相似，只是MapPartition操作是在DataSet中基于分区对数据进行处理，函数调用中会按照分区将数据通过Iteator的形式传入，每个分区中的元素数与并行度有关，并返回任意数量的结果值。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">source.mapPartition(<span class="keyword">new</span> MapPartitionFunction&lt;String, Long&gt;() &#123;</span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">           <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mapPartition</span><span class="params">(Iterable&lt;String&gt; values, Collector&lt;Long&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">               <span class="keyword">long</span> c = <span class="number">0</span>;</span><br><span class="line">               <span class="keyword">for</span> (String value : values) &#123;</span><br><span class="line">                   c++;</span><br><span class="line">               &#125;</span><br><span class="line">               <span class="comment">//输出每个分区元素个数</span></span><br><span class="line">               out.collect(c);</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;).print();</span><br></pre></td></tr></table></figure><h3 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h3><p>过滤数据，如果返回true则保留数据，如果返回false则过滤掉</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Long&gt; source = env.fromElements(<span class="number">1L</span>, <span class="number">2L</span>, <span class="number">3L</span>,<span class="number">4L</span>,<span class="number">5L</span>);</span><br><span class="line">        source.filter(<span class="keyword">new</span> FilterFunction&lt;Long&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Long value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value % <span class="number">2</span> == <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();</span><br></pre></td></tr></table></figure><h3 id="Project"><a href="#Project" class="headerlink" title="Project"></a><strong>Project</strong></h3><p>仅能用在Tuple类型的数据集，投影操作，选取Tuple数据的字段的子集</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Tuple3&lt;Long, Integer, String&gt;&gt; source = env.fromElements(</span><br><span class="line">              Tuple3.of(<span class="number">1L</span>, <span class="number">20</span>, <span class="string">"tom"</span>), </span><br><span class="line">              Tuple3.of(<span class="number">2L</span>, <span class="number">25</span>, <span class="string">"jack"</span>), </span><br><span class="line">              Tuple3.of(<span class="number">3L</span>, <span class="number">22</span>, <span class="string">"bob"</span>));</span><br><span class="line">      <span class="comment">// 去第一个和第三个元素</span></span><br><span class="line">      source.project(<span class="number">0</span>, <span class="number">2</span>).print();</span><br></pre></td></tr></table></figure><h3 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a><strong>Reduce</strong></h3><p>通过两两合并，将数据集中的元素合并成一个元素，可以在整个数据集上使用，也可以在分组之后的数据集上使用。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Tuple2&lt;String, Integer&gt;&gt; source = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Hadoop"</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Spark"</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1</span>));</span><br><span class="line">        source</span><br><span class="line">                .groupBy(<span class="number">0</span>)</span><br><span class="line">                .reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Tuple2.of(value1.f0, value1.f1 + value2.f1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();</span><br></pre></td></tr></table></figure><h3 id="ReduceGroup"><a href="#ReduceGroup" class="headerlink" title="ReduceGroup"></a><strong>ReduceGroup</strong></h3><p>将数据集中的元素合并成一个元素，可以在整个数据集上使用，也可以在分组之后的数据集上使用。reduce函数的输入值是一个分组元素的Iterable。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Tuple2&lt;String, Long&gt;&gt; source = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Hadoop"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Spark"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1L</span>));</span><br><span class="line">        source</span><br><span class="line">                .groupBy(<span class="number">0</span>)</span><br><span class="line">                .reduceGroup(<span class="keyword">new</span> GroupReduceFunction&lt;Tuple2&lt;String,Long&gt;, Tuple2&lt;String,Long&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Iterable&lt;Tuple2&lt;String, Long&gt;&gt; values, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        Long sum = <span class="number">0L</span>;</span><br><span class="line">                        String word = <span class="string">""</span>;</span><br><span class="line">                        <span class="keyword">for</span>(Tuple2&lt;String, Long&gt; value:values)&#123;</span><br><span class="line">                            sum += value.f1;</span><br><span class="line">                            word = value.f0;</span><br><span class="line"></span><br><span class="line">                        &#125;</span><br><span class="line">                        out.collect(Tuple2.of(word,sum));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).print();</span><br></pre></td></tr></table></figure><h3 id="Aggregate"><a href="#Aggregate" class="headerlink" title="Aggregate"></a><strong>Aggregate</strong></h3><p>通过Aggregate Function将一组元素值合并成单个值，可以在整个DataSet数据集上使用，也可以在分组之后的数据集上使用。仅仅用在Tuple类型的数据集上，主要包括Sum,Min,Max函数</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Tuple2&lt;String, Long&gt;&gt; source = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Hadoop"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Spark"</span>, <span class="number">1L</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Flink"</span>, <span class="number">1L</span>));</span><br><span class="line">        source</span><br><span class="line">                .groupBy(<span class="number">0</span>)</span><br><span class="line">                .aggregate(SUM,<span class="number">1</span>)<span class="comment">// 按第2个值求和</span></span><br><span class="line">                 .print();</span><br></pre></td></tr></table></figure><h3 id="Distinct"><a href="#Distinct" class="headerlink" title="Distinct"></a><strong>Distinct</strong></h3><p>DataSet数据集元素去重</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Tuple&gt; source = env.fromElements(Tuple1.of(<span class="string">"Flink"</span>),Tuple1.of(<span class="string">"Flink"</span>),Tuple1.of(<span class="string">"hadoop"</span>));</span><br><span class="line">        source.distinct(<span class="number">0</span>).print();<span class="comment">// 按照tuple的第一个字段去重</span></span><br><span class="line"><span class="comment">// 结果：</span></span><br><span class="line">(Flink)</span><br><span class="line">(hadoop)</span><br></pre></td></tr></table></figure><h3 id="Join"><a href="#Join" class="headerlink" title="Join"></a><strong>Join</strong></h3><p>默认的join是产生一个Tuple2数据类型的DataSet，关联的key可以通过key表达式、Key-selector函数、字段位置以及CaseClass字段指定。对于两个Tuple类型的数据集可以通过字段位置进行关联，左边数据集的字段通过where方法指定，右边数据集的字段通过equalTo()方法指定。比如：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSource&lt;Tuple2&lt;Integer,String&gt;&gt; source1 = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="number">1</span>,<span class="string">"jack"</span>),</span><br><span class="line">                Tuple2.of(<span class="number">2</span>,<span class="string">"tom"</span>),</span><br><span class="line">                Tuple2.of(<span class="number">3</span>,<span class="string">"Bob"</span>));</span><br><span class="line">        DataSource&lt;Tuple2&lt;String, Integer&gt;&gt; source2 = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"order1"</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"order2"</span>, <span class="number">2</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"order3"</span>, <span class="number">3</span>));</span><br><span class="line">        source1.join(source2).where(<span class="number">0</span>).equalTo(<span class="number">1</span>).print();</span><br></pre></td></tr></table></figure><p>可以在关联的过程中指定自定义Join Funciton, Funciton的入参为左边数据集中的数据元素和右边数据集的中的数据元素所组成的元祖，并返回一个经过计算处理后的数据。如：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 用户id，购买商品名称，购买商品数量</span></span><br><span class="line">        DataSource&lt;Tuple3&lt;Integer,String,Integer&gt;&gt; source1 = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1</span>,<span class="string">"item1"</span>,<span class="number">2</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2</span>,<span class="string">"item2"</span>,<span class="number">3</span>),</span><br><span class="line">                Tuple3.of(<span class="number">3</span>,<span class="string">"item3"</span>,<span class="number">4</span>));</span><br><span class="line">        <span class="comment">//商品名称与商品单价</span></span><br><span class="line">        DataSource&lt;Tuple2&lt;String, Integer&gt;&gt; source2 = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"item1"</span>, <span class="number">10</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"item2"</span>, <span class="number">20</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"item3"</span>, <span class="number">15</span>));</span><br><span class="line">        source1.join(source2)</span><br><span class="line">                .where(<span class="number">1</span>)</span><br><span class="line">                .equalTo(<span class="number">0</span>)</span><br><span class="line">                .with(<span class="keyword">new</span> JoinFunction&lt;Tuple3&lt;Integer,String,Integer&gt;, Tuple2&lt;String,Integer&gt;, Tuple3&lt;Integer,String,Double&gt;&gt;() &#123;</span><br><span class="line">                    <span class="comment">// 用户每种商品购物总金额</span></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, String, Double&gt; <span class="title">join</span><span class="params">(Tuple3&lt;Integer, String, Integer&gt; first, Tuple2&lt;String, Integer&gt; second)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> Tuple3.of(first.f0,first.f1,first.f2 * second.f1.doubleValue());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).print();</span><br></pre></td></tr></table></figure><p>为了能够更好地引导Flink底层去正确地处理数据集，可以在DataSet数据集关联中，通过Size Hint标记数据集的大小，Flink可以根据用户给定的hint(提示)调整计算策略，例如可以使用joinWithTiny或joinWithHuge提示第二个数据集的大小。示例如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;Integer, String&gt;&gt; input1 = <span class="comment">// [...]</span></span><br><span class="line">DataSet&lt;Tuple2&lt;Integer, String&gt;&gt; input2 = <span class="comment">// [...]</span></span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;Tuple2&lt;Integer, String&gt;, Tuple2&lt;Integer, String&gt;&gt;&gt;</span><br><span class="line">            result1 =</span><br><span class="line">            <span class="comment">// 提示第二个数据集为小数据集</span></span><br><span class="line">            input1.joinWithTiny(input2)</span><br><span class="line">                  .where(<span class="number">0</span>)</span><br><span class="line">                  .equalTo(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;Tuple2&lt;Integer, String&gt;, Tuple2&lt;Integer, String&gt;&gt;&gt;</span><br><span class="line">            result2 =</span><br><span class="line">            <span class="comment">// h提示第二个数据集为大数据集</span></span><br><span class="line">            input1.joinWithHuge(input2)</span><br><span class="line">                  .where(<span class="number">0</span>)</span><br><span class="line">                  .equalTo(<span class="number">0</span>);</span><br></pre></td></tr></table></figure><p>Flink的runtime可以使用多种方式执行join。在不同的情况下，每种可能的方式都会胜过其他方式。系统会尝试自动选择一种合理的方法，但是允许用户手动选择一种策略， 可以让Flink更加灵活且高效地执行Join操作。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;SomeType&gt; input1 = <span class="comment">// [...]</span></span><br><span class="line">DataSet&lt;AnotherType&gt; input2 = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// 广播第一个输入并从中构建一个哈希表，第二个输入将对其进行探测，适用于第一个数据集非常小的场景</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result =</span><br><span class="line">      input1.join(input2, JoinHint.BROADCAST_HASH_FIRST)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br><span class="line"><span class="comment">// 广播第二个输入并从中构建一个哈希表，第一个输入将对其进行探测，适用于第二个数据集非常小的场景</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result =</span><br><span class="line">      input1.join(input2, JoinHint.BROADCAST_HASH_SECOND)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br><span class="line"><span class="comment">// 将两个数据集重新分区，并将第一个数据集转换成哈希表，适用于第一个数据集比第二个数据集小，但两个数据集都比较大的场景</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result =</span><br><span class="line">      input1.join(input2, JoinHint.REPARTITION_HASH_FIRST)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br><span class="line"><span class="comment">// 将两个数据集重新分区，并将第二个数据集转换成哈希表，适用于第二个数据集比第一个数据集小，但两个数据集都比较大的场景</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result =</span><br><span class="line">      input1.join(input2, JoinHint.REPARTITION_HASH_SECOND)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br><span class="line"><span class="comment">// 将两个数据集重新分区，并将每个分区排序，适用于两个数据集都已经排好序的场景</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result =</span><br><span class="line">      input1.join(input2, JoinHint.REPARTITION_SORT_MERGE)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br><span class="line"><span class="comment">// 相当于不指定，有系统自行处理</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result =</span><br><span class="line">      input1.join(input2, JoinHint.OPTIMIZER_CHOOSES)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br></pre></td></tr></table></figure><h3 id="OuterJoin"><a href="#OuterJoin" class="headerlink" title="OuterJoin"></a>OuterJoin</h3><p>OuterJoin对两个数据集进行外关联，包含left、right、full outer join三种关联方式，分别对应DataSet API中的leftOuterJoin、rightOuterJoin以及fullOuterJoin方法。注意外连接仅适用于Java 和 Scala DataSet API.</p><p>使用方式几乎和join类似：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//左外连接</span></span><br><span class="line">source1.leftOuterJoin(source2).where(<span class="number">1</span>).equalTo(<span class="number">0</span>);</span><br><span class="line"><span class="comment">//右外链接</span></span><br><span class="line">source1.rightOuterJoin(source2).where(<span class="number">1</span>).equalTo(<span class="number">0</span>);</span><br></pre></td></tr></table></figure><p>此外，外连接也提供了相应的关联算法提示，可以跟据左右数据集的分布情况选择合适的优化策略，提升数据处理的效率。下面代码可以参考上面join的解释。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;SomeType&gt; input1 = <span class="comment">// [...]</span></span><br><span class="line">DataSet&lt;AnotherType&gt; input2 = <span class="comment">// [...]</span></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result1 =</span><br><span class="line">      input1.leftOuterJoin(input2, JoinHint.REPARTITION_SORT_MERGE)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;SomeType, AnotherType&gt; result2 =</span><br><span class="line">      input1.rightOuterJoin(input2, JoinHint.BROADCAST_HASH_FIRST)</span><br><span class="line">            .where(<span class="string">"id"</span>).equalTo(<span class="string">"key"</span>);</span><br></pre></td></tr></table></figure><p>对于外连接的关联算法，与join有所不同。每种外连接只支持部分算法。如下：</p><ul><li><p>LeftOuterJoin支持：</p><ul><li><p>OPTIMIZER_CHOOSES</p></li><li><p>BROADCAST_HASH_SECOND</p></li><li><p>REPARTITION_HASH_SECOND</p></li><li><p>REPARTITION_SORT_MERGE</p><ul><li>RightOuterJoin支持：<pre><code>- OPTIMIZER_CHOOSES- BROADCAST_HASH_FIRST- REPARTITION_HASH_FIRST- REPARTITION_SORT_MERGE</code></pre></li><li>FullOuterJoin支持：<ul><li>OPTIMIZER_CHOOSES</li><li>REPARTITION_SORT_MERGE</li></ul></li></ul></li></ul></li></ul><h3 id="CoGroup"><a href="#CoGroup" class="headerlink" title="CoGroup"></a><strong>CoGroup</strong></h3><p> CoGroup是对分组之后的DataSet进行join操作，将两个DataSet数据集合并在一起，会先各自对每个DataSet按照key进行分组，然后将分组之后的DataSet传输到用户定义的CoGroupFunction，将两个数据集根据相同的Key记录组合在一起，相同Key的记录会存放在一个Group中，如果指定key仅在一个数据集中有记录，则co-groupFunction会将这个Group与空的Group关联。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 用户id，购买商品名称，购买商品数量</span></span><br><span class="line">        DataSource&lt;Tuple3&lt;Integer,String,Integer&gt;&gt; source1 = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1</span>,<span class="string">"item1"</span>,<span class="number">2</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2</span>,<span class="string">"item2"</span>,<span class="number">3</span>),</span><br><span class="line">                Tuple3.of(<span class="number">3</span>,<span class="string">"item2"</span>,<span class="number">4</span>));</span><br><span class="line">        <span class="comment">//商品名称与商品单价</span></span><br><span class="line">        DataSource&lt;Tuple2&lt;String, Integer&gt;&gt; source2 = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"item1"</span>, <span class="number">10</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"item2"</span>, <span class="number">20</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"item3"</span>, <span class="number">15</span>));</span><br><span class="line"></span><br><span class="line">        source1.coGroup(source2)</span><br><span class="line">                .where(<span class="number">1</span>)</span><br><span class="line">                .equalTo(<span class="number">0</span>)</span><br><span class="line">                .with(<span class="keyword">new</span> CoGroupFunction&lt;Tuple3&lt;Integer,String,Integer&gt;, Tuple2&lt;String,Integer&gt;, Tuple2&lt;String,Double&gt;&gt;() &#123;</span><br><span class="line">                    <span class="comment">// 每个Iterable存储的是分好组的数据，即相同key的数据组织在一起</span></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">coGroup</span><span class="params">(Iterable&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; first, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; second, Collector&lt;Tuple2&lt;String, Double&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">//存储每种商品购买数量</span></span><br><span class="line">                        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">                        <span class="keyword">for</span>(Tuple3&lt;Integer, String, Integer&gt; val1:first)&#123;</span><br><span class="line">                        sum += val1.f2;</span><br><span class="line"></span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="comment">// 每种商品数量 * 商品单价</span></span><br><span class="line">                    <span class="keyword">for</span>(Tuple2&lt;String, Integer&gt; val2:second)&#123;</span><br><span class="line">                        out.collect(Tuple2.of(val2.f0,sum * val2.f1.doubleValue()));</span><br><span class="line"></span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).print();</span><br></pre></td></tr></table></figure><h3 id="Cross"><a href="#Cross" class="headerlink" title="Cross"></a><strong>Cross</strong></h3><p>将两个数据集合并成一个数据集，返回被连接的两个数据集所有数据行的笛卡儿积，返回的数据行数等于第一个数据集中符合查询条件的数据行数乘以第二个数据集中符合查询条件的数据行数。Cross操作可以通过应用Cross Funciton将关联的数据集合并成目标格式的数据集，如果不指定Cross Funciton则返回Tuple2类型的数据集。Cross操作是计算密集型的算子，建议在使用时加上算法提示，比如<em>crossWithTiny()</em> and <em>crossWithHuge()</em>.</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//[id,x,y],坐标值</span></span><br><span class="line">        DataSet&lt;Tuple3&lt;Integer, Integer, Integer&gt;&gt; coords1 = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1</span>, <span class="number">20</span>, <span class="number">18</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2</span>, <span class="number">15</span>, <span class="number">20</span>),</span><br><span class="line">                Tuple3.of(<span class="number">3</span>, <span class="number">25</span>, <span class="number">10</span>));</span><br><span class="line">        DataSet&lt;Tuple3&lt;Integer, Integer, Integer&gt;&gt; coords2 = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1</span>, <span class="number">20</span>, <span class="number">18</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2</span>, <span class="number">15</span>, <span class="number">20</span>),</span><br><span class="line">                Tuple3.of(<span class="number">3</span>, <span class="number">25</span>, <span class="number">10</span>));</span><br><span class="line">        <span class="comment">// 求任意两点之间的欧氏距离</span></span><br><span class="line"></span><br><span class="line">        coords1.cross(coords2)</span><br><span class="line">                .with(<span class="keyword">new</span> CrossFunction&lt;Tuple3&lt;Integer, Integer, Integer&gt;, Tuple3&lt;Integer, Integer, Integer&gt;, Tuple3&lt;Integer, Integer, Double&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, Integer, Double&gt; <span class="title">cross</span><span class="params">(Tuple3&lt;Integer, Integer, Integer&gt; val1, Tuple3&lt;Integer, Integer, Integer&gt; val2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">// 计算欧式距离</span></span><br><span class="line">                        <span class="keyword">double</span> dist = sqrt(pow(val1.f1 - val2.f1, <span class="number">2</span>) + pow(val1.f2 - val2.f2, <span class="number">2</span>));</span><br><span class="line">                        <span class="comment">// 返回两点之间的欧式距离</span></span><br><span class="line">                        <span class="keyword">return</span> Tuple3.of(val1.f0,val2.f0,dist);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).print();</span><br></pre></td></tr></table></figure><h3 id="Union"><a href="#Union" class="headerlink" title="Union"></a><strong>Union</strong></h3><p>合并两个DataSet数据集，两个数据集的数据元素格式必须相同，多个数据集可以连续合并.</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals1 = env.fromElements(</span><br><span class="line">           Tuple2.of(<span class="string">"jack"</span>,<span class="number">20</span>),</span><br><span class="line">           Tuple2.of(<span class="string">"Tom"</span>,<span class="number">21</span>));</span><br><span class="line">   DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals2 = env.fromElements(</span><br><span class="line">           Tuple2.of(<span class="string">"Robin"</span>,<span class="number">25</span>),</span><br><span class="line">           Tuple2.of(<span class="string">"Bob"</span>,<span class="number">30</span>));</span><br><span class="line">   DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals3 = env.fromElements(</span><br><span class="line">           Tuple2.of(<span class="string">"Jasper"</span>,<span class="number">24</span>),</span><br><span class="line">           Tuple2.of(<span class="string">"jarry"</span>,<span class="number">21</span>));</span><br><span class="line">   DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; unioned = vals1</span><br><span class="line">           .union(vals2)</span><br><span class="line">           .union(vals3);</span><br><span class="line">   unioned.print();</span><br></pre></td></tr></table></figure><h3 id="Rebalance"><a href="#Rebalance" class="headerlink" title="Rebalance"></a><strong>Rebalance</strong></h3><p>对数据集中的数据进行平均分布，使得每个分区上的数据量相同,减轻数据倾斜造成的影响，注意仅仅是<code>Map-like</code>类型的算子(比如map，flatMap)才可以用在Rebalance算子之后。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;String&gt; in = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// rebalance DataSet,然后使用map算子.</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, String&gt;&gt; out = in.rebalance()</span><br><span class="line">                                        .map(<span class="keyword">new</span> Mapper());</span><br></pre></td></tr></table></figure><h3 id="Hash-Partition"><a href="#Hash-Partition" class="headerlink" title="Hash-Partition"></a><strong>Hash-Partition</strong></h3><p>根据给定的Key进行Hash分区，key相同的数据会被放入同一个分区内。可以使用通过元素的位置、元素的名称或者key selector函数指定key。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; in = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// 根据第一个值进行hash分区，然后使用 MapPartition转换操作.</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, String&gt;&gt; out = in.partitionByHash(<span class="number">0</span>)</span><br><span class="line">                                        .mapPartition(<span class="keyword">new</span> PartitionMapper());</span><br></pre></td></tr></table></figure><h3 id="Range-Partition"><a href="#Range-Partition" class="headerlink" title="Range-Partition"></a><strong>Range-Partition</strong></h3><p>根据给定的Key进行Range分区，key相同的数据会被放入同一个分区内。可以使用通过元素的位置、元素的名称或者key selector函数指定key。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; in = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// 根据第一个值进行Range分区，然后使用 MapPartition转换操作.</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, String&gt;&gt; out = in.partitionByRange(<span class="number">0</span>)</span><br><span class="line">                                        .mapPartition(<span class="keyword">new</span> PartitionMapper());</span><br></pre></td></tr></table></figure><h3 id="Custom-Partitioning"><a href="#Custom-Partitioning" class="headerlink" title="Custom Partitioning"></a><strong>Custom Partitioning</strong></h3><p>除了上面的分区外，还支持自定义分区函数。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String,Integer&gt;&gt; in = <span class="comment">// [...]</span></span><br><span class="line">DataSet&lt;Integer&gt; result = in.partitionCustom(partitioner, key)</span><br><span class="line">                            .mapPartition(<span class="keyword">new</span> PartitionMapper());</span><br></pre></td></tr></table></figure><h3 id="Sort-Partition"><a href="#Sort-Partition" class="headerlink" title="Sort Partition"></a><strong>Sort Partition</strong></h3><p>在本地对DataSet数据集中的所有分区根据指定字段进行重排序，排序方式通过Order.ASCENDING以及Order.DESCENDING关键字指定。支持指定多个字段进行分区排序，如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; in = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// 按照第一个字段升序排列，第二个字段降序排列.</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, String&gt;&gt; out = in.sortPartition(<span class="number">1</span>, Order.ASCENDING)</span><br><span class="line">                                        .sortPartition(<span class="number">0</span>, Order.DESCENDING)</span><br><span class="line">                                        .mapPartition(<span class="keyword">new</span> PartitionMapper());</span><br></pre></td></tr></table></figure><h3 id="First-n"><a href="#First-n" class="headerlink" title="First-n"></a>First-n</h3><p>返回数据集的n条随机结果，可以应用于常规类型数据集、Grouped类型数据集以及排序数据集上。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; in = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// 返回数据集中的任意5个元素</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; out1 = in.first(<span class="number">5</span>);</span><br><span class="line"><span class="comment">//返回每个分组内的任意两个元素</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; out2 = in.groupBy(<span class="number">0</span>)</span><br><span class="line">                                          .first(<span class="number">2</span>);</span><br><span class="line"><span class="comment">// 返回每个分组内的前三个元素</span></span><br><span class="line"><span class="comment">// 分组后的数据集按照第二个字段进行升序排序</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; out3 = in.groupBy(<span class="number">0</span>)</span><br><span class="line">                                          .sortGroup(<span class="number">1</span>, Order.ASCENDING)</span><br><span class="line">                                          .first(<span class="number">3</span>);</span><br></pre></td></tr></table></figure><h3 id="MinBy-MaxBy"><a href="#MinBy-MaxBy" class="headerlink" title="MinBy / MaxBy"></a>MinBy / MaxBy</h3><p>从数据集中返回指定字段或组合对应最小或最大的记录，如果选择的字段具有多个相同值，则在集合中随机选择一条记录返回。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; source = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">"jack"</span>,<span class="number">20</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Tom"</span>,<span class="number">21</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Robin"</span>,<span class="number">25</span>),</span><br><span class="line">                Tuple2.of(<span class="string">"Bob"</span>,<span class="number">30</span>));</span><br><span class="line"><span class="comment">// 按照第2个元素比较，找出第二个元素为最小值的那个tuple</span></span><br><span class="line"><span class="comment">// 在整个DataSet上使用minBy</span></span><br><span class="line">ReduceOperator&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2Reduce = source.minBy(<span class="number">1</span>);</span><br><span class="line">tuple2Reduce.print();<span class="comment">// 返回(jack,20)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 也可以在分组的DataSet上使用minBy</span></span><br><span class="line">source.groupBy(<span class="number">0</span>) <span class="comment">// 按照第一个字段进行分组</span></span><br><span class="line">      .minBy(<span class="number">1</span>)  <span class="comment">// 找出每个分组内的按照第二个元素为最小值的那个tuple</span></span><br><span class="line">      .print();</span><br></pre></td></tr></table></figure><h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>广播变量是分布式计算框架中经常会用到的一种数据共享方式。其主要作用是将小数据集采用网络传输的方式，在每台机器上维护一个只读的缓存变量，所在的计算节点实例均可以在本地内存中直接读取被广播的数据集，这样能够避免在数据计算过程中多次通过远程的方式从其他节点中读取小数据集，从而提升整体任务的计算性能。</p><p>广播变量可以理解为一个公共的共享变量，可以把DataSet广播出去，这样不同的task都可以读取该数据，广播的数据只会在每个节点上存一份。如果不使用广播变量，则会在每个节点上的task中都要复制一份dataset数据集，导致浪费内存。</p><p>使用广播变量的基本步骤如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//第一步创建需要广播的数据集</span></span><br><span class="line">DataSet&lt;Integer&gt; toBroadcast = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">DataSet&lt;String&gt; data = env.fromElements(<span class="string">"a"</span>, <span class="string">"b"</span>);</span><br><span class="line"></span><br><span class="line">data.map(<span class="keyword">new</span> RichMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      <span class="comment">// 第三步访问集合形式的广播变量数据集</span></span><br><span class="line">      Collection&lt;Integer&gt; broadcastSet = getRuntimeContext().getBroadcastVariable(<span class="string">"broadcastSetName"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).withBroadcastSet(toBroadcast, <span class="string">"broadcastSetName"</span>); <span class="comment">// 第二步广播数据集</span></span><br></pre></td></tr></table></figure><p>从上面的代码可以看出，DataSet API支持在RichFunction接口中通过RuntimeContext读取到广播变量。</p><p>首先在RichFunction中实现Open()方法，然后调用getRuntimeContext()方法获取应用的RuntimeContext，接着调用getBroadcastVariable()方法通过广播名称获取广播变量。同时Flink直接通过collect操作将数据集转换为本地Collection。需要注意的是，Collection对象的数据类型必须和定义的数据集的类型保持一致，否则会出现类型转换问题。</p><p>注意事项：</p><ul><li>由于广播变量的内容是保存在每个节点的内存中，所以广播变量数据集不易过大。</li><li>广播变量初始化之后，不支持修改，这样方能保证每个节点的数据都是一样的。</li><li>如果多个算子都要使用一份数据集，那么需要在多个算子的后面分别注册广播变量。</li><li>只能在批处理中使用广播变量。</li></ul><h3 id="使用Demo"><a href="#使用Demo" class="headerlink" title="使用Demo"></a>使用Demo</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BroadcastExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;Tuple2&lt;Integer,String&gt;&gt; RawBroadCastData = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        RawBroadCastData.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>,<span class="string">"jack"</span>));</span><br><span class="line">        RawBroadCastData.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">2</span>,<span class="string">"tom"</span>));</span><br><span class="line">        RawBroadCastData.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">3</span>,<span class="string">"Bob"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 模拟数据源，[userId,userName]</span></span><br><span class="line">        DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; userInfoBroadCastData = env.fromCollection(RawBroadCastData);</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;Tuple2&lt;Integer,Double&gt;&gt; rawUserAount = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        rawUserAount.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>,<span class="number">1000.00</span>));</span><br><span class="line">        rawUserAount.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">2</span>,<span class="number">500.20</span>));</span><br><span class="line">        rawUserAount.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">3</span>,<span class="number">800.50</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理数据：用户id，用户购买金额 ，[UserId,amount]</span></span><br><span class="line">        DataSet&lt;Tuple2&lt;Integer, Double&gt;&gt; userAmount = env.fromCollection(rawUserAount);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 转换为map集合类型的DataSet</span></span><br><span class="line">        DataSet&lt;HashMap&lt;Integer, String&gt;&gt; userInfoBroadCast = userInfoBroadCastData.map(<span class="keyword">new</span> MapFunction&lt;Tuple2&lt;Integer, String&gt;, HashMap&lt;Integer, String&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> HashMap&lt;Integer, String&gt; <span class="title">map</span><span class="params">(Tuple2&lt;Integer, String&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                HashMap&lt;Integer, String&gt; userInfo = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">                userInfo.put(value.f0, value.f1);</span><br><span class="line">                <span class="keyword">return</span> userInfo;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">       DataSet&lt;String&gt; result = userAmount.map(<span class="keyword">new</span> RichMapFunction&lt;Tuple2&lt;Integer, Double&gt;, String&gt;() &#123;</span><br><span class="line">            <span class="comment">// 存放广播变量返回的list集合数据</span></span><br><span class="line">            List&lt;HashMap&lt;String, String&gt;&gt; broadCastList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="comment">// 存放广播变量的值</span></span><br><span class="line">            HashMap&lt;String, String&gt; allMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                <span class="comment">//获取广播数据,返回的是一个list集合</span></span><br><span class="line">                <span class="keyword">this</span>.broadCastList = getRuntimeContext().getBroadcastVariable(<span class="string">"userInfo"</span>);</span><br><span class="line">                <span class="keyword">for</span> (HashMap&lt;String, String&gt; value : broadCastList) &#123;</span><br><span class="line">                    allMap.putAll(value);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(Tuple2&lt;Integer, Double&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String userName = allMap.get(value.f0);</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"用户id： "</span> + value.f0 + <span class="string">" | "</span>+ <span class="string">"用户名： "</span> + userName + <span class="string">" | "</span> + <span class="string">"购买金额： "</span> + value.f1;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).withBroadcastSet(userInfoBroadCast, <span class="string">"userInfo"</span>);</span><br><span class="line"></span><br><span class="line">        result.print();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="分布式缓存"><a href="#分布式缓存" class="headerlink" title="分布式缓存"></a>分布式缓存</h2><h3 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h3><p>Flink提供了一个分布式缓存(distributed cache),类似于Hadoop，以使文件在本地可被用户函数的并行实例访问。分布式缓存的工作机制是为程序注册一个文件或目录(本地或者远程文件系统，如HDFS等)，通过ExecutionEnvironment注册一个缓存文件，并起一个别名。当程序执行的时候，Flink会自动把注册的文件或目录复制到所有TaskManager节点的本地文件系统，用户可以通过注册是起的别名来查找文件或目录，然后在TaskManager节点的本地文件系统访问该文件。</p><p>分布式缓存的使用步骤：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// 注册一个HDFS文件</span></span><br><span class="line">env.registerCachedFile(<span class="string">"hdfs:///path/to/your/file"</span>, <span class="string">"hdfsFile"</span>)</span><br><span class="line"><span class="comment">// 注册一个本地文件</span></span><br><span class="line">env.registerCachedFile(<span class="string">"file:///path/to/exec/file"</span>, <span class="string">"localExecFile"</span>, <span class="keyword">true</span>)</span><br><span class="line"><span class="comment">// 访问数据</span></span><br><span class="line">getRuntimeContext().getDistributedCache().getFile(<span class="string">"hdfsFile"</span>);</span><br></pre></td></tr></table></figure><p>获取缓存文件的方式和广播变量相似，也是实现RichFunction接口，并通过RichFunction接口获得RuntimeContext对象，然后通过RuntimeContext提供的接口获取对应的本地缓存文件。</p><h3 id="使用Demo-1"><a href="#使用Demo-1" class="headerlink" title="使用Demo"></a>使用Demo</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributeCacheExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//获取运行环境</span></span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         *  注册一个本地文件</span></span><br><span class="line"><span class="comment">         *   文件内容为：</span></span><br><span class="line"><span class="comment">         *   1,"jack"</span></span><br><span class="line"><span class="comment">         *   2,"tom"</span></span><br><span class="line"><span class="comment">         *   3,"Bob"</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        env.registerCachedFile(<span class="string">"file:///E://userinfo.txt"</span>, <span class="string">"localFileUserInfo"</span>, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;Tuple2&lt;Integer,Double&gt;&gt; rawUserAount = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        rawUserAount.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">1</span>,<span class="number">1000.00</span>));</span><br><span class="line">        rawUserAount.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">2</span>,<span class="number">500.20</span>));</span><br><span class="line">        rawUserAount.add(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">3</span>,<span class="number">800.50</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理数据：用户id，用户购买金额 ，[UserId,amount]</span></span><br><span class="line">        DataSet&lt;Tuple2&lt;Integer, Double&gt;&gt; userAmount = env.fromCollection(rawUserAount);</span><br><span class="line"></span><br><span class="line">        DataSet&lt;String&gt; result= userAmount.map(<span class="keyword">new</span> RichMapFunction&lt;Tuple2&lt;Integer, Double&gt;, String&gt;() &#123;</span><br><span class="line">            <span class="comment">// 保存缓存数据</span></span><br><span class="line">            HashMap&lt;String, String&gt; allMap = <span class="keyword">new</span> HashMap&lt;String, String&gt;();</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                <span class="comment">// 获取分布式缓存的数据</span></span><br><span class="line">                File userInfoFile = getRuntimeContext().getDistributedCache().getFile(<span class="string">"localFileUserInfo"</span>);</span><br><span class="line">                List&lt;String&gt; userInfo = FileUtils.readLines(userInfoFile);</span><br><span class="line">                <span class="keyword">for</span> (String value : userInfo) &#123;</span><br><span class="line"></span><br><span class="line">                    String[] split = value.split(<span class="string">","</span>);</span><br><span class="line">                    allMap.put(split[<span class="number">0</span>], split[<span class="number">1</span>]);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(Tuple2&lt;Integer, Double&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String userName = allMap.get(value.f0);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> <span class="string">"用户id： "</span> + value.f0 + <span class="string">" | "</span> + <span class="string">"用户名： "</span> + userName + <span class="string">" | "</span> + <span class="string">"购买金额： "</span> + value.f1;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        result.print();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要讲解了Flink DataSet API的基本使用。首先介绍了一个DataSet API的WordCount案例，接着介绍了DataSet API的数据源与Sink操作，以及基本的使用。然后对每一个转换操作进行了详细的解释，并给出了具体的使用案例。最后讲解了广播变量和分布式缓存的概念，并就如何使用这两种高级功能，提供了完整的Demo案例。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink DataStream API 中的多面手——Process Function详解</title>
      <link href="/2020/04/30/Flink-DataStream-API-%E4%B8%AD%E7%9A%84%E5%A4%9A%E9%9D%A2%E6%89%8B%E2%80%94%E2%80%94Process-Function%E8%AF%A6%E8%A7%A3/"/>
      <url>/2020/04/30/Flink-DataStream-API-%E4%B8%AD%E7%9A%84%E5%A4%9A%E9%9D%A2%E6%89%8B%E2%80%94%E2%80%94Process-Function%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>在<a href="https://mp.weixin.qq.com/s/ycz_N5m6RjsW9ZBhNMvACw" target="_blank" rel="noopener">Flink的时间与watermarks详解</a>这篇文章中，阐述了Flink的时间与水位线的相关内容。你可能不禁要发问，该如何访问时间戳和水位线呢？首先通过普通的DataStream API是无法访问的，需要借助Flink提供的一个底层的API——Process  Function。Process Function不仅能够访问时间戳与水位线，而且还可以注册在将来的某个特定时间触发的计时器(timers)。除此之外，还可以将数据通过Side Outputs发送到多个输出流中。这样以来，可以实现数据分流的功能，同时也是处理迟到数据的一种方式。下面我们将从源码入手，结合具体的使用案例来说明该如何使用Process  Function。</p><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Flink提供了很多Process Function，每种Process Function都有各自的功能，这些Process Function主要包括：</p><blockquote><ul><li><p>ProcessFunction</p></li><li><p>KeyedProcessFunction</p></li><li><p>CoProcessFunction</p></li><li><p>ProcessJoinFunction</p></li><li><p>ProcessWindowFunction</p></li><li><p>ProcessAllWindowFunction</p></li><li><p>BaseBroadcastProcessFunction</p><pre><code>* KeyedBroadcastProcessFunction* BroadcastProcessFunction</code></pre></li></ul></blockquote><p>继承关系图如下：</p><p><img src="//jiamaoxiang.top/2020/04/30/Flink-DataStream-API-中的多面手——Process-Function详解/processfunction.png" alt></p><p>从上面的继承关系中可以看出，都实现了RichFunction接口，所以支持使用<code>open()</code>、<code>close()</code>、<code>getRuntimeContext()</code>等方法的调用。从名字上可以看出，这些函数都有不同的适用场景，但是基本的功能是类似的，下面会以KeyedProcessFunction为例来讨论这些函数的通用功能。</p><h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h2><h3 id="KeyedProcessFunction"><a href="#KeyedProcessFunction" class="headerlink" title="KeyedProcessFunction"></a>KeyedProcessFunction</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 处理KeyedStream流的低级API函数</span></span><br><span class="line"><span class="comment"> * 对于输入流中的每个元素都会触发调用processElement方法.该方法会产生0个或多个输出.</span></span><br><span class="line"><span class="comment"> * 其实现类可以通过Context访问数据的时间戳和计时器(timers).当计时器(timers)触发时，会回调onTimer方法.</span></span><br><span class="line"><span class="comment"> * onTimer方法会产生0个或者多个输出，并且会注册一个未来的计时器.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 注意：如果要访问keyed state和计时器(timers)，必须在KeyedStream上使用KeyedProcessFunction.</span></span><br><span class="line"><span class="comment"> * 另外，KeyedProcessFunction的父类AbstractRichFunction实现了RichFunction接口，所以，可以使用</span></span><br><span class="line"><span class="comment"> * open()，close()及getRuntimeContext()方法.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;K&gt; key的类型</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;I&gt; 输入元素的数据类型</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;O&gt; 输出元素的数据类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyedProcessFunction</span>&lt;<span class="title">K</span>, <span class="title">I</span>, <span class="title">O</span>&gt; <span class="keyword">extends</span> <span class="title">AbstractRichFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 处理输入流中的每个元素</span></span><br><span class="line"><span class="comment"> * 该方法会输出0个或者多个输出，类似于FlatMap的功能</span></span><br><span class="line"><span class="comment"> * 除此之外，该方法还可以更新内部状态或者设置计时器(timer)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> value 输入元素</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> ctx  Context，可以访问输入元素的时间戳，并其可以获取一个时间服务器(TimerService)，用于注册计时器(timers)并查询时间</span></span><br><span class="line"><span class="comment"> *  Context只有在processElement被调用期间有效.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> out  返回的结果值</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(I value, Context ctx, Collector&lt;O&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 是一个回调函数，当在TimerService中注册的计时器(timers)被触发时，会回调该函数</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> timestamp 触发计时器(timers)的时间戳</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> ctx  OnTimerContext，允许访问时间戳，TimeDomain枚举类提供了两种时间类型：</span></span><br><span class="line"><span class="comment"> * EVENT_TIME与PROCESSING_TIME</span></span><br><span class="line"><span class="comment"> * 并其可以获取一个时间服务器(TimerService)，用于注册计时器(timers)并查询时间</span></span><br><span class="line"><span class="comment"> * OnTimerContext只有在onTimer方法被调用期间有效</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> out 结果输出</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;O&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 仅仅在processElement()方法或者onTimer方法被调用期间有效</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Context</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 当前被处理元素的时间戳，或者是触发计时器(timers)时的时间戳</span></span><br><span class="line"><span class="comment"> * 该值可能为null，比如当程序中设置的时间语义为：TimeCharacteristic#ProcessingTime</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> Long <span class="title">timestamp</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 访问时间和注册的计时器(timers)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TimerService <span class="title">timerService</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将元素输出到side output (侧输出)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> outputTag 侧输出的标记</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> value 输出的记录</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;X&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> &lt;X&gt; <span class="function"><span class="keyword">void</span> <span class="title">output</span><span class="params">(OutputTag&lt;X&gt; outputTag, X value)</span></span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 获取被处理元素的key</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> K <span class="title">getCurrentKey</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 当onTimer方法被调用时，才可以使用OnTimerContext</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">OnTimerContext</span> <span class="keyword">extends</span> <span class="title">Context</span> </span>&#123;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 触发计时器(timers)的时间类型，包括两种：EVENT_TIME与PROCESSING_TIME</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TimeDomain <span class="title">timeDomain</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 获取触发计时器(timer)元素的key</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> K <span class="title">getCurrentKey</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的源码中，主要有两个方法，分析如下：</p><ul><li>processElement(I value, Context ctx, Collector<o> out)</o></li></ul><p>该方法会对流中的每条记录都调用一次，输出0个或者多个元素，类似于FlatMap的功能，通过Collector将结果发出。除此之外，该函数有一个Context 参数，用户可以通过Context 访问时间戳、当前记录的key值以及TimerService(关于TimerService，下面会详细解释)。另外还可以使用output方法将数据发送到side output，实现分流或者处理迟到数据的功能。</p><ul><li>onTimer(long timestamp, OnTimerContext ctx, Collector<o> out)</o></li></ul><p>该方法是一个回调函数，当在TimerService中注册的计时器(timers)被触发时，会回调该函数。其中<code>@param timestamp</code>参数表示触发计时器(timers)的时间戳，Collector可以将记录发出。细心的你可能会发现，这两个方法都有一个上下文参数，上面的方法传递的是Context 参数，onTimer方法传递的是OnTimerContext参数，这两个参数对象可以实现相似的功能。OnTimerContext还可以返回触发计时器的时间域(EVENT_TIME与PROCESSING_TIME)。</p><h3 id="TimerService"><a href="#TimerService" class="headerlink" title="TimerService"></a>TimerService</h3><p>在KeyedProcessFunction源码中，使用TimerService来访问时间和计时器，下面来看一下源码：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">TimerService</span> </span>&#123;</span><br><span class="line">String UNSUPPORTED_REGISTER_TIMER_MSG = <span class="string">"Setting timers is only supported on a keyed streams."</span>;</span><br><span class="line">String UNSUPPORTED_DELETE_TIMER_MSG = <span class="string">"Deleting timers is only supported on a keyed streams."</span>;</span><br><span class="line"><span class="comment">// 返回当前的处理时间</span></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">currentProcessingTime</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">// 返回当前event-time水位线(watermark)</span></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">currentWatermark</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 注册一个计时器(timers)，当processing time的时间等于该计时器时钟时会被调用</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> time</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">registerProcessingTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 注册一个计时器(timers),当event time的水位线(watermark)到达该时间时会被触发</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> time</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">registerEventTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据给定的触发时间(trigger time)来删除processing-time计时器</span></span><br><span class="line"><span class="comment"> * 如果这个timer不存在，那么该方法不会起作用，</span></span><br><span class="line"><span class="comment"> * 即该计时器(timer)之前已经被注册了，并且没有过时</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> time</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">deleteProcessingTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br><span class="line">    </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据给定的触发时间(trigger time)来删除event-time 计时器</span></span><br><span class="line"><span class="comment"> * 如果这个timer不存在，那么该方法不会起作用，</span></span><br><span class="line"><span class="comment"> * 即该计时器(timer)之前已经被注册了，并且没有过时</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> time</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">deleteEventTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>TimerService提供了以下几种方法：</p><ul><li>currentProcessingTime()</li></ul><p>返回当前的处理时间</p><ul><li>currentWatermark()</li></ul><p>返回当前event-time水位线(watermark)时间戳</p><ul><li>registerProcessingTimeTimer(long time)</li></ul><p>针对当前key，注册一个processing time计时器(timers)，当processing time的时间等于该计时器时钟时会被调用</p><ul><li>registerEventTimeTimer(long time)</li></ul><p>针对当前key，注册一个event time计时器(timers)，当水位线时间戳大于等于该计时器时钟时会被调用</p><ul><li>deleteProcessingTimeTimer(long time)</li></ul><p>针对当前key，删除一个之前注册过的processing time计时器(timers)，如果这个timer不存在，那么该方法不会起作用</p><ul><li>deleteEventTimeTimer(long time)</li></ul><p>针对当前key，删除一个之前注册过的event time计时器(timers)，如果这个timer不存在，那么该方法不会起作用</p><p>当计时器触发时，会回调onTimer()函数，系统对于ProcessElement()方法和onTimer()方法的调用是同步的</p><p>注意:上面的源码中有两个Error 信息,这就说明计时器只能在keyed streams上使用，常见的用途是在某些key值不在使用后清除keyed state，或者实现一些基于时间的自定义窗口逻辑。如果要在一个非KeyedStream上使用计时器，可以使用KeySelector返回一个固定的分区值(比如返回一个常数)，这样所有的数据只会发送到一个分区。</p><h2 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h2><p>下面将使用Process Function的side output功能进行分流处理，具体代码如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessFunctionExample</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义side output标签</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;UserBehaviors&gt; buyTags = <span class="keyword">new</span> OutputTag&lt;UserBehaviors&gt;(<span class="string">"buy"</span>) &#123;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;UserBehaviors&gt; cartTags = <span class="keyword">new</span> OutputTag&lt;UserBehaviors&gt;(<span class="string">"cart"</span>) &#123;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> OutputTag&lt;UserBehaviors&gt; favTags = <span class="keyword">new</span> OutputTag&lt;UserBehaviors&gt;(<span class="string">"fav"</span>) &#123;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitStreamFunction</span> <span class="keyword">extends</span> <span class="title">ProcessFunction</span>&lt;<span class="title">UserBehaviors</span>, <span class="title">UserBehaviors</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(UserBehaviors value, Context ctx, Collector&lt;UserBehaviors&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">switch</span> (value.behavior) &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">"buy"</span>:</span><br><span class="line">                    ctx.output(buyTags, value);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">"cart"</span>:</span><br><span class="line">                    ctx.output(cartTags, value);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">"fav"</span>:</span><br><span class="line">                    ctx.output(favTags, value);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">default</span>:</span><br><span class="line">                    out.collect(value);</span><br><span class="line">           &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 模拟数据源[userId,behavior,product]</span></span><br><span class="line">        SingleOutputStreamOperator&lt;UserBehaviors&gt; splitStream = env.fromElements(</span><br><span class="line">                <span class="keyword">new</span> UserBehaviors(<span class="number">1L</span>, <span class="string">"buy"</span>, <span class="string">"iphone"</span>),</span><br><span class="line">                <span class="keyword">new</span> UserBehaviors(<span class="number">1L</span>, <span class="string">"cart"</span>, <span class="string">"huawei"</span>),</span><br><span class="line">                <span class="keyword">new</span> UserBehaviors(<span class="number">1L</span>, <span class="string">"buy"</span>, <span class="string">"logi"</span>),</span><br><span class="line">                <span class="keyword">new</span> UserBehaviors(<span class="number">1L</span>, <span class="string">"fav"</span>, <span class="string">"oppo"</span>),</span><br><span class="line">                <span class="keyword">new</span> UserBehaviors(<span class="number">2L</span>, <span class="string">"buy"</span>, <span class="string">"huawei"</span>),</span><br><span class="line">                <span class="keyword">new</span> UserBehaviors(<span class="number">2L</span>, <span class="string">"buy"</span>, <span class="string">"onemore"</span>),</span><br><span class="line">                <span class="keyword">new</span> UserBehaviors(<span class="number">2L</span>, <span class="string">"fav"</span>, <span class="string">"iphone"</span>)).process(<span class="keyword">new</span> SplitStreamFunction());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取分流之后购买行为的数据</span></span><br><span class="line">        splitStream.getSideOutput(buyTags).print(<span class="string">"data_buy"</span>);</span><br><span class="line">        <span class="comment">//获取分流之后加购行为的数据</span></span><br><span class="line">        splitStream.getSideOutput(cartTags).print(<span class="string">"data_cart"</span>);</span><br><span class="line">        <span class="comment">//获取分流之后收藏行为的数据</span></span><br><span class="line">        splitStream.getSideOutput(favTags).print(<span class="string">"data_fav"</span>);</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"ProcessFunctionExample"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先介绍了Flink提供的几种底层Process Function API，这些API可以访问时间戳和水位线，同时支持注册一个计时器，进行调用回调函数onTimer()。接着从源码的角度解读了这些API的共同部分，详细解释了每个方法的具体含义和使用方式。最后，给出了一个Process Function常见使用场景案例，使用其实现分流处理。除此之外，用户还可以使用这些函数，通过注册计时器，在回调函数中定义处理逻辑，使用非常的灵活。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink内部Exactly Once三板斧:状态、状态后端与检查点</title>
      <link href="/2020/04/25/Flink%E5%86%85%E9%83%A8Exactly-Once%E4%B8%89%E6%9D%BF%E6%96%A7-%E7%8A%B6%E6%80%81%E3%80%81%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF%E4%B8%8E%E6%A3%80%E6%9F%A5%E7%82%B9/"/>
      <url>/2020/04/25/Flink%E5%86%85%E9%83%A8Exactly-Once%E4%B8%89%E6%9D%BF%E6%96%A7-%E7%8A%B6%E6%80%81%E3%80%81%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF%E4%B8%8E%E6%A3%80%E6%9F%A5%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p>Flink是一个分布式的流处理引擎，而流处理的其中一个特点就是7X24。那么，如何保障Flink作业的持续运行呢？Flink的内部会将应用状态(state)存储到本地内存或者嵌入式的kv数据库(RocksDB)中，由于采用的是分布式架构，Flink需要对本地生成的状态进行持久化存储，以避免因应用或者节点机器故障等原因导致数据的丢失，Flink是通过checkpoint(检查点)的方式将状态写入到远程的持久化存储，从而就可以实现不同语义的结果保障。通过本文，你可以了解到什么是Flink的状态，Flink的状态是怎么存储的，Flink可选择的状态后端(statebackend)有哪些，什么是全局一致性检查点，Flink内部如何通过检查点实现Exactly Once的结果保障。另外，本文内容较长，建议关注加收藏。</p><a id="more"></a><h2 id="什么是状态"><a href="#什么是状态" class="headerlink" title="什么是状态"></a>什么是状态</h2><h3 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h3><p>关于什么是状态，我们先不做过多的分析。首先看一个代码案例，其中案例1是Spark的WordCount代码，案例2是Flink的WorkCount代码。</p><ul><li>案例1：Spark WC</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">object WordCount &#123;</span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args:Array[String])</span></span>&#123;</span><br><span class="line">  val conf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line">  val ssc = <span class="keyword">new</span> StreamingContext(conf, Seconds(<span class="number">5</span>))</span><br><span class="line">  val lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line">  val words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">  val pairs = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">  val wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line">  wordCounts.print()</span><br><span class="line">  ssc.start()</span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输入：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">C:\WINDOWS\system32&gt;nc -lp 9999</span><br><span class="line">hello spark</span><br><span class="line">hello spark</span><br></pre></td></tr></table></figure><p>输出：</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/spark.png" alt></p><ul><li>案例2：Flink WC</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(<span class="number">1</span>);</span><br><span class="line">        DataStreamSource&lt;String&gt; streamSource = env.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>);</span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String,Integer&gt;&gt; words = streamSource.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String,Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] splits = value.split(<span class="string">"\\s"</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word : splits) &#123;</span><br><span class="line">                    out.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        words.keyBy(<span class="number">0</span>).sum(<span class="number">1</span>).print();</span><br><span class="line">        env.execute(<span class="string">"WC"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输入：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">C:\WINDOWS\system32&gt;nc -lp <span class="number">9999</span></span><br><span class="line">hello Flink</span><br><span class="line">hello Flink</span><br></pre></td></tr></table></figure><p>输出：</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/Flink.png" alt></p><p>从上面的两个例子可以看出，在使用Spark进行词频统计时，当前的统计结果不受历史统计结果的影响，只计算接收的当前数据的结果，这个就可以理解为无状态的计算。再来看一下Flink的例子，可以看出当第二次词频统计时，把第一次的结果值也统计在了一起，即Flink把上一次的计算结果保存在了状态里，第二次计算的时候会先拿到上一次的结果状态，然后结合新到来的数据再进行计算，这就可以理解成有状态的计算，如下图所示。</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/statetask.png" alt></p><h3 id="状态的类别"><a href="#状态的类别" class="headerlink" title="状态的类别"></a>状态的类别</h3><p>Flink提供了两种基本类型的状态：分别是 <code>Keyed State</code> 和<code>Operator State</code>。根据不同的状态管理方式，每种状态又有两种存在形式，分别为：<code>managed(托管状态)</code>和<code>raw(原生状态)</code>。具体如下表格所示。需要注意的是，由于Flink推荐使用managed state，所以下文主要讨论managed state，对于raw state，本文不会做过多的讨论。</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/state_kinds.png" alt></p><h4 id="managed-state-amp-raw-state区别"><a href="#managed-state-amp-raw-state区别" class="headerlink" title="managed state &amp; raw state区别"></a>managed state &amp; raw state区别</h4><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/managedstate_rowstate.png" alt></p><h4 id="Keyed-State-amp-Operator-State"><a href="#Keyed-State-amp-Operator-State" class="headerlink" title="Keyed State &amp; Operator State"></a>Keyed State &amp; Operator State</h4><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/keystate_operatorstate.png" alt></p><h3 id="Keyed-State"><a href="#Keyed-State" class="headerlink" title="Keyed State"></a>Keyed State</h3><p>Keyed State只能由作用在KeyedStream上面的函数使用，该状态与某个key进行绑定，即每一个key对应一个state。Keyed State按照key进行维护和访问的，Flink会为每一个Key都维护一个状态实例，该状态实例总是位于处理该key记录的算子任务上，因此同一个key的记录可以访问到一样的状态。如下图所示，可以通过在一条流上使用keyBy()方法来生成一个KeyedStream。Flink提供了很多种keyed state，具体如下：</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/keyedstate.png" alt></p><ul><li><strong>ValueState<t></t></strong></li></ul><p>用于保存类型为T的单个值。用户可以通过ValueState.value()来获取该状态值，通过ValueState.update()来更新该状态。使用<code>ValueStateDescriptor</code>来获取状态句柄。</p><ul><li><strong>ListState<t></t></strong></li></ul><p>用于保存类型为T的元素列表，即key的状态值是一个列表。用户可以使用ListState.add()或者ListState.addAll()将新元素添加到列表中，通过ListState.get()访问状态元素，该方法会返回一个可遍历所有元素的Iterable<t>对象，注意ListState不支持删除单个元素，但是用户可以使用update(List<t> values)来更新整个列表。使用 <code>ListStateDescriptor</code>来获取状态句柄。</t></t></p><ul><li><strong>ReducingState<t></t></strong></li></ul><p>调用add()方法添加值时，会立即返回一个使用ReduceFunction聚合后的值，用户可以使用ReducingState.get()来获取该状态值。使用 <code>ReducingStateDescriptor</code>来获取状态句柄。</p><ul><li><strong>AggregatingState&lt;IN, OUT&gt;</strong></li></ul><p>与ReducingState<t>类似，不同的是它使用的是AggregateFunction来聚合内部的值，AggregatingState.get()方法会计算最终的结果并将其返回。使用 <code>AggregatingStateDescriptor</code>来获取状态句柄</t></p><ul><li><strong>MapState&lt;UK, UV&gt;</strong></li></ul><p>用于保存一组key、value的映射，类似于java的Map集合。用户可以通过get(UK key)方法获取key对应的状态，可以通过put(UK k,UV value)方法添加一个键值，可以通过remove(UK key)删除给定key的值，可以通过contains(UK key)判断是否存在对应的key。使用 <code>MapStateDescriptor</code>来获取状态句柄。</p><ul><li><strong>FoldingState&lt;T, ACC&gt;</strong></li></ul><p>在Flink 1.4的版本中标记过时，在未来的版本中会被移除，使用AggregatingState进行代替。</p><p>值得注意的是，上面的状态原语都支持通过State.clear()方法来进行清除状态。另外，上述的状态原语仅用于与状态进行交互，真正的状态是存储在状态后端（后面会介绍状态后端）的，通过该状态原语相当于持有了状态的句柄(handle)。</p><h4 id="keyed-State使用案例"><a href="#keyed-State使用案例" class="headerlink" title="keyed State使用案例"></a>keyed State使用案例</h4><p>下面给出一个MapState的使用案例，关于ValueState的使用情况可以参考官网，具体如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapStateExample</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//统计每个用户每种行为的个数</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UserBehaviorCnt</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>&lt;<span class="title">Tuple3</span>&lt;<span class="title">Long</span>, <span class="title">String</span>, <span class="title">String</span>&gt;, <span class="title">Tuple3</span>&lt;<span class="title">Long</span>, <span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//定义一个MapState句柄</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">transient</span> MapState&lt;String, Integer&gt; behaviorCntState;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化状态</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>.open(parameters);</span><br><span class="line">            MapStateDescriptor&lt;String, Integer&gt; userBehaviorMapStateDesc = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">                    <span class="string">"userBehavior"</span>,  <span class="comment">// 状态描述符的名称</span></span><br><span class="line">                    TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;String&gt;() &#123;&#125;),  <span class="comment">// MapState状态的key的数据类型</span></span><br><span class="line">                    TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Integer&gt;() &#123;&#125;)  <span class="comment">// MapState状态的value的数据类型</span></span><br><span class="line">            );</span><br><span class="line">            behaviorCntState = getRuntimeContext().getMapState(userBehaviorMapStateDesc); <span class="comment">// 获取状态</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Tuple3&lt;Long, String, String&gt; value, Collector&lt;Tuple3&lt;Long, String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            Integer behaviorCnt = <span class="number">1</span>;</span><br><span class="line">            <span class="comment">// 如果当前状态包括该行为，则+1</span></span><br><span class="line">            <span class="keyword">if</span> (behaviorCntState.contains(value.f1)) &#123;</span><br><span class="line">                behaviorCnt = behaviorCntState.get(value.f1) + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 更新状态</span></span><br><span class="line">            behaviorCntState.put(value.f1, behaviorCnt);</span><br><span class="line">            out.collect(Tuple3.of(value.f0, value.f1, behaviorCnt));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 模拟数据源[userId,behavior,product]</span></span><br><span class="line">        DataStreamSource&lt;Tuple3&lt;Long, String, String&gt;&gt; userBehaviors = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"buy"</span>, <span class="string">"iphone"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"cart"</span>, <span class="string">"huawei"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"buy"</span>, <span class="string">"logi"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"fav"</span>, <span class="string">"oppo"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2L</span>, <span class="string">"buy"</span>, <span class="string">"huawei"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2L</span>, <span class="string">"buy"</span>, <span class="string">"onemore"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2L</span>, <span class="string">"fav"</span>, <span class="string">"iphone"</span>));</span><br><span class="line">        userBehaviors</span><br><span class="line">                .keyBy(<span class="number">0</span>)</span><br><span class="line">                .flatMap(<span class="keyword">new</span> UserBehaviorCnt())</span><br><span class="line">                .print();</span><br><span class="line">        env.execute(<span class="string">"MapStateExample"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果输出：</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/MapStateExample.png" alt></p><h4 id="状态的生命周期管理-TTL"><a href="#状态的生命周期管理-TTL" class="headerlink" title="状态的生命周期管理(TTL)"></a>状态的生命周期管理(TTL)</h4><p>对于任何类型Keyed State都可以设定状态的生命周期（TTL）,即状态的存活时间，以确保能够在规定时间内及时地清理状态数据。如果配置了状态的TTL，那么当状态过期时，存储的状态会被清除。状态生命周期功能可以通过StateTtlConfig配置，然后将StateTtlConfig配置传入StateDescriptor中的enableTimeToLive方法中即可。代码示例如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">                 <span class="comment">// 指定TTL时长为10S</span></span><br><span class="line">                .newBuilder(Time.seconds(<span class="number">10</span>))</span><br><span class="line">                 <span class="comment">// 只对创建和写入操作有效</span></span><br><span class="line">                .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)</span><br><span class="line">                 <span class="comment">// 不返回过期的数据</span></span><br><span class="line">                .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) </span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化状态</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>.open(parameters);</span><br><span class="line">            MapStateDescriptor&lt;String, Integer&gt; userBehaviorMapStateDesc = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">                    <span class="string">"userBehavior"</span>,  <span class="comment">// 状态描述符的名称</span></span><br><span class="line">                    TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;String&gt;() &#123;&#125;),  <span class="comment">// MapState状态的key的数据类型</span></span><br><span class="line">                    TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Integer&gt;() &#123;&#125;)  <span class="comment">// MapState状态的value的数据类型</span></span><br><span class="line"></span><br><span class="line">            );</span><br><span class="line">            <span class="comment">// 设置stateTtlConfig</span></span><br><span class="line">            userBehaviorMapStateDesc.enableTimeToLive(ttlConfig);</span><br><span class="line">            behaviorCntState = getRuntimeContext().getMapState(userBehaviorMapStateDesc); <span class="comment">// 获取状态</span></span><br><span class="line"></span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>在StateTtlConfig创建时，newBuilder方法是必须要指定的，newBuilder中设定过期时间的参数。对于其他参数都是可选的或使用默认值。其中setUpdateType方法中传入的类型有三种：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> UpdateType &#123;</span><br><span class="line"><span class="comment">//禁用TTL,永远不会过期</span></span><br><span class="line">Disabled,</span><br><span class="line">    <span class="comment">// 创建和写入时更新TTL</span></span><br><span class="line">OnCreateAndWrite,</span><br><span class="line"><span class="comment">// 与OnCreateAndWrite类似，但是在读操作时也会更新TTL</span></span><br><span class="line">OnReadAndWrite</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>值得注意的是，过期的状态数据根据UpdateType参数进行配置，只有被写入或者读取的时间才会更新TTL，也就是说如果某个状态指标一直不被使用或者更新，则永远不会触发对该状态数据的清理操作，这种情况可能会导致系统中的状态数据越来越大。目前用户可以使用StateTtlConfig.cleanupFullSnapshot设定当触发State Snapshot的时候清理状态数据，但是改配置不适合用于RocksDB做增量Checkpointing的操作。</p><p>上面的StateTtlConfig创建时，可以指定setStateVisibility，用于状态的可见性配置，根据过期数据是否被清理来确定是否返回状态数据。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 是否返回过期的数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> StateVisibility &#123;</span><br><span class="line"><span class="comment">//如果数据没有被清理，就可以返回</span></span><br><span class="line">ReturnExpiredIfNotCleanedUp,</span><br><span class="line"><span class="comment">//永远不返回过期的数据,默认值</span></span><br><span class="line">NeverReturnExpired</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Operator-State"><a href="#Operator-State" class="headerlink" title="Operator State"></a>Operator State</h3><p>Operator State的作用于是某个算子任务，这意味着所有在同一个并行任务之内的记录都能访问到相同的状态 。算子状态不能通过其他任务访问，无论该任务是相同的算子。如下图所示。</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/operatorstate.png" alt></p><p>Operator State是一种non-keyed state，与并行的操作算子实例相关联，例如在Kafka Connector中，每个Kafka消费端算子实例都对应到Kafka的一个分区中，维护Topic分区和Offsets偏移量作为算子的Operator State。在Flink中可以实现ListCheckpointed<t extends serializable>接口或者CheckpointedFunction 接口来实现一个Operator State。</t></p><p>首先，我们先看一下这两个接口的具体实现，然后再给出这两种接口的具体使用案例。先看一下ListCheckpointed接口的源码，如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ListCheckpointed</span>&lt;<span class="title">T</span> <span class="keyword">extends</span> <span class="title">Serializable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 获取某个算子实例的当前状态，该状态包括该算子实例之前被调用时的所有结果</span></span><br><span class="line"><span class="comment"> * 以列表的形式返回一个函数状态的快照</span></span><br><span class="line"><span class="comment"> * Flink触发生成检查点时调用该方法</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> checkpointId checkpoint的ID,是一个唯一的、单调递增的值</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> timestamp Job Manager触发checkpoint时的时间戳</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span>  返回一个operator state list,如果为null时,返回空list</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">List&lt;T&gt; <span class="title">snapshotState</span><span class="params">(<span class="keyword">long</span> checkpointId, <span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 初始化函数状态时调用，可能是在作业启动时或者故障恢复时</span></span><br><span class="line"><span class="comment"> * 根据提供的列表恢复函数状态</span></span><br><span class="line"><span class="comment"> * 注意：当实现该方法时，需要在RichFunction#open()方法之前调用该方法</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> state 被恢复算子实例的state列表 ，可能为空</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">restoreState</span><span class="params">(List&lt;T&gt; state)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用Operator ListState时，在进行扩缩容时，重分布的策略(状态恢复的模式)如下图所示：</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/operatorliststate%E6%89%A9%E7%BC%A9%E5%AE%B9.png" alt></p><p>上面的重分布策略为<strong>Even-split Redistribution</strong>，即每个算子实例中含有部分状态元素的List列表，整个状态数据是所有List列表的合集。当触发restore/redistribution动作时，通过将状态数据平均分配成与算子并行度相同数量的List列表，每个task实例中有一个List，其可以为空或者含有多个元素。</p><p>我们再来看一下CheckpointedFunction接口，源码如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 会在生成检查点之前调用</span></span><br><span class="line"><span class="comment"> * 该方法的目的是确保检查点开始之前所有状态对象都已经更新完毕</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> context 使用FunctionSnapshotContext作为参数</span></span><br><span class="line"><span class="comment"> *                从FunctionSnapshotContext可以获取checkpoint的元数据信息，</span></span><br><span class="line"><span class="comment"> *                比如checkpoint编号，JobManager在初始化checkpoint时的时间戳</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 在创建checkpointedFunction的并行实例时被调用，</span></span><br><span class="line"><span class="comment"> * 在应用启动或者故障重启时触发该方法的调用</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> context 传入FunctionInitializationContext对象，</span></span><br><span class="line"><span class="comment"> *                   可以使用该对象访问OperatorStateStore和 KeyedStateStore对象，</span></span><br><span class="line"><span class="comment"> *                   这两个对象可以获取状态的句柄，即通过Flink runtime来注册函数状态并返回state对象</span></span><br><span class="line"><span class="comment"> *                   比如：ValueState、ListState等</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CheckpointedFunction接口是用于指定有状态函数的最底层的接口，该接口提供了用于注册和维护keyed state 与operator state的hook(即可以同时使用keyed state 和operator state)，另外也是唯一支持使用list union state。关于Union List State,使用的是Flink为Operator state提供的另一种重分布的策略：<strong>Union Redistribution</strong>，即每个算子实例中含有所有状态元素的List列表，当触发restore/redistribution动作时，每个算子都能够获取到完整的状态元素列表。具体如下图所示：</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/operatorunionlist.png" alt></p><h4 id="ListCheckpointed"><a href="#ListCheckpointed" class="headerlink" title="ListCheckpointed"></a>ListCheckpointed</h4><p>ListCheckpointed接口和CheckpointedFunction接口相比在灵活性上相对弱一些，只能支持List类型的状态，并且在数据恢复的时候仅支持<strong>even-redistribution</strong>策略。该接口不像Flink提供的Keyed State(比如Value State、ListState)那样直接在状态后端(state backend)注册，需要将operator state实现为成员变量，然后通过接口提供的回调函数与状态后端进行交互。使用代码案例如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ListCheckpointedExample</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UserBehaviorCnt</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>&lt;<span class="title">Tuple3</span>&lt;<span class="title">Long</span>, <span class="title">String</span>, <span class="title">String</span>&gt;, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;&gt; <span class="keyword">implements</span> <span class="title">ListCheckpointed</span>&lt;<span class="title">Long</span>&gt; </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> Long userBuyBehaviorCnt = <span class="number">0L</span>;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Tuple3&lt;Long, String, String&gt; value, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(value.f1.equals(<span class="string">"buy"</span>))&#123;</span><br><span class="line">                userBuyBehaviorCnt ++;</span><br><span class="line">                out.collect(Tuple2.of(<span class="string">"buy"</span>,userBuyBehaviorCnt));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> List&lt;Long&gt; <span class="title">snapshotState</span><span class="params">(<span class="keyword">long</span> checkpointId, <span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">//返回单个元素的List集合，该集合元素是用户购买行为的数量</span></span><br><span class="line">            <span class="keyword">return</span> Collections.singletonList(userBuyBehaviorCnt);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">restoreState</span><span class="params">(List&lt;Long&gt; state)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">// 在进行扩缩容之后，进行状态恢复，需要把其他subtask的状态加在一起</span></span><br><span class="line">            <span class="keyword">for</span> (Long cnt : state) &#123;</span><br><span class="line">                userBuyBehaviorCnt += <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 模拟数据源[userId,behavior,product]</span></span><br><span class="line">        DataStreamSource&lt;Tuple3&lt;Long, String, String&gt;&gt; userBehaviors = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"buy"</span>, <span class="string">"iphone"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"cart"</span>, <span class="string">"huawei"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"buy"</span>, <span class="string">"logi"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"fav"</span>, <span class="string">"oppo"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2L</span>, <span class="string">"buy"</span>, <span class="string">"huawei"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2L</span>, <span class="string">"buy"</span>, <span class="string">"onemore"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2L</span>, <span class="string">"fav"</span>, <span class="string">"iphone"</span>));</span><br><span class="line"></span><br><span class="line">        userBehaviors</span><br><span class="line">                .flatMap(<span class="keyword">new</span> UserBehaviorCnt())</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"ListCheckpointedExample"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="CheckpointedFunction"><a href="#CheckpointedFunction" class="headerlink" title="CheckpointedFunction"></a>CheckpointedFunction</h4><p>CheckpointedFunction接口提供了更加丰富的操作，比如支持Union list state，可以访问keyedState，关于重分布策略，如果使用Even-split Redistribution策略，则通过context. getListState(descriptor)获取Operator State；如果使用UnionRedistribution策略，则通过context. getUnionList State(descriptor)来获取。使用案例如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CheckpointFunctionExample</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UserBehaviorCnt</span> <span class="keyword">implements</span> <span class="title">CheckpointedFunction</span>, <span class="title">FlatMapFunction</span>&lt;<span class="title">Tuple3</span>&lt;<span class="title">Long</span>, <span class="title">String</span>, <span class="title">String</span>&gt;, <span class="title">Tuple3</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>, <span class="title">Long</span>&gt;&gt; </span>&#123;</span><br><span class="line">        <span class="comment">// 统计每个operator实例的用户行为数量的本地变量</span></span><br><span class="line">        <span class="keyword">private</span> Long opUserBehaviorCnt = <span class="number">0L</span>;</span><br><span class="line">        <span class="comment">// 每个key的state,存储key对应的相关状态</span></span><br><span class="line">        <span class="keyword">private</span> ValueState&lt;Long&gt; keyedCntState;</span><br><span class="line">        <span class="comment">// 定义operator state，存储算子的状态</span></span><br><span class="line">        <span class="keyword">private</span> ListState&lt;Long&gt; opCntState;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Tuple3&lt;Long, String, String&gt; value, Collector&lt;Tuple3&lt;Long, Long, Long&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (value.f1.equals(<span class="string">"buy"</span>)) &#123;</span><br><span class="line">                <span class="comment">// 更新算子状态本地变量值</span></span><br><span class="line">                opUserBehaviorCnt += <span class="number">1</span>;</span><br><span class="line">                Long keyedCount = keyedCntState.value();</span><br><span class="line">                <span class="comment">// 更新keyedstate的状态 ,判断状态是否为null，否则空指针异常</span></span><br><span class="line">                keyedCntState.update(keyedCount == <span class="keyword">null</span> ? <span class="number">1L</span> : keyedCount + <span class="number">1</span> );</span><br><span class="line">                <span class="comment">// 结果输出</span></span><br><span class="line">                out.collect(Tuple3.of(value.f0, keyedCntState.value(), opUserBehaviorCnt));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">// 使用opUserBehaviorCnt本地变量更新operator state</span></span><br><span class="line">            opCntState.clear();</span><br><span class="line">            opCntState.add(opUserBehaviorCnt);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 通过KeyedStateStore,定义keyedState的StateDescriptor描述符</span></span><br><span class="line">            ValueStateDescriptor valueStateDescriptor = <span class="keyword">new</span> ValueStateDescriptor(<span class="string">"keyedCnt"</span>, TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Long&gt;() &#123;</span><br><span class="line">            &#125;));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 通过OperatorStateStore,定义OperatorState的StateDescriptor描述符</span></span><br><span class="line">            ListStateDescriptor opStateDescriptor = <span class="keyword">new</span> ListStateDescriptor(<span class="string">"opCnt"</span>, TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Long&gt;() &#123;</span><br><span class="line">            &#125;));</span><br><span class="line">            <span class="comment">// 初始化keyed state状态值</span></span><br><span class="line">            keyedCntState = context.getKeyedStateStore().getState(valueStateDescriptor);</span><br><span class="line">            <span class="comment">// 初始化operator state状态</span></span><br><span class="line">            opCntState = context.getOperatorStateStore().getListState(opStateDescriptor);</span><br><span class="line">            <span class="comment">// 初始化本地变量operator state</span></span><br><span class="line">            <span class="keyword">for</span> (Long state : opCntState.get()) &#123;</span><br><span class="line">                opUserBehaviorCnt += state;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 模拟数据源[userId,behavior,product]</span></span><br><span class="line">        DataStreamSource&lt;Tuple3&lt;Long, String, String&gt;&gt; userBehaviors = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"buy"</span>, <span class="string">"iphone"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"cart"</span>, <span class="string">"huawei"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"buy"</span>, <span class="string">"logi"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1L</span>, <span class="string">"fav"</span>, <span class="string">"oppo"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2L</span>, <span class="string">"buy"</span>, <span class="string">"huawei"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2L</span>, <span class="string">"buy"</span>, <span class="string">"onemore"</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2L</span>, <span class="string">"fav"</span>, <span class="string">"iphone"</span>));</span><br><span class="line"></span><br><span class="line">        userBehaviors</span><br><span class="line">                .keyBy(<span class="number">0</span>)</span><br><span class="line">                .flatMap(<span class="keyword">new</span> UserBehaviorCnt())</span><br><span class="line">                .print();</span><br><span class="line">        env.execute(<span class="string">"CheckpointFunctionExample"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="什么是状态后端"><a href="#什么是状态后端" class="headerlink" title="什么是状态后端"></a>什么是状态后端</h2><p>上面使用的状态都需要存储到状态后端(StateBackend)，然后在checkpoint触发时，将状态持久化到外部存储系统。Flink提供了三种类型的状态后端，分别是基于内存的状态后端(<strong>MemoryStateBackend</strong>、基于文件系统的状态后端(<strong>FsStateBackend</strong>)以及基于RockDB作为存储介质的<strong>RocksDB StateBackend</strong>。这三种类型的StateBackend都能够有效地存储Flink流式计算过程中产生的状态数据，在默认情况下Flink使用的是MemoryStateBackend，区别见下表。下面分别对每种状态后端的特点进行说明。</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/F:%5Cnpm%5Cmywebsite%5Csource_posts%5CFlink%E5%86%85%E9%83%A8Exactly-Once%E4%B8%89%E6%9D%BF%E6%96%A7-%E7%8A%B6%E6%80%81%E3%80%81%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF%E4%B8%8E%E6%A3%80%E6%9F%A5%E7%82%B9%5Cstatebackend.png" alt></p><h3 id="状态后端的类别"><a href="#状态后端的类别" class="headerlink" title="状态后端的类别"></a>状态后端的类别</h3><h4 id="MemoryStateBackend"><a href="#MemoryStateBackend" class="headerlink" title="MemoryStateBackend"></a>MemoryStateBackend</h4><p>MemoryStateBackend将状态数据全部存储在JVM堆内存中，包括用户在使用DataStream API中创建的Key/Value State，窗口中缓存的状态数据，以及触发器等数据。MemoryStateBackend具有非常快速和高效的特点，但也具有非常多的限制，最主要的就是内存的容量限制，一旦存储的状态数据过多就会导致系统内存溢出等问题，从而影响整个应用的正常运行。同时如果机器出现问题，整个主机内存中的状态数据都会丢失，进而无法恢复任务中的状态数据。因此从数据安全的角度建议用户尽可能地避免在生产环境中使用MemoryStateBackend。Flink将MemoryStateBackend作为默认状态后端。</p><p>MemoryStateBackend比较适合用于测试环境中，并用于本地调试和验证，不建议在生产环境中使用。但如果应用状态数据量不是很大，例如使用了大量的非状态计算算子，也可以在生产环境中使MemoryStateBackend.</p><h4 id="FsStateBackend"><a href="#FsStateBackend" class="headerlink" title="FsStateBackend"></a>FsStateBackend</h4><p>FsStateBackend是基于文件系统的一种状态后端，这里的文件系统可以是本地文件系统，也可以是HDFS分布式文件系统。创建FsStateBackend的构造函数如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">FsStateBackend(Path checkpointDataUri, <span class="keyword">boolean</span> asynchronousSnapshots)</span><br></pre></td></tr></table></figure><p>其中path如果为本地路径，其格式为“file:///data/flink/checkpoints”，如果path为HDFS路径，其格式为“hdfs://nameservice/flink/checkpoints”。FsStateBackend中第二个Boolean类型的参数指定是否以同步的方式进行状态数据记录，默认采用异步的方式将状态数据同步到文件系统中，异步方式能够尽可能避免在Checkpoint的过程中影响流式计算任务。如果用户想采用同步的方式进行状态数据的检查点数据，则将第二个参数指定为True即可。</p><p>相比于MemoryStateBackend, FsStateBackend更适合任务状态非常大的情况，例如应用中含有时间范围非常长的窗口计算，或Key/value State状态数据量非常大的场景，这时系统内存不足以支撑状态数据的存储。同时FsStateBackend最大的好处是相对比较稳定，在checkpoint时，将状态持久化到像HDFS分布式文件系统中，能最大程度保证状态数据的安全性。</p><h4 id="RocksDBStateBackend"><a href="#RocksDBStateBackend" class="headerlink" title="RocksDBStateBackend"></a>RocksDBStateBackend</h4><p>与前面的状态后端不同，RocksDBStateBackend需要单独引入相关的依赖包。RocksDB 是一个 key/value 的内存存储系统，类似于HBase，是一种内存磁盘混合的 LSM DB。当写数据时会先写进write buffer(类似于HBase的memstore)，然后在flush到磁盘文件，当读取数据时会现在block cache(类似于HBase的block cache)，所以速度会很快。</p><p>RocksDBStateBackend在性能上要比FsStateBackend高一些，主要是因为借助于RocksDB存储了最新热数据，然后通过异步的方式再同步到文件系统中，但RocksDBStateBackend和MemoryStateBackend相比性能就会较弱一些。</p><p>需要注意 RocksDB 不支持同步的 Checkpoint，构造方法中没有同步快照这个选项。不过 RocksDB 支持增量的 Checkpoint，也是目前唯一增量 Checkpoint 的 Backend，意味着并不需要把所有 sst 文件上传到 Checkpoint 目录，仅需要上传新生成的 sst 文件即可。它的 Checkpoint 存储在外部文件系统（本地或HDFS），其容量限制只要单个 TaskManager 上 State 总量不超过它的内存+磁盘，单 Key最大 2G，总大小不超过配置的文件系统容量即可。对于超大状态的作业，例如天级窗口聚合等场景下可以使会用该状态后端。</p><h3 id="配置状态后端"><a href="#配置状态后端" class="headerlink" title="配置状态后端"></a>配置状态后端</h3><p>Flink默认使用的状态后端是MemoryStateBackend，所以不需要显示配置。对于其他的状态后端，都需要进行显性配置。在Flink中包含了两种级别的StateBackend配置：一种是在程序中进行配置，该配置只对当前应用有效；另外一种是通过 <code>flink-conf.yaml</code>进行全局配置，一旦配置就会对整个Flink集群上的所有应用有效。</p><ul><li>应用级别配置</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> FsStateBackend(<span class="string">"hdfs://namenode:40010/flink/checkpoints"</span>));</span><br></pre></td></tr></table></figure><p>如果使用RocksDBStateBackend则需要单独引入rockdb依赖库,如下：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-statebackend-rocksdb_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>使用方式与FsStateBackend类似，如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> RocksDBStateBackend(<span class="string">"hdfs://namenode:40010/flink/checkpoints"</span>));</span><br></pre></td></tr></table></figure><ul><li>集群级别配置</li></ul><p>具体的配置项在flink-conf.yaml文件中，如下代码所示，参数state.backend指明StateBackend类型，state.checkpoints.dir配置具体的状态存储路径，代码中使用filesystem作为StateBackend，然后指定相应的HDFS文件路径作为state的checkpoint文件夹。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用filesystem存储</span></span><br><span class="line">state.backend: filesystem</span><br><span class="line"><span class="comment"># checkpoint存储路径</span></span><br><span class="line">state.checkpoints.dir: hdfs://namenode:40010/flink/checkpoints</span><br></pre></td></tr></table></figure><p>如果想用RocksDBStateBackend配置集群级别的状态后端，可以使用下面的配置：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 操作RocksDBStateBackend的线程数量，默认值为1</span></span><br><span class="line">state.backend.rocksdb.checkpoint.transfer.thread.num: 1</span><br><span class="line"><span class="comment"># 指定RocksDB存储状态数据的本地文件路径</span></span><br><span class="line">state.backend.rocksdb.localdir: /var/rockdb/checkpoints</span><br><span class="line"><span class="comment"># 用于指定定时器服务的工厂类实现类，默认为“HEAP”，也可以指定为“RocksDB”</span></span><br><span class="line">state.backend.rocksdb.timer-service.factory: HEAP</span><br></pre></td></tr></table></figure><h2 id="什么是Checkpoint-检查点"><a href="#什么是Checkpoint-检查点" class="headerlink" title="什么是Checkpoint(检查点)"></a>什么是Checkpoint(检查点)</h2><p>上面讲解了Flink的状态以及状态后端，状态是存储在状态后端。为了保证state容错，Flink提供了处理故障的措施，这种措施称之为checkpoint(一致性检查点)。checkpoint是Flink实现容错的核心功能，主要是周期性地触发checkpoint，将state生成快照持久化到外部存储系统(比如HDFS)。这样一来，如果Flink程序出现故障，那么就可以从上一次checkpoint中进行状态恢复，从而提供容错保障。另外，通过checkpoint机制，Flink可以实现Exactly-once语义(Flink内部的Exactly-once,关于端到端的exactly_once,Flink是通过两阶段提交协议实现的)。下面将会详细分析Flink的checkpoint机制。</p><h3 id="检查点的生成"><a href="#检查点的生成" class="headerlink" title="检查点的生成"></a>检查点的生成</h3><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/checkpoint_%E6%A6%82%E8%A7%88.png" alt></p><p>如上图，输入流是用户行为数据，包括购买(buy)和加入购物车(cart)两种，每种行为数据都有一个偏移量，统计每种行为的个数。</p><p>第一步：JobManager checkpoint coordinator 触发checkpoint。</p><p>第二步：假设当消费到[cart，3]这条数据时，触发了checkpoint。那么此时数据源会把消费的偏移量3写入持久化存储。</p><p>第三步：当写入结束后，source会将state handle(状态存储路径)反馈给JobManager的checkpoint coordinator。</p><p>第四步：接着算子count buy与count cart也会进行同样的步骤</p><p>第五步：等所有的算子都完成了上述步骤之后，即当 Checkpoint coordinator 收集齐所有 task 的 state handle，就认为这一次的 Checkpoint 全局完成了，向持久化存储中再备份一个 Checkpoint meta 文件，那么整个checkpoint也就完成了，如果中间有一个不成功，那么本次checkpoin就宣告失败。</p><h3 id="检查点的恢复"><a href="#检查点的恢复" class="headerlink" title="检查点的恢复"></a>检查点的恢复</h3><p>通过上面的分析，或许你已经对Flink的checkpoint有了初步的认识。那么接下来，我们看一下是如何从检查点恢复的。</p><ul><li>任务失败</li></ul><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/%E4%BD%9C%E4%B8%9A%E5%A4%B1%E8%B4%A5.png" alt></p><ul><li>重启作业</li></ul><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/1%E9%87%8D%E5%90%AF%E4%BD%9C%E4%B8%9A.png" alt></p><ul><li>恢复检查点</li></ul><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/2%E6%A3%80%E6%9F%A5%E7%82%B9%E6%81%A2%E5%A4%8D.png" alt></p><ul><li>继续处理数据</li></ul><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/%E7%BB%A7%E7%BB%AD%E8%BF%90%E8%A1%8C.png" alt></p><p>上述过程具体总结如下：</p><ul><li>第一步：重启作业</li><li>第二步：从上一次检查点恢复状态数据</li><li>第三步：继续处理新的数据</li></ul><h3 id="Flink内部Exactly-Once实现"><a href="#Flink内部Exactly-Once实现" class="headerlink" title="Flink内部Exactly-Once实现"></a>Flink内部Exactly-Once实现</h3><p>Flink提供了精确一次的处理语义，精确一次的处理语义可以理解为：数据可能会重复计算，但是结果状态只有一个。Flink通过Checkpoint机制实现了精确一次的处理语义，Flink在触发Checkpoint时会向Source端插入checkpoint barrier，checkpoint barriers是从source端插入的，并且会向下游算子进行传递。checkpoint barriers携带一个checkpoint ID，用于标识属于哪一个checkpoint，checkpoint barriers将流逻辑是哪个分为了两部分。对于双流的情况，通过barrier对齐的方式实现精确一次的处理语义。</p><p>关于什么是checkpoint barrier，可以看一下CheckpointBarrier类的源码描述，如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Checkpoint barriers用来在数据流中实现checkpoint对齐的.</span></span><br><span class="line"><span class="comment"> * Checkpoint barrier由JobManager的checkpoint coordinator插入到Source中,</span></span><br><span class="line"><span class="comment"> * Source会把barrier广播发送到下游算子,当一个算子接收到了其中一个输入流的Checkpoint barrier时,</span></span><br><span class="line"><span class="comment"> * 它就会知道已经处理完了本次checkpoint与上次checkpoint之间的数据.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 一旦某个算子接收到了所有输入流的checkpoint barrier时，</span></span><br><span class="line"><span class="comment"> * 意味着该算子的已经处理完了截止到当前checkpoint的数据，</span></span><br><span class="line"><span class="comment"> * 可以触发checkpoint，并将barrier向下游传递</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 根据用户选择的处理语义，在checkpoint完成之前会缓存后一次checkpoint的数据，</span></span><br><span class="line"><span class="comment"> * 直到本次checkpoint完成(exactly once)</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * checkpoint barrier的id是严格单调递增的</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CheckpointBarrier</span> <span class="keyword">extends</span> <span class="title">RuntimeEvent</span> </span>&#123;...&#125;</span><br></pre></td></tr></table></figure><p>可以看出checkpoint barrier主要功能是实现checkpoint对齐的，从而可以实现Exactly-Once处理语义。</p><p>下面将会对checkpoint过程进行分解，具体如下：</p><p>图1，包括两个流，每个任务都会消费一条用户行为数据(包括购买(buy)和加购(cart))，数字代表该数据的偏移量，count buy任务统计购买行为的个数，coun cart统计加购行为的个数。</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/%E7%8A%B6%E6%80%811.png" alt></p><p>图2，触发checkpoint，JobManager会向每个数据源发送一个新的checkpoint编号，以此来启动检查点生成流程。</p><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/%E7%8A%B6%E6%80%812.png" alt></p><ul><li>图3，当Source任务收到消息后，会停止发出数据，然后利用状态后端触发生成本地状态检查点，并把该checkpoint barrier以及checkpoint id广播至所有传出的数据流分区。状态后端会在checkpoint完成之后通知任务，随后任务会向Job Manager发送确认消息。在将checkpoint barrier发出之后，Source任务恢复正常工作。</li></ul><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/%E7%8A%B6%E6%80%813.png" alt></p><ul><li>图4，Source任务发出的checkpoint barrier会发送到与之相连的下游算子任务，当任务收到一个新的checkpoint barrier时，会继续等待其他输入分区的checkpoint barrier到来，这个过程称之为<strong>barrier 对齐</strong>，checkpoint barrier到来之前会把到来的数据线缓存起来。</li></ul><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/%E7%8A%B6%E6%80%814.png" alt></p><ul><li>图5，任务收齐了全部输入分区的checkpoint barrier之后，会通知状态后端开始生成checkpoint，同时会把checkpoint barrier广播至下游算子。</li></ul><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/%E7%8A%B6%E6%80%815.png" alt></p><ul><li>图6，任务在发出checkpoint barrier之后，开始处理因barrier对齐产生的缓存数据，在缓存的数据处理完之后，就会继续处理输入流数据。</li></ul><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/%E7%8A%B6%E6%80%816.png" alt></p><ul><li>图7，最终checkpoint barrier会被传送到sink端，sink任务接收到checkpoint barrier之后，会向其他算子任务一样，将自身的状态写入checkpoint，之后向Job Manager发送确认消息。Job Manager接收到所有任务返回的确认消息之后，就会将此次检查点标记为完成。</li></ul><p><img src="//jiamaoxiang.top/2020/04/25/Flink内部Exactly-Once三板斧-状态、状态后端与检查点/%E7%8A%B6%E6%80%817.png" alt></p><h3 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// checkpoint的时间间隔，如果状态比较大，可以适当调大该值</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>);</span><br><span class="line"><span class="comment">// 配置处理语义，默认是exactly-once</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line"><span class="comment">// 两个checkpoint之间的最小时间间隔，防止因checkpoint时间过长，导致checkpoint积压</span></span><br><span class="line">env.getCheckpointConfig().setMinPauseBetweenCheckpoints(<span class="number">500</span>);</span><br><span class="line"><span class="comment">// checkpoint执行的上限时间，如果超过该阈值，则会中断checkpoint</span></span><br><span class="line">env.getCheckpointConfig().setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line"><span class="comment">// 最大并行执行的检查点数量，默认为1，可以指定多个，从而同时出发多个checkpoint，提升效率</span></span><br><span class="line">env.getCheckpointConfig().setMaxConcurrentCheckpoints(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// 设定周期性外部检查点，将状态数据持久化到外部系统中，</span></span><br><span class="line"><span class="comment">// 使用该方式不会在任务正常停止的过程中清理掉检查点数据</span></span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line"><span class="comment">// allow job recovery fallback to checkpoint when there is a more recent savepoint</span></span><br><span class="line">env.getCheckpointConfig().setPreferCheckpointForRecovery(<span class="keyword">true</span>);</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先从Flink的状态入手，通过Spark的WordCount和Flink的Work Count进行说明什么是状态。接着对状态的分类以及状态的使用进行了详细说明。然后对Flink提供的三种状态后端进行讨论，并给出了状态后端的使用说明。最后，以图解加文字的形式详细解释了Flink的checkpoint机制，并给出了使用Checkpoint时的程序配置。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的时间与watermarks详解</title>
      <link href="/2020/04/17/Flink%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E4%B8%8E%E7%AA%97%E5%8F%A3%E7%9A%84%E7%AE%97%E5%AD%90/"/>
      <url>/2020/04/17/Flink%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E4%B8%8E%E7%AA%97%E5%8F%A3%E7%9A%84%E7%AE%97%E5%AD%90/</url>
      
        <content type="html"><![CDATA[<p>当我们在使用Flink的时候，避免不了要和时间(time)、水位线(watermarks)打交道，理解这些概念是开发分布式流处理应用的基础。那么Flink支持哪些时间语义？Flink是如何处理乱序事件的？什么是水位线？水位线是如何生成的？水位线的传播方式是什么？让我们带着这些问题来开始本文的内容。</p><a id="more"></a><h2 id="时间语义"><a href="#时间语义" class="headerlink" title="时间语义"></a>时间语义</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>时间是Flink等流处理中最重要的概念之一，在 Flink 中 Time  可以分为三种：Event-Time，Processing-Time 以及 Ingestion-Time，如下图所示：</p><p><img src="//jiamaoxiang.top/2020/04/17/Flink基于时间与窗口的算子/%E6%97%B6%E9%97%B4%E5%9B%BE.png" alt></p><ul><li>Event Time</li></ul><p><strong>事件时间</strong>，事件(Event)本身的时间，即数据流中事件实际发生的时间，通常使用事件发生时的时间戳来描述，这些事件的时间戳通常在进入流处理应用之前就已经存在了，事件时间反映了事件真实的发生时间。所以，基于事件时间的计算操作，其结果是具有确定性的，无论数据流的处理速度如何、事件到达算子的顺序是否会乱，最终生成的结果都是一样的。</p><ul><li>Ingestion Time</li></ul><p><strong>摄入时间</strong>，事件进入Flink的时间，即将每一个事件在数据源算子的处理时间作为事件时间的时间戳，并自动生成水位线(watermarks,关于watermarks下文会详细分析)。</p><p>Ingestion Time从概念上讲介于Event Time和Processing Time之间。与Processing Time相比 ，它的性能消耗更多一些，但结果却更可预测。由于 Ingestion Time使用稳定的时间戳（在数据源处分配了一次），因此对记录的不同窗口操作将引用相同的时间戳，而在Processing Time中每个窗口算子都可以将记录分配给不同的窗口。</p><p>与Event Time相比，Ingestion Time无法处理任何乱序事件或迟到的数据，即无法提供确定的结果，但是程序不必指定如何生成水位线。在内部，Ingestion Time与Event Time非常相似，但是可以实现自动分配时间戳和自动生成水位线的功能。</p><ul><li>Processing Time</li></ul><p><strong>处理时间</strong>，根据处理机器的系统时钟决定数据流当前的时间，即事件被处理时当前系统的时间。还以窗口算子为例(关于window，下文会详细分析)，基于处理时间的窗口操作是以机器时间来进行触发的，由于数据到达窗口的速率不同，所以窗口算子中使用处理时间会导致不确定的结果。在使用处理时间时，无需等待水位线的到来后进行触发窗口，所以可以提供较低的延迟。</p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>经过上面的分析，应该对Flink的时间语义有了大致的了解。不知道你会不会有这样一个疑问：既然事件时间已经能够解决所有的问题了，那为何还要用处理时间呢？其实处理时间有其特定的使用场景，处理时间由于不用考虑事件的延迟与乱序，所以其处理数据的延迟较低。因此如果一些应用比较重视处理速度而非准确性，那么就可以使用处理时间，比如要实时监控仪表盘。总之，虽然处理时间的延迟较低，但是其结果具有不确定性，事件时间虽然有延迟，但是能够保证处理的结果具有准确性，并且可以处理延迟甚至无序的数据。</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>上一小结讲述了三种时间语义的基本概念，接下来将从代码层面讲解在程序中该如何配置这三种时间语义。首先来看一段代码：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/** The time characteristic that is used if none other is set. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> TimeCharacteristic DEFAULT_TIME_CHARACTERISTIC = TimeCharacteristic.ProcessingTime;</span><br><span class="line"><span class="comment">//省略的代码</span></span><br><span class="line"><span class="comment">/** The time characteristic used by the data streams. */</span></span><br><span class="line"><span class="keyword">private</span> TimeCharacteristic timeCharacteristic = DEFAULT_TIME_CHARACTERISTIC;</span><br></pre></td></tr></table></figure><p>上述两行代码摘自StreamExecutionEnvironment类，可以看出，Flink在流处理程序中默认的时间语义是Processing Time，那么该如何修改默认的时间语义呢？很简单，再来看一段代码，下面的代码片段同样来自于StreamExecutionEnvironment类：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果使用Processing Time或者Event Time，默认的水位线间隔时间是200毫秒</span></span><br><span class="line"><span class="comment"> * 可以通过ExecutionConfig#setAutoWatermarkInterval(long)设置</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> characteristic The time characteristic.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setStreamTimeCharacteristic</span><span class="params">(TimeCharacteristic characteristic)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.timeCharacteristic = Preconditions.checkNotNull(characteristic);</span><br><span class="line"><span class="keyword">if</span> (characteristic == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">getConfig().setAutoWatermarkInterval(<span class="number">0</span>);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">getConfig().setAutoWatermarkInterval(<span class="number">200</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述的方法可以配置不同的时间语义，参数TimeCharacteristic是一个枚举类，包括ProcessingTime，IngestionTime，EventTime三个元素。具体使用方式如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);</span></span><br><span class="line"><span class="comment">//env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span></span><br></pre></td></tr></table></figure><h2 id="watermarks"><a href="#watermarks" class="headerlink" title="watermarks"></a>watermarks</h2><p>在解释watermarks(水位线)之前，先看一个我们身边发生的真实案例。高考，是大家非常熟悉的场景。如果把高考的考试安排简单地看作是一个流处理应用，那么，每一个考试科目的开始时间到结束时间就是一个窗口，每个考生可以理解成一条记录，考生到达考场的时间可以理解成记录的时间戳，而考试可以理解成某种算子操作。大家都知道，高考考试在开考后15分钟是不允许进场的，这个规定可以理解成一个水位线，比如，上午第一场语文考试，开考时间是9:30，允许在9:45之前进入考场，那么9:45这个时间可以理解成一个水位线。在开考之前，有的同学喜欢提前到考场，有的同学喜欢卡点到考场。假设有个同学叫<strong>考必胜</strong>,ta是卡着时间点到的考场，但是早上由于吃了不干净的东西，突然感觉肚子不适，无奈之下在厕所里耽误了16分钟，那么按照规定，此时考必胜是不能够进入考场的，因为此时已经默认所有考生都已经在考场了，此时考试也已经触发，那么卡必胜就可以理解为迟到的事件。以上就是对窗口、事件时间以及水位线的简单理解，下面开始详细解释什么水位线。</p><h3 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h3><p>在上一节中，详细讲解了Flink提供的三种时间语义，在讲解这三种时间语义的时候，提到了一个名词—水位线，那么究竟什么是水位线呢？先来看一个例子，假如要每5分钟统计一次过去1个小时内的热门商品的topN，这是一个典型的滑动窗口操作，那么基于事件时间的窗口该在什么时候出发计算呢？换句话说，我们要等多久才能够确定已经接收到了特定时间点之前的所有事件，另一方面，由于网络延迟等原因，会产生乱序的数据，在进行窗口操作时，不能够无限期的等待下去，需要一个机制来告诉窗口在某个特定时间来触发window计算，即认为小于等于该时间点的数据都已经到来了。这个机制就是watermark(水位线)，可以用来处理乱序事件。</p><p>水位线是一个全局的进度指标，表示可以确定不会再有延迟的事件到来的某个时间点。从本质上讲，水位线提供了一个逻辑时钟，用来通知系统当前的事件时间。比如，当一个算子接收到了W(T)时刻的水位线，就可以大胆的认为不会再接收到任何时间戳小于或等于W(T)的事件了。水位线对于基于事件时间的窗口和处理乱序数据是非常关键的，算子一旦接收到了某个水位线，就相当于接到一支穿云箭的信号：所有特定时间区间的数据都已集结完毕，可以进行窗口触发计算。</p><p>既然已经说了，事件是会存在乱序的，那这个乱序的程度究竟有多大呢，这个就不太好确定了，总之总会有些迟到的事件慢慢悠悠的到来。所以，水位线其实是一种在<strong>准确性</strong>与<strong>延迟</strong>之间的权衡，如果水位线设置的非常苛刻，即不允许有掉队的数据出现，虽然准确性提高了，但这在无形之中增加了数据处理的延迟。反之，如果水位线设置的非常激进，即允许有迟到的数据发生，那么虽然降低了数据处理的延迟，但数据的准确性会较低。</p><p>所以，水位线是中庸之道，过犹不及。在很多现实应用中，系统无法获取足够多的信息来确定完美的水位线，那么该怎么办呢？Flink提供了某些机制来处理那些可能晚于水位线的迟到时间，用户可以根据应用的需求不同，可以将这些漏网之鱼(迟到的数据)舍弃掉，或者写入日志，或者利用他们修正之前的结果。</p><p>上面说到没有完美的水位线，可能还是很抽象。接下来，我们再看一幅图，从图中可以很直观地观察真实的水位线与理想中的完美水位线之间的关系，如下图：</p><p><img src="//jiamaoxiang.top/2020/04/17/Flink基于时间与窗口的算子/TimeSkew.png" alt></p><p>上图的浅灰色直虚线表示理想的水位线，深灰色的弯曲虚线表示现实中的水位线，黑色直线表示两者之间的偏差。在理想状态下，这种偏差为0，因为总是在时间发生时就会立即处理，即事件的真实时间与处理事件的时间是一致的。比如，12:01产生的事件刚好在12:01时被处理，12:02产生的事件刚好在12:02时被处理。但是现实总会有迟到的数据产生，比如网络延迟的原因，所以真实的情况会像深灰色的弯曲虚线表示的那样，即12:01产生的数据可能会在12:01之后被处理，12:02产生的数据在12:02时被处理，12:03时产生的数据会被在12:03之后处理。这种动态的偏差在分布式处理系统中是非常常见的。</p><h3 id="水位线图解"><a href="#水位线图解" class="headerlink" title="水位线图解"></a>水位线图解</h3><p>在上一小节，通过语言描述对水位线的概念进行了详细解读，在本小节会通过图解的方式解析水位线的含义，这样更能加深对水位线的理解。如下图所示：</p><p><img src="//jiamaoxiang.top/2020/04/17/Flink基于时间与窗口的算子/%E6%B0%B4%E4%BD%8D%E7%BA%BF%E5%9B%BE%E8%A7%A31.png" alt></p><p>如上图，矩形表示一条记录，三角表示该条记录的时间戳(真实发生时间)，圆圈表示水位线。可以看到上面的数据是乱序的，比如当算子接收到为2的水位线时，就可以认为时间戳小于等于2的数据都已经到来了，此时可以触发计算。同理，接收到为5的水位线时，就可以认为时间戳小于或等于5的数据都已经到来了，此时可以触发计算。</p><p>可以看出水位线是单调递增的，并且和记录的时间戳存在联系，一个时间戳为T的水位线表示接下来所有记录的时间戳一定都会大于T。</p><h3 id="水位线的传播"><a href="#水位线的传播" class="headerlink" title="水位线的传播"></a>水位线的传播</h3><p>现在，或许你已经对水位线是什么有了一个初步的认识，接下来将会介绍水位线是怎么在Flink内部传播的。关于水位线的传播策略可以归纳为3点：</p><ul><li>首先，水位线是以广播的形式在算子之间进行传播</li><li>Long.MAX_VALUE表示事件时间的结束，即未来不会有数据到来了</li><li>单个分区的输入取最大值，多个分区的输入取最小值</li></ul><p>关于Long.MAX_VALUE的解释，先看一段代码，如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="comment">/** </span></span><br><span class="line"><span class="comment"> * 当一个source关闭时，会输出一个Long.MAX_VALUE的水位线，当一个算子接收到该水位线时，</span></span><br><span class="line"><span class="comment"> * 相当于接收到一个信号：未来不会再有数据输入了</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Watermark</span> <span class="keyword">extends</span> <span class="title">StreamElement</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//表示事件时间的结束</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Watermark MAX_WATERMARK = <span class="keyword">new</span> Watermark(Long.MAX_VALUE);</span><br><span class="line">    <span class="comment">//省略的代码</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>关于另外两条策略的解释，可以从下图中得到：</p><p><img src="//jiamaoxiang.top/2020/04/17/Flink基于时间与窗口的算子/%E6%B0%B4%E4%BD%8D%E7%BA%BF%E4%BC%A0%E6%92%AD.png" alt></p><p>如上图，一个任务会为它的每个分区都维护一个分区水位线(partition watermark)，当收到每个分区传来的水位线时，任务首先会让当前分区水位线的值与接收的水位线值相比较，如果新接收的水位线值大于当前分区水位线值，则会将对应的分区水位线值更新为较大的水位线值(如上图中的2步骤)，接着，任务会把事件时钟调整为当前分区水位线值的最小值，如上图步骤2 ，由于当前分区水位线的最小值为3，所以将事件时间时钟更新为3，然后将值为3的水位线广播到下游任务。步骤3与步骤4的处理逻辑同上。</p><p>同时我们可以注意到这种设计其实有一个局限，具体体现在没有对分区(partition)是否来自于不同的流进行区分，比如对于两条流或多条流的Union或Connect操作，同样是按照全部分区水位线中最小值来更新事件时间时钟，这就导致所有的输入记录都会按照基于同一个事件时间时钟来处理，这种一刀切的做法对于同一个流的不同分区而言是无可厚非的，但是对于多条流而言，强制使用一个时钟进行同步会对整个集群带来较大的性能开销，比如当两个流的水位线相差很大是，其中的一个流要等待最慢的那条流，而较快的流的记录会在状态中缓存，直到事件时间时钟到达允许处理它们的那个时间点。</p><h3 id="水位线的生成方式"><a href="#水位线的生成方式" class="headerlink" title="水位线的生成方式"></a>水位线的生成方式</h3><p>通常情况下，在接收到数据源之后应该马上为其生成水位线，即越靠近数据源越好。Flink提供两种方式生成水位线，其中一种方式为在数据源完成的，即利用SourceFunction在应用读入数据流的时候分配时间戳与水位线。另一种方式是通过实现接口的自定义函数，该方式又包括两种实现方式：一种为周期性生成水位线，即实现AssignerWithPeriodicWatermarks接口，另一种为定点生成水位线，即实AssignerWithPunctuatedWatermarks接口。具体如下图所示：</p><p><img src="//jiamaoxiang.top/2020/04/17/Flink基于时间与窗口的算子/%E6%B0%B4%E4%BD%8D%E7%BA%BF%E7%94%9F%E6%88%90%E6%96%B9%E5%BC%8F.png" alt></p><h4 id="数据源方式"><a href="#数据源方式" class="headerlink" title="数据源方式"></a>数据源方式</h4><p>该方式主要是实现自定义数据源，数据源分配时间戳和水位线主要是通过内部的SourceContext对象实现的，先看一下SourceFunction的源码，如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">SourceFunction</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Function</span>, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">SourceContext</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">collect</span><span class="params">(T element)</span></span>;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">* 用于输出记录并附属一个与之关联的时间戳</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">collectWithTimestamp</span><span class="params">(T element, <span class="keyword">long</span> timestamp)</span></span>;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">* 用于输出传入的水位线</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">emitWatermark</span><span class="params">(Watermark mark)</span></span>;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">* 将自身标记为空闲状态</span></span><br><span class="line"><span class="comment">* 某个某个分区不在产生数据，会阻碍全局水位线前进，</span></span><br><span class="line"><span class="comment">* 因为收不到新的记录，意味着不会发出新的水位线，</span></span><br><span class="line"><span class="comment">* 根据水位线的传播策略，会导致整个应用都停止工作</span></span><br><span class="line"><span class="comment">* Flink提供一种机制，将数据源函数暂时标记为空闲，</span></span><br><span class="line"><span class="comment">* 在空闲状态下，Flink水位线的传播机制会忽略掉空闲的数据流分区</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">markAsTemporarilyIdle</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">Object <span class="title">getCheckpointLock</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上面对的代码可以看出，通过SourceContext对象的方法可以实现时间戳与水位线的分配。</p><h4 id="自定义函数的方式"><a href="#自定义函数的方式" class="headerlink" title="自定义函数的方式"></a>自定义函数的方式</h4><p>使用自定义函数的方式分配时间戳，只需要调用assignTimestampsAndWatermarks()方法，传入一个实现AssignerWithPeriodicWatermarks或者AssignerWithPunctuatedWatermarks接口的分配器即可，如下代码所示：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()</span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line">        SingleOutputStreamOperator&lt;UserBehavior&gt; userBehavior = env</span><br><span class="line">                .addSource(<span class="keyword">new</span> MysqlSource())</span><br><span class="line">                .assignTimestampsAndWatermarks(<span class="keyword">new</span> MyTimestampsAndWatermarks());</span><br></pre></td></tr></table></figure><ul><li><strong>周期分配器(AssignerWithPeriodicWatermarks)</strong></li></ul><p>该分配器是实现了一个AssignerWithPeriodicWatermarks的用户自定义函数，通过重写extractTimestamp()方法来提取时间戳，提取出来的时间戳会附加在各自的记录上，查询得到的水位线会注入到数据流中。</p><p>周期性的生成水位线是指以固定的时间间隔来发出水位线并推进事件时间的前进，关于默认的时间间隔在上文中也有提到，根据选择的时间语义确定默认的时间间隔，如果使用Processing Time或者Event Time，默认的水位线间隔时间是200毫秒，当然用户也可以自己设定时间间隔，关于如何设定，先看一段代码，代码来自于ExecutionConfig类：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 设置生成水位线的时间间隔</span></span><br><span class="line"><span class="comment">   * 注：自动生成watermarks的时间间隔不能是负数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ExecutionConfig <span class="title">setAutoWatermarkInterval</span><span class="params">(<span class="keyword">long</span> interval)</span> </span>&#123;</span><br><span class="line">Preconditions.checkArgument(interval &gt;= <span class="number">0</span>, <span class="string">"Auto watermark interval must not be negative."</span>);</span><br><span class="line"><span class="keyword">this</span>.autoWatermarkInterval = interval;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所以，如果要调整默认的200毫秒的间隔，可以调用setAutoWatermarkInterval()方法，具体使用如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">//每3秒生成一次水位线</span></span><br><span class="line">env.getConfig().setAutoWatermarkInterval(<span class="number">3000</span>);</span><br></pre></td></tr></table></figure><p>上面指定了每隔3秒生成一次水位线，即每隔3秒会自动向流里注入一个水位线，在代码层面，Flink会每隔3秒钟调用一次AssignerWithPeriodicWatermarks的getCurrentWatermark()方法，每次调用该方法时，如果得到的值不为空并且大于上一个水位线的时间戳，那么就会向流中注入一个新的水位线。这项检查可以有效地保证了事件时间的递增的特性，一旦检查失败也就不会生成水位线。下面给出一个实现周期分配水位线的例子：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyTimestampsAndWatermarks</span> <span class="keyword">implements</span> <span class="title">AssignerWithPeriodicWatermarks</span>&lt;<span class="title">UserBehavior</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// 定义1分钟的容忍间隔时间，即允许数据的最大乱序时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> maxOutofOrderness = <span class="number">60</span> * <span class="number">1000</span>;</span><br><span class="line">    <span class="comment">// 观察到的最大时间戳</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> currentMaxTs = Long.MIN_VALUE;      </span><br><span class="line"></span><br><span class="line">    <span class="meta">@Nullable</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Watermark <span class="title">getCurrentWatermark</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 生成具有1分钟容忍度的水位线</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Watermark(currentMaxTs - maxOutofOrderness);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(UserBehavior element, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//获取当前记录的时间戳</span></span><br><span class="line">        <span class="keyword">long</span> currentTs = element.timestamp;</span><br><span class="line">        <span class="comment">// 更新最大的时间戳</span></span><br><span class="line">        currentMaxTs = Math.max(currentMaxTs, currentTs);</span><br><span class="line">        <span class="comment">// 返回记录的时间戳</span></span><br><span class="line">        <span class="keyword">return</span> currentTs;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过查看TimestampAssignerd 继承关系可以发现(继承关系如下图)，除此之外，Flink还提供了两种内置的水位线分配器，分别为：AscendingTimestampExtractor和BoundedOutOfOrdernessTimestampExtractor两个抽象类。</p><p><img src="//jiamaoxiang.top/2020/04/17/Flink基于时间与窗口的算子/%E5%86%85%E7%BD%AE%E6%B0%B4%E4%BD%8D%E7%BA%BF%E5%88%86%E9%85%8D%E5%99%A8.png" alt></p><p>关于<strong>AscendingTimestampExtractor</strong>，一般是在数据集的时间戳是单调递增的且没有乱序时使用，该方法使用当前的时间戳生成水位线，使用方式如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;UserBehavior&gt; userBehavior = env</span><br><span class="line">                .addSource(<span class="keyword">new</span> MysqlSource())</span><br><span class="line">                .assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;UserBehavior&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(UserBehavior element)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> element.timestamp*<span class="number">1000</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br></pre></td></tr></table></figure><p>关于<strong>BoundedOutOfOrdernessTimestampExtractor</strong>，是在数据集中存在乱序数据的情况下使用，即数据有延迟(任意新到来的元素与已经到来的时间戳最大的元素之间的时间差)，这种方式可以接收一个表示最大预期延迟参数，具体如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;UserBehavior&gt; userBehavior = env</span><br><span class="line">                .addSource(<span class="keyword">new</span> MysqlSource())</span><br><span class="line">                .assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;UserBehavior&gt;(Time.seconds(<span class="number">10</span>)) &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(UserBehavior element)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> element.timestamp*<span class="number">1000</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; );</span><br></pre></td></tr></table></figure><p>上述的代码接收了一个10秒钟延迟的参数，这10秒钟意味着如果当前元素的事件时间与到达的元素的最大时间戳的差值在10秒之内，那么该元素会被处理，如果差值超过10秒，表示其本应该参与的计算，已经完成了，Flink称之为迟到的数据，Flink提供了不同的策略来处理这些迟到的数据。</p><ul><li><strong>定点水位线分配器(AssignerWithPunctuatedWatermarks)</strong></li></ul><p>该方式是基于某些事件(指示系统进度的特殊元祖或标记)触发水位线的生成与发送，基于特定的事件向流中注入一个水位线，流中的每一个元素都有机会判断是否生成一个水位线，如果得到的水位线不为空并且大于之前的水位线，就生成水位线并注入流中。</p><p>实现AssignerWithPunctuatedWatermarks接口，重写checkAndGetNextWatermark()方法，该方法会在针对每个事件的extractTimestamp()方法后立即调用，以此来决定是否生成一个新的水位线，如果该方法返回一个非空并且大于之前值的水位线，就会将这个新的水位线发出。</p><p>下面将会实现一个简单的定点水位线分配器</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyPunctuatedAssigner</span> <span class="keyword">implements</span> <span class="title">AssignerWithPunctuatedWatermarks</span>&lt;<span class="title">UserBehavior</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// 定义1分钟的容忍间隔时间，即允许数据的最大乱序时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> maxOutofOrderness = <span class="number">60</span> * <span class="number">1000</span>;      </span><br><span class="line">    <span class="meta">@Nullable</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Watermark <span class="title">checkAndGetNextWatermark</span><span class="params">(UserBehavior element, <span class="keyword">long</span> extractedTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 如果读取数据的用户行为是购买，就生成水位线</span></span><br><span class="line">        <span class="keyword">if</span>(element.action.equals(<span class="string">"buy"</span>))&#123;</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">new</span> Watermark(extractedTimestamp - maxOutofOrderness);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="comment">// 不发出水位线</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(UserBehavior element, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> element.timestamp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="迟到的数据"><a href="#迟到的数据" class="headerlink" title="迟到的数据"></a>迟到的数据</h3><p>上文已经说过，现实中很难生成一个完美的水位线，水位线就是在延迟与准确性之前做的一种权衡。那么，如果生成的水位线过于紧迫，即水位线可能会大于后来数据的时间戳，这就意味着数据有延迟，关于延迟数据的处理，Flink提供了一些机制，具体如下：</p><ul><li>直接将迟到的数据丢弃</li><li>将迟到的数据输出到单独的数据流中，即使用sideOutputLateData(new OutputTag&lt;&gt;()）实现侧输出</li><li>根据迟到的事件更新并发出结果</li></ul><p>由于篇幅限制，关于迟到数据的具体处理在本文先不做太多的讨论，在后续的文章中会对其详细进行说明。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文从Flink的时间语义开始说起，详细介绍了三种时间语义的概念、特点及使用方式，接着对Flink处理乱序数据的一种机制—水位线进行详细说明，主要描述了水位线的基本概念，传播方式、生成方式，并对其中的细节部分进行了图解，可以加深对水位线的理解。最后，简单说明了一下Flink对于迟到数据的处理方式。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink DataStream API编程指南</title>
      <link href="/2020/04/12/Flink-DataStream-API%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/"/>
      <url>/2020/04/12/Flink-DataStream-API%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<p>Flink DataStream API主要分为三个部分，分别为Source、Transformation以及Sink，其中Source是数据源，Flink内置了很多数据源，比如最常用的Kafka。Transformation是具体的转换操作，主要是用户定义的处理数据的逻辑，比如Map，FlatMap等。Sink(数据汇)是数据的输出，可以把处理之后的数据输出到存储设备上，Flink内置了许多的Sink，比如Kafka，HDFS等。另外除了Flink内置的Source和Sink外，用户可以实现自定义的Source与Sink。考虑到内置的Source与Sink使用起来比较简单且方便，所以，关于内置的Source与Sink的使用方式不在本文的讨论范围之内，本文会先从自定义Source开始说起，然后详细描述一些常见算子的使用方式，最后会实现一个自定义的Sink。</p><a id="more"></a><h2 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h2><p>Flink内部实现了比较常用的数据源，比如基于文件的，基于Socket的，基于集合的等等，如果这些都不能满足需求，用户可以自定义数据源，下面将会以MySQL为例，实现一个自定义的数据源。本文的所有操作将使用该数据源，具体代码如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Created</span> with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@author</span> : jmx</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Date</span>: 2020/4/14</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Time</span>: 17:34</span></span><br><span class="line"><span class="comment"> * note: RichParallelSourceFunction与SourceContext必须加泛型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MysqlSource</span> <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>&lt;<span class="title">UserBehavior</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> Connection conn;</span><br><span class="line">    <span class="keyword">public</span> PreparedStatement pps;</span><br><span class="line">    <span class="keyword">private</span> String driver;</span><br><span class="line">    <span class="keyword">private</span> String url;</span><br><span class="line">    <span class="keyword">private</span> String user;</span><br><span class="line">    <span class="keyword">private</span> String pass;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 该方法只会在最开始的时候被调用一次</span></span><br><span class="line"><span class="comment">     * 此方法用于实现获取连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//初始化数据库连接参数</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        URL fileUrl = TestProperties.class.getClassLoader().getResource(<span class="string">"mysql.ini"</span>);</span><br><span class="line">        FileInputStream inputStream = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(fileUrl.toURI()));</span><br><span class="line">        properties.load(inputStream);</span><br><span class="line">        inputStream.close();</span><br><span class="line">        driver = properties.getProperty(<span class="string">"driver"</span>);</span><br><span class="line">        url = properties.getProperty(<span class="string">"url"</span>);</span><br><span class="line">        user = properties.getProperty(<span class="string">"user"</span>);</span><br><span class="line">        pass = properties.getProperty(<span class="string">"pass"</span>);</span><br><span class="line">        <span class="comment">//获取数据连接</span></span><br><span class="line">        conn = getConection();</span><br><span class="line">        String scanSQL = <span class="string">"SELECT * FROM user_behavior_log"</span>;</span><br><span class="line">        pps = conn.prepareStatement(scanSQL);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;UserBehavior&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ResultSet resultSet = pps.executeQuery();</span><br><span class="line">        <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">            ctx.collect(UserBehavior.of(</span><br><span class="line">                    resultSet.getLong(<span class="string">"user_id"</span>),</span><br><span class="line">                    resultSet.getLong(<span class="string">"item_id"</span>),</span><br><span class="line">                    resultSet.getInt(<span class="string">"cat_id"</span>),</span><br><span class="line">                    resultSet.getInt(<span class="string">"merchant_id"</span>),</span><br><span class="line">                    resultSet.getInt(<span class="string">"brand_id"</span>),</span><br><span class="line">                    resultSet.getString(<span class="string">"action"</span>),</span><br><span class="line">                    resultSet.getString(<span class="string">"gender"</span>),</span><br><span class="line">                    resultSet.getLong(<span class="string">"timestamp"</span>)));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 实现关闭连接</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (pps != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                pps.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                conn.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取数据库连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> SQLException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Connection <span class="title">getConection</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        Connection connnection = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//加载驱动</span></span><br><span class="line">            Class.forName(driver);</span><br><span class="line">            <span class="comment">//获取连接</span></span><br><span class="line">            connnection = DriverManager.getConnection(</span><br><span class="line">                    url,</span><br><span class="line">                    user,</span><br><span class="line">                    pass);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> connnection;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>首先继承RichParallelSourceFunction，实现继承的方法，主要包括open()方法、run()方法及close方法。上述的</p><p>RichParallelSourceFunction是支持设置多并行度的，关于RichParallelSourceFunction与RichSourceFunction的区别，前者支持用户设置多并行度，后者不支持通过setParallelism()方法设置并行度，默认的并行度为1，否则会报如下错误：<figure class="highlight plain"><figcaption><span>in thread "main" java.lang.IllegalArgumentException: The maximum parallelism of non parallel operator must be 1.```</span></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">另外，RichParallelSourceFunction提供了额外的open()方法与close()方法，如果定义Source时需要获取链接，那么可以在open()方法中进行初始化，然后在close()方法中关闭资源链接，关于Rich***Function与普通Function的区别，下文会详细解释，在这里先有个印象。上述的代码中的配置信息是通过配置文件传递的，由于篇幅限制，我会把本文的代码放置在github，见文末github地址。</span><br><span class="line"></span><br><span class="line">## 基本转换</span><br><span class="line"></span><br><span class="line">Flink提供了大量的算子操作供用户使用，常见的算子主要包括以下几种，注意：本文不讨论关于基于时间与窗口的算子，这些内容会在《Flink基于时间与窗口的算子》中进行详细介绍。</span><br><span class="line"></span><br><span class="line">**说明**：本文的操作是基于上文自定义的MySQL Source，对应的数据解释如下：</span><br><span class="line"></span><br><span class="line">```java</span><br><span class="line">userId;     // 用户ID</span><br><span class="line">itemId;     // 商品ID</span><br><span class="line">catId;      // 商品类目ID</span><br><span class="line">merchantId; // 卖家ID</span><br><span class="line">brandId;    // 品牌ID</span><br><span class="line">action;     // 用户行为, 包括(&quot;pv&quot;, &quot;buy&quot;, &quot;cart&quot;, &quot;fav&quot;)</span><br><span class="line">gender;     // 性别</span><br><span class="line">timestamp;  // 行为发生的时间戳，单位秒</span><br></pre></td></tr></table></figure></p><h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><h4 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h4><p> DataStream → DataStream 的转换，输入一个元素，返回一个元素，如下操作：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;String&gt; userBehaviorMap = userBehavior.map(<span class="keyword">new</span> RichMapFunction&lt;UserBehavior, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(UserBehavior value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String action = <span class="string">""</span>;</span><br><span class="line">                <span class="keyword">switch</span> (value.action) &#123;</span><br><span class="line">                    <span class="keyword">case</span> <span class="string">"pv"</span>:</span><br><span class="line">                        action = <span class="string">"浏览"</span>;</span><br><span class="line">                    <span class="keyword">case</span> <span class="string">"cart"</span>:</span><br><span class="line">                        action = <span class="string">"加购"</span>;</span><br><span class="line">                    <span class="keyword">case</span> <span class="string">"fav"</span>:</span><br><span class="line">                        action = <span class="string">"收藏"</span>;</span><br><span class="line">                    <span class="keyword">case</span> <span class="string">"buy"</span>:</span><br><span class="line">                        action = <span class="string">"购买"</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> action;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><h4 id="示意图"><a href="#示意图" class="headerlink" title="示意图"></a>示意图</h4><p>将雨滴形状转换成相对应的圆形形状的map操作</p><p><img src="//jiamaoxiang.top/2020/04/12/Flink-DataStream-API编程指南/map.png" alt></p><h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><h4 id="解释-1"><a href="#解释-1" class="headerlink" title="解释"></a>解释</h4><p>DataStream → DataStream，输入一个元素，返回零个、一个或多个元素。事实上，flatMap算子可以看做是filter与map的泛化，即它能够实现这两种操作。flatMap算子对应的FlatMapFunction定义了flatMap方法，可以通过向collector对象传递数据的方式返回0个，1个或者多个事件作为结果。如下操作：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;UserBehavior&gt; userBehaviorflatMap = userBehavior.flatMap(<span class="keyword">new</span> RichFlatMapFunction&lt;UserBehavior, UserBehavior&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(UserBehavior value, Collector&lt;UserBehavior&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">if</span> (value.gender.equals(<span class="string">"女"</span>)) &#123;</span><br><span class="line">                    out.collect(value);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><h4 id="示意图-1"><a href="#示意图-1" class="headerlink" title="示意图"></a>示意图</h4><p>将黄色的雨滴过滤掉，将蓝色雨滴转为圆形，保留绿色雨滴</p><p><img src="//jiamaoxiang.top/2020/04/12/Flink-DataStream-API编程指南/flatmap.png" alt></p><h3 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h3><h4 id="解释-2"><a href="#解释-2" class="headerlink" title="解释"></a>解释</h4><p>DataStream → DataStream，过滤算子，对数据进行判断，符合条件即返回true的数据会被保留，否则被过滤。如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;UserBehavior&gt; userBehaviorFilter = userBehavior.filter(<span class="keyword">new</span> RichFilterFunction&lt;UserBehavior&gt;() &#123;</span><br><span class="line">          <span class="meta">@Override</span></span><br><span class="line">          <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(UserBehavior value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">              <span class="keyword">return</span> value.action.equals(<span class="string">"buy"</span>);<span class="comment">//保留购买行为的数据</span></span><br><span class="line">          &#125;</span><br><span class="line">      &#125;);</span><br></pre></td></tr></table></figure><h4 id="示意图-2"><a href="#示意图-2" class="headerlink" title="示意图"></a>示意图</h4><p>将红色与绿色雨滴过滤掉，保留蓝色雨滴。</p><p><img src="//jiamaoxiang.top/2020/04/12/Flink-DataStream-API编程指南/filter.png" alt></p><h3 id="keyBy"><a href="#keyBy" class="headerlink" title="keyBy"></a>keyBy</h3><h4 id="解释-3"><a href="#解释-3" class="headerlink" title="解释"></a>解释</h4><p>DataStream→KeyedStream，从逻辑上将流划分为不相交的分区。具有相同键的所有记录都分配给同一分区。在内部，keyBy（）是通过哈希分区实现的。<br>定义键值有3中方式：<br>(1)使用字段位置，如keyBy(1)，此方式是针对元组数据类型，比如tuple，使用元组相应元素的位置来定义键值;<br>(2)字段表达式,用于元组、POJO以及样例类;<br>(3)键值选择器，即keySelector，可以从输入事件中提取键值</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; userBehaviorkeyBy = userBehavior.map(<span class="keyword">new</span> RichMapFunction&lt;UserBehavior, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">map</span><span class="params">(UserBehavior value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Tuple2.of(value.action.toString(), <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(<span class="number">0</span>) <span class="comment">// scala元组编号从1开始，java元组编号是从0开始</span></span><br><span class="line">           .sum(<span class="number">1</span>); <span class="comment">//滚动聚合</span></span><br></pre></td></tr></table></figure><h4 id="示意图-3"><a href="#示意图-3" class="headerlink" title="示意图"></a>示意图</h4><p>基于形状对事件进行分区的keyBy操作</p><p><img src="//jiamaoxiang.top/2020/04/12/Flink-DataStream-API编程指南/keyBy.png" alt></p><h3 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h3><h4 id="解释-4"><a href="#解释-4" class="headerlink" title="解释"></a>解释</h4><p>KeyedStream → DataStream，对数据进行滚动聚合操作，结合当前元素和上一次Reduce返回的值进行聚合，然后返回一个新的值.将一个ReduceFunction应用在一个keyedStream上,每到来一个事件都会与当前reduce的结果进行聚合，<br>产生一个新的DataStream,该算子不会改变数据类型，因此输入流与输出流的类型永远保持一致。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; userBehaviorReduce = userBehavior.map(<span class="keyword">new</span> RichMapFunction&lt;UserBehavior, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">map</span><span class="params">(UserBehavior value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Tuple2.of(value.action.toString(), <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(<span class="number">0</span>) <span class="comment">// scala元组编号从1开始，java元组编号是从0开始</span></span><br><span class="line">          .reduce(<span class="keyword">new</span> RichReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">              <span class="meta">@Override</span></span><br><span class="line">              <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                  <span class="keyword">return</span> Tuple2.of(value1.f0,value1.f1 + value2.f1);<span class="comment">//滚动聚合,功能与sum类似</span></span><br><span class="line">              &#125;</span><br><span class="line">          &#125;);</span><br></pre></td></tr></table></figure><h4 id="示意图-4"><a href="#示意图-4" class="headerlink" title="示意图"></a>示意图</h4><p><img src="//jiamaoxiang.top/2020/04/12/Flink-DataStream-API编程指南/reduce.png" alt></p><h3 id="Aggregations-滚动聚合"><a href="#Aggregations-滚动聚合" class="headerlink" title="Aggregations(滚动聚合)"></a>Aggregations(滚动聚合)</h3><p>KeyedStream → DataStream，Aggregations(滚动聚合),滚动聚合转换作用于KeyedStream流上，生成一个包含聚合结果(比如sum求和，min最小值)的DataStream，滚动聚合的转换会为每个流过该算子的key值保存一个聚合结果，<br>当有新的元素流过该算子时，会根据之前的结果值和当前的元素值，更新相应的结果值</p><ul><li><p>sum():滚动聚合流过该算子的指定字段的和；</p></li><li><p>min():滚动计算流过该算子的指定字段的最小值</p></li><li><p>max():滚动计算流过该算子的指定字段的最大值</p></li><li><p>minBy():滚动计算当目前为止流过该算子的最小值，返回该值对应的事件；</p></li><li><p>maxBy():滚动计算当目前为止流过该算子的最大值，返回该值对应的事件；</p></li></ul><h3 id="union"><a href="#union" class="headerlink" title="union"></a>union</h3><h4 id="解释-5"><a href="#解释-5" class="headerlink" title="解释"></a>解释</h4><p>DataStream* → DataStream，将多条流合并，新的的流会包括所有流的数据，值得注意的是，两个流的数据类型必须一致，另外，来自两条流的事件会以FIFO(先进先出)的方式合并，所以并不能保证两条流的顺序，此外，union算子不会对数据去重，每个输入事件都会被发送到下游算子。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">userBehaviorkeyBy.union(userBehaviorReduce).print();<span class="comment">//将两条流union在一起，可以支持多条流(大于2)的union</span></span><br></pre></td></tr></table></figure><h4 id="示意图-5"><a href="#示意图-5" class="headerlink" title="示意图"></a>示意图</h4><p><img src="//jiamaoxiang.top/2020/04/12/Flink-DataStream-API编程指南/union.png" alt></p><h3 id="connect"><a href="#connect" class="headerlink" title="connect"></a>connect</h3><h4 id="解释-6"><a href="#解释-6" class="headerlink" title="解释"></a>解释</h4><p>DataStream,DataStream → ConnectedStreams，将两个流的事件进行组合，返回一个ConnectedStreams对象，两个流的数据类型可以不一致,ConnectedStreams对象提供了类似于map(),flatMap()功能的算子，如CoMapFunction与CoFlatMapFunction分别表示map()与flatMap算子，这两个算子会分别作用于两条流，注意：CoMapFunction 或CoFlatMapFunction被调用的时候并不能控制事件的顺序只要有事件流过该算子，该算子就会被调用。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ConnectedStreams&lt;UserBehavior, Tuple2&lt;String, Integer&gt;&gt; behaviorConnectedStreams = userBehaviorFilter.connect(userBehaviorkeyBy);</span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple3&lt;String, String, Integer&gt;&gt; behaviorConnectedStreamsmap = behaviorConnectedStreams.map(<span class="keyword">new</span> RichCoMapFunction&lt;UserBehavior, Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple3&lt;String, String, Integer&gt; <span class="title">map1</span><span class="params">(UserBehavior value1)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Tuple3.of(<span class="string">"first"</span>, value1.action, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple3&lt;String, String, Integer&gt; <span class="title">map2</span><span class="params">(Tuple2&lt;String, Integer&gt; value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Tuple3.of(<span class="string">"second"</span>, value2.f0, value2.f1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><h3 id="split"><a href="#split" class="headerlink" title="split"></a>split</h3><h4 id="解释-7"><a href="#解释-7" class="headerlink" title="解释"></a>解释</h4><p>DataStream → SplitStream，将流分割成两条或多条流，与union相反。分割之后的流与输入流的数据类型一致，<br>对于每个到来的事件可以被路由到0个、1个或多个输出流中。可以实现过滤与复制事件的功能，DataStream.split()接收一个OutputSelector函数，用来定义分流的规则，即将满足不同条件的流分配到用户命名的一个输出。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> SplitStream&lt;UserBehavior&gt; userBehaviorSplitStream = userBehavior.split(<span class="keyword">new</span> OutputSelector&lt;UserBehavior&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(UserBehavior value)</span> </span>&#123;</span><br><span class="line">                ArrayList&lt;String&gt; userBehaviors = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">                <span class="keyword">if</span> (value.action.equals(<span class="string">"buy"</span>)) &#123;</span><br><span class="line">                    userBehaviors.add(<span class="string">"buy"</span>);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    userBehaviors.add(<span class="string">"other"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> userBehaviors;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">userBehaviorSplitStream.select(<span class="string">"buy"</span>).print();</span><br></pre></td></tr></table></figure><h4 id="示意图-6"><a href="#示意图-6" class="headerlink" title="示意图"></a>示意图</h4><p><img src="//jiamaoxiang.top/2020/04/12/Flink-DataStream-API编程指南/split.png" alt></p><h2 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h2><p>Flink提供了许多内置的Sink，比如writeASText，print，HDFS，Kaka等等，下面将基于MySQL实现一个自定义的Sink，可以与自定义的MysqlSource进行对比，具体如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Created</span> with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@author</span> : jmx</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Date</span>: 2020/4/16</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Time</span>: 22:53</span></span><br><span class="line"><span class="comment"> *  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MysqlSink</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">UserBehavior</span>&gt; </span>&#123;</span><br><span class="line">    PreparedStatement pps;</span><br><span class="line">    <span class="keyword">public</span> Connection conn;</span><br><span class="line">    <span class="keyword">private</span> String driver;</span><br><span class="line">    <span class="keyword">private</span> String url;</span><br><span class="line">    <span class="keyword">private</span> String user;</span><br><span class="line">    <span class="keyword">private</span> String pass;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 在open() 方法初始化连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//初始化数据库连接参数</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        URL fileUrl = TestProperties.class.getClassLoader().getResource(<span class="string">"mysql.ini"</span>);</span><br><span class="line">        FileInputStream inputStream = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(fileUrl.toURI()));</span><br><span class="line">        properties.load(inputStream);</span><br><span class="line">        inputStream.close();</span><br><span class="line">        driver = properties.getProperty(<span class="string">"driver"</span>);</span><br><span class="line">        url = properties.getProperty(<span class="string">"url"</span>);</span><br><span class="line">        user = properties.getProperty(<span class="string">"user"</span>);</span><br><span class="line">        pass = properties.getProperty(<span class="string">"pass"</span>);</span><br><span class="line">        <span class="comment">//获取数据连接</span></span><br><span class="line">        conn = getConnection();</span><br><span class="line">        String insertSql = <span class="string">"insert into user_behavior values(?, ?, ?, ?,?, ?, ?, ?);"</span>;</span><br><span class="line">        pps = conn.prepareStatement(insertSql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 实现关闭连接</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (conn != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                conn.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (pps != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                pps.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 调用invoke() 方法，进行数据插入</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(UserBehavior value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        pps.setLong(<span class="number">1</span>, value.userId);</span><br><span class="line">        pps.setLong(<span class="number">2</span>, value.itemId);</span><br><span class="line">        pps.setInt(<span class="number">3</span>, value.catId);</span><br><span class="line">        pps.setInt(<span class="number">4</span>, value.merchantId);</span><br><span class="line">        pps.setInt(<span class="number">5</span>, value.brandId);</span><br><span class="line">        pps.setString(<span class="number">6</span>, value.action);</span><br><span class="line">        pps.setString(<span class="number">7</span>, value.gender);</span><br><span class="line">        pps.setLong(<span class="number">8</span>, value.timestamp);</span><br><span class="line">        pps.executeUpdate();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取数据库连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> SQLException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Connection <span class="title">getConnection</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        Connection connnection = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//加载驱动</span></span><br><span class="line">            Class.forName(driver);</span><br><span class="line">            <span class="comment">//获取连接</span></span><br><span class="line">            connnection = DriverManager.getConnection(</span><br><span class="line">                    url,</span><br><span class="line">                    user,</span><br><span class="line">                    pass);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> connnection;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="关于RichFunction"><a href="#关于RichFunction" class="headerlink" title="关于RichFunction"></a>关于RichFunction</h2><p>细心的读者可以发现，在前文的算子操作案例中，使用的都是RichFunction，因为在很多时候需要在函数处理数据之前先进行一些初始化操作，或者获取函数的上下文信息，DataStream API提供了一类RichFunction，与普通的函数相比，该函数提供了许多额外的功能。</p><p>使用RichFunction的时候，可以实现两个额外的方法：</p><ul><li>open(),是初始化方法，会在每个人物首次调用转换方法(比如map)前调用一次。通常用于进行一次的设置工作，注意Configuration参数只在DataSet API中使用，而并没有在DataStream API中使用，因此在使用DataStream API时，可以将其忽略。</li><li>close()，函数的终止方法 ，会在每个任务最后一次调用转换方法后调用一次，通常用于资源释放等操作。</li></ul><p>此外用户还可以通过getRuntimeContext()方法访问函数的上下文信息(RuntimeContext),例如函数的并行度，函数所在subtask的编号以及执行函数的任务名称，同时也可以访问分区状态。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先实现了自定义MySQL Source，然后基于MySql 的Source进行了一系列的算子操作，并对常见的算子操作进行详细剖析，最后实现了一个自定义MySQL Sink，并对RichFunction进行了解释。</p><p><strong>代码地址</strong>:<a href="https://github.com/jiamx/study-flink" target="_blank" rel="noopener">https://github.com/jiamx/study-flink</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何使用Hive进行OLAP分析</title>
      <link href="/2020/04/09/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Hive%E8%BF%9B%E8%A1%8COLAP%E5%88%86%E6%9E%90/"/>
      <url>/2020/04/09/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Hive%E8%BF%9B%E8%A1%8COLAP%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>本文首先介绍了什么是OLAP，接着介绍Hive中提供的几种OLAP分析的函数，并对每一种函数进行了详细说明，并给出了相关的图示解释，最后以一个案例说明了这几种函数的使用方式，可以进一步加深理解。</p><a id="more"></a><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>在线分析处理(OLAP,Online Analytical Processing)是通过带层次的维度和跨维度进行多维分析的，简单理解为一种多维数据分析的方式，通过OLAP可以展示数据仓库中数据的多维逻辑视图。在多维分析中，数据是按照维度(观察数据的角度)来表示的，比如商品、城市、客户。而维通常按层次(层次维度)组织的，如城市、省、国家，再比如时间也是有层次的，如天、周、月、季度和年。不同的管理者可以从不同的维度(视角)去观察这些数据，这些在多个不同维度上对数据进行综合考察的手段就是通常所说的数据仓库多维查询，最常见的就如上卷(roll-up)和下钻(drill-down)了,所谓上卷，指的是选定特定的数据范围之后，对其进行汇总统计以获取更高层次的信息。所谓下钻，指的是选定特定的数据范围之后，需要进一步查看细节的数据。从另一种意义上说，钻取就是针对多维展现的数据，进一步探究其内部的组成和来源。值得注意的是，上卷和下钻要求维度具有层级结构，即数仓中所说的层次维度。</p><h2 id="如何实现数据的多维分析"><a href="#如何实现数据的多维分析" class="headerlink" title="如何实现数据的多维分析"></a>如何实现数据的多维分析</h2><p>Hive提供了多维数据分析的函数，如GROUPING SETS,GROUPING_ID,CUBE,ROLLUP,通过这些分析函数，可以轻而易举的实现多维数据分析。下面将会通过一个案例来了解这些函数的具体含义以及该如何使用这些函数。注意：在hive中使用这些函数之前，要确保开启了map端聚合，即set hive.map.aggr=true，否则会报如下错误：</p><p><img src="//jiamaoxiang.top/2020/04/09/如何使用Hive进行OLAP分析/map%E7%AB%AF%E8%81%9A%E5%90%88.png" alt></p><h3 id="简单介绍"><a href="#简单介绍" class="headerlink" title="简单介绍"></a>简单介绍</h3><ul><li>GROUPING SETS</li></ul><p>在一个group by查询中，通过该子句可以对不同维度或同一维度的不同层次进行聚合，简单理解为一条sql可以实现多种不同的分组规则，用户可以在该函数中传入自己定义的多种分组字段，本质上等价于多个group by语句进行UNION，对于GROUPING SETS子句中的空白集’（）’表示对总体进行聚集。</p><p><strong>示例模板</strong></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 使用GROUPING SETS查询</span></span><br><span class="line"><span class="keyword">SELECT</span> a,</span><br><span class="line">       b,</span><br><span class="line">       <span class="keyword">SUM</span>(c)</span><br><span class="line"><span class="keyword">FROM</span> tab1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> a,b</span><br><span class="line"><span class="keyword">GROUPING</span> <span class="keyword">SETS</span> ((a,b), a,b, ());</span><br><span class="line"><span class="comment">-- 与GROUP BY等价关系</span></span><br><span class="line"><span class="keyword">SELECT</span> a, b, <span class="keyword">SUM</span>( c ) <span class="keyword">FROM</span> tab1 <span class="keyword">GROUP</span> <span class="keyword">BY</span> a, b</span><br><span class="line"><span class="keyword">UNION</span></span><br><span class="line"><span class="keyword">SELECT</span> a, <span class="literal">null</span>, <span class="keyword">SUM</span>( c ) <span class="keyword">FROM</span> tab1 <span class="keyword">GROUP</span> <span class="keyword">BY</span> a, <span class="literal">null</span></span><br><span class="line"><span class="keyword">UNION</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="literal">null</span>, b, <span class="keyword">SUM</span>( c ) <span class="keyword">FROM</span> tab1 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="literal">null</span>, b</span><br><span class="line"><span class="keyword">UNION</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="literal">null</span>, <span class="literal">null</span>, <span class="keyword">SUM</span>( c ) <span class="keyword">FROM</span> tab1;</span><br></pre></td></tr></table></figure><ul><li>GROUPING__ID</li></ul><p>当使用聚合时，有时候会出现数据本身为null值，很难区分究竟是数据列本身为null值还是聚合数据行为null，即无法区分查询结果中的null值是属于列本身的还是聚合的结果行，因此需要一种方法识别出列中的null值。grouping_id 函数就是此场景下的解决方案。注意该函数是有两个下划线。这个函数为每种聚合数据行生成唯一的组id。它的返回值看起来像整型数值，其实是字符串类型，这个值使用了位图策略（bitvector，位向量），即它的二进制形式中的每一位表示对应列是否参与分组，如果某一列参与了分组，对应位就被置为1，否则为0。通过这种方式可以区分出数据本身中的null值。看到这是不是还是一头雾水，没关系，来看下面的示例：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_grouping__id(<span class="keyword">id</span> <span class="built_in">int</span>,amount <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>));</span><br><span class="line"><span class="comment">-- 插入数据</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> test_grouping__id <span class="keyword">values</span>(<span class="number">1</span>,<span class="literal">null</span>),(<span class="number">1</span>,<span class="number">1</span>),(<span class="number">2</span>,<span class="number">2</span>),(<span class="number">3</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="literal">null</span>),(<span class="number">4</span>,<span class="number">5</span>);</span><br><span class="line"><span class="comment">--执行查询</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">id</span>,</span><br><span class="line">       amount,</span><br><span class="line">       grouping__id,</span><br><span class="line">       <span class="keyword">count</span>(*) cnt</span><br><span class="line"><span class="keyword">FROM</span> test_grouping__id</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">id</span>,</span><br><span class="line">         amount</span><br><span class="line"><span class="keyword">GROUPING</span> <span class="keyword">sets</span>(<span class="keyword">id</span>,(<span class="keyword">id</span>,amount),())</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> grouping__id</span><br></pre></td></tr></table></figure><p><strong>查询结果分析</strong></p><p>查询结果如下图所示：绿色框表示未进行分组，即进行全局聚合，grouping_id等于0，表示没有字段参与分组。蓝色框表示按照id进行分组，对应的grouping_id为1，表示只有一个字段参与了分组。橘色的框表示按照id和amount两个字段进行分组，grouping_id为3，即有两个字段参与了分组，转成十进制为2^0 + 2^1  = 3。</p><p><img src="//jiamaoxiang.top/2020/04/09/如何使用Hive进行OLAP分析/grouping__id.png" alt></p><p>以上面为例，分组字段为id、amount，转成二进制表示形式为：</p><p><img src="//jiamaoxiang.top/2020/04/09/如何使用Hive进行OLAP分析/%E4%BA%8C%E8%BF%9B%E5%88%B6.png" alt></p><ul><li>ROLLUP</li></ul><p>通用的语法为WITH ROLLUP,需要与group by一起用于在维的层次结构级别上计算聚合。功能为可以按照group by的分组字段进行组合，计算出不同分组的结果。注意对于分组字段的组合会与最左边的字段为主。使用ROLLUP的GROUP BY a，b，c假定层次结构是“ a”向下钻取到“ b”，“ b”向下钻取到“ c”。则可以通过GROUP BY a，b，c，WITH ROLLUP进行实现，该语句等价于GROUP BY a，b，c GROUPING SETS（（a，b，c），（a，b），（a），（））。即使用WITH ROLLUP，首先会对全局聚合(不分组)，然后会按GROUP BY字段组合，进行聚合，但是最左侧的分组字段必须参与分组，比如a字段是最左侧的字段，则a必定参与分组组合。</p><p><strong>示例模板</strong></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 使用WITH ROLLUP查询</span></span><br><span class="line"><span class="keyword">SELECT</span> a,</span><br><span class="line">       b,</span><br><span class="line">       c</span><br><span class="line">       <span class="keyword">SUM</span>(d)</span><br><span class="line"><span class="keyword">FROM</span> tab1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> a,b,c</span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">ROLLUP</span></span><br><span class="line"><span class="comment">-- 等价于下面的方式</span></span><br><span class="line"><span class="keyword">SELECT</span> a,</span><br><span class="line">       b,</span><br><span class="line">       c,</span><br><span class="line">       <span class="keyword">SUM</span>(d)</span><br><span class="line"><span class="keyword">FROM</span> tab1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> a,b,c</span><br><span class="line"><span class="keyword">GROUPING</span> <span class="keyword">SETS</span> ((a,b,c), (a,b), (a),());</span><br></pre></td></tr></table></figure><ul><li>CUBE</li></ul><p>CUBE表示一个立方体，apache的kylin使用就是这种预计算方式。即会对给定的维度(分组字段)进行多种组合之后，形成不同分组规则的数据结果。一旦我们在一组维度上计算出CUBE，就可以得到这些维度上所有可能的聚合聚合结果。比如：<strong>GROUP BY a，b，c WITH CUBE</strong>，等价于<strong>GROUP BY a，b，c GROUPING SETS（（a，b，c），（a，b），（b，c）， （a，c），（a），（b），（c），（））</strong>。</p><p>其实，可以将上面的情况抽象成排列组合的问题，即从分组字段集合(假设有n个字段)中随意取出0~n个字段，那么会有多少中组合方式，如下面公式所示：</p><p><img src="//jiamaoxiang.top/2020/04/09/如何使用Hive进行OLAP分析/%E7%BB%84%E5%90%881.png" alt></p><p>结合上面的例子，<strong>GROUP BY a，b，c WITH CUBE</strong>，那么所有的组合方式有：（a，b，c），（a，b），（b，c）， （a，c），（a），（b），（c），（）,一共有8种组合，即2^3 = 8。</p><p><strong>示例模板</strong></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 使用WITH CUBE查询</span></span><br><span class="line"><span class="keyword">SELECT</span> a,</span><br><span class="line">       b,</span><br><span class="line">       c</span><br><span class="line">       <span class="keyword">SUM</span>(d)</span><br><span class="line"><span class="keyword">FROM</span> tab1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> a,b,c</span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">CUBE</span></span><br><span class="line"><span class="comment">-- 等价于下面的方式</span></span><br><span class="line"><span class="keyword">SELECT</span> a,</span><br><span class="line">       b,</span><br><span class="line">       c,</span><br><span class="line">       <span class="keyword">SUM</span>(d)</span><br><span class="line"><span class="keyword">FROM</span> tab1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> a,b,c</span><br><span class="line"><span class="keyword">GROUPING</span> <span class="keyword">SETS</span> ((a,b,c),(a,b),(b,c), (a,c),(a),(b),(c),());</span><br></pre></td></tr></table></figure><h3 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h3><h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><p>有一份用户行为数据集，包括用户的所有行为（包括pv点击、buy购买、cart加购、fav收藏），具体如下表所示：</p><table><thead><tr><th>字段名</th><th align="left">列名称</th><th align="left">说明</th></tr></thead><tbody><tr><td>user_id</td><td align="left">用户ID</td><td align="left">整数类型，用户ID</td></tr><tr><td>item_id</td><td align="left">商品ID</td><td align="left">整数类型，商品ID</td></tr><tr><td>category_id</td><td align="left">商品类目ID</td><td align="left">整数类型，商品所属类目ID</td></tr><tr><td>behavior</td><td align="left">行为类型</td><td align="left">字符串，枚举类型，包括(‘pv’, ‘buy’, ‘cart’, ‘fav’)</td></tr><tr><td>access_time</td><td align="left">时间戳</td><td align="left">行为发生的时间戳，单位秒</td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_behavior</span><br><span class="line">             (</span><br><span class="line">                user_id <span class="built_in">int</span> ,</span><br><span class="line">                item_id <span class="built_in">int</span>,</span><br><span class="line">                category_id <span class="built_in">int</span>,</span><br><span class="line">                behavior <span class="keyword">string</span>,</span><br><span class="line">               access_time <span class="keyword">string</span></span><br><span class="line">               )</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span>;</span><br><span class="line"><span class="comment">-- 装载数据</span></span><br><span class="line">1,101,1,pv,1511658000</span><br><span class="line">2,102,1,pv,1511658000</span><br><span class="line">3,103,1,pv,1511658000</span><br><span class="line">4,104,2,cart,1511659329</span><br><span class="line">5,105,2,buy,1511659326</span><br><span class="line">6,106,3,fav,1511659323</span><br><span class="line">7,101,1,pv,1511658010</span><br><span class="line">8,102,1,buy,1511658200</span><br><span class="line">9,103,1,cart,1511658030</span><br><span class="line">10,107,3,fav,1511659332</span><br></pre></td></tr></table></figure><h4 id="GROUPING-SETS使用"><a href="#GROUPING-SETS使用" class="headerlink" title="GROUPING SETS使用"></a>GROUPING SETS使用</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询每种商品品类、每种用户行为的访问次数</span></span><br><span class="line"><span class="comment">-- 查询每种用户行为的访问次数</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">item_id,</span><br><span class="line">category_id,</span><br><span class="line">behavior,</span><br><span class="line"><span class="keyword">COUNT</span>(*) <span class="keyword">AS</span> cnt,</span><br><span class="line">GROUPING__ID </span><br><span class="line"><span class="keyword">FROM</span> user_behavior </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> item_id,category_id,behavior </span><br><span class="line"><span class="keyword">GROUPING</span> <span class="keyword">SETS</span> ((category_id,behavior),behavior)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br></pre></td></tr></table></figure><p><strong>结果如下：</strong></p><p><img src="//jiamaoxiang.top/2020/04/09/如何使用Hive进行OLAP分析/grouping_set.png" alt></p><h4 id="ROLLUP使用"><a href="#ROLLUP使用" class="headerlink" title="ROLLUP使用"></a>ROLLUP使用</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询每种商品品类的访问次数</span></span><br><span class="line"><span class="comment">-- 查询每种商品品类、每种用户行为的次数</span></span><br><span class="line"><span class="comment">-- 查询用户的总访问次数</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">category_id,</span><br><span class="line">behavior,</span><br><span class="line"><span class="keyword">COUNT</span>(*) <span class="keyword">AS</span> cnt,</span><br><span class="line">GROUPING__ID </span><br><span class="line"><span class="keyword">FROM</span> user_behavior </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> category_id,behavior </span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">ROLLUP</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br></pre></td></tr></table></figure><p><strong>结果如下：</strong></p><p><img src="//jiamaoxiang.top/2020/04/09/如何使用Hive进行OLAP分析/rollup.png" alt></p><h4 id="CUBE使用"><a href="#CUBE使用" class="headerlink" title="CUBE使用"></a>CUBE使用</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询每种商品品类的访问次数</span></span><br><span class="line"><span class="comment">-- 查询每种用户行为的次数</span></span><br><span class="line"><span class="comment">-- 查询每种商品品类、每种用户行为的次数</span></span><br><span class="line"><span class="comment">-- 查询用户的总访问次数</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">category_id,</span><br><span class="line">behavior,</span><br><span class="line"><span class="keyword">COUNT</span>(*) <span class="keyword">AS</span> cnt,</span><br><span class="line">GROUPING__ID </span><br><span class="line"><span class="keyword">FROM</span> user_behavior </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> category_id,behavior </span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">CUBE</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br></pre></td></tr></table></figure><p><strong>结果如下：</strong></p><p><img src="//jiamaoxiang.top/2020/04/09/如何使用Hive进行OLAP分析/cube.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先介绍了什么是OLAP，接着介绍Hive中提供的几种OLAP分析的函数，并对每一种函数进行了详细说明，并给出了相关的图示解释，最后以一个案例说明了这几种函数的使用方式，可以进一步加深理解。</p>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>你真的了解Flink Kafka source吗？</title>
      <link href="/2020/04/02/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3Flink-Kafka-connector%E5%90%97%EF%BC%9F/"/>
      <url>/2020/04/02/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3Flink-Kafka-connector%E5%90%97%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>Flink 提供了专门的 Kafka 连接器，向 Kafka topic 中读取或者写入数据。Flink Kafka Consumer 集成了 Flink 的 Checkpoint 机制，可提供 exactly-once 的处理语义。为此，Flink 并不完全依赖于跟踪 Kafka 消费组的偏移量，而是在内部跟踪和检查偏移量。</p><a id="more"></a><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>当我们在使用Spark Streaming、Flink等计算框架进行数据实时处理时，使用Kafka作为一款发布与订阅的消息系统成为了标配。Spark Streaming与Flink都提供了相对应的Kafka Consumer，使用起来非常的方便，只需要设置一下Kafka的参数，然后添加kafka的source就万事大吉了。如果你真的觉得事情就是如此的so easy，感觉妈妈再也不用担心你的学习了，那就真的是too young too simple sometimes naive了。本文以Flink 的Kafka Source为讨论对象，首先从基本的使用入手，然后深入源码逐一剖析，一并为你拨开Flink Kafka connector的神秘面纱。值得注意的是，本文假定读者具备了Kafka的相关知识，关于Kafka的相关细节问题，不在本文的讨论范围之内。</p><h2 id="Flink-Kafka-Consumer介绍"><a href="#Flink-Kafka-Consumer介绍" class="headerlink" title="Flink Kafka Consumer介绍"></a>Flink Kafka Consumer介绍</h2><p>Flink Kafka Connector有很多个版本，可以根据你的kafka和Flink的版本选择相应的包（maven artifact id）和类名。本文所涉及的Flink版本为1.10，Kafka的版本为2.3.4。Flink所提供的Maven依赖于类名如下表所示：</p><table><thead><tr><th align="left">Maven 依赖</th><th align="left">自从哪个版本 开始支持</th><th align="left">类名</th><th align="left">Kafka 版本</th><th align="left">注意</th></tr></thead><tbody><tr><td align="left">flink-connector-kafka-0.8_2.11</td><td align="left">1.0.0</td><td align="left">FlinkKafkaConsumer08 FlinkKafkaProducer08</td><td align="left">0.8.x</td><td align="left">这个连接器在内部使用 Kafka 的 <a href="https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+SimpleConsumer+Example" target="_blank" rel="noopener">SimpleConsumer</a> API。偏移量由 Flink 提交给 ZK。</td></tr><tr><td align="left">flink-connector-kafka-0.9_2.11</td><td align="left">1.0.0</td><td align="left">FlinkKafkaConsumer09 FlinkKafkaProducer09</td><td align="left">0.9.x</td><td align="left">这个连接器使用新的 Kafka <a href="http://kafka.apache.org/documentation.html#newconsumerapi" target="_blank" rel="noopener">Consumer API</a></td></tr><tr><td align="left">flink-connector-kafka-0.10_2.11</td><td align="left">1.2.0</td><td align="left">FlinkKafkaConsumer010 FlinkKafkaProducer010</td><td align="left">0.10.x</td><td align="left">这个连接器支持 <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message" target="_blank" rel="noopener">带有时间戳的 Kafka 消息</a>，用于生产和消费。</td></tr><tr><td align="left">flink-connector-kafka-0.11_2.11</td><td align="left">1.4.0</td><td align="left">FlinkKafkaConsumer011 FlinkKafkaProducer011</td><td align="left">&gt;=  0.11.x</td><td align="left">Kafka 从 0.11.x 版本开始不支持 Scala 2.10。此连接器支持了 <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging" target="_blank" rel="noopener">Kafka 事务性的消息传递</a>来为生产者提供 Exactly once 语义。</td></tr><tr><td align="left">flink-connector-kafka_2.11</td><td align="left">1.7.0</td><td align="left">FlinkKafkaConsumer FlinkKafkaProducer</td><td align="left">&gt;= 1.0.0</td><td align="left">这个通用的 Kafka 连接器尽力与 Kafka client 的最新版本保持同步。该连接器使用的 Kafka client 版本可能会在 Flink 版本之间发生变化。从 Flink 1.9 版本开始，它使用 Kafka 2.2.0 client。当前 Kafka 客户端向后兼容 0.10.0 或更高版本的 Kafka broker。 但是对于 Kafka 0.11.x 和 0.10.x 版本，我们建议你分别使用专用的 flink-connector-kafka-0.11_2.11 和 flink-connector-kafka-0.10_2.11 连接器。</td></tr></tbody></table><h2 id="Demo示例"><a href="#Demo示例" class="headerlink" title="Demo示例"></a>Demo示例</h2><h3 id="添加Maven依赖"><a href="#添加Maven依赖" class="headerlink" title="添加Maven依赖"></a>添加Maven依赖</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;!--本文使用的是通用型的connector--&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.10.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="简单代码案例"><a href="#简单代码案例" class="headerlink" title="简单代码案例"></a>简单代码案例</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConnector</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment senv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">// 开启checkpoint，时间间隔为毫秒</span></span><br><span class="line">        senv.enableCheckpointing(<span class="number">5000L</span>);</span><br><span class="line">        <span class="comment">// 选择状态后端</span></span><br><span class="line">        senv.setStateBackend((StateBackend) <span class="keyword">new</span> FsStateBackend(<span class="string">"file:///E://checkpoint"</span>));</span><br><span class="line">        <span class="comment">//senv.setStateBackend((StateBackend) new FsStateBackend("hdfs://kms-1:8020/checkpoint"));</span></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// kafka broker地址</span></span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"kms-2:9092,kms-3:9092,kms-4:9092"</span>);</span><br><span class="line">        <span class="comment">// 仅kafka0.8版本需要配置</span></span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"kms-2:2181,kms-3:2181,kms-4:2181"</span>);</span><br><span class="line">        <span class="comment">// 消费者组</span></span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line">        <span class="comment">// 自动偏移量提交</span></span><br><span class="line">        props.put(<span class="string">"enable.auto.commit"</span>, <span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">// 偏移量提交的时间间隔，毫秒</span></span><br><span class="line">        props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="number">5000</span>);</span><br><span class="line">        <span class="comment">// kafka 消息的key序列化器</span></span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        <span class="comment">// kafka 消息的value序列化器</span></span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        <span class="comment">// 指定kafka的消费者从哪里开始消费数据</span></span><br><span class="line">        <span class="comment">// 共有三种方式，</span></span><br><span class="line">        <span class="comment">// #earliest</span></span><br><span class="line">        <span class="comment">// 当各分区下有已提交的offset时，从提交的offset开始消费；</span></span><br><span class="line">        <span class="comment">// 无提交的offset时，从头开始消费</span></span><br><span class="line">        <span class="comment">// #latest</span></span><br><span class="line">        <span class="comment">// 当各分区下有已提交的offset时，从提交的offset开始消费；</span></span><br><span class="line">        <span class="comment">// 无提交的offset时，消费新产生的该分区下的数据</span></span><br><span class="line">        <span class="comment">// #none</span></span><br><span class="line">        <span class="comment">// topic各分区都存在已提交的offset时，</span></span><br><span class="line">        <span class="comment">// 从offset后开始消费；</span></span><br><span class="line">        <span class="comment">// 只要有一个分区不存在已提交的offset，则抛出异常</span></span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; consumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(</span><br><span class="line">                <span class="string">"qfbap_ods.code_city"</span>,</span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props);</span><br><span class="line">        <span class="comment">//设置checkpoint后在提交offset，即oncheckpoint模式</span></span><br><span class="line">        <span class="comment">// 该值默认为true，</span></span><br><span class="line">        consumer.setCommitOffsetsOnCheckpoints(<span class="keyword">true</span>);</span><br><span class="line">     </span><br><span class="line">        <span class="comment">// 最早的数据开始消费</span></span><br><span class="line">        <span class="comment">// 该模式下，Kafka 中的 committed offset 将被忽略，不会用作起始位置。</span></span><br><span class="line">        <span class="comment">//consumer.setStartFromEarliest();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 消费者组最近一次提交的偏移量，默认。</span></span><br><span class="line">        <span class="comment">// 如果找不到分区的偏移量，那么将会使用配置中的 auto.offset.reset 设置</span></span><br><span class="line">        <span class="comment">//consumer.setStartFromGroupOffsets();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 最新的数据开始消费</span></span><br><span class="line">        <span class="comment">// 该模式下，Kafka 中的 committed offset 将被忽略，不会用作起始位置。</span></span><br><span class="line">        <span class="comment">//consumer.setStartFromLatest();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定具体的偏移量时间戳,毫秒</span></span><br><span class="line">        <span class="comment">// 对于每个分区，其时间戳大于或等于指定时间戳的记录将用作起始位置。</span></span><br><span class="line">        <span class="comment">// 如果一个分区的最新记录早于指定的时间戳，则只从最新记录读取该分区数据。</span></span><br><span class="line">        <span class="comment">// 在这种模式下，Kafka 中的已提交 offset 将被忽略，不会用作起始位置。</span></span><br><span class="line">        <span class="comment">//consumer.setStartFromTimestamp(1585047859000L);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 为每个分区指定偏移量</span></span><br><span class="line">        <span class="comment">/*Map&lt;KafkaTopicPartition, Long&gt; specificStartOffsets = new HashMap&lt;&gt;();</span></span><br><span class="line"><span class="comment">        specificStartOffsets.put(new KafkaTopicPartition("qfbap_ods.code_city", 0), 23L);</span></span><br><span class="line"><span class="comment">        specificStartOffsets.put(new KafkaTopicPartition("qfbap_ods.code_city", 1), 31L);</span></span><br><span class="line"><span class="comment">        specificStartOffsets.put(new KafkaTopicPartition("qfbap_ods.code_city", 2), 43L);</span></span><br><span class="line"><span class="comment">        consumer1.setStartFromSpecificOffsets(specificStartOffsets);*/</span></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 请注意：当 Job 从故障中自动恢复或使用 savepoint 手动恢复时，</span></span><br><span class="line"><span class="comment">         * 这些起始位置配置方法不会影响消费的起始位置。</span></span><br><span class="line"><span class="comment">         * 在恢复时，每个 Kafka 分区的起始位置由存储在 savepoint 或 checkpoint 中的 offset 确定</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; source = senv.addSource(consumer);</span><br><span class="line">        <span class="comment">// TODO</span></span><br><span class="line">        source.print();</span><br><span class="line">        senv.execute(<span class="string">"test kafka connector"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="参数配置解读"><a href="#参数配置解读" class="headerlink" title="参数配置解读"></a>参数配置解读</h3><p>在Demo示例中，给出了详细的配置信息，下面将对上面的参数配置进行逐一分析。</p><h4 id="kakfa的properties参数配置"><a href="#kakfa的properties参数配置" class="headerlink" title="kakfa的properties参数配置"></a>kakfa的properties参数配置</h4><ul><li><p>bootstrap.servers：kafka broker地址</p></li><li><p>zookeeper.connect：仅kafka0.8版本需要配置</p></li><li><p>group.id：消费者组</p></li><li><p>enable.auto.commit：</p><p>自动偏移量提交，该值的配置不是最终的偏移量提交模式，需要考虑用户是否开启了checkpoint，</p><p>在下面的源码分析中会进行解读</p></li><li><p>auto.commit.interval.ms：偏移量提交的时间间隔，毫秒</p></li><li><p>key.deserializer：</p><p>kafka 消息的key序列化器，如果不指定会使用ByteArrayDeserializer序列化器</p></li><li><p>value.deserializer：</p></li></ul><p>kafka 消息的value序列化器，如果不指定会使用ByteArrayDeserializer序列化器</p><ul><li><p>auto.offset.reset：</p><p>指定kafka的消费者从哪里开始消费数据，共有三种方式，</p><ul><li>第一种：earliest<br>当各分区下有已提交的offset时，从提交的offset开始消费； 无提交的offset时，从头开始消费</li><li>第二种：latest<br>当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据</li><li>第三种：none<br>topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常</li></ul><p>注意：上面的指定消费模式并不是最终的消费模式，取决于用户在Flink程序中配置的消费模式</p></li></ul><h4 id="Flink程序用户配置的参数"><a href="#Flink程序用户配置的参数" class="headerlink" title="Flink程序用户配置的参数"></a>Flink程序用户配置的参数</h4><ul><li>consumer.setCommitOffsetsOnCheckpoints(true)</li></ul><p>​    解释：设置checkpoint后在提交offset，即oncheckpoint模式，该值默认为true，该参数会影响偏移量的提交方式，下面的源码中会进行分析</p><ul><li><p>consumer.setStartFromEarliest()</p><p>解释： 最早的数据开始消费 ，该模式下，Kafka 中的 committed offset 将被忽略，不会用作起始位置。该方法为继承父类FlinkKafkaConsumerBase的方法。</p></li><li><p>consumer.setStartFromGroupOffsets()</p><p>解释：消费者组最近一次提交的偏移量，默认。 如果找不到分区的偏移量，那么将会使用配置中的 auto.offset.reset 设置，该方法为继承父类FlinkKafkaConsumerBase的方法。</p></li><li><p>consumer.setStartFromLatest()</p><p>解释：最新的数据开始消费，该模式下，Kafka 中的 committed offset 将被忽略，不会用作起始位置。该方法为继承父类FlinkKafkaConsumerBase的方法。</p></li><li><p>consumer.setStartFromTimestamp(1585047859000L)</p><p>解释：指定具体的偏移量时间戳,毫秒。对于每个分区，其时间戳大于或等于指定时间戳的记录将用作起始位置。 如果一个分区的最新记录早于指定的时间戳，则只从最新记录读取该分区数据。在这种模式下，Kafka 中的已提交 offset 将被忽略，不会用作起始位置。</p></li><li><p>consumer.setStartFromSpecificOffsets(specificStartOffsets)</p></li></ul><p>解释：为每个分区指定偏移量，该方法为继承父类FlinkKafkaConsumerBase的方法。</p><p>请注意：当 Job 从故障中自动恢复或使用 savepoint 手动恢复时，这些起始位置配置方法不会影响消费的起始位置。在恢复时，每个 Kafka 分区的起始位置由存储在 savepoint 或 checkpoint 中的 offset 确定。</p><h2 id="Flink-Kafka-Consumer源码解读"><a href="#Flink-Kafka-Consumer源码解读" class="headerlink" title="Flink Kafka Consumer源码解读"></a>Flink Kafka Consumer源码解读</h2><h3 id="继承关系"><a href="#继承关系" class="headerlink" title="继承关系"></a>继承关系</h3><p>Flink Kafka Consumer继承了FlinkKafkaConsumerBase抽象类，而FlinkKafkaConsumerBase抽象类又继承了RichParallelSourceFunction，所以要实现一个自定义的source时，有两种实现方式：一种是通过实现SourceFunction接口来自定义并行度为1的数据源；另一种是通过实现ParallelSourceFunction接口或者继承RichParallelSourceFunction来自定义具有并行度的数据源。FlinkKafkaConsumer的继承关系如下图所示。</p><p><img src="//jiamaoxiang.top/2020/04/02/你真的了解Flink-Kafka-connector吗？/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3Flink-Kafka-connector%E5%90%97%EF%BC%9F%5C%E7%BB%A7%E6%89%BF%E5%9B%BE.png" alt></p><h3 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h3><h4 id="FlinkKafkaConsumer源码"><a href="#FlinkKafkaConsumer源码" class="headerlink" title="FlinkKafkaConsumer源码"></a>FlinkKafkaConsumer源码</h4><p>先看一下FlinkKafkaConsumer的源码，为了方面阅读，本文将尽量给出本比较完整的源代码片段，具体如下所示：代码较长，在这里可以先有有一个总体的印象，下面会对重要的代码片段详细进行分析。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkKafkaConsumer</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">FlinkKafkaConsumerBase</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 配置轮询超时超时时间，使用flink.poll-timeout参数在properties进行配置</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String KEY_POLL_TIMEOUT = <span class="string">"flink.poll-timeout"</span>;</span><br><span class="line"><span class="comment">// 如果没有可用数据，则等待轮询所需的时间（以毫秒为单位）。 如果为0，则立即返回所有可用的记录</span></span><br><span class="line"><span class="comment">//默认轮询超时时间</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> DEFAULT_POLL_TIMEOUT = <span class="number">100L</span>;</span><br><span class="line"><span class="comment">// 用户提供的kafka 参数配置</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> Properties properties;</span><br><span class="line"><span class="comment">// 如果没有可用数据，则等待轮询所需的时间（以毫秒为单位）。 如果为0，则立即返回所有可用的记录</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">long</span> pollTimeout;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topic                   消费的主题名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> valueDeserializer       反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props                   用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(String topic, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(Collections.singletonList(topic), valueDeserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * 该构造方法允许传入KafkaDeserializationSchema，该反序列化类支持访问kafka消费的额外信息</span></span><br><span class="line"><span class="comment"> * 比如：key/value对，offsets(偏移量)，topic(主题名称)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topic                消费的主题名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer         反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props                用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(String topic, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(Collections.singletonList(topic), deserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * 该构造方法允许传入多个topic(主题)，支持消费多个主题</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topics          消费的主题名称，多个主题为List集合</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer    反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props           用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(List&lt;String&gt; topics, DeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(topics, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(deserializer), props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * 该构造方法允许传入多个topic(主题)，支持消费多个主题,</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topics         消费的主题名称，多个主题为List集合</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer   反序列化类型，用于将kafka的字节消息转换为Flink的对象,支持获取额外信息</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props          用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(List&lt;String&gt; topics, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(topics, <span class="keyword">null</span>, deserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于正则表达式订阅多个topic</span></span><br><span class="line"><span class="comment"> * 如果开启了分区发现，即FlinkKafkaConsumer.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值为非负数</span></span><br><span class="line"><span class="comment"> * 只要是能够正则匹配上，主题一旦被创建就会立即被订阅</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> subscriptionPattern   主题的正则表达式</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> valueDeserializer   反序列化类型，用于将kafka的字节消息转换为Flink的对象,支持获取额外信息</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props               用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(Pattern subscriptionPattern, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(<span class="keyword">null</span>, subscriptionPattern, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(valueDeserializer), props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于正则表达式订阅多个topic</span></span><br><span class="line"><span class="comment"> * 如果开启了分区发现，即FlinkKafkaConsumer.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值为非负数</span></span><br><span class="line"><span class="comment"> * 只要是能够正则匹配上，主题一旦被创建就会立即被订阅</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> subscriptionPattern   主题的正则表达式</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer          该反序列化类支持访问kafka消费的额外信息,比如：key/value对，offsets(偏移量)，topic(主题名称)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props                 用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(Pattern subscriptionPattern, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(<span class="keyword">null</span>, subscriptionPattern, deserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">List&lt;String&gt; topics,</span></span></span><br><span class="line"><span class="function"><span class="params">Pattern subscriptionPattern,</span></span></span><br><span class="line"><span class="function"><span class="params">KafkaDeserializationSchema&lt;T&gt; deserializer,</span></span></span><br><span class="line"><span class="function"><span class="params">Properties props)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 调用父类(FlinkKafkaConsumerBase)构造方法，PropertiesUtil.getLong方法第一个参数为Properties，第二个参数为key，第三个参数为value默认值</span></span><br><span class="line"><span class="keyword">super</span>(</span><br><span class="line">topics,</span><br><span class="line">subscriptionPattern,</span><br><span class="line">deserializer,</span><br><span class="line">getLong(</span><br><span class="line">checkNotNull(props, <span class="string">"props"</span>),</span><br><span class="line">KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS, PARTITION_DISCOVERY_DISABLED),</span><br><span class="line">!getBoolean(props, KEY_DISABLE_METRICS, <span class="keyword">false</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">this</span>.properties = props;</span><br><span class="line">setDeserializer(<span class="keyword">this</span>.properties);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 配置轮询超时时间，如果在properties中配置了KEY_POLL_TIMEOUT参数，则返回具体的配置值，否则返回默认值DEFAULT_POLL_TIMEOUT</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (properties.containsKey(KEY_POLL_TIMEOUT)) &#123;</span><br><span class="line"><span class="keyword">this</span>.pollTimeout = Long.parseLong(properties.getProperty(KEY_POLL_TIMEOUT));</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">this</span>.pollTimeout = DEFAULT_POLL_TIMEOUT;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Cannot parse poll timeout for '"</span> + KEY_POLL_TIMEOUT + <span class="string">'\''</span>, e);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">   <span class="comment">// 父类(FlinkKafkaConsumerBase)方法重写，该方法的作用是返回一个fetcher实例，</span></span><br><span class="line"><span class="comment">// fetcher的作用是连接kafka的broker，拉去数据并进行反序列化，然后将数据输出为数据流(data stream)</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">protected</span> AbstractFetcher&lt;T, ?&gt; createFetcher(</span><br><span class="line">SourceContext&lt;T&gt; sourceContext,</span><br><span class="line">Map&lt;KafkaTopicPartition, Long&gt; assignedPartitionsWithInitialOffsets,</span><br><span class="line">SerializedValue&lt;AssignerWithPeriodicWatermarks&lt;T&gt;&gt; watermarksPeriodic,</span><br><span class="line">SerializedValue&lt;AssignerWithPunctuatedWatermarks&lt;T&gt;&gt; watermarksPunctuated,</span><br><span class="line">StreamingRuntimeContext runtimeContext,</span><br><span class="line">OffsetCommitMode offsetCommitMode,</span><br><span class="line">MetricGroup consumerMetricGroup,</span><br><span class="line"><span class="keyword">boolean</span> useMetrics) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 确保当偏移量的提交模式为ON_CHECKPOINTS(条件1：开启checkpoint，条件2：consumer.setCommitOffsetsOnCheckpoints(true))时，禁用自动提交</span></span><br><span class="line"><span class="comment">// 该方法为父类(FlinkKafkaConsumerBase)的静态方法</span></span><br><span class="line"><span class="comment">// 这将覆盖用户在properties中配置的任何设置</span></span><br><span class="line"><span class="comment">// 当offset的模式为ON_CHECKPOINTS，或者为DISABLED时，会将用户配置的properties属性进行覆盖</span></span><br><span class="line"><span class="comment">// 具体是将ENABLE_AUTO_COMMIT_CONFIG = "enable.auto.commit"的值重置为"false</span></span><br><span class="line">        <span class="comment">// 可以理解为：如果开启了checkpoint，并且设置了consumer.setCommitOffsetsOnCheckpoints(true)，默认为true，</span></span><br><span class="line"><span class="comment">// 就会将kafka properties的enable.auto.commit强制置为false</span></span><br><span class="line">adjustAutoCommitConfig(properties, offsetCommitMode);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> KafkaFetcher&lt;&gt;(</span><br><span class="line">sourceContext,</span><br><span class="line">assignedPartitionsWithInitialOffsets,</span><br><span class="line">watermarksPeriodic,</span><br><span class="line">watermarksPunctuated,</span><br><span class="line">runtimeContext.getProcessingTimeService(),</span><br><span class="line">runtimeContext.getExecutionConfig().getAutoWatermarkInterval(),</span><br><span class="line">runtimeContext.getUserCodeClassLoader(),</span><br><span class="line">runtimeContext.getTaskNameWithSubtasks(),</span><br><span class="line">deserializer,</span><br><span class="line">properties,</span><br><span class="line">pollTimeout,</span><br><span class="line">runtimeContext.getMetricGroup(),</span><br><span class="line">consumerMetricGroup,</span><br><span class="line">useMetrics);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//父类(FlinkKafkaConsumerBase)方法重写</span></span><br><span class="line"><span class="comment">// 返回一个分区发现类，分区发现可以使用kafka broker的高级consumer API发现topic和partition的元数据</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> AbstractPartitionDiscoverer <span class="title">createPartitionDiscoverer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">KafkaTopicsDescriptor topicsDescriptor,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> indexOfThisSubtask,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">int</span> numParallelSubtasks)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> KafkaPartitionDiscoverer(topicsDescriptor, indexOfThisSubtask, numParallelSubtasks, properties);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *判断是否在kafka的参数开启了自动提交，即enable.auto.commit=true，</span></span><br><span class="line"><span class="comment"> * 并且auto.commit.interval.ms&gt;0,</span></span><br><span class="line"><span class="comment"> * 注意：如果没有没有设置enable.auto.commit的参数，则默认为true</span></span><br><span class="line"><span class="comment"> *       如果没有设置auto.commit.interval.ms的参数，则默认为5000毫秒</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">getIsAutoCommitEnabled</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="keyword">return</span> getBoolean(properties, ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">true</span>) &amp;&amp;</span><br><span class="line">PropertiesUtil.getLong(properties, ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="number">5000</span>) &gt; <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 确保配置了kafka消息的key与value的反序列化方式，</span></span><br><span class="line"><span class="comment"> * 如果没有配置，则使用ByteArrayDeserializer序列化器，</span></span><br><span class="line"><span class="comment"> * 该类的deserialize方法是直接将数据进行return，未做任何处理</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">setDeserializer</span><span class="params">(Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">final</span> String deSerName = ByteArrayDeserializer.class.getName();</span><br><span class="line"></span><br><span class="line">Object keyDeSer = props.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">Object valDeSer = props.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (keyDeSer != <span class="keyword">null</span> &amp;&amp; !keyDeSer.equals(deSerName)) &#123;</span><br><span class="line">LOG.warn(<span class="string">"Ignoring configured key DeSerializer (&#123;&#125;)"</span>, ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (valDeSer != <span class="keyword">null</span> &amp;&amp; !valDeSer.equals(deSerName)) &#123;</span><br><span class="line">LOG.warn(<span class="string">"Ignoring configured value DeSerializer (&#123;&#125;)"</span>, ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">&#125;</span><br><span class="line">props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, deSerName);</span><br><span class="line">props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, deSerName);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>上面的代码已经给出了非常详细的注释，下面将对比较关键的部分进行分析。</p><ul><li><p>构造方法分析</p><p><img src="//jiamaoxiang.top/2020/04/02/你真的了解Flink-Kafka-connector吗？/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3Flink-Kafka-connector%E5%90%97%EF%BC%9F%5C%E6%9E%84%E9%80%A0%E6%96%B9%E6%B3%95%E9%87%8D%E5%86%99.png" alt></p></li></ul><p>FlinkKakfaConsumer提供了7种构造方法，如上图所示。不同的构造方法分别具有不同的功能，通过传递的参数也可以大致分析出每种构造方法特有的功能，为了方便理解，本文将对其进行分组讨论，具体如下：</p><p><strong>单topic</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topic                   消费的主题名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> valueDeserializer       反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props                   用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(String topic, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(Collections.singletonList(topic), valueDeserializer, props);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * 该构造方法允许传入KafkaDeserializationSchema，该反序列化类支持访问kafka消费的额外信息</span></span><br><span class="line"><span class="comment"> * 比如：key/value对，offsets(偏移量)，topic(主题名称)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topic                消费的主题名称</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer         反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props                用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(String topic, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(Collections.singletonList(topic), deserializer, props);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面两种构造方法只支持单个topic，区别在于反序列化的方式不一样。第一种使用的是DeserializationSchema，第二种使用的是KafkaDeserializationSchema，其中使用带有KafkaDeserializationSchema参数的构造方法可以获取更多的附属信息，比如在某些场景下需要获取key/value对，offsets(偏移量)，topic(主题名称)等信息，可以选择使用此方式的构造方法。以上两种方法都调用了私有的构造方法，私有构造方法的分析见下面。</p><p><strong>多topic</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * 该构造方法允许传入多个topic(主题)，支持消费多个主题</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topics          消费的主题名称，多个主题为List集合</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer    反序列化类型，用于将kafka的字节消息转换为Flink的对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props           用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(List&lt;String&gt; topics, DeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(topics, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(deserializer), props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个kafka的consumer source</span></span><br><span class="line"><span class="comment"> * 该构造方法允许传入多个topic(主题)，支持消费多个主题,</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> topics         消费的主题名称，多个主题为List集合</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer   反序列化类型，用于将kafka的字节消息转换为Flink的对象,支持获取额外信息</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props          用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(List&lt;String&gt; topics, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(topics, <span class="keyword">null</span>, deserializer, props);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的两种多topic的构造方法，可以使用一个list集合接收多个topic进行消费，区别在于反序列化的方式不一样。第一种使用的是DeserializationSchema，第二种使用的是KafkaDeserializationSchema，其中使用带有KafkaDeserializationSchema参数的构造方法可以获取更多的附属信息，比如在某些场景下需要获取key/value对，offsets(偏移量)，topic(主题名称)等信息，可以选择使用此方式的构造方法。以上两种方法都调用了私有的构造方法，私有构造方法的分析见下面。</p><p><strong>正则匹配topic</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于正则表达式订阅多个topic</span></span><br><span class="line"><span class="comment"> * 如果开启了分区发现，即FlinkKafkaConsumer.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值为非负数</span></span><br><span class="line"><span class="comment"> * 只要是能够正则匹配上，主题一旦被创建就会立即被订阅</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> subscriptionPattern   主题的正则表达式</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> valueDeserializer   反序列化类型，用于将kafka的字节消息转换为Flink的对象,支持获取额外信息</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props               用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(Pattern subscriptionPattern, DeserializationSchema&lt;T&gt; valueDeserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(<span class="keyword">null</span>, subscriptionPattern, <span class="keyword">new</span> KafkaDeserializationSchemaWrapper&lt;&gt;(valueDeserializer), props);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于正则表达式订阅多个topic</span></span><br><span class="line"><span class="comment"> * 如果开启了分区发现，即FlinkKafkaConsumer.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值为非负数</span></span><br><span class="line"><span class="comment"> * 只要是能够正则匹配上，主题一旦被创建就会立即被订阅</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> subscriptionPattern   主题的正则表达式</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deserializer          该反序列化类支持访问kafka消费的额外信息,比如：key/value对，offsets(偏移量)，topic(主题名称)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> props                 用户传入的kafka参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(Pattern subscriptionPattern, KafkaDeserializationSchema&lt;T&gt; deserializer, Properties props)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>(<span class="keyword">null</span>, subscriptionPattern, deserializer, props);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实际的生产环境中可能有这样一些需求，比如有一个flink作业需要将多种不同的数据聚合到一起，而这些数据对应着不同的kafka topic，随着业务增长，新增一类数据，同时新增了一个kafka topic，如何在不重启作业的情况下作业自动感知新的topic。首先需要在构建FlinkKafkaConsumer时的properties中设置flink.partition-discovery.interval-millis参数为非负值，表示开启动态发现的开关，以及设置的时间间隔。此时FLinkKafkaConsumer内部会启动一个单独的线程定期去kafka获取最新的meta信息。具体的调用执行信息，参见下面的私有构造方法</p><p><strong>私有构造方法</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="title">FlinkKafkaConsumer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">List&lt;String&gt; topics,</span></span></span><br><span class="line"><span class="function"><span class="params">Pattern subscriptionPattern,</span></span></span><br><span class="line"><span class="function"><span class="params">KafkaDeserializationSchema&lt;T&gt; deserializer,</span></span></span><br><span class="line"><span class="function"><span class="params">Properties props)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用父类(FlinkKafkaConsumerBase)构造方法，PropertiesUtil.getLong方法第一个参数为Properties，第二个参数为key，第三个参数为value默认值。KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS值是开启分区发现的配置参数，在properties里面配置flink.partition-discovery.interval-millis=5000(大于0的数),如果没有配置则使用PARTITION_DISCOVERY_DISABLED=Long.MIN_VALUE(表示禁用分区发现)</span></span><br><span class="line"><span class="keyword">super</span>(</span><br><span class="line">topics,</span><br><span class="line">subscriptionPattern,</span><br><span class="line">deserializer,</span><br><span class="line">getLong(</span><br><span class="line">checkNotNull(props, <span class="string">"props"</span>),</span><br><span class="line">KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS, PARTITION_DISCOVERY_DISABLED),</span><br><span class="line">!getBoolean(props, KEY_DISABLE_METRICS, <span class="keyword">false</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">this</span>.properties = props;</span><br><span class="line">setDeserializer(<span class="keyword">this</span>.properties);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 配置轮询超时时间，如果在properties中配置了KEY_POLL_TIMEOUT参数，则返回具体的配置值，否则返回默认值DEFAULT_POLL_TIMEOUT</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (properties.containsKey(KEY_POLL_TIMEOUT)) &#123;</span><br><span class="line"><span class="keyword">this</span>.pollTimeout = Long.parseLong(properties.getProperty(KEY_POLL_TIMEOUT));</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">this</span>.pollTimeout = DEFAULT_POLL_TIMEOUT;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Cannot parse poll timeout for '"</span> + KEY_POLL_TIMEOUT + <span class="string">'\''</span>, e);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>其他方法分析</li></ul><p><strong>KafkaFetcher对象创建</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="comment">// 父类(FlinkKafkaConsumerBase)方法重写，该方法的作用是返回一个fetcher实例，</span></span><br><span class="line"><span class="comment">// fetcher的作用是连接kafka的broker，拉去数据并进行反序列化，然后将数据输出为数据流(data stream)</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">protected</span> AbstractFetcher&lt;T, ?&gt; createFetcher(</span><br><span class="line">SourceContext&lt;T&gt; sourceContext,</span><br><span class="line">Map&lt;KafkaTopicPartition, Long&gt; assignedPartitionsWithInitialOffsets,</span><br><span class="line">SerializedValue&lt;AssignerWithPeriodicWatermarks&lt;T&gt;&gt; watermarksPeriodic,</span><br><span class="line">SerializedValue&lt;AssignerWithPunctuatedWatermarks&lt;T&gt;&gt; watermarksPunctuated,</span><br><span class="line">StreamingRuntimeContext runtimeContext,</span><br><span class="line">OffsetCommitMode offsetCommitMode,</span><br><span class="line">MetricGroup consumerMetricGroup,</span><br><span class="line"><span class="keyword">boolean</span> useMetrics) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">       <span class="comment">// 确保当偏移量的提交模式为ON_CHECKPOINTS(条件1：开启checkpoint，条件2：consumer.setCommitOffsetsOnCheckpoints(true))时，禁用自动提交</span></span><br><span class="line"><span class="comment">// 该方法为父类(FlinkKafkaConsumerBase)的静态方法</span></span><br><span class="line"><span class="comment">// 这将覆盖用户在properties中配置的任何设置</span></span><br><span class="line"><span class="comment">// 当offset的模式为ON_CHECKPOINTS，或者为DISABLED时，会将用户配置的properties属性进行覆盖</span></span><br><span class="line"><span class="comment">// 具体是将ENABLE_AUTO_COMMIT_CONFIG = "enable.auto.commit"的值重置为"false</span></span><br><span class="line">       <span class="comment">// 可以理解为：如果开启了checkpoint，并且设置了consumer.setCommitOffsetsOnCheckpoints(true)，默认为true，</span></span><br><span class="line"><span class="comment">// 就会将kafka properties的enable.auto.commit强制置为false</span></span><br><span class="line">adjustAutoCommitConfig(properties, offsetCommitMode);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> KafkaFetcher&lt;&gt;(</span><br><span class="line">sourceContext,</span><br><span class="line">assignedPartitionsWithInitialOffsets,</span><br><span class="line">watermarksPeriodic,</span><br><span class="line">watermarksPunctuated,</span><br><span class="line">runtimeContext.getProcessingTimeService(),</span><br><span class="line">runtimeContext.getExecutionConfig().getAutoWatermarkInterval(),</span><br><span class="line">runtimeContext.getUserCodeClassLoader(),</span><br><span class="line">runtimeContext.getTaskNameWithSubtasks(),</span><br><span class="line">deserializer,</span><br><span class="line">properties,</span><br><span class="line">pollTimeout,</span><br><span class="line">runtimeContext.getMetricGroup(),</span><br><span class="line">consumerMetricGroup,</span><br><span class="line">useMetrics);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该方法的作用是返回一个fetcher实例，fetcher的作用是连接kafka的broker，拉去数据并进行反序列化，然后将数据输出为数据流(data stream)，在这里对自动偏移量提交模式进行了强制调整，即确保当偏移量的提交模式为ON_CHECKPOINTS(条件1：开启checkpoint，条件2：consumer.setCommitOffsetsOnCheckpoints(true))时，禁用自动提交。这将覆盖用户在properties中配置的任何设置，简单可以理解为：如果开启了checkpoint，并且设置了consumer.setCommitOffsetsOnCheckpoints(true)，默认为true，就会将kafka properties的enable.auto.commit强制置为false。关于offset的提交模式，见下文的偏移量提交模式分析。</p><p><strong>判断是否设置了自动提交</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">getIsAutoCommitEnabled</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="keyword">return</span> getBoolean(properties, ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">true</span>) &amp;&amp;</span><br><span class="line">PropertiesUtil.getLong(properties, ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="number">5000</span>) &gt; <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>判断是否在kafka的参数开启了自动提交，即enable.auto.commit=true，并且auto.commit.interval.ms&gt;0, 注意：如果没有没有设置enable.auto.commit的参数，则默认为true, 如果没有设置auto.commit.interval.ms的参数，则默认为5000毫秒。该方法会在FlinkKafkaConsumerBase的open方法进行初始化的时候调用。</p><p><strong>反序列化</strong></p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">setDeserializer</span><span class="params">(Properties props)</span> </span>&#123;</span><br><span class="line">         <span class="comment">// 默认的反序列化方式 </span></span><br><span class="line"><span class="keyword">final</span> String deSerName = ByteArrayDeserializer.class.getName();</span><br><span class="line">         <span class="comment">//获取用户配置的properties关于key与value的反序列化模式</span></span><br><span class="line">Object keyDeSer = props.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">Object valDeSer = props.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">         <span class="comment">// 如果配置了，则使用用户配置的值</span></span><br><span class="line"><span class="keyword">if</span> (keyDeSer != <span class="keyword">null</span> &amp;&amp; !keyDeSer.equals(deSerName)) &#123;</span><br><span class="line">LOG.warn(<span class="string">"Ignoring configured key DeSerializer (&#123;&#125;)"</span>, ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (valDeSer != <span class="keyword">null</span> &amp;&amp; !valDeSer.equals(deSerName)) &#123;</span><br><span class="line">LOG.warn(<span class="string">"Ignoring configured value DeSerializer (&#123;&#125;)"</span>, ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">&#125;</span><br><span class="line">        <span class="comment">// 没有配置，则使用ByteArrayDeserializer进行反序列化</span></span><br><span class="line">props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, deSerName);</span><br><span class="line">props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, deSerName);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>确保配置了kafka消息的key与value的反序列化方式，如果没有配置，则使用ByteArrayDeserializer序列化器，<br>ByteArrayDeserializer类的deserialize方法是直接将数据进行return，未做任何处理。</p><h4 id="FlinkKafkaConsumerBase源码"><a href="#FlinkKafkaConsumerBase源码" class="headerlink" title="FlinkKafkaConsumerBase源码"></a>FlinkKafkaConsumerBase源码</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkKafkaConsumerBase</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span></span></span><br><span class="line"><span class="class"><span class="title">CheckpointListener</span>,</span></span><br><span class="line"><span class="class"><span class="title">ResultTypeQueryable</span>&lt;<span class="title">T</span>&gt;,</span></span><br><span class="line"><span class="class"><span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAX_NUM_PENDING_CHECKPOINTS = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> PARTITION_DISCOVERY_DISABLED = Long.MIN_VALUE;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String KEY_DISABLE_METRICS = <span class="string">"flink.disable-metrics"</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS = <span class="string">"flink.partition-discovery.interval-millis"</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String OFFSETS_STATE_NAME = <span class="string">"topic-partition-offset-states"</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">boolean</span> enableCommitOnCheckpoints = <span class="keyword">true</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 偏移量的提交模式，仅能通过在FlinkKafkaConsumerBase#open(Configuration)进行配置</span></span><br><span class="line"><span class="comment"> * 该值取决于用户是否开启了checkpoint</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> OffsetCommitMode offsetCommitMode;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 配置从哪个位置开始消费kafka的消息，</span></span><br><span class="line"><span class="comment"> * 默认为StartupMode#GROUP_OFFSETS，即从当前提交的偏移量开始消费</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> StartupMode startupMode = StartupMode.GROUP_OFFSETS;</span><br><span class="line"><span class="keyword">private</span> Map&lt;KafkaTopicPartition, Long&gt; specificStartupOffsets;</span><br><span class="line"><span class="keyword">private</span> Long startupOffsetsTimestamp;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 确保当偏移量的提交模式为ON_CHECKPOINTS时，禁用自动提交，</span></span><br><span class="line"><span class="comment"> * 这将覆盖用户在properties中配置的任何设置。</span></span><br><span class="line"><span class="comment"> * 当offset的模式为ON_CHECKPOINTS，或者为DISABLED时，会将用户配置的properties属性进行覆盖</span></span><br><span class="line"><span class="comment"> * 具体是将ENABLE_AUTO_COMMIT_CONFIG = "enable.auto.commit"的值重置为"false，即禁用自动提交</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> properties       kafka配置的properties，会通过该方法进行覆盖</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> offsetCommitMode    offset提交模式</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">adjustAutoCommitConfig</span><span class="params">(Properties properties, OffsetCommitMode offsetCommitMode)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS || offsetCommitMode == OffsetCommitMode.DISABLED) &#123;</span><br><span class="line">properties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="string">"false"</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 决定是否在开启checkpoint时，在checkpoin之后提交偏移量，</span></span><br><span class="line"><span class="comment"> * 只有用户配置了启用checkpoint，该参数才会其作用</span></span><br><span class="line"><span class="comment"> * 如果没有开启checkpoint，则使用kafka的配置参数：enable.auto.commit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> commitOnCheckpoints</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setCommitOffsetsOnCheckpoints</span><span class="params">(<span class="keyword">boolean</span> commitOnCheckpoints)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.enableCommitOnCheckpoints = commitOnCheckpoints;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 从最早的偏移量开始消费，</span></span><br><span class="line"><span class="comment"> *该模式下，Kafka 中的已经提交的偏移量将被忽略，不会用作起始位置。</span></span><br><span class="line"><span class="comment"> *可以通过consumer1.setStartFromEarliest()进行设置</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromEarliest</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.startupMode = StartupMode.EARLIEST;</span><br><span class="line"><span class="keyword">this</span>.startupOffsetsTimestamp = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">this</span>.specificStartupOffsets = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 从最新的数据开始消费,</span></span><br><span class="line"><span class="comment"> *  该模式下，Kafka 中的 已提交的偏移量将被忽略，不会用作起始位置。</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromLatest</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.startupMode = StartupMode.LATEST;</span><br><span class="line"><span class="keyword">this</span>.startupOffsetsTimestamp = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">this</span>.specificStartupOffsets = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *指定具体的偏移量时间戳,毫秒</span></span><br><span class="line"><span class="comment"> *对于每个分区，其时间戳大于或等于指定时间戳的记录将用作起始位置。</span></span><br><span class="line"><span class="comment"> * 如果一个分区的最新记录早于指定的时间戳，则只从最新记录读取该分区数据。</span></span><br><span class="line"><span class="comment"> * 在这种模式下，Kafka 中的已提交 offset 将被忽略，不会用作起始位置。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromTimestamp</span><span class="params">(<span class="keyword">long</span> startupOffsetsTimestamp)</span> </span>&#123;</span><br><span class="line">checkArgument(startupOffsetsTimestamp &gt;= <span class="number">0</span>, <span class="string">"The provided value for the startup offsets timestamp is invalid."</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">long</span> currentTimestamp = System.currentTimeMillis();</span><br><span class="line">checkArgument(startupOffsetsTimestamp &lt;= currentTimestamp,</span><br><span class="line"><span class="string">"Startup time[%s] must be before current time[%s]."</span>, startupOffsetsTimestamp, currentTimestamp);</span><br><span class="line"></span><br><span class="line"><span class="keyword">this</span>.startupMode = StartupMode.TIMESTAMP;</span><br><span class="line"><span class="keyword">this</span>.startupOffsetsTimestamp = startupOffsetsTimestamp;</span><br><span class="line"><span class="keyword">this</span>.specificStartupOffsets = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 从具体的消费者组最近提交的偏移量开始消费，为默认方式</span></span><br><span class="line"><span class="comment"> * 如果没有发现分区的偏移量，使用auto.offset.reset参数配置的值</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromGroupOffsets</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.startupMode = StartupMode.GROUP_OFFSETS;</span><br><span class="line"><span class="keyword">this</span>.startupOffsetsTimestamp = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">this</span>.specificStartupOffsets = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *为每个分区指定偏移量进行消费</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FlinkKafkaConsumerBase&lt;T&gt; <span class="title">setStartFromSpecificOffsets</span><span class="params">(Map&lt;KafkaTopicPartition, Long&gt; specificStartupOffsets)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.startupMode = StartupMode.SPECIFIC_OFFSETS;</span><br><span class="line"><span class="keyword">this</span>.startupOffsetsTimestamp = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">this</span>.specificStartupOffsets = checkNotNull(specificStartupOffsets);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration configuration)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// determine the offset commit mode</span></span><br><span class="line"><span class="comment">// 决定偏移量的提交模式，</span></span><br><span class="line"><span class="comment">// 第一个参数为是否开启了自动提交，</span></span><br><span class="line"><span class="comment">// 第二个参数为是否开启了CommitOnCheckpoint模式</span></span><br><span class="line"><span class="comment">// 第三个参数为是否开启了checkpoint</span></span><br><span class="line"><span class="keyword">this</span>.offsetCommitMode = OffsetCommitModes.fromConfiguration(</span><br><span class="line">getIsAutoCommitEnabled(),</span><br><span class="line">enableCommitOnCheckpoints,</span><br><span class="line">((StreamingRuntimeContext) getRuntimeContext()).isCheckpointingEnabled());</span><br><span class="line">       </span><br><span class="line">   <span class="comment">// 省略的代码</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 省略的代码</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个fetcher用于连接kafka的broker，拉去数据并进行反序列化，然后将数据输出为数据流(data stream)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> sourceContext   数据输出的上下文</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> subscribedPartitionsToStartOffsets  当前sub task需要处理的topic分区集合，即topic的partition与offset的Map集合</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> watermarksPeriodic    可选,一个序列化的时间戳提取器，生成periodic类型的 watermark</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> watermarksPunctuated  可选,一个序列化的时间戳提取器，生成punctuated类型的 watermark</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> runtimeContext        task的runtime context上下文</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> offsetCommitMode      offset的提交模式,有三种，分别为：DISABLED(禁用偏移量自动提交),ON_CHECKPOINTS(仅仅当checkpoints完成之后，才提交偏移量给kafka)</span></span><br><span class="line"><span class="comment"> * KAFKA_PERIODIC(使用kafka自动提交函数，周期性自动提交偏移量)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> kafkaMetricGroup   Flink的Metric</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> useMetrics         是否使用Metric</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span>                   返回一个fetcher实例</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">abstract</span> AbstractFetcher&lt;T, ?&gt; createFetcher(</span><br><span class="line">SourceContext&lt;T&gt; sourceContext,</span><br><span class="line">Map&lt;KafkaTopicPartition, Long&gt; subscribedPartitionsToStartOffsets,</span><br><span class="line">SerializedValue&lt;AssignerWithPeriodicWatermarks&lt;T&gt;&gt; watermarksPeriodic,</span><br><span class="line">SerializedValue&lt;AssignerWithPunctuatedWatermarks&lt;T&gt;&gt; watermarksPunctuated,</span><br><span class="line">StreamingRuntimeContext runtimeContext,</span><br><span class="line">OffsetCommitMode offsetCommitMode,</span><br><span class="line">MetricGroup kafkaMetricGroup,</span><br><span class="line"><span class="keyword">boolean</span> useMetrics) <span class="keyword">throws</span> Exception;</span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">getIsAutoCommitEnabled</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">// 省略的代码</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述代码是FlinkKafkaConsumerBase的部分代码片段，基本上对其做了详细注释，里面的有些方法是FlinkKafkaConsumer继承的，有些是重写的。之所以在这里给出，可以对照FlinkKafkaConsumer的源码，从而方便理解。</p><h3 id="偏移量提交模式分析"><a href="#偏移量提交模式分析" class="headerlink" title="偏移量提交模式分析"></a>偏移量提交模式分析</h3><p>Flink Kafka Consumer 允许有配置如何将 offset 提交回 Kafka broker（或 0.8 版本的 Zookeeper）的行为。请注意：Flink Kafka Consumer 不依赖于提交的 offset 来实现容错保证。提交的 offset 只是一种方法，用于公开 consumer 的进度以便进行监控。</p><p>配置 offset 提交行为的方法是否相同，取决于是否为 job 启用了 checkpointing。在这里先给出提交模式的具体结论，下面会对两种方式进行具体的分析。基本的结论为：</p><ul><li><p>开启checkpoint</p><ul><li><p>情况1：用户通过调用 consumer 上的 setCommitOffsetsOnCheckpoints(true) 方法来启用 offset 的提交(默认情况下为 true )<br>那么当 checkpointing 完成时，Flink Kafka Consumer 将提交的 offset 存储在 checkpoint 状态中。<br>这确保 Kafka broker 中提交的 offset 与 checkpoint 状态中的 offset 一致。<br>注意，在这个场景中，Properties 中的自动定期 offset 提交设置会被完全忽略。<br>此情况使用的是ON_CHECKPOINTS</p></li><li><p>情况2：用户通过调用 consumer 上的 setCommitOffsetsOnCheckpoints(“false”) 方法来禁用 offset 的提交，则使用DISABLED模式提交offset</p></li></ul></li><li><p>未开启checkpoint<br>Flink Kafka Consumer 依赖于内部使用的 Kafka client 自动定期 offset 提交功能，因此，要禁用或启用 offset 的提交</p></li><li><p>情况1：配置了Kafka properties的参数配置了”enable.auto.commit” = “true”或者 Kafka 0.8 的 auto.commit.enable=true，使用KAFKA_PERIODIC模式提交offset，即自动提交offset</p><ul><li>情况2：没有配置enable.auto.commit参数，使用DISABLED模式提交offset，这意味着kafka不知道当前的消费者组的消费者每次消费的偏移量。</li></ul></li></ul><h4 id="提交模式源码分析"><a href="#提交模式源码分析" class="headerlink" title="提交模式源码分析"></a>提交模式源码分析</h4><ul><li>offset的提交模式</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> OffsetCommitMode &#123;</span><br><span class="line"><span class="comment">// 禁用偏移量自动提交</span></span><br><span class="line">DISABLED,</span><br><span class="line"><span class="comment">// 仅仅当checkpoints完成之后，才提交偏移量给kafka</span></span><br><span class="line">ON_CHECKPOINTS,</span><br><span class="line"><span class="comment">// 使用kafka自动提交函数，周期性自动提交偏移量</span></span><br><span class="line">KAFKA_PERIODIC;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>提交模式的调用</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OffsetCommitModes</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> OffsetCommitMode <span class="title">fromConfiguration</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">boolean</span> enableAutoCommit,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">boolean</span> enableCommitOnCheckpoint,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">boolean</span> enableCheckpointing)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 如果开启了checkinpoint，执行下面判断</span></span><br><span class="line"><span class="keyword">if</span> (enableCheckpointing) &#123;</span><br><span class="line"><span class="comment">// 如果开启了checkpoint，进一步判断是否在checkpoin启用时提交(setCommitOffsetsOnCheckpoints(true))，如果是则使用ON_CHECKPOINTS模式</span></span><br><span class="line"><span class="comment">// 否则使用DISABLED模式</span></span><br><span class="line"><span class="keyword">return</span> (enableCommitOnCheckpoint) ? OffsetCommitMode.ON_CHECKPOINTS : OffsetCommitMode.DISABLED;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// 若Kafka properties的参数配置了"enable.auto.commit" = "true"，则使用KAFKA_PERIODIC模式提交offset</span></span><br><span class="line"><span class="comment">// 否则使用DISABLED模式</span></span><br><span class="line"><span class="keyword">return</span> (enableAutoCommit) ? OffsetCommitMode.KAFKA_PERIODIC : OffsetCommitMode.DISABLED;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要介绍了Flink Kafka Consumer，首先对FlinkKafkaConsumer的不同版本进行了对比，然后给出了一个完整的Demo案例，并对案例的配置参数进行了详细解释，接着分析了FlinkKafkaConsumer的继承关系，并分别对FlinkKafkaConsumer以及其父类FlinkKafkaConsumerBase的源码进行了解读，最后从源码层面分析了Flink Kafka Consumer的偏移量提交模式，并对每一种提交模式进行了梳理。</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink1.10集成Hive快速入门</title>
      <link href="/2020/03/31/Flink1-10%E9%9B%86%E6%88%90Hive%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"/>
      <url>/2020/03/31/Flink1-10%E9%9B%86%E6%88%90Hive%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<p>Hive 是大数据领域最早出现的 SQL 引擎，发展至今有着丰富的功能和广泛的用户基础。之后出现的 SQL 引擎，如 Spark SQL、Impala 等，都在一定程度上提供了与 Hive 集成的功能，从而方便用户使用现有的数据仓库、进行作业迁移等。</p><a id="more"></a><p>Flink从1.9开始支持集成Hive，不过1.9版本为beta版，不推荐在生产环境中使用。在最新版Flink1.10版本，标志着对 Blink的整合宣告完成，随着对 Hive 的生产级别集成，Hive作为数据仓库系统的绝对核心，承担着绝大多数的离线数据ETL计算和数据管理，期待Flink未来对Hive的完美支持。</p><p>而 HiveCatalog 会与一个 Hive Metastore 的实例连接，提供元数据持久化的能力。要使用 Flink 与 Hive 进行交互，用户需要配置一个 HiveCatalog，并通过 HiveCatalog 访问 Hive 中的元数据。</p><h2 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h2><p>要与Hive集成，需要在Flink的lib目录下添加额外的依赖jar包，以使集成在Table API程序或SQL Client中的SQL中起作用。或者，可以将这些依赖项放在文件夹中，并分别使用Table API程序或SQL Client 的<code>-C</code> 或<code>-l</code>选项将它们添加到classpath中。本文使用第一种方式，即将jar包直接复制到$FLINK_HOME/lib目录下。本文使用的Hive版本为2.3.4(对于不同版本的Hive，可以参照官网选择不同的jar包依赖)，总共需要3个jar包，如下：</p><ul><li>flink-connector-hive_2.11-1.10.0.jar</li><li>flink-shaded-hadoop-2-uber-2.7.5-8.0.jar</li><li>hive-exec-2.3.4.jar</li></ul><p>其中hive-exec-2.3.4.jar在hive的lib文件夹下，另外两个需要自行下载，下载地址：<a href="https://repo1.maven.org/maven2/org/apache/flink/flink-connector-hive_2.11/1.10.0/" target="_blank" rel="noopener">flink-connector-hive_2.11-1.10.0.jar</a>，<a href="//https://maven.aliyun.com/mvn/search">flink-shaded-hadoop-2-uber-2.7.5-8.0.jar</a></p><p><img src="//jiamaoxiang.top/2020/03/31/Flink1-10集成Hive快速入门/demo.png" alt></p><p><strong>切莫拔剑四顾心茫然，话不多说，直接上代码。</strong></p><h2 id="构建程序"><a href="#构建程序" class="headerlink" title="构建程序"></a>构建程序</h2><h3 id="添加Maven依赖"><a href="#添加Maven依赖" class="headerlink" title="添加Maven依赖"></a>添加Maven依赖</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Flink Dependency --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-java-bridge_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.10.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Hive Dependency --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="实例代码"><a href="#实例代码" class="headerlink" title="实例代码"></a>实例代码</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.flink.sql.hiveintegration;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.catalog.hive.HiveCatalog;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Created</span> with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@author</span> : jmx</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Date</span>: 2020/3/31</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Time</span>: 13:22</span></span><br><span class="line"><span class="comment"> *  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkHiveIntegration</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">                .newInstance()</span><br><span class="line">                .useBlinkPlanner() <span class="comment">// 使用BlinkPlanner</span></span><br><span class="line">                .inBatchMode() <span class="comment">// Batch模式，默认为StreamingMode</span></span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//使用StreamingMode</span></span><br><span class="line">       <span class="comment">/* EnvironmentSettings settings = EnvironmentSettings</span></span><br><span class="line"><span class="comment">                .newInstance()</span></span><br><span class="line"><span class="comment">                .useBlinkPlanner() // 使用BlinkPlanner</span></span><br><span class="line"><span class="comment">                .inStreamingMode() // StreamingMode</span></span><br><span class="line"><span class="comment">                .build();*/</span></span><br><span class="line"></span><br><span class="line">        TableEnvironment tableEnv = TableEnvironment.create(settings);</span><br><span class="line"></span><br><span class="line">        String name = <span class="string">"myhive"</span>;      <span class="comment">// Catalog名称，定义一个唯一的名称表示</span></span><br><span class="line">        String defaultDatabase = <span class="string">"qfbap_ods"</span>;  <span class="comment">// 默认数据库名称</span></span><br><span class="line">        String hiveConfDir = <span class="string">"/opt/modules/apache-hive-2.3.4-bin/conf"</span>;  <span class="comment">// hive-site.xml路径</span></span><br><span class="line">        String version = <span class="string">"2.3.4"</span>;       <span class="comment">// Hive版本号</span></span><br><span class="line"></span><br><span class="line">        HiveCatalog hive = <span class="keyword">new</span> HiveCatalog(name, defaultDatabase, hiveConfDir, version);</span><br><span class="line"></span><br><span class="line">        tableEnv.registerCatalog(<span class="string">"myhive"</span>, hive);</span><br><span class="line">        tableEnv.useCatalog(<span class="string">"myhive"</span>);</span><br><span class="line">        <span class="comment">// 创建数据库，目前不支持创建hive表</span></span><br><span class="line">        String createDbSql = <span class="string">"CREATE DATABASE IF NOT EXISTS myhive.test123"</span>;</span><br><span class="line"></span><br><span class="line">        tableEnv.sqlUpdate(createDbSql);  </span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Flink-SQL-Client集成Hive"><a href="#Flink-SQL-Client集成Hive" class="headerlink" title="Flink SQL Client集成Hive"></a>Flink SQL Client集成Hive</h2><p>Flink的表和SQL API可以处理用SQL语言编写的查询，但是这些查询需要嵌入到用Java或Scala编写的程序中。此外，这些程序在提交到集群之前需要与构建工具打包。这或多或少地限制了Java/Scala程序员对Flink的使用。</p><p>SQL客户端旨在提供一种简单的方式，无需一行Java或Scala代码，即可将表程序编写、调试和提交到Flink集群。Flink SQL客户端CLI允许通过命令行的形式运行分布式程序。使用Flink SQL cli访问Hive，需要配置sql-client-defaults.yaml文件。</p><h3 id="sql-client-defaults-yaml配置"><a href="#sql-client-defaults-yaml配置" class="headerlink" title="sql-client-defaults.yaml配置"></a>sql-client-defaults.yaml配置</h3><p>目前 HiveTableSink 不支持流式写入（未实现 AppendStreamTableSink）。需要将执行模式改成 batch<br>模式，否则会报如下错误：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">org.apache.flink.table.api.TableException: Stream Tables can only be emitted by AppendStreamTableSink, RetractStreamTableSink, or UpsertStreamTableSink.</span><br></pre></td></tr></table></figure><p>需要修改的配置内容如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#...省略的配置项...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line"><span class="comment"># Catalogs</span></span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line"><span class="comment"># 配置catalogs,可以配置多个.</span></span><br><span class="line">catalogs: <span class="comment"># empty list</span></span><br><span class="line">  - name: myhive</span><br><span class="line">    <span class="built_in">type</span>: hive</span><br><span class="line">    hive-conf-dir: /opt/modules/apache-hive-2.3.4-bin/conf</span><br><span class="line">    hive-version: 2.3.4</span><br><span class="line">    default-database: qfbap_ods</span><br><span class="line"></span><br><span class="line"><span class="comment">#...省略的配置项...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line"><span class="comment"># Execution properties</span></span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Properties that change the fundamental execution behavior of a table program.</span></span><br><span class="line"></span><br><span class="line">execution:</span><br><span class="line">  <span class="comment"># select the implementation responsible for planning table programs</span></span><br><span class="line">  <span class="comment"># possible values are 'blink' (used by default) or 'old'</span></span><br><span class="line">  planner: blink</span><br><span class="line">  <span class="comment"># 'batch' or 'streaming' execution</span></span><br><span class="line">  <span class="built_in">type</span>: batch</span><br></pre></td></tr></table></figure><h3 id="启动Flink-SQL-Cli"><a href="#启动Flink-SQL-Cli" class="headerlink" title="启动Flink SQL Cli"></a>启动Flink SQL Cli</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/sql-client.sh  embedded</span><br></pre></td></tr></table></figure><p>启动之后，就可以在此Cli下执行SQL命令访问Hive的表了，基本的操作如下：</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 命令行帮助</span></span><br><span class="line">Flink SQL&gt; help</span><br><span class="line"><span class="comment">-- 查看当前会话的catalog，其中myhive为自己配置的，default_catalog为默认的</span></span><br><span class="line">Flink SQL&gt; show catalogs;</span><br><span class="line">default_catalog</span><br><span class="line">myhive</span><br><span class="line"><span class="comment">-- 使用catalog</span></span><br><span class="line">Flink SQL&gt; use catalog myhive;</span><br><span class="line"><span class="comment">-- 查看当前catalog的数据库</span></span><br><span class="line">Flink SQL&gt; show databases;</span><br><span class="line"><span class="comment">-- 创建数据库</span></span><br><span class="line">Flink SQL&gt; create database testdb;</span><br><span class="line"><span class="comment">-- 删除数据库</span></span><br><span class="line">Flink SQL&gt; drop database testdb;</span><br><span class="line"><span class="comment">-- 创建表</span></span><br><span class="line">Flink SQL&gt; create table tbl(id int,name string);</span><br><span class="line"><span class="comment">-- 删除表</span></span><br><span class="line">Flink SQL&gt; drop table tbl;</span><br><span class="line"><span class="comment">-- 查询表</span></span><br><span class="line">Flink SQL&gt; select * from  code_city;</span><br><span class="line"><span class="comment">-- 插入数据</span></span><br><span class="line">Flink SQL&gt; insert overwrite code_city select id,city,province,event_time from code_city_delta ;</span><br><span class="line">Flink SQL&gt; INSERT into code_city values(1,'南京','江苏','');</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文以最新版本的Flink为例，对Flink集成Hive进行了实操。首先通过代码的方式与Hive进行集成，然后介绍了如何使用Flink SQL 客户端访问Hive，并对其中会遇到的坑进行了描述，最后给出了Flink SQL Cli的详细使用。相信在未来的版本中Flink SQL会越来越完善，期待Flink未来对Hive的完美支持。</p><p>欢迎添加我的公众号，随时随地了解更多精彩内容。</p><p><img src="//jiamaoxiang.top/2020/03/31/Flink1-10集成Hive快速入门/%E4%B8%89%E7%BB%B4%E7%A0%81.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的八种分区策略源码解读</title>
      <link href="/2020/03/30/Flink%E7%9A%84%E5%85%AB%E7%A7%8D%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/"/>
      <url>/2020/03/30/Flink%E7%9A%84%E5%85%AB%E7%A7%8D%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p>Flink包含8中分区策略，这8中分区策略(分区器)分别如下面所示，本文将从源码的角度一一解读每个分区器的实现方式。</p><a id="more"></a><ul><li><strong>GlobalPartitioner</strong></li><li><strong>ShufflePartitioner</strong></li><li><strong>RebalancePartitioner</strong></li><li><strong>RescalePartitioner</strong></li><li><strong>BroadcastPartitioner</strong></li><li><strong>ForwardPartitioner</strong></li><li><strong>KeyGroupStreamPartitioner</strong></li><li><strong>CustomPartitionerWrapper</strong></li></ul><h2 id="继承关系图"><a href="#继承关系图" class="headerlink" title="继承关系图"></a>继承关系图</h2><h3 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h3><h4 id="名称"><a href="#名称" class="headerlink" title="名称"></a>名称</h4><p><strong>ChannelSelector</strong></p><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ChannelSelector</span>&lt;<span class="title">T</span> <span class="keyword">extends</span> <span class="title">IOReadableWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 初始化channels数量，channel可以理解为下游Operator的某个实例(并行算子的某个subtask).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setup</span><span class="params">(<span class="keyword">int</span> numberOfChannels)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *根据当前的record以及Channel总数，</span></span><br><span class="line"><span class="comment"> *决定应将record发送到下游哪个Channel。</span></span><br><span class="line"><span class="comment"> *不同的分区策略会实现不同的该方法。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(T record)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*是否以广播的形式发送到下游所有的算子实例</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">isBroadcast</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="抽象类"><a href="#抽象类" class="headerlink" title="抽象类"></a>抽象类</h3><h4 id="名称-1"><a href="#名称-1" class="headerlink" title="名称"></a>名称</h4><p><strong>StreamPartitioner</strong></p><h4 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span></span></span><br><span class="line"><span class="class"><span class="title">ChannelSelector</span>&lt;<span class="title">SerializationDelegate</span>&lt;<span class="title">StreamRecord</span>&lt;<span class="title">T</span>&gt;&gt;&gt;, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">int</span> numberOfChannels;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(<span class="keyword">int</span> numberOfChannels)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.numberOfChannels = numberOfChannels;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isBroadcast</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="继承关系图-1"><a href="#继承关系图-1" class="headerlink" title="继承关系图"></a>继承关系图</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/Flink%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E7%B1%BB%E7%BB%A7%E6%89%BF%E5%85%B3%E7%B3%BB%E5%9B%BE.png" alt></p><h2 id="GlobalPartitioner"><a href="#GlobalPartitioner" class="headerlink" title="GlobalPartitioner"></a>GlobalPartitioner</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>该分区器会将所有的数据都发送到下游的某个算子实例(subtask id = 0)</p><h3 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 发送所有的数据到下游算子的第一个task(ID = 0)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GlobalPartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="comment">//只返回0，即只发送给下游算子的第一个task</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"GLOBAL"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解"><a href="#图解" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/globa.png" alt></p><h2 id="ShufflePartitioner"><a href="#ShufflePartitioner" class="headerlink" title="ShufflePartitioner"></a>ShufflePartitioner</h2><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p>随机选择一个下游算子实例进行发送</p><h3 id="源码解读-1"><a href="#源码解读-1" class="headerlink" title="源码解读"></a>源码解读</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 随机的选择一个channel进行发送</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ShufflePartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Random random = <span class="keyword">new</span> Random();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="comment">//产生[0,numberOfChannels)伪随机数，随机发送到下游的某个task</span></span><br><span class="line"><span class="keyword">return</span> random.nextInt(numberOfChannels);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> ShufflePartitioner&lt;T&gt;();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"SHUFFLE"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解-1"><a href="#图解-1" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/shuffle.png" alt></p><h2 id="BroadcastPartitioner"><a href="#BroadcastPartitioner" class="headerlink" title="BroadcastPartitioner"></a>BroadcastPartitioner</h2><h3 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h3><p>发送到下游所有的算子实例</p><h3 id="源码解读-2"><a href="#源码解读-2" class="headerlink" title="源码解读"></a>源码解读</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 发送到所有的channel</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BroadcastPartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Broadcast模式是直接发送到下游的所有task，所以不需要通过下面的方法选择发送的通道</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">"Broadcast partitioner does not support select channels."</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isBroadcast</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"BROADCAST"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解-2"><a href="#图解-2" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/broadcast.png" alt></p><h2 id="RebalancePartitioner"><a href="#RebalancePartitioner" class="headerlink" title="RebalancePartitioner"></a>RebalancePartitioner</h2><h3 id="简介-3"><a href="#简介-3" class="headerlink" title="简介"></a>简介</h3><p>通过循环的方式依次发送到下游的task</p><h3 id="源码解读-3"><a href="#源码解读-3" class="headerlink" title="源码解读"></a>源码解读</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *通过循环的方式依次发送到下游的task</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RebalancePartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> nextChannelToSendTo;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(<span class="keyword">int</span> numberOfChannels)</span> </span>&#123;</span><br><span class="line"><span class="keyword">super</span>.setup(numberOfChannels);</span><br><span class="line"><span class="comment">//初始化channel的id，返回[0,numberOfChannels)的伪随机数</span></span><br><span class="line">nextChannelToSendTo = ThreadLocalRandom.current().nextInt(numberOfChannels);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="comment">//循环依次发送到下游的task，比如：nextChannelToSendTo初始值为0，numberOfChannels(下游算子的实例个数，并行度)值为2</span></span><br><span class="line"><span class="comment">//则第一次发送到ID = 1的task，第二次发送到ID = 0的task，第三次发送到ID = 1的task上...依次类推</span></span><br><span class="line">nextChannelToSendTo = (nextChannelToSendTo + <span class="number">1</span>) % numberOfChannels;</span><br><span class="line"><span class="keyword">return</span> nextChannelToSendTo;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"REBALANCE"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解-3"><a href="#图解-3" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/rebalance.png" alt></p><h2 id="RescalePartitioner"><a href="#RescalePartitioner" class="headerlink" title="RescalePartitioner"></a>RescalePartitioner</h2><h3 id="简介-4"><a href="#简介-4" class="headerlink" title="简介"></a>简介</h3><p>基于上下游Operator的并行度，将记录以循环的方式输出到下游Operator的每个实例。<br>  举例: 上游并行度是2，下游是4，则上游一个并行度以循环的方式将记录输出到下游的两个并行度上;上游另一个并行度以循环的方式将记录输出到下游另两个并行度上。<br> 若上游并行度是4，下游并行度是2，则上游两个并行度将记录输出到下游一个并行度上；上游另两个并行度将记录输出到下游另一个并行度上。</p><h3 id="源码解读-4"><a href="#源码解读-4" class="headerlink" title="源码解读"></a>源码解读</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RescalePartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> nextChannelToSendTo = -<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (++nextChannelToSendTo &gt;= numberOfChannels) &#123;</span><br><span class="line">nextChannelToSendTo = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> nextChannelToSendTo;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"RESCALE"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解-4"><a href="#图解-4" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/rescale.png" alt></p><h4 id="尖叫提示"><a href="#尖叫提示" class="headerlink" title="尖叫提示"></a>尖叫提示</h4><p>Flink 中的执行图可以分成四层：StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图。</p><p><strong>StreamGraph</strong>：是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构。</p><p><strong>JobGraph</strong>：StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。</p><p><strong>ExecutionGraph</strong>：JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。</p><p>物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。</p><p> 而StreamingJobGraphGenerator就是StreamGraph转换为JobGraph。在这个类中，把ForwardPartitioner和RescalePartitioner列为POINTWISE分配模式，其他的为ALL_TO_ALL分配模式。代码如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (partitioner <span class="keyword">instanceof</span> ForwardPartitioner || partitioner <span class="keyword">instanceof</span> RescalePartitioner) &#123;</span><br><span class="line">jobEdge = downStreamVertex.connectNewDataSetAsInput(</span><br><span class="line">headVertex,</span><br><span class="line"></span><br><span class="line">   <span class="comment">// 上游算子(生产端)的实例(subtask)连接下游算子(消费端)的一个或者多个实例(subtask)</span></span><br><span class="line">DistributionPattern.POINTWISE,</span><br><span class="line">resultPartitionType);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">jobEdge = downStreamVertex.connectNewDataSetAsInput(</span><br><span class="line">headVertex,</span><br><span class="line"><span class="comment">// 上游算子(生产端)的实例(subtask)连接下游算子(消费端)的所有实例(subtask)</span></span><br><span class="line">DistributionPattern.ALL_TO_ALL,</span><br><span class="line">resultPartitionType);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="ForwardPartitioner"><a href="#ForwardPartitioner" class="headerlink" title="ForwardPartitioner"></a>ForwardPartitioner</h2><h3 id="简介-5"><a href="#简介-5" class="headerlink" title="简介"></a>简介</h3><p>发送到下游对应的第一个task，保证上下游算子并行度一致，即上有算子与下游算子是1:1的关系</p><h3 id="源码解读-5"><a href="#源码解读-5" class="headerlink" title="源码解读"></a>源码解读</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 发送到下游对应的第一个task</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ForwardPartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"FORWARD"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解-5"><a href="#图解-5" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/forward.png" alt></p><h4 id="尖叫提示-1"><a href="#尖叫提示-1" class="headerlink" title="尖叫提示"></a>尖叫提示</h4><p>在上下游的算子没有指定分区器的情况下，如果上下游的算子并行度一致，则使用ForwardPartitioner，否则使用RebalancePartitioner，对于ForwardPartitioner，必须保证上下游算子并行度一致，否则会抛出异常</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//在上下游的算子没有指定分区器的情况下，如果上下游的算子并行度一致，则使用ForwardPartitioner，否则使用RebalancePartitioner</span></span><br><span class="line"><span class="keyword">if</span> (partitioner == <span class="keyword">null</span> &amp;&amp; upstreamNode.getParallelism() == downstreamNode.getParallelism()) &#123;</span><br><span class="line">partitioner = <span class="keyword">new</span> ForwardPartitioner&lt;Object&gt;();</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (partitioner == <span class="keyword">null</span>) &#123;</span><br><span class="line">partitioner = <span class="keyword">new</span> RebalancePartitioner&lt;Object&gt;();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (partitioner <span class="keyword">instanceof</span> ForwardPartitioner) &#123;</span><br><span class="line"><span class="comment">//如果上下游的并行度不一致，会抛出异常</span></span><br><span class="line"><span class="keyword">if</span> (upstreamNode.getParallelism() != downstreamNode.getParallelism()) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">"Forward partitioning does not allow "</span> +</span><br><span class="line"><span class="string">"change of parallelism. Upstream operation: "</span> + upstreamNode + <span class="string">" parallelism: "</span> + upstreamNode.getParallelism() +</span><br><span class="line"><span class="string">", downstream operation: "</span> + downstreamNode + <span class="string">" parallelism: "</span> + downstreamNode.getParallelism() +</span><br><span class="line"><span class="string">" You must use another partitioning strategy, such as broadcast, rebalance, shuffle or global."</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="KeyGroupStreamPartitioner"><a href="#KeyGroupStreamPartitioner" class="headerlink" title="KeyGroupStreamPartitioner"></a>KeyGroupStreamPartitioner</h2><h3 id="简介-6"><a href="#简介-6" class="headerlink" title="简介"></a>简介</h3><p>根据key的分组索引选择发送到相对应的下游subtask</p><h3 id="源码解读-6"><a href="#源码解读-6" class="headerlink" title="源码解读"></a>源码解读</h3><ul><li>org.apache.flink.streaming.runtime.partitioner.KeyGroupStreamPartitioner</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据key的分组索引选择发送到相对应的下游subtask</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;K&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyGroupStreamPartitioner</span>&lt;<span class="title">T</span>, <span class="title">K</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span> <span class="title">ConfigurableStreamPartitioner</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line">K key;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">key = keySelector.getKey(record.getInstance().getValue());</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Could not extract key from "</span> + record.getInstance().getValue(), e);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//调用KeyGroupRangeAssignment类的assignKeyToParallelOperator方法,代码如下所示</span></span><br><span class="line"><span class="keyword">return</span> KeyGroupRangeAssignment.assignKeyToParallelOperator(key, maxParallelism, numberOfChannels);</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>org.apache.flink.runtime.state.KeyGroupRangeAssignment</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyGroupRangeAssignment</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据key分配一个并行算子实例的索引，该索引即为该key要发送的下游算子实例的路由信息，</span></span><br><span class="line"><span class="comment"> * 即该key发送到哪一个task</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">assignKeyToParallelOperator</span><span class="params">(Object key, <span class="keyword">int</span> maxParallelism, <span class="keyword">int</span> parallelism)</span> </span>&#123;</span><br><span class="line">Preconditions.checkNotNull(key, <span class="string">"Assigned key must not be null!"</span>);</span><br><span class="line"><span class="keyword">return</span> computeOperatorIndexForKeyGroup(maxParallelism, parallelism, assignToKeyGroup(key, maxParallelism));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *根据key分配一个分组id(keyGroupId)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">assignToKeyGroup</span><span class="params">(Object key, <span class="keyword">int</span> maxParallelism)</span> </span>&#123;</span><br><span class="line">Preconditions.checkNotNull(key, <span class="string">"Assigned key must not be null!"</span>);</span><br><span class="line"><span class="comment">//获取key的hashcode</span></span><br><span class="line"><span class="keyword">return</span> computeKeyGroupForKeyHash(key.hashCode(), maxParallelism);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据key分配一个分组id(keyGroupId),</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">computeKeyGroupForKeyHash</span><span class="params">(<span class="keyword">int</span> keyHash, <span class="keyword">int</span> maxParallelism)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//与maxParallelism取余，获取keyGroupId</span></span><br><span class="line"><span class="keyword">return</span> MathUtils.murmurHash(keyHash) % maxParallelism;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//计算分区index，即该key group应该发送到下游的哪一个算子实例</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">computeOperatorIndexForKeyGroup</span><span class="params">(<span class="keyword">int</span> maxParallelism, <span class="keyword">int</span> parallelism, <span class="keyword">int</span> keyGroupId)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> keyGroupId * parallelism / maxParallelism;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="图解-6"><a href="#图解-6" class="headerlink" title="图解"></a>图解</h3><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/key.png" alt></p><h2 id="CustomPartitionerWrapper"><a href="#CustomPartitionerWrapper" class="headerlink" title="CustomPartitionerWrapper"></a>CustomPartitionerWrapper</h2><h3 id="简介-7"><a href="#简介-7" class="headerlink" title="简介"></a>简介</h3><p>通过<code>Partitioner</code>实例的<code>partition</code>方法(自定义的)将记录输出到下游。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomPartitionerWrapper</span>&lt;<span class="title">K</span>, <span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">StreamPartitioner</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">Partitioner&lt;K&gt; partitioner;</span><br><span class="line">KeySelector&lt;T, K&gt; keySelector;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">CustomPartitionerWrapper</span><span class="params">(Partitioner&lt;K&gt; partitioner, KeySelector&lt;T, K&gt; keySelector)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.partitioner = partitioner;</span><br><span class="line"><span class="keyword">this</span>.keySelector = keySelector;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">selectChannel</span><span class="params">(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record)</span> </span>&#123;</span><br><span class="line">K key;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">key = keySelector.getKey(record.getInstance().getValue());</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Could not extract key from "</span> + record.getInstance(), e);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//实现Partitioner接口，重写partition方法</span></span><br><span class="line"><span class="keyword">return</span> partitioner.partition(key, numberOfChannels);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamPartitioner&lt;T&gt; <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"CUSTOM"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>比如：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">      <span class="comment">// key: 根据key的值来分区</span></span><br><span class="line">      <span class="comment">// numPartitions: 下游算子并行度</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String key, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> key.length() % numPartitions;<span class="comment">//在此处定义分区策略</span></span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="//jiamaoxiang.top/2020/03/30/Flink的八种分区策略源码解读/%E6%B1%87%E6%80%BB.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于Canal与Flink实现数据实时增量同步(二)</title>
      <link href="/2020/03/24/%E5%9F%BA%E4%BA%8ECanal%E4%B8%8EFlink%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%A2%9E%E9%87%8F%E5%90%8C%E6%AD%A5-%E4%BA%8C/"/>
      <url>/2020/03/24/%E5%9F%BA%E4%BA%8ECanal%E4%B8%8EFlink%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%A2%9E%E9%87%8F%E5%90%8C%E6%AD%A5-%E4%BA%8C/</url>
      
        <content type="html"><![CDATA[<p>本文主要从Binlog实时采集和离线处理Binlog还原业务数据两个方面，来介绍如何实现DB数据准确、高效地进入Hive数仓。</p><a id="more"></a><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在数据仓库建模中，未经任何加工处理的原始业务层数据，我们称之为ODS(Operational Data Store)数据。在互联网企业中，常见的ODS数据有业务日志数据（Log）和业务DB数据（DB）两类。对于业务DB数据来说，从MySQL等关系型数据库的业务数据进行采集，然后导入到Hive中，是进行数据仓库生产的重要环节。如何准确、高效地把MySQL数据同步到Hive中？一般常用的解决方案是批量取数并Load：直连MySQL去Select表中的数据，然后存到本地文件作为中间存储，最后把文件Load到Hive表中。这种方案的优点是实现简单，但是随着业务的发展，缺点也逐渐暴露出来：</p><ul><li><p>性能瓶颈：随着业务规模的增长，Select From MySQL -&gt; Save to Localfile -&gt; Load to Hive这种数据流花费的时间越来越长，无法满足下游数仓生产的时间要求。</p></li><li><p>直接从MySQL中Select大量数据，对MySQL的影响非常大，容易造成慢查询，影响业务线上的正常服务。</p></li><li><p>由于Hive本身的语法不支持更新、删除等SQL原语(高版本Hive支持，但是需要分桶+ORC存储格式)，对于MySQL中发生Update/Delete的数据无法很好地进行支持。</p></li></ul><p>为了彻底解决这些问题，我们逐步转向CDC (Change Data Capture) + Merge的技术方案，即实时Binlog采集 + 离线处理Binlog还原业务数据这样一套解决方案。Binlog是MySQL的二进制日志，记录了MySQL中发生的所有数据变更，MySQL集群自身的主从同步就是基于Binlog做的。</p><h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><p>首先，采用Flink负责把Kafka上的Binlog数据拉取到HDFS上。</p><p>然后，对每张ODS表，首先需要一次性制作快照（Snapshot），把MySQL里的存量数据读取到Hive上，这一过程底层采用直连MySQL去Select数据的方式，可以使用Sqoop进行一次性全量导入。</p><p>最后，对每张ODS表，每天基于存量数据和当天增量产生的Binlog做Merge，从而还原出业务数据。</p><p>Binlog是流式产生的，通过对Binlog的实时采集，把部分数据处理需求由每天一次的批处理分摊到实时流上。无论从性能上还是对MySQL的访问压力上，都会有明显地改善。Binlog本身记录了数据变更的类型（Insert/Update/Delete），通过一些语义方面的处理，完全能够做到精准的数据还原。</p><h2 id="实现方案"><a href="#实现方案" class="headerlink" title="实现方案"></a>实现方案</h2><h3 id="Flink处理Kafka的binlog日志"><a href="#Flink处理Kafka的binlog日志" class="headerlink" title="Flink处理Kafka的binlog日志"></a>Flink处理Kafka的binlog日志</h3><p>使用kafka source，对读取的数据进行JSON解析，将解析的字段拼接成字符串，符合Hive的schema格式，具体代码如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.etl.kafka2hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONArray;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.parser.Feature;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FilterFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringEncoder;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.core.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.StateBackend;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.filesystem.FsStateBackend;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.CheckpointConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.RollingPolicy;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Created</span> with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@author</span> : jmx</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Date</span>: 2020/3/27</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Time</span>: 12:52</span></span><br><span class="line"><span class="comment"> *  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsSink</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String fieldDelimiter = <span class="string">","</span>;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// checkpoint</span></span><br><span class="line">        env.enableCheckpointing(<span class="number">10_000</span>);</span><br><span class="line">        <span class="comment">//env.setStateBackend((StateBackend) new FsStateBackend("file:///E://checkpoint"));</span></span><br><span class="line">        env.setStateBackend((StateBackend) <span class="keyword">new</span> FsStateBackend(<span class="string">"hdfs://kms-1:8020/checkpoint"</span>));</span><br><span class="line">        CheckpointConfig config = env.getCheckpointConfig();</span><br><span class="line">        config.enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// source</span></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.setProperty(<span class="string">"bootstrap.servers"</span>, <span class="string">"kms-2:9092,kms-3:9092,kms-4:9092"</span>);</span><br><span class="line">        <span class="comment">// only required for Kafka 0.8</span></span><br><span class="line">        props.setProperty(<span class="string">"zookeeper.connect"</span>, <span class="string">"kms-2:2181,kms-3:2181,kms-4:2181"</span>);</span><br><span class="line">        props.setProperty(<span class="string">"group.id"</span>, <span class="string">"test123"</span>);</span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; consumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(</span><br><span class="line">                <span class="string">"qfbap_ods.code_city"</span>, <span class="keyword">new</span> SimpleStringSchema(), props);</span><br><span class="line">        consumer.setStartFromEarliest();</span><br><span class="line">        DataStream&lt;String&gt; stream = env.addSource(consumer);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// transform</span></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; cityDS = stream</span><br><span class="line">                .filter(<span class="keyword">new</span> FilterFunction&lt;String&gt;() &#123;</span><br><span class="line">                    <span class="comment">// 过滤掉DDL操作</span></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(String jsonVal)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        JSONObject record = JSON.parseObject(jsonVal, Feature.OrderedField);</span><br><span class="line">                        <span class="keyword">return</span> record.getString(<span class="string">"isDdl"</span>).equals(<span class="string">"false"</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        StringBuilder fieldsBuilder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">                        <span class="comment">// 解析JSON数据</span></span><br><span class="line">                        JSONObject record = JSON.parseObject(value, Feature.OrderedField);</span><br><span class="line">                        <span class="comment">// 获取最新的字段值</span></span><br><span class="line">                        JSONArray data = record.getJSONArray(<span class="string">"data"</span>);</span><br><span class="line">                        <span class="comment">// 遍历，字段值的JSON数组，只有一个元素</span></span><br><span class="line">                        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; data.size(); i++) &#123;</span><br><span class="line">                            <span class="comment">// 获取到JSON数组的第i个元素</span></span><br><span class="line">                            JSONObject obj = data.getJSONObject(i);</span><br><span class="line">                            <span class="keyword">if</span> (obj != <span class="keyword">null</span>) &#123;</span><br><span class="line">                                fieldsBuilder.append(record.getLong(<span class="string">"id"</span>)); <span class="comment">// 序号id</span></span><br><span class="line">                                fieldsBuilder.append(fieldDelimiter); <span class="comment">// 字段分隔符</span></span><br><span class="line">                                fieldsBuilder.append(record.getLong(<span class="string">"es"</span>)); <span class="comment">//业务时间戳</span></span><br><span class="line">                                fieldsBuilder.append(fieldDelimiter);</span><br><span class="line">                                fieldsBuilder.append(record.getLong(<span class="string">"ts"</span>)); <span class="comment">// 日志时间戳</span></span><br><span class="line">                                fieldsBuilder.append(fieldDelimiter);</span><br><span class="line">                                fieldsBuilder.append(record.getString(<span class="string">"type"</span>)); <span class="comment">// 操作类型</span></span><br><span class="line">                                <span class="keyword">for</span> (Map.Entry&lt;String, Object&gt; entry : obj.entrySet()) &#123;</span><br><span class="line"></span><br><span class="line">                                    fieldsBuilder.append(fieldDelimiter);</span><br><span class="line">                                    fieldsBuilder.append(entry.getValue()); <span class="comment">// 表字段数据</span></span><br><span class="line">                                &#125;</span><br><span class="line"></span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">return</span> fieldsBuilder.toString();</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//cityDS.print();</span></span><br><span class="line">        <span class="comment">//stream.print();</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// sink</span></span><br><span class="line">        <span class="comment">// 以下条件满足其中之一就会滚动生成新的文件</span></span><br><span class="line">        RollingPolicy&lt;String, String&gt; rollingPolicy = DefaultRollingPolicy.create()</span><br><span class="line">                .withRolloverInterval(<span class="number">60L</span> * <span class="number">1000L</span>) <span class="comment">//滚动写入新文件的时间，默认60s。根据具体情况调节</span></span><br><span class="line">                .withMaxPartSize(<span class="number">1024</span> * <span class="number">1024</span> * <span class="number">128L</span>) <span class="comment">//设置每个文件的最大大小 ,默认是128M，这里设置为128M</span></span><br><span class="line">                .withInactivityInterval(<span class="number">60L</span> * <span class="number">1000L</span>) <span class="comment">//默认60秒,未写入数据处于不活跃状态超时会滚动新文件</span></span><br><span class="line">                .build();</span><br><span class="line">        </span><br><span class="line">        StreamingFileSink&lt;String&gt; sink = StreamingFileSink</span><br><span class="line">                <span class="comment">//.forRowFormat(new Path("file:///E://binlog_db/city"), new SimpleStringEncoder&lt;String&gt;())</span></span><br><span class="line">                .forRowFormat(<span class="keyword">new</span> Path(<span class="string">"hdfs://kms-1:8020/binlog_db/code_city_delta"</span>), <span class="keyword">new</span> SimpleStringEncoder&lt;String&gt;())</span><br><span class="line">                .withBucketAssigner(<span class="keyword">new</span> EventTimeBucketAssigner())</span><br><span class="line">                .withRollingPolicy(rollingPolicy)</span><br><span class="line">                .withBucketCheckInterval(<span class="number">1000</span>)  <span class="comment">// 桶检查间隔，这里设置1S</span></span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        cityDS.addSink(sink);</span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于Flink Sink到HDFS，<code>StreamingFileSink</code> 替代了先前的 <code>BucketingSink</code>，用来将上游数据存储到 HDFS 的不同目录中。它的核心逻辑是分桶，默认的分桶方式是 <code>DateTimeBucketAssigner</code>，即按照处理时间分桶。处理时间指的是消息到达 Flink 程序的时间，这点并不符合我们的需求。因此，我们需要自己编写代码将事件时间从消息体中解析出来，按规则生成分桶的名称，具体代码如下：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.etl.kafka2hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.core.io.SimpleVersionedSerializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.BucketAssigner;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.bucketassigners.SimpleVersionedStringSerializer;</span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Created</span> with IntelliJ IDEA.</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@author</span> : jmx</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Date</span>: 2020/3/27</span></span><br><span class="line"><span class="comment"> *  <span class="doctag">@Time</span>: 12:49</span></span><br><span class="line"><span class="comment"> *  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EventTimeBucketAssigner</span> <span class="keyword">implements</span> <span class="title">BucketAssigner</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getBucketId</span><span class="params">(String element, Context context)</span> </span>&#123;</span><br><span class="line">        String partitionValue;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            partitionValue = getPartitionValue(element);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            partitionValue = <span class="string">"00000000"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"dt="</span> + partitionValue;<span class="comment">//分区目录名称</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SimpleVersionedSerializer&lt;String&gt; <span class="title">getSerializer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> SimpleVersionedStringSerializer.INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> String <span class="title">getPartitionValue</span><span class="params">(String element)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 取出最后拼接字符串的es字段值，该值为业务时间</span></span><br><span class="line">        <span class="keyword">long</span> eventTime = Long.parseLong(element.split(<span class="string">","</span>)[<span class="number">1</span>]);</span><br><span class="line">        Date eventDate = <span class="keyword">new</span> Date(eventTime);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> SimpleDateFormat(<span class="string">"yyyyMMdd"</span>).format(eventDate);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="离线还原MySQL数据"><a href="#离线还原MySQL数据" class="headerlink" title="离线还原MySQL数据"></a>离线还原MySQL数据</h3><p>经过上述步骤，即可将Binlog日志记录写入到HDFS的对应的分区中，接下来就需要根据增量的数据和存量的数据还原最新的数据。Hive 表保存在 HDFS 上，该文件系统不支持修改，因此我们需要一些额外工作来写入数据变更。常用的方式包括：JOIN、Hive 事务、或改用 HBase、kudu。</p><p>如昨日的存量数据code_city,今日增量的数据为code_city_delta，可以通过 <code>FULL OUTER JOIN</code>，将存量和增量数据合并成一张最新的数据表，并作为明天的存量数据：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> code_city</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">        <span class="keyword">COALESCE</span>( t2.id, t1.id ) <span class="keyword">AS</span> <span class="keyword">id</span>,</span><br><span class="line">        <span class="keyword">COALESCE</span> ( t2.city, t1.city ) <span class="keyword">AS</span> city,</span><br><span class="line">        <span class="keyword">COALESCE</span> ( t2.province, t1.province ) <span class="keyword">AS</span> province,</span><br><span class="line">        <span class="keyword">COALESCE</span> ( t2.event_time, t1.event_time ) <span class="keyword">AS</span> event_time </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">        code_city t1</span><br><span class="line">        <span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> (</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">        <span class="keyword">id</span>,</span><br><span class="line">        city,</span><br><span class="line">        province,</span><br><span class="line">        event_time </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">        (<span class="comment">-- 取最后一条状态数据</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">        <span class="keyword">id</span>,</span><br><span class="line">        city,</span><br><span class="line">        province,</span><br><span class="line">        dml_type,</span><br><span class="line">        event_time,</span><br><span class="line">        row_number ( ) <span class="keyword">over</span> ( <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">id</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> event_time <span class="keyword">DESC</span> ) <span class="keyword">AS</span> <span class="keyword">rank</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">        code_city_delta </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">        dt = <span class="string">'20200324'</span> <span class="comment">-- 分区数据</span></span><br><span class="line">        ) temp </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">        <span class="keyword">rank</span> = <span class="number">1</span> </span><br><span class="line">        ) t2 <span class="keyword">ON</span> t1.id = t2.id;</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文主要从Binlog流式采集和基于Binlog的ODS数据还原两方面，介绍了通过Flink实现实时的ETL，此外还可以将binlog日志写入kudu、HBase等支持事务操作的NoSQL中，这样就可以省去数据表还原的步骤。本文是《基于Canal与Flink实现数据实时增量同步》的第二篇，关于canal解析Binlog日志写入kafka的实现步骤，参见《基于Canal与Flink实现数据实时增量同步一》。</p><p><strong>refrence：</strong></p><p>[1]<a href="https://tech.meituan.com/2018/12/06/binlog-dw.html" target="_blank" rel="noopener">https://tech.meituan.com/2018/12/06/binlog-dw.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式数据集成框架gobblin快速入门</title>
      <link href="/2020/03/22/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90%E6%A1%86%E6%9E%B6gobblin%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"/>
      <url>/2020/03/22/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90%E6%A1%86%E6%9E%B6gobblin%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/apache/incubator-gobblin" target="_blank" rel="noopener">Apache Gobblin </a>是一个通用的分布式数据集成框架，用于从各种数据源（数据库，REST API，FTP / SFTP服务器，文件管理器等）提取，转换和加载大量数据到Hadoop上。使得大数据集成变得更加简单，例如<strong>流</strong>和<strong>批处理</strong>数据生态系统的数据摄取，复制，组织和生命周期管理。gobblin由LinkedIn开源，现为Apache的孵化项目。</p><hr><h2 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h2><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><p>下载gobblin的发布包，下载地址为：<a href="https://github.com/apache/incubator-gobblin/releases" target="_blank" rel="noopener">https://github.com/apache/incubator-gobblin/releases</a>,本文档下载的版本为<a href="https://github.com/apache/incubator-gobblin/releases/tag/release-0.14.0" target="_blank" rel="noopener">release-0.14.0</a>，包名称为：incubator-gobblin-release-0.14.0.tar.gz</p><h3 id="解压编译"><a href="#解压编译" class="headerlink" title="解压编译"></a>解压编译</h3><ul><li>解压源码tar包，进入解压文件夹</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar -xzvf incubator-gobblin-release-0.14.0.tar.gz -C /opt/module/</span></span><br><span class="line"><span class="comment"># cd /opt/module/incubator-gobblin-release-0.14.0</span></span><br></pre></td></tr></table></figure><ul><li>编译</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./gradlew :gobblin-distribution:buildDistributionTar</span><br></pre></td></tr></table></figure><p>编译过程大概几分钟，编译完成后会生成一个build文件夹，进入该文件夹会看到生成的tar包名称为：apache-gobblin-incubating-bin-0.14.0.tar.gz</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cd /opt/module/incubator-gobblin-release-0.14.0/build/gobblin-distribution/distributions</span></span><br></pre></td></tr></table></figure><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>将上面编译好的tar包解压，解压后的文件名称为gobblin-dist</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar -xzvf gobblin-incubating-bin-0.14.0.tar.gz  -C /opt/module/</span></span><br><span class="line"><span class="comment"># ll</span></span><br><span class="line">drwxr-xr-x 2 root root  4096 3月  22 17:35 bin</span><br><span class="line">drwxr-xr-x 6 root root  4096 3月  22 17:35 conf</span><br><span class="line">drwxr-xr-x 2 root root 16384 3月  22 17:35 lib</span><br></pre></td></tr></table></figure><h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2>]]></content>
      
      
      <categories>
          
          <category> gobblin </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gobblin </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于Canal与Flink实现数据实时增量同步(一)</title>
      <link href="/2020/03/05/%E5%9F%BA%E4%BA%8ECanal%E4%B8%8EFlink%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%A2%9E%E9%87%8F%E5%90%8C%E6%AD%A5-%E4%B8%80/"/>
      <url>/2020/03/05/%E5%9F%BA%E4%BA%8ECanal%E4%B8%8EFlink%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%A2%9E%E9%87%8F%E5%90%8C%E6%AD%A5-%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<p>canal是阿里巴巴旗下的一款开源项目，纯Java开发。基于数据库增量日志解析，提供增量数据订阅&amp;消费，目前主要支持了MySQL（也支持mariaDB）。</p><a id="more"></a><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><h3 id="常见的binlog命令"><a href="#常见的binlog命令" class="headerlink" title="常见的binlog命令"></a>常见的binlog命令</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 是否启用binlog日志</span></span><br><span class="line">show variables like <span class="string">'log_bin'</span>;</span><br><span class="line"><span class="comment"># 查看binlog类型</span></span><br><span class="line">show global variables like <span class="string">'binlog_format'</span>;</span><br><span class="line"><span class="comment"># 查看详细的日志配置信息</span></span><br><span class="line">show global variables like <span class="string">'%log%'</span>;</span><br><span class="line"><span class="comment"># mysql数据存储目录</span></span><br><span class="line">show variables like <span class="string">'%dir%'</span>;</span><br><span class="line"><span class="comment"># 查看binlog的目录</span></span><br><span class="line">show global variables like <span class="string">"%log_bin%"</span>;</span><br><span class="line"><span class="comment"># 查看当前服务器使用的biglog文件及大小</span></span><br><span class="line">show binary logs;</span><br><span class="line"><span class="comment"># 查看最新一个binlog日志文件名称和Position</span></span><br><span class="line">show master status;</span><br></pre></td></tr></table></figure><h3 id="配置MySQL的binlog"><a href="#配置MySQL的binlog" class="headerlink" title="配置MySQL的binlog"></a>配置MySQL的binlog</h3><p>对于自建 MySQL , 需要先开启 Binlog 写入功能，配置 binlog-format 为 ROW 模式，my.cnf 中配置如下</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line"><span class="built_in">log</span>-bin=mysql-bin <span class="comment"># 开启 binlog</span></span><br><span class="line">binlog-format=ROW <span class="comment"># 选择 ROW 模式</span></span><br><span class="line">server_id=1 <span class="comment"># 配置 MySQL replaction 需要定义，不要和 canal 的 slaveId 重复</span></span><br></pre></td></tr></table></figure><h3 id="授权"><a href="#授权" class="headerlink" title="授权"></a>授权</h3><p>授权 canal 链接 MySQL 账号具有作为 MySQL slave 的权限, 如果已有账户可直接 grant</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">USER</span> canal <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'canal'</span>;  </span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span>, <span class="keyword">REPLICATION</span> <span class="keyword">SLAVE</span>, <span class="keyword">REPLICATION</span> <span class="keyword">CLIENT</span> <span class="keyword">ON</span> *.* <span class="keyword">TO</span> <span class="string">'canal'</span>@<span class="string">'%'</span>;</span><br><span class="line"><span class="comment">-- GRANT ALL PRIVILEGES ON *.* TO 'canal'@'%' ;</span></span><br><span class="line"><span class="keyword">FLUSH</span> <span class="keyword">PRIVILEGES</span>;</span><br></pre></td></tr></table></figure><h2 id="部署canal"><a href="#部署canal" class="headerlink" title="部署canal"></a>部署canal</h2><h3 id="安装canal"><a href="#安装canal" class="headerlink" title="安装canal"></a>安装canal</h3><ul><li>下载：<a href="https://github.com/alibaba/canal/releases" target="_blank" rel="noopener">点此下载</a></li><li>解压缩</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[kms@kms-1 softwares]$ tar -xzvf canal.deployer-1.1.4.tar.gz  -C /opt/modules/canal/</span><br></pre></td></tr></table></figure><ul><li>目录结构</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">drwxr-xr-x 2 root root 4096 Mar  5 14:19 bin</span><br><span class="line">drwxr-xr-x 5 root root 4096 Mar  5 13:54 conf</span><br><span class="line">drwxr-xr-x 2 root root 4096 Mar  5 13:04 lib</span><br><span class="line">drwxrwxrwx 4 root root 4096 Mar  5 14:19 logs</span><br></pre></td></tr></table></figure><h3 id="配置修改"><a href="#配置修改" class="headerlink" title="配置修改"></a>配置修改</h3><ul><li>修改conf/example/instance.properties，修改内容如下：</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## mysql serverId</span></span><br><span class="line">canal.instance.mysql.slaveId = 1234</span><br><span class="line"><span class="comment">#position info，需要改成自己的数据库信息</span></span><br><span class="line">canal.instance.master.address = kms-1.apache.com:3306 </span><br><span class="line"><span class="comment">#username/password，需要改成自己的数据库信息</span></span><br><span class="line">canal.instance.dbUsername = canal  </span><br><span class="line">canal.instance.dbPassword = canal</span><br><span class="line"><span class="comment"># mq config，kafka topic名称</span></span><br><span class="line">canal.mq.topic=<span class="built_in">test</span></span><br></pre></td></tr></table></figure><ul><li>修改conf/canal.properties，修改内容如下：</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 配置zookeeper地址</span></span><br><span class="line">canal.zkServers =kms-2:2181,kms-3:2181,kms-4:2181</span><br><span class="line"><span class="comment"># 可选项: tcp(默认), kafka, RocketMQ，</span></span><br><span class="line">canal.serverMode = kafka</span><br><span class="line"><span class="comment"># 配置kafka地址</span></span><br><span class="line">canal.mq.servers = kms-2:9092,kms-3:9092,kms-4:9092</span><br></pre></td></tr></table></figure><h3 id="启动canal"><a href="#启动canal" class="headerlink" title="启动canal"></a>启动canal</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh bin/startup.sh</span><br></pre></td></tr></table></figure><h3 id="关闭canal"><a href="#关闭canal" class="headerlink" title="关闭canal"></a>关闭canal</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh bin/stop.sh</span><br></pre></td></tr></table></figure><h2 id="部署Canal-Admin-可选"><a href="#部署Canal-Admin-可选" class="headerlink" title="部署Canal Admin(可选)"></a>部署Canal Admin(可选)</h2><p>canal-admin设计上是为canal提供整体配置管理、节点运维等面向运维的功能，提供相对友好的WebUI操作界面，方便更多用户快速和安全的操作。</p><h3 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h3><p>canal-admin的限定依赖：</p><ul><li>MySQL，用于存储配置和节点等相关数据</li><li>canal版本，要求&gt;=1.1.4 (需要依赖canal-server提供面向admin的动态运维管理接口)</li></ul><h3 id="安装canal-admin"><a href="#安装canal-admin" class="headerlink" title="安装canal-admin"></a>安装canal-admin</h3><ul><li><p>下载</p><p><a href="https://github.com/alibaba/canal/releases" target="_blank" rel="noopener">点此下载</a></p></li><li><p>解压缩</p></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[kms@kms-1 softwares]$ tar -xzvf canal.admin-1.1.4.tar.gz  -C /opt/modules/canal-admin/</span><br></pre></td></tr></table></figure><ul><li>目录结构</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">drwxrwxr-x 2 kms kms 4096 Mar  6 11:25 bin</span><br><span class="line">drwxrwxr-x 3 kms kms 4096 Mar  6 11:25 conf</span><br><span class="line">drwxrwxr-x 2 kms kms 4096 Mar  6 11:25 lib</span><br><span class="line">drwxrwxr-x 2 kms kms 4096 Sep  2  2019 logs</span><br></pre></td></tr></table></figure><ul><li>配置修改</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi conf/application.yml</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">server:</span><br><span class="line">  port: 8089</span><br><span class="line">spring:</span><br><span class="line">  jackson:</span><br><span class="line">    date-format: yyyy-MM-dd HH:mm:ss</span><br><span class="line">    time-zone: GMT+8</span><br><span class="line"></span><br><span class="line">spring.datasource:</span><br><span class="line">  address: kms-1:3306</span><br><span class="line">  database: canal_manager</span><br><span class="line">  username: canal</span><br><span class="line">  password: canal</span><br><span class="line">  driver-class-name: com.mysql.jdbc.Driver</span><br><span class="line">  url: jdbc:mysql://<span class="variable">$&#123;spring.datasource.address&#125;</span>/<span class="variable">$&#123;spring.datasource.database&#125;</span>?useUnicode=<span class="literal">true</span>&amp;characterEncoding=UTF-8&amp;useSSL=<span class="literal">false</span></span><br><span class="line">  hikari:</span><br><span class="line">    maximum-pool-size: 30</span><br><span class="line">    minimum-idle: 1</span><br><span class="line"></span><br><span class="line">canal:</span><br><span class="line">  adminUser: admin</span><br><span class="line">  adminPasswd: admin</span><br></pre></td></tr></table></figure><ul><li>初始化原数据库</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql -uroot -p</span><br><span class="line"><span class="comment"># 导入初始化SQL</span></span><br><span class="line"><span class="comment">#注：(1)初始化SQL脚本里会默认创建canal_manager的数据库，建议使用root等有超级权限的账号进行初始化 </span></span><br><span class="line"><span class="comment">#    (2)canal_manager.sql默认会在conf目录下</span></span><br><span class="line">&gt; mysql&gt; <span class="built_in">source</span> /opt/modules/canal-admin/conf/canal_manager.sql</span><br></pre></td></tr></table></figure><ul><li>启动canal-admin</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh bin/startup.sh</span><br></pre></td></tr></table></figure><ul><li>访问</li></ul><p>可以通过 <a href="http://kms-1:8089/" target="_blank" rel="noopener">http://kms-1:8089/</a> 访问，默认密码：admin/123456 </p><ul><li>canal-server端配置</li></ul><p>使用canal_local.properties的配置覆盖canal.properties,将下面配置内容配置在canal_local.properties文件里面，就可以了。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># register ip</span></span><br><span class="line">canal.register.ip =</span><br><span class="line"><span class="comment"># canal admin config</span></span><br><span class="line">canal.admin.manager = 127.0.0.1:8089</span><br><span class="line">canal.admin.port = 11110</span><br><span class="line">canal.admin.user = admin</span><br><span class="line">canal.admin.passwd = 4ACFE3202A5FF5CF467898FC58AAB1D615029441</span><br><span class="line"><span class="comment"># admin auto register</span></span><br><span class="line">canal.admin.register.auto = <span class="literal">true</span></span><br><span class="line">canal.admin.register.cluster =</span><br></pre></td></tr></table></figure><ul><li>启动canal-serve</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh bin/startup.sh  <span class="built_in">local</span></span><br></pre></td></tr></table></figure><p>注意：先启canal-server,然后再启动canal-admin，之后登陆canal-admin就可以添加serve和instance了。</p><h2 id="启动kafka控制台消费者测试"><a href="#启动kafka控制台消费者测试" class="headerlink" title="启动kafka控制台消费者测试"></a>启动kafka控制台消费者测试</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server kms-2:9092,kms-3:9092,kms-4:9092  --topic <span class="built_in">test</span> --from-beginning</span><br></pre></td></tr></table></figure><p>此时MySQL数据表若有变化，会将row类型的log写进Kakfa，具体格式为JSON：</p><ul><li>insert操作</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"data"</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"id"</span>:<span class="string">"338"</span>,</span><br><span class="line">            <span class="string">"city"</span>:<span class="string">"成都"</span>,</span><br><span class="line">            <span class="string">"province"</span>:<span class="string">"四川省"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"database"</span>:<span class="string">"qfbap_ods"</span>,</span><br><span class="line">    <span class="string">"es"</span>:1583394964000,</span><br><span class="line">    <span class="string">"id"</span>:2,</span><br><span class="line">    <span class="string">"isDdl"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="string">"mysqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:<span class="string">"int(11)"</span>,</span><br><span class="line">        <span class="string">"city"</span>:<span class="string">"varchar(256)"</span>,</span><br><span class="line">        <span class="string">"province"</span>:<span class="string">"varchar(256)"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"old"</span>:null,</span><br><span class="line">    <span class="string">"pkNames"</span>:[</span><br><span class="line">        <span class="string">"id"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"sql"</span>:<span class="string">""</span>,</span><br><span class="line">    <span class="string">"sqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:4,</span><br><span class="line">        <span class="string">"city"</span>:12,</span><br><span class="line">        <span class="string">"province"</span>:12</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"table"</span>:<span class="string">"code_city"</span>,</span><br><span class="line">    <span class="string">"ts"</span>:1583394964361,</span><br><span class="line">    <span class="string">"type"</span>:<span class="string">"INSERT"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>update操作</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"data"</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"id"</span>:<span class="string">"338"</span>,</span><br><span class="line">            <span class="string">"city"</span>:<span class="string">"绵阳市"</span>,</span><br><span class="line">            <span class="string">"province"</span>:<span class="string">"四川省"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"database"</span>:<span class="string">"qfbap_ods"</span>,</span><br><span class="line">    <span class="string">"es"</span>:1583395177000,</span><br><span class="line">    <span class="string">"id"</span>:3,</span><br><span class="line">    <span class="string">"isDdl"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="string">"mysqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:<span class="string">"int(11)"</span>,</span><br><span class="line">        <span class="string">"city"</span>:<span class="string">"varchar(256)"</span>,</span><br><span class="line">        <span class="string">"province"</span>:<span class="string">"varchar(256)"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"old"</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"city"</span>:<span class="string">"成都"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"pkNames"</span>:[</span><br><span class="line">        <span class="string">"id"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"sql"</span>:<span class="string">""</span>,</span><br><span class="line">    <span class="string">"sqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:4,</span><br><span class="line">        <span class="string">"city"</span>:12,</span><br><span class="line">        <span class="string">"province"</span>:12</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"table"</span>:<span class="string">"code_city"</span>,</span><br><span class="line">    <span class="string">"ts"</span>:1583395177408,</span><br><span class="line">    <span class="string">"type"</span>:<span class="string">"UPDATE"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>delete操作</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"data"</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"id"</span>:<span class="string">"338"</span>,</span><br><span class="line">            <span class="string">"city"</span>:<span class="string">"绵阳市"</span>,</span><br><span class="line">            <span class="string">"province"</span>:<span class="string">"四川省"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"database"</span>:<span class="string">"qfbap_ods"</span>,</span><br><span class="line">    <span class="string">"es"</span>:1583395333000,</span><br><span class="line">    <span class="string">"id"</span>:4,</span><br><span class="line">    <span class="string">"isDdl"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="string">"mysqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:<span class="string">"int(11)"</span>,</span><br><span class="line">        <span class="string">"city"</span>:<span class="string">"varchar(256)"</span>,</span><br><span class="line">        <span class="string">"province"</span>:<span class="string">"varchar(256)"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"old"</span>:null,</span><br><span class="line">    <span class="string">"pkNames"</span>:[</span><br><span class="line">        <span class="string">"id"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"sql"</span>:<span class="string">""</span>,</span><br><span class="line">    <span class="string">"sqlType"</span>:&#123;</span><br><span class="line">        <span class="string">"id"</span>:4,</span><br><span class="line">        <span class="string">"city"</span>:12,</span><br><span class="line">        <span class="string">"province"</span>:12</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"table"</span>:<span class="string">"code_city"</span>,</span><br><span class="line">    <span class="string">"ts"</span>:1583395333208,</span><br><span class="line">    <span class="string">"type"</span>:<span class="string">"DELETE"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="JSON日志格式解释"><a href="#JSON日志格式解释" class="headerlink" title="JSON日志格式解释"></a>JSON日志格式解释</h3><ul><li>data：最新的数据，为JSON数组，如果是插入则表示最新插入的数据，如果是更新，则表示更新后的最新数据，如果是删除，则表示被删除的数据</li><li>database：数据库名称</li><li>es：事件时间，13位的时间戳</li><li>id：事件操作的序列号，1,2,3…</li><li>isDdl：是否是DDL操作</li><li>mysqlType：字段类型</li><li>old：旧数据</li><li>pkNames：主键名称</li><li>sql：SQL语句</li><li>sqlType：是经过canal转换处理的，比如unsigned int会被转化为Long，unsigned long会被转换为BigDecimal</li><li>table：表名</li><li>ts：日志时间</li><li>type：操作类型，比如DELETE，UPDATE，INSERT</li></ul><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>本文首先介绍了MySQL binlog日志的配置以及Canal的搭建，然后描述了通过canal数据传输到Kafka的配置，最后对canal解析之后的JSON数据进行了详细解释。本文是基于Canal与Flink实现数据实时增量同步的第一篇，在下一篇介绍如何使用Flink实现实时增量数据同步。</p><hr>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQL中的相关子查询解析</title>
      <link href="/2019/12/10/SQL%E4%B8%AD%E7%9A%84%E7%9B%B8%E5%85%B3%E5%AD%90%E6%9F%A5%E8%AF%A2%E8%A7%A3%E6%9E%90/"/>
      <url>/2019/12/10/SQL%E4%B8%AD%E7%9A%84%E7%9B%B8%E5%85%B3%E5%AD%90%E6%9F%A5%E8%AF%A2%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>分步骤解析SQL的相关子查询</p><a id="more"></a><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>在SQL语言中，一个SELECT-FROM-WHERE语句称为一个查询块。讲一个查询块嵌套在另一个查询块的WHERE子句或者HAVing短语的条件中的查询称为嵌套查询。其中上层查询块称为外层查询或者父查询，下层查询称为内查询或者子查询。</p><p>根据子查询是否依赖于父查询，可以分为不相关子查询和相关子查询。其中子查询的查询条件不依赖于父查询，称为不相关子查询，如果子查询的查询条件依赖于父查询，则这类子查询称之为相关子查询。</p><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><table><thead><tr><th align="center">学号</th><th align="center">课程号</th><th align="center">成绩</th></tr></thead><tbody><tr><td align="center">201215121</td><td align="center">1</td><td align="center">92</td></tr><tr><td align="center">201215121</td><td align="center">2</td><td align="center">85</td></tr><tr><td align="center">201215121</td><td align="center">3</td><td align="center">88</td></tr><tr><td align="center">201215122</td><td align="center">2</td><td align="center">90</td></tr><tr><td align="center">201215122</td><td align="center">3</td><td align="center">80</td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> SC (Sno <span class="built_in">char</span>(<span class="number">9</span>), Cno <span class="built_in">char</span>(<span class="number">4</span>),Grade <span class="built_in">SMALLINT</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> SC;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> SC <span class="keyword">values</span> (<span class="string">'201215121'</span>,<span class="string">'1'</span>, <span class="number">92</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> SC <span class="keyword">values</span> (<span class="string">'201215121'</span>,<span class="string">'2'</span>, <span class="number">85</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> SC <span class="keyword">values</span> (<span class="string">'201215121'</span>,<span class="string">'3'</span>, <span class="number">88</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> SC <span class="keyword">values</span> (<span class="string">'201215122'</span>,<span class="string">'2'</span>,<span class="number">90</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> SC <span class="keyword">values</span> (<span class="string">'201215122'</span>,<span class="string">'3'</span>,<span class="number">80</span>);</span><br></pre></td></tr></table></figure><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><ul><li>找出每个学生超过他自己选修课程平均分的课程号</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> Sno,Cno </span><br><span class="line"><span class="keyword">FROM</span> SC x </span><br><span class="line"><span class="keyword">WHERE</span> Grade &gt;= ( <span class="keyword">SELECT</span> <span class="keyword">avg</span>( Grade )</span><br><span class="line">                 <span class="keyword">FROM</span> SC y </span><br><span class="line"><span class="keyword">WHERE</span> y.Sno = x.Sno )</span><br></pre></td></tr></table></figure><p>x是SC的别名，又称为元祖变量，可以用来表示SC的一个元祖。内层查询时求一个学生所有选修课程的平均成绩的，至于是哪一个学生的平均成绩要看参数<code>x.Sno</code>的值，而该值是与父查询相关的，因此这类查询称之为相关子查询。</p><p>这个语句的一种可能执行过程采用以下三个步骤。</p><p>1.从外层查询中取出SC的一个元祖x，将元祖x的Sno值(201215121)传送给内层查询。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">AVG</span>(Grade)</span><br><span class="line"><span class="keyword">FROM</span>  SC y</span><br><span class="line"><span class="keyword">WHERE</span> y.Sno=<span class="string">'201215121'</span>;</span><br></pre></td></tr></table></figure><p>2.执行内层查询，得到值88(近似值)，用该值代替内层查询，得到外层查询：</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> Sno,Cno</span><br><span class="line"><span class="keyword">FROM</span>  SC x</span><br><span class="line"><span class="keyword">WHERE</span> Grade &gt; <span class="number">88</span>;</span><br></pre></td></tr></table></figure><p>3.执行这个查询，得到 (201215121,1)</p><p>然后外层查询取出下一个元祖重复上述1至2步骤的处理，直到外层的SC元祖全部处理完毕。</p><hr><ul><li><p><strong>reference</strong></p><p>[1]王珊, 萨师煊. 数据库系统概论(第5版)</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeeCode数据库部分题目汇总</title>
      <link href="/2019/12/09/LeeCode%E6%95%B0%E6%8D%AE%E5%BA%93%E9%83%A8%E5%88%86%E9%A2%98%E7%9B%AE%E6%B1%87%E6%80%BB/"/>
      <url>/2019/12/09/LeeCode%E6%95%B0%E6%8D%AE%E5%BA%93%E9%83%A8%E5%88%86%E9%A2%98%E7%9B%AE%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<p>LeeCode数据库部分SQL题目总结</p><a id="more"></a><h2 id="176-第二高的薪水"><a href="#176-第二高的薪水" class="headerlink" title="176. 第二高的薪水"></a>176. 第二高的薪水</h2><h3 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h3><p>编写一个 SQL 查询，获取 Employee 表中第二高的薪水（Salary）</p><table><thead><tr><th align="center">Id</th><th align="center">Salary</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">100</td></tr><tr><td align="center">2</td><td align="center">200</td></tr><tr><td align="center">3</td><td align="center">300</td></tr></tbody></table><p>例如上述 Employee 表，SQL查询应该返回 200 作为第二高的薪水。如果不存在第二高的薪水，那么查询应返回 null</p><table><thead><tr><th align="center">SecondHighestSalary</th></tr></thead><tbody><tr><td align="center">200</td></tr></tbody></table><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Employee (<span class="keyword">Id</span> <span class="built_in">int</span>, Salary <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Employee;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, Salary) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'100'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, Salary) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'200'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, Salary) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'300'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句"><a href="#SQL语句" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">MAX</span>(Salary) SecondHighestSalary</span><br><span class="line"><span class="keyword">FROM</span> Employee</span><br><span class="line"><span class="keyword">WHERE</span> Salary &lt;</span><br><span class="line">(<span class="keyword">SELECT</span> <span class="keyword">MAX</span>(Salary) <span class="keyword">FROM</span> Employee)</span><br></pre></td></tr></table></figure><h2 id="178-分数排名"><a href="#178-分数排名" class="headerlink" title="178.分数排名"></a>178.分数排名</h2><h3 id="描述-1"><a href="#描述-1" class="headerlink" title="描述"></a>描述</h3><p>编写一个 SQL 查询来实现分数排名。如果两个分数相同，则两个分数排名（Rank）相同。请注意，平分后的下一个名次应该是下一个连续的整数值。换句话说，名次之间不应该有“间隔”。</p><table><thead><tr><th align="center">Id</th><th align="center">Score</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">3.50</td></tr><tr><td align="center">2</td><td align="center">3.65</td></tr><tr><td align="center">3</td><td align="center">4.00</td></tr><tr><td align="center">4</td><td align="center">3.85</td></tr><tr><td align="center">5</td><td align="center">4.00</td></tr><tr><td align="center">6</td><td align="center">3.65</td></tr></tbody></table><p>例如，根据上述给定的 Scores 表，你的查询应该返回（按分数从高到低排列）：</p><table><thead><tr><th align="center">Score</th><th align="center">Rank</th></tr></thead><tbody><tr><td align="center">4.00</td><td align="center">1</td></tr><tr><td align="center">4.00</td><td align="center">1</td></tr><tr><td align="center">3.85</td><td align="center">2</td></tr><tr><td align="center">3.65</td><td align="center">3</td></tr><tr><td align="center">3.65</td><td align="center">3</td></tr><tr><td align="center">3.50</td><td align="center">4</td></tr></tbody></table><h3 id="数据准备-1"><a href="#数据准备-1" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Scores (<span class="keyword">Id</span> <span class="built_in">int</span>, Score <span class="built_in">DECIMAL</span>(<span class="number">3</span>,<span class="number">2</span>));</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Scores;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Scores (<span class="keyword">Id</span>, Score) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'3.5'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Scores (<span class="keyword">Id</span>, Score) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'3.65'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Scores (<span class="keyword">Id</span>, Score) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'4.0'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Scores (<span class="keyword">Id</span>, Score) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'3.85'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Scores (<span class="keyword">Id</span>, Score) <span class="keyword">values</span> (<span class="string">'5'</span>, <span class="string">'4.0'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Scores (<span class="keyword">Id</span>, Score) <span class="keyword">values</span> (<span class="string">'6'</span>, <span class="string">'3.65'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-1"><a href="#SQL语句-1" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">Score,</span><br><span class="line">@<span class="keyword">rank</span> := @<span class="keyword">rank</span> + (@prev &lt;&gt; (@prev := Score)) <span class="keyword">Rank</span></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Scores,</span><br><span class="line">(<span class="keyword">SELECT</span> @<span class="keyword">rank</span> := <span class="number">0</span>, @prev := <span class="number">-1</span>) init</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> Score <span class="keyword">desc</span></span><br></pre></td></tr></table></figure><h2 id="180-连续出现的数字"><a href="#180-连续出现的数字" class="headerlink" title="180. 连续出现的数字"></a>180. 连续出现的数字</h2><h3 id="描述-2"><a href="#描述-2" class="headerlink" title="描述"></a>描述</h3><p>编写一个 SQL 查询，查找所有至少连续出现三次的数字。</p><table><thead><tr><th align="center">Id</th><th align="center">Num</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">1</td></tr><tr><td align="center">2</td><td align="center">1</td></tr><tr><td align="center">3</td><td align="center">1</td></tr><tr><td align="center">4</td><td align="center">2</td></tr><tr><td align="center">5</td><td align="center">1</td></tr><tr><td align="center">6</td><td align="center">2</td></tr><tr><td align="center">7</td><td align="center">2</td></tr></tbody></table><p>例如，给定上面的 Logs 表， 1 是唯一连续出现至少三次的数字。</p><table><thead><tr><th align="center">ConsecutiveNums</th></tr></thead><tbody><tr><td align="center">1</td></tr></tbody></table><h3 id="数据准备-2"><a href="#数据准备-2" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Num</span> <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> <span class="keyword">Logs</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'5'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'6'</span>, <span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Logs</span> (<span class="keyword">Id</span>, <span class="keyword">Num</span>) <span class="keyword">values</span> (<span class="string">'7'</span>, <span class="string">'2'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-2"><a href="#SQL语句-2" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> l1.Num ConsecutiveNums</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">Logs</span> l1,</span><br><span class="line"><span class="keyword">Logs</span> l2,</span><br><span class="line"><span class="keyword">Logs</span> l3</span><br><span class="line"><span class="keyword">WHERE</span> l1.Id=l2.Id<span class="number">-1</span></span><br><span class="line"><span class="keyword">AND</span> l2.Id =l3.Id<span class="number">-1</span></span><br><span class="line"><span class="keyword">AND</span> l1.Num =l2.Num</span><br><span class="line"><span class="keyword">AND</span> l2.Num =l3.Num</span><br></pre></td></tr></table></figure><h2 id="181-超过经理收入的员工"><a href="#181-超过经理收入的员工" class="headerlink" title="181. 超过经理收入的员工"></a>181. 超过经理收入的员工</h2><h3 id="描述-3"><a href="#描述-3" class="headerlink" title="描述"></a>描述</h3><p>Employee 表包含所有员工，他们的经理也属于员工。每个员工都有一个 Id，此外还有一列对应员工的经理的 Id。</p><table><thead><tr><th align="center">Id</th><th align="center">Name</th><th align="center">Salary</th><th align="center">ManagerId</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">Joe</td><td align="center">70000</td><td align="center">3</td></tr><tr><td align="center">2</td><td align="center">Henry</td><td align="center">80000</td><td align="center">4</td></tr><tr><td align="center">3</td><td align="center">Sam</td><td align="center">60000</td><td align="center">null</td></tr><tr><td align="center">4</td><td align="center">Max</td><td align="center">90000</td><td align="center">null</td></tr></tbody></table><p>给定 Employee 表，编写一个 SQL 查询，该查询可以获取收入超过他们经理的员工的姓名。在上面的表格中，Joe 是唯一一个收入超过他的经理的员工</p><table><thead><tr><th align="center">Employee</th></tr></thead><tbody><tr><td align="center">Joe</td></tr></tbody></table><h3 id="数据准备-3"><a href="#数据准备-3" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Employee (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), Salary <span class="built_in">int</span>, ManagerId <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Employee;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, ManagerId) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'Joe'</span>, <span class="string">'70000'</span>, <span class="string">'3'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, ManagerId) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Henry'</span>, <span class="string">'80000'</span>, <span class="string">'4'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, ManagerId) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'Sam'</span>, <span class="string">'60000'</span>, <span class="string">'None'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, ManagerId) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'Max'</span>, <span class="string">'90000'</span>, <span class="string">'None'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-3"><a href="#SQL语句-3" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">     a.NAME <span class="keyword">AS</span> Employee</span><br><span class="line"><span class="keyword">FROM</span> Employee <span class="keyword">AS</span> a <span class="keyword">JOIN</span> Employee <span class="keyword">AS</span> b</span><br><span class="line">     <span class="keyword">ON</span> a.ManagerId = b.Id</span><br><span class="line">     <span class="keyword">AND</span> a.Salary &gt; b.Salary</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="182-查找重复的电子邮箱"><a href="#182-查找重复的电子邮箱" class="headerlink" title="182. 查找重复的电子邮箱"></a>182. 查找重复的电子邮箱</h2><h3 id="描述-4"><a href="#描述-4" class="headerlink" title="描述"></a>描述</h3><p>编写一个 SQL 查询，查找 Person 表中所有重复的电子邮箱。示例：</p><table><thead><tr><th align="center">Id</th><th align="center">Email</th></tr></thead><tbody><tr><td align="center">1</td><td align="center"><a href="mailto:a@b.com" target="_blank" rel="noopener">a@b.com</a></td></tr><tr><td align="center">2</td><td align="center"><a href="mailto:c@d.com" target="_blank" rel="noopener">c@d.com</a></td></tr><tr><td align="center">3</td><td align="center"><a href="mailto:a@b.com" target="_blank" rel="noopener">a@b.com</a></td></tr></tbody></table><p>根据以上输入，你的查询应返回以下结果：</p><table><thead><tr><th align="center">Email</th></tr></thead><tbody><tr><td align="center"><a href="mailto:a@b.com" target="_blank" rel="noopener">a@b.com</a></td></tr></tbody></table><p>说明：所有电子邮箱都是小写字母。</p><h3 id="数据准备-4"><a href="#数据准备-4" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Person (<span class="keyword">Id</span> <span class="built_in">int</span>, Email <span class="built_in">varchar</span>(<span class="number">255</span>));</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Person;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Person (<span class="keyword">Id</span>, Email) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'a@b.com'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Person (<span class="keyword">Id</span>, Email) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'c@d.com'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Person (<span class="keyword">Id</span>, Email) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'a@b.com'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-4"><a href="#SQL语句-4" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 方法1：</span></span><br><span class="line"><span class="keyword">select</span> Email <span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">  <span class="keyword">select</span> Email, <span class="keyword">count</span>(Email) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line">  <span class="keyword">from</span> Person</span><br><span class="line">  <span class="keyword">group</span> <span class="keyword">by</span> Email</span><br><span class="line">) <span class="keyword">as</span> statistic</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">num</span> &gt; <span class="number">1</span></span><br><span class="line">;</span><br><span class="line"><span class="comment">-- 方法2</span></span><br><span class="line"><span class="keyword">select</span> Email</span><br><span class="line"><span class="keyword">from</span> Person</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> Email</span><br><span class="line"><span class="keyword">having</span> <span class="keyword">count</span>(Email) &gt; <span class="number">1</span>;</span><br><span class="line"><span class="comment">-- 方法3</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">     <span class="keyword">distinct</span>(P1.Email) <span class="string">'Email'</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">      Person P1,</span><br><span class="line">      Person P2</span><br><span class="line"><span class="keyword">where</span> P1.Id &lt;&gt; P2.Id <span class="keyword">and</span> P1.Email = P2.Email</span><br></pre></td></tr></table></figure><h2 id="183-从不订购的客户"><a href="#183-从不订购的客户" class="headerlink" title="183. 从不订购的客户"></a>183. 从不订购的客户</h2><h3 id="描述-5"><a href="#描述-5" class="headerlink" title="描述"></a>描述</h3><p>某网站包含两个表，Customers 表和 Orders 表。编写一个 SQL 查询，找出所有从不订购任何东西的客户。</p><p>Customers 表：</p><table><thead><tr><th align="center">Id</th><th align="center">Name</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">Joe</td></tr><tr><td align="center">2</td><td align="center">Henry</td></tr><tr><td align="center">3</td><td align="center">Sam</td></tr><tr><td align="center">4</td><td align="center">Max</td></tr></tbody></table><p>Orders 表：</p><table><thead><tr><th align="center">Id</th><th align="center">CustomerId</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">3</td></tr><tr><td align="center">2</td><td align="center">1</td></tr></tbody></table><p>例如给定上述表格，你的查询应返回：</p><table><thead><tr><th align="center">Customers</th></tr></thead><tbody><tr><td align="center">Henry</td></tr><tr><td align="center">Max</td></tr></tbody></table><h3 id="数据准备-5"><a href="#数据准备-5" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Customers (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Name</span> <span class="built_in">varchar</span>(<span class="number">255</span>));</span><br><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Orders (<span class="keyword">Id</span> <span class="built_in">int</span>, CustomerId <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Customers;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Customers (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'Joe'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Customers (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Henry'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Customers (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'Sam'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Customers (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'Max'</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Orders;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Orders (<span class="keyword">Id</span>, CustomerId) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'3'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Orders (<span class="keyword">Id</span>, CustomerId) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'1'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-5"><a href="#SQL语句-5" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 方法1：</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">a.NAME <span class="string">'Customers'</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Customers a</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> Orders b <span class="keyword">ON</span> a.Id = b.CustomerId </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">b.Id <span class="keyword">IS</span> <span class="literal">NULL</span></span><br><span class="line"><span class="comment">-- 方法2：</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">NAME</span></span><br><span class="line"><span class="string">'Customers'</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Customers </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line"><span class="keyword">Id</span> <span class="keyword">NOT</span> <span class="keyword">IN</span> ( <span class="keyword">SELECT</span> CustomerId <span class="keyword">FROM</span> Orders )</span><br></pre></td></tr></table></figure><h2 id="184-部门工资最高的员工"><a href="#184-部门工资最高的员工" class="headerlink" title="184. 部门工资最高的员工"></a>184. 部门工资最高的员工</h2><h3 id="描述-6"><a href="#描述-6" class="headerlink" title="描述"></a>描述</h3><p>Employee 表包含所有员工信息，每个员工有其对应的 Id, salary 和 department Id。</p><table><thead><tr><th>Id</th><th>Name</th><th>Salary</th><th>DepartmentId</th></tr></thead><tbody><tr><td>1</td><td>Joe</td><td>70000</td><td>1</td></tr><tr><td>2</td><td>Henry</td><td>80000</td><td>2</td></tr><tr><td>3</td><td>Sam</td><td>60000</td><td>2</td></tr><tr><td>4</td><td>Max</td><td>90000</td><td>1</td></tr></tbody></table><p>Department 表包含公司所有部门的信息。</p><table><thead><tr><th>Id</th><th>Name</th></tr></thead><tbody><tr><td>1</td><td>IT</td></tr><tr><td>2</td><td>Sales</td></tr></tbody></table><p>编写一个 SQL 查询，找出每个部门工资最高的员工。例如，根据上述给定的表格，Max 在 IT 部门有最高工资，Henry 在 Sales 部门有最高工资。</p><table><thead><tr><th>Department</th><th>Employee</th><th>Salary</th></tr></thead><tbody><tr><td>IT</td><td>Max</td><td>90000</td></tr><tr><td>Sales</td><td>Henry</td><td>80000</td></tr></tbody></table><h3 id="数据准备-6"><a href="#数据准备-6" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Employee (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), Salary <span class="built_in">int</span>, DepartmentId <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Department (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Name</span> <span class="built_in">varchar</span>(<span class="number">255</span>));</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Employee;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'Joe'</span>, <span class="string">'70000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Henry'</span>, <span class="string">'80000'</span>, <span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'Sam'</span>, <span class="string">'60000'</span>, <span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'Max'</span>, <span class="string">'90000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Department;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Department (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'IT'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Department (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Sales'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-6"><a href="#SQL语句-6" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    Department.name <span class="keyword">AS</span> <span class="string">'Department'</span>,</span><br><span class="line">    Employee.name <span class="keyword">AS</span> <span class="string">'Employee'</span>,</span><br><span class="line">    Salary</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">    Employee</span><br><span class="line">        <span class="keyword">JOIN</span></span><br><span class="line">    Department <span class="keyword">ON</span> Employee.DepartmentId = Department.Id</span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">    (Employee.DepartmentId , Salary) <span class="keyword">IN</span></span><br><span class="line">    (   <span class="keyword">SELECT</span></span><br><span class="line">            DepartmentId, <span class="keyword">MAX</span>(Salary)</span><br><span class="line">        <span class="keyword">FROM</span></span><br><span class="line">            Employee</span><br><span class="line">        <span class="keyword">GROUP</span> <span class="keyword">BY</span> DepartmentId</span><br><span class="line">)</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h2 id="185-部门工资前三高的所有员工"><a href="#185-部门工资前三高的所有员工" class="headerlink" title="185.部门工资前三高的所有员工"></a>185.部门工资前三高的所有员工</h2><h3 id="描述-7"><a href="#描述-7" class="headerlink" title="描述"></a>描述</h3><p>Employee 表包含所有员工信息，每个员工有其对应的工号 Id，姓名 Name，工资 Salary 和部门编号 DepartmentId 。</p><table><thead><tr><th align="center">Id</th><th align="center">Name</th><th align="center">Salary</th><th align="center">DepartmentId</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">Joe</td><td align="center">85000</td><td align="center">1</td></tr><tr><td align="center">2</td><td align="center">Henry</td><td align="center">80000</td><td align="center">2</td></tr><tr><td align="center">3</td><td align="center">Sam</td><td align="center">60000</td><td align="center">2</td></tr><tr><td align="center">4</td><td align="center">Max</td><td align="center">90000</td><td align="center">1</td></tr><tr><td align="center">5</td><td align="center">Janet</td><td align="center">69000</td><td align="center">1</td></tr><tr><td align="center">6</td><td align="center">Randy</td><td align="center">85000</td><td align="center">1</td></tr><tr><td align="center">7</td><td align="center">Will</td><td align="center">70000</td><td align="center">1</td></tr></tbody></table><p>Department 表包含公司所有部门的信息。</p><table><thead><tr><th align="center">Id</th><th align="center">Name</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">IT</td></tr><tr><td align="center">2</td><td align="center">Sales</td></tr></tbody></table><p>编写一个 SQL 查询，找出每个部门获得前三高工资的所有员工。例如，根据上述给定的表，查询结果应返回：</p><table><thead><tr><th align="center">Department</th><th align="center">Employee</th><th align="center">Salary</th></tr></thead><tbody><tr><td align="center">IT</td><td align="center">Max</td><td align="center">90000</td></tr><tr><td align="center">IT</td><td align="center">Randy</td><td align="center">85000</td></tr><tr><td align="center">IT</td><td align="center">Joe</td><td align="center">85000</td></tr><tr><td align="center">IT</td><td align="center">Will</td><td align="center">70000</td></tr><tr><td align="center">Sales</td><td align="center">Henry</td><td align="center">80000</td></tr><tr><td align="center">Sales</td><td align="center">Sam</td><td align="center">60000</td></tr></tbody></table><p>解释：</p><p>IT 部门中，Max 获得了最高的工资，Randy 和 Joe 都拿到了第二高的工资，Will 的工资排第三。销售部门（Sales）只有两名员工，Henry 的工资最高，Sam 的工资排第二。</p><h3 id="数据准备-7"><a href="#数据准备-7" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Employee (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Name</span> <span class="built_in">varchar</span>(<span class="number">255</span>), Salary <span class="built_in">int</span>, DepartmentId <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Department (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="keyword">Name</span> <span class="built_in">varchar</span>(<span class="number">255</span>));</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Employee;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'Joe'</span>, <span class="string">'85000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Henry'</span>, <span class="string">'80000'</span>, <span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'Sam'</span>, <span class="string">'60000'</span>, <span class="string">'2'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'Max'</span>, <span class="string">'90000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'5'</span>, <span class="string">'Janet'</span>, <span class="string">'69000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'6'</span>, <span class="string">'Randy'</span>, <span class="string">'85000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Employee (<span class="keyword">Id</span>, <span class="keyword">Name</span>, Salary, DepartmentId) <span class="keyword">values</span> (<span class="string">'7'</span>, <span class="string">'Will'</span>, <span class="string">'70000'</span>, <span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Department;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Department (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'IT'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Department (<span class="keyword">Id</span>, <span class="keyword">Name</span>) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Sales'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-7"><a href="#SQL语句-7" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">d.Name <span class="keyword">AS</span> <span class="string">'Department'</span>, e1.Name <span class="keyword">AS</span> <span class="string">'Employee'</span>, e1.Salary</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Employee e1</span><br><span class="line"><span class="keyword">JOIN</span></span><br><span class="line">Department d <span class="keyword">ON</span> e1.DepartmentId = d.Id</span><br><span class="line"><span class="keyword">WHERE</span> <span class="comment">-- 相关子查询，父查询传递一个元祖到子查询，遍历子查询的的数据，如果满足不超过3个人的工资大于传过来的工资，则保留该元祖的数据，否则就过滤掉</span></span><br><span class="line"><span class="number">3</span> &gt; (<span class="keyword">SELECT</span></span><br><span class="line"><span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> e2.Salary) <span class="comment">-- 对于重复的工资，计数一次，从而保证相同的工资的排名相同</span></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Employee e2</span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">e2.Salary &gt; e1.Salary</span><br><span class="line"><span class="keyword">AND</span> e1.DepartmentId = e2.DepartmentId</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="196-删除重复的邮箱"><a href="#196-删除重复的邮箱" class="headerlink" title="196.删除重复的邮箱"></a>196.删除重复的邮箱</h2><h3 id="描述-8"><a href="#描述-8" class="headerlink" title="描述"></a>描述</h3><p>编写一个 SQL 查询，来删除 Person 表中所有重复的电子邮箱，重复的邮箱里只保留 Id 最小 的那个。</p><table><thead><tr><th>Id</th><th>Email</th></tr></thead><tbody><tr><td>1</td><td><a href="mailto:john@example.com" target="_blank" rel="noopener">john@example.com</a></td></tr><tr><td>2</td><td><a href="mailto:bob@example.com" target="_blank" rel="noopener">bob@example.com</a></td></tr><tr><td>3</td><td><a href="mailto:john@example.com" target="_blank" rel="noopener">john@example.com</a></td></tr></tbody></table><p>Id 是这个表的主键。<br>例如，在运行你的查询语句之后，上面的 Person 表应返回以下几行:</p><table><thead><tr><th align="center">Id</th><th align="center">Email</th></tr></thead><tbody><tr><td align="center">1</td><td align="center"><a href="mailto:john@example.com" target="_blank" rel="noopener">john@example.com</a></td></tr><tr><td align="center">2</td><td align="center"><a href="mailto:bob@example.com" target="_blank" rel="noopener">bob@example.com</a></td></tr></tbody></table><h3 id="数据准备-8"><a href="#数据准备-8" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Person (<span class="keyword">Id</span> <span class="built_in">int</span>,Email <span class="built_in">varchar</span>(<span class="number">20</span>));</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Person;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Person <span class="keyword">values</span> (<span class="string">'1'</span>,  <span class="string">'john@example.com'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Person <span class="keyword">values</span> (<span class="string">'2'</span>,  <span class="string">'bob@example.com'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Person <span class="keyword">values</span> (<span class="string">'3'</span>,  <span class="string">'john@example.com'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-8"><a href="#SQL语句-8" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DELETE</span> p1.* </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Person p1,</span><br><span class="line">Person p2 </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">p1.Email = p2.Email </span><br><span class="line"><span class="keyword">AND</span> p1.Id &gt; p2.Id</span><br></pre></td></tr></table></figure><h2 id="197-上升的温度"><a href="#197-上升的温度" class="headerlink" title="197.上升的温度"></a>197.上升的温度</h2><h3 id="描述-9"><a href="#描述-9" class="headerlink" title="描述"></a>描述</h3><p>给定一个 Weather 表，编写一个 SQL 查询，来查找与之前（昨天的）日期相比温度更高的所有日期的 Id。</p><table><thead><tr><th align="center">Id(INT)</th><th align="center">RecordDate(DATE)</th><th align="center">Temperature(INT)</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">2015-01-01</td><td align="center">10</td></tr><tr><td align="center">2</td><td align="center">2015-01-02</td><td align="center">25</td></tr><tr><td align="center">3</td><td align="center">2015-01-03</td><td align="center">20</td></tr><tr><td align="center">4</td><td align="center">2015-01-04</td><td align="center">30</td></tr></tbody></table><p>例如，根据上述给定的 Weather 表格，返回如下 Id:</p><table><thead><tr><th align="center">id</th></tr></thead><tbody><tr><td align="center">2</td></tr><tr><td align="center">4</td></tr></tbody></table><h3 id="数据准备-9"><a href="#数据准备-9" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Weather (<span class="keyword">Id</span> <span class="built_in">int</span>, <span class="built_in">Date</span> <span class="built_in">date</span>, Temperature <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Weather;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Weather (<span class="keyword">Id</span>, <span class="built_in">Date</span>, Temperature) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'2015-01-01'</span>, <span class="string">'10'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Weather (<span class="keyword">Id</span>, <span class="built_in">Date</span>, Temperature) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'2015-01-02'</span>, <span class="string">'25'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Weather (<span class="keyword">Id</span>, <span class="built_in">Date</span>, Temperature) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'2015-01-03'</span>, <span class="string">'20'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Weather (<span class="keyword">Id</span>, <span class="built_in">Date</span>, Temperature) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'2015-01-04'</span>, <span class="string">'30'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-9"><a href="#SQL语句-9" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">a.Id </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Weather a</span><br><span class="line"><span class="keyword">JOIN</span> Weather b <span class="keyword">ON</span> <span class="keyword">DATEDIFF</span>(a.RecordDate,b.RecordDate) = <span class="number">1</span></span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">a.Temperature &gt; b.Temperature</span><br></pre></td></tr></table></figure><h2 id="262-行程与用户"><a href="#262-行程与用户" class="headerlink" title="262.行程与用户"></a>262.行程与用户</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>Trips 表中存所有出租车的行程信息。每段行程有唯一键 Id，Client_Id 和 Driver_Id 是 Users 表中 Users_Id 的外键。Status 是枚举类型，枚举成员为 (‘completed’, ‘cancelled_by_driver’, ‘cancelled_by_client’)。</p><table><thead><tr><th align="center">Id</th><th align="center">Client_Id</th><th align="center">Driver_Id</th><th align="center">City_Id</th><th align="center">Status</th><th align="center">Request_at</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">1</td><td align="center">10</td><td align="center">1</td><td align="center">completed</td><td align="center">2013-10-01</td></tr><tr><td align="center">2</td><td align="center">2</td><td align="center">11</td><td align="center">1</td><td align="center">cancelled_by_driver</td><td align="center">2013-10-01</td></tr><tr><td align="center">3</td><td align="center">3</td><td align="center">12</td><td align="center">6</td><td align="center">cancelled</td><td align="center">2013-10-01</td></tr><tr><td align="center">4</td><td align="center">4</td><td align="center">13</td><td align="center">6</td><td align="center">cancelled_by_client</td><td align="center">2013-10-01</td></tr><tr><td align="center">5</td><td align="center">1</td><td align="center">10</td><td align="center">1</td><td align="center">completed</td><td align="center">2013-10-02</td></tr><tr><td align="center">6</td><td align="center">2</td><td align="center">11</td><td align="center">6</td><td align="center">completed</td><td align="center">2013-10-02</td></tr><tr><td align="center">7</td><td align="center">3</td><td align="center">12</td><td align="center">6</td><td align="center">completed</td><td align="center">2013-10-02</td></tr><tr><td align="center">8</td><td align="center">2</td><td align="center">12</td><td align="center">12</td><td align="center">completed</td><td align="center">2013-10-03</td></tr><tr><td align="center">9</td><td align="center">3</td><td align="center">10</td><td align="center">12</td><td align="center">completed</td><td align="center">2013-10-03</td></tr><tr><td align="center">10</td><td align="center">4</td><td align="center">13</td><td align="center">12</td><td align="center">cancelled_by_driver</td><td align="center">2013-10-03</td></tr></tbody></table><p>Users 表存所有用户。每个用户有唯一键 Users_Id。Banned 表示这个用户是否被禁止，Role 则是一个表示（‘client’, ‘driver’, ‘partner’）的枚举类型。</p><table><thead><tr><th align="center">Users_Id</th><th align="center">Banned</th><th align="center">Role</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">No</td><td align="center">client</td></tr><tr><td align="center">2</td><td align="center">Yes</td><td align="center">client</td></tr><tr><td align="center">3</td><td align="center">No</td><td align="center">client</td></tr><tr><td align="center">4</td><td align="center">No</td><td align="center">client</td></tr><tr><td align="center">10</td><td align="center">No</td><td align="center">driver</td></tr><tr><td align="center">11</td><td align="center">No</td><td align="center">driver</td></tr><tr><td align="center">12</td><td align="center">No</td><td align="center">driver</td></tr><tr><td align="center">13</td><td align="center">No</td><td align="center">driver</td></tr></tbody></table><p>写一段 SQL 语句查出 2013年10月1日 至 2013年10月3日 期间非禁止用户的取消率。基于上表，你的 SQL 语句应返回如下结果，取消率（Cancellation Rate）保留两位小数。</p><p>取消率的计算方式如下：(被司机或乘客取消的非禁止用户生成的订单数量) / (非禁止用户生成的订单总数)</p><table><thead><tr><th align="center">Day</th><th align="center">Cancellation Rate</th></tr></thead><tbody><tr><td align="center">2013-10-01</td><td align="center">0.33</td></tr><tr><td align="center">2013-10-02</td><td align="center">0.00</td></tr><tr><td align="center">2013-10-03</td><td align="center">0.50</td></tr></tbody></table><h3 id="数据准备-10"><a href="#数据准备-10" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> Trips (<span class="keyword">Id</span> <span class="built_in">int</span>, Client_Id <span class="built_in">int</span>, Driver_Id <span class="built_in">int</span>,</span><br><span class="line">City_Id <span class="built_in">int</span>, <span class="keyword">Status</span> ENUM(<span class="string">'completed'</span>, <span class="string">'cancelled_by_driver'</span>, <span class="string">'cancelled_by_client'</span>),</span><br><span class="line">Request_at <span class="built_in">varchar</span>(<span class="number">50</span>));</span><br><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> <span class="keyword">Users</span> (Users_Id <span class="built_in">int</span>,</span><br><span class="line">Banned <span class="built_in">varchar</span>(<span class="number">50</span>), <span class="keyword">Role</span> ENUM(<span class="string">'client'</span>, <span class="string">'driver'</span>, <span class="string">'partner'</span>));</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> Trips;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'1'</span>, <span class="string">'10'</span>, <span class="string">'1'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-01'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'2'</span>, <span class="string">'11'</span>, <span class="string">'1'</span>, <span class="string">'cancelled_by_driver'</span>, <span class="string">'2013-10-01'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'3'</span>, <span class="string">'12'</span>, <span class="string">'6'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-01'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'4'</span>, <span class="string">'13'</span>, <span class="string">'6'</span>, <span class="string">'cancelled_by_client'</span>, <span class="string">'2013-10-01'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'5'</span>, <span class="string">'1'</span>, <span class="string">'10'</span>, <span class="string">'1'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-02'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'6'</span>, <span class="string">'2'</span>, <span class="string">'11'</span>, <span class="string">'6'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-02'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'7'</span>, <span class="string">'3'</span>, <span class="string">'12'</span>, <span class="string">'6'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-02'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'8'</span>, <span class="string">'2'</span>, <span class="string">'12'</span>, <span class="string">'12'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-03'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'9'</span>, <span class="string">'3'</span>, <span class="string">'10'</span>, <span class="string">'12'</span>, <span class="string">'completed'</span>, <span class="string">'2013-10-03'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> Trips (<span class="keyword">Id</span>, Client_Id, Driver_Id, City_Id, <span class="keyword">Status</span>,</span><br><span class="line">Request_at) <span class="keyword">values</span> (<span class="string">'10'</span>, <span class="string">'4'</span>, <span class="string">'13'</span>, <span class="string">'12'</span>, <span class="string">'cancelled_by_driver'</span>, <span class="string">'2013-10-03'</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> <span class="keyword">Users</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'1'</span>, <span class="string">'No'</span>, <span class="string">'client'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'2'</span>, <span class="string">'Yes'</span>, <span class="string">'client'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'3'</span>, <span class="string">'No'</span>, <span class="string">'client'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'4'</span>, <span class="string">'No'</span>, <span class="string">'client'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'10'</span>, <span class="string">'No'</span>, <span class="string">'driver'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'11'</span>, <span class="string">'No'</span>, <span class="string">'driver'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'12'</span>, <span class="string">'No'</span>, <span class="string">'driver'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">Users</span> (Users_Id, Banned, <span class="keyword">Role</span>) <span class="keyword">values</span> (<span class="string">'13'</span>, <span class="string">'No'</span>, <span class="string">'driver'</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-10"><a href="#SQL语句-10" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 方法1：</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">temp1.Request_at <span class="keyword">AS</span> <span class="keyword">DAY</span>,</span><br><span class="line"><span class="keyword">IF</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">cast</span>( ( temp2.cancelled_order / temp1.total_order ) <span class="keyword">AS</span> <span class="built_in">DECIMAL</span> ( <span class="number">3</span>, <span class="number">2</span> ) ) <span class="keyword">IS</span> <span class="literal">NULL</span>,</span><br><span class="line"><span class="number">0.00</span>,</span><br><span class="line"><span class="keyword">cast</span>( ( temp2.cancelled_order / temp1.total_order ) <span class="keyword">AS</span> <span class="built_in">DECIMAL</span> ( <span class="number">3</span>, <span class="number">2</span> ) ) </span><br><span class="line">) <span class="keyword">AS</span> <span class="string">'Cancellation Rate'</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">t1.Request_at,</span><br><span class="line"><span class="keyword">count</span>( * ) <span class="keyword">AS</span> total_order </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">( <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Trips <span class="keyword">WHERE</span> Request_at &gt;= <span class="string">'2013-10-01'</span> <span class="keyword">AND</span> Request_at &lt;= <span class="string">'2013-10-03'</span> ) t1</span><br><span class="line"><span class="keyword">JOIN</span> ( <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> <span class="keyword">Users</span> <span class="keyword">WHERE</span> Banned = <span class="string">'NO'</span> ) t2 <span class="keyword">ON</span> t1.Client_Id = t2.Users_Id </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">t1.Request_at </span><br><span class="line">) temp1</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> (</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">t1.Request_at,</span><br><span class="line"><span class="keyword">count</span>( * ) <span class="keyword">AS</span> cancelled_order </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">( <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Trips <span class="keyword">WHERE</span> Request_at &gt;= <span class="string">'2013-10-01'</span> <span class="keyword">AND</span> Request_at &lt;= <span class="string">'2013-10-03'</span> <span class="keyword">AND</span> ( <span class="keyword">STATUS</span> = <span class="string">'cancelled_by_driver'</span> <span class="keyword">OR</span> <span class="keyword">STATUS</span> = <span class="string">'cancelled_by_client'</span> ) ) t1</span><br><span class="line"><span class="keyword">JOIN</span> ( <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> <span class="keyword">Users</span> <span class="keyword">WHERE</span> Banned = <span class="string">'NO'</span> ) t2 <span class="keyword">ON</span> t1.Client_Id = t2.Users_Id </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">t1.Request_at </span><br><span class="line">) temp2 <span class="keyword">ON</span> temp1.Request_at = temp2.Request_at</span><br><span class="line"><span class="comment">-- ---------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 方法2：</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">temp.request_at <span class="keyword">DAY</span>,</span><br><span class="line"><span class="keyword">round</span>( <span class="keyword">sum</span>( <span class="keyword">CASE</span> temp.STATUS <span class="keyword">WHEN</span> <span class="string">'completed'</span> <span class="keyword">THEN</span> <span class="number">0</span> <span class="keyword">ELSE</span> <span class="number">1</span> <span class="keyword">END</span> ) / <span class="keyword">count</span>( temp.STATUS ), <span class="number">2</span> ) <span class="string">'Cancellation Rate'</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">( <span class="keyword">SELECT</span> <span class="keyword">STATUS</span>, request_at <span class="keyword">FROM</span> trips t <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> <span class="keyword">users</span> u <span class="keyword">ON</span> t.client_id = u.users_id <span class="keyword">WHERE</span> u.banned = <span class="string">'no'</span> ) temp </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">request_at <span class="keyword">BETWEEN</span> <span class="string">'2013-10-01'</span> </span><br><span class="line"><span class="keyword">AND</span> <span class="string">'2013-10-03'</span> </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">temp.request_at</span><br></pre></td></tr></table></figure><h2 id="511-游戏玩家分析I"><a href="#511-游戏玩家分析I" class="headerlink" title="511.游戏玩家分析I"></a>511.游戏玩家分析I</h2><h3 id="描述-10"><a href="#描述-10" class="headerlink" title="描述"></a>描述</h3><p>找出每个玩家第一次登录的日期。Activity表如下：</p><table><thead><tr><th align="center">player_id</th><th align="center">device_id</th><th align="center">event_date</th><th align="center">games_played</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">2</td><td align="center">2016<strong>-</strong>03<strong>-</strong>01</td><td align="center">5</td></tr><tr><td align="center">1</td><td align="center">2</td><td align="center">2016-03-02</td><td align="center">6</td></tr><tr><td align="center">2</td><td align="center">3</td><td align="center"><strong>2017</strong>-<strong>06</strong>-25</td><td align="center">1</td></tr><tr><td align="center">3</td><td align="center">1</td><td align="center">2016-03-02</td><td align="center">0</td></tr><tr><td align="center">3</td><td align="center">4</td><td align="center">2018<strong>-</strong>07<strong>-</strong>03</td><td align="center">5</td></tr></tbody></table><p>结果Result表如下：</p><table><thead><tr><th align="center">player_id</th><th align="center">first_login</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">2016-03-01</td></tr><tr><td align="center">2</td><td align="center">2017<strong>-</strong>06<strong>-</strong>25</td></tr><tr><td align="center">3</td><td align="center">2016-03-02</td></tr></tbody></table><h3 id="数据准备-11"><a href="#数据准备-11" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> <span class="keyword">If</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> activity(player_id <span class="built_in">int</span>,device_id <span class="built_in">int</span>,event_date <span class="built_in">date</span>,games_played <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">Truncate</span> <span class="keyword">table</span> activity;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> activity <span class="keyword">values</span> (<span class="number">1</span>,<span class="number">2</span>,<span class="string">'2016-03-01'</span>,<span class="number">5</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> activity <span class="keyword">values</span> (<span class="number">1</span>,<span class="number">2</span>,<span class="string">'2016-03-02'</span>,<span class="number">6</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> activity <span class="keyword">values</span> (<span class="number">2</span>,<span class="number">3</span>,<span class="string">'2017-06-25'</span>,<span class="number">1</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> activity <span class="keyword">values</span> (<span class="number">3</span>,<span class="number">1</span>,<span class="string">'2016-03-02'</span>,<span class="number">0</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> activity <span class="keyword">values</span> (<span class="number">3</span>,<span class="number">4</span>,<span class="string">'2018-07-03'</span>,<span class="number">5</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-11"><a href="#SQL语句-11" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> player_id,<span class="keyword">min</span>(event_date) first_login <span class="keyword">from</span> activity <span class="keyword">group</span> <span class="keyword">by</span> player_id ;</span><br></pre></td></tr></table></figure><h2 id="512-游戏玩家分析II"><a href="#512-游戏玩家分析II" class="headerlink" title="512.  游戏玩家分析II"></a>512.  游戏玩家分析II</h2><h3 id="描述-11"><a href="#描述-11" class="headerlink" title="描述"></a>描述</h3><p>显示每个玩家首次登录的设备号(同时显示玩家ID)。</p><h3 id="数据准备-12"><a href="#数据准备-12" class="headerlink" title="数据准备"></a>数据准备</h3><p>见511题</p><h3 id="SQL语句-12"><a href="#SQL语句-12" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">player_id,</span><br><span class="line">device_id </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">activity </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">( player_id, event_date ) <span class="keyword">IN</span> ( <span class="keyword">SELECT</span> player_id, <span class="keyword">min</span>( event_date ) first_login <span class="keyword">FROM</span> activity <span class="keyword">GROUP</span> <span class="keyword">BY</span> player_id )</span><br></pre></td></tr></table></figure><h2 id="534-游戏玩家分析III"><a href="#534-游戏玩家分析III" class="headerlink" title="534 游戏玩家分析III"></a>534 游戏玩家分析III</h2><h3 id="描述-12"><a href="#描述-12" class="headerlink" title="描述"></a>描述</h3><p>编写一个 SQL 查询，同时显示每组玩家、日期以及玩家到目前为止玩了多少游戏。也就是说，在此日期之前玩家所玩的游戏总数。详细情况请查看示例。</p><p>结果为：</p><table><thead><tr><th align="center">player_id</th><th align="center">event_date</th><th align="center">games_played_so_far</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">2016-03-01</td><td align="center">5</td></tr><tr><td align="center">1</td><td align="center">2016-03-02</td><td align="center">11</td></tr><tr><td align="center">1</td><td align="center">2017-06-25</td><td align="center">1</td></tr><tr><td align="center">3</td><td align="center">2016-03-02</td><td align="center">0</td></tr><tr><td align="center">3</td><td align="center">2018-07-03</td><td align="center">5</td></tr></tbody></table><h3 id="数据准备-13"><a href="#数据准备-13" class="headerlink" title="数据准备"></a>数据准备</h3><p>见511题</p><h3 id="SQL语句-13"><a href="#SQL语句-13" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 方法一</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">B.player_id,</span><br><span class="line">B.event_date,</span><br><span class="line"><span class="keyword">SUM</span>( A.games_played ) <span class="keyword">AS</span> <span class="string">`games_played_so_far`</span> </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">Activity <span class="keyword">AS</span> A</span><br><span class="line"><span class="keyword">JOIN</span> Activity <span class="keyword">AS</span> B <span class="keyword">ON</span> ( A.player_id = B.player_id <span class="keyword">AND</span> A.event_date &lt;= B.event_date ) </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">B.player_id,</span><br><span class="line">B.event_date</span><br><span class="line"><span class="comment">-- 方法二</span></span><br><span class="line"><span class="keyword">SELECT</span> C.player_id,C.event_date,C.games_played_so_far</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">A.player_id,</span><br><span class="line">A.event_date,</span><br><span class="line">@sum_cnt:=</span><br><span class="line"><span class="keyword">if</span>(A.player_id = @pre_id <span class="keyword">AND</span> A.event_date != @pre_date,</span><br><span class="line">@sum_cnt + A.games_played,</span><br><span class="line">A.games_played </span><br><span class="line">)</span><br><span class="line"><span class="keyword">AS</span> <span class="string">`games_played_so_far`</span>,</span><br><span class="line">@pre_id:=A.player_id <span class="keyword">AS</span> <span class="string">`player_ids`</span>,</span><br><span class="line">@pre_date:=A.event_date <span class="keyword">AS</span> <span class="string">`event_dates`</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">activity <span class="keyword">AS</span> A,(<span class="keyword">SELECT</span> @pre_id:=<span class="literal">NULL</span>,@pre_date:=<span class="literal">NULL</span>,@sum_cnt:=<span class="number">0</span>) <span class="keyword">AS</span> B</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">BY</span> A.player_id,A.event_date</span><br><span class="line">) <span class="keyword">AS</span> C</span><br></pre></td></tr></table></figure><h2 id="550-游戏玩家分析IV"><a href="#550-游戏玩家分析IV" class="headerlink" title="550 游戏玩家分析IV"></a>550 游戏玩家分析IV</h2><h3 id="描述-13"><a href="#描述-13" class="headerlink" title="描述"></a>描述</h3><p>列出首次登录后，紧接着第二天又登录的人数占总人数的比例。比如511题中的数据，只有玩家1连续两天登录了，而总玩家有3个，所以连着两天登录的用户比例为：1/3 ~0.33</p><h3 id="数据准备-14"><a href="#数据准备-14" class="headerlink" title="数据准备"></a>数据准备</h3><p>见511题</p><h3 id="SQL语句-14"><a href="#SQL语句-14" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line"><span class="keyword">ROUND</span>(</span><br><span class="line">(</span><br><span class="line"><span class="comment">-- 求第二天连续登陆的用户数</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line"><span class="keyword">count</span>( <span class="keyword">DISTINCT</span> player_id ) <span class="keyword">AS</span> con_cnt </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">a.player_id,</span><br><span class="line"><span class="keyword">DATEDIFF</span>( b.event_date, a.event_date ) <span class="keyword">AS</span> diff </span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">activity a</span><br><span class="line"><span class="keyword">JOIN</span> activity b <span class="keyword">ON</span> ( a.player_id = b.player_id <span class="keyword">AND</span> a.event_date &lt; b.event_date ) </span><br><span class="line">) t1 </span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">diff = <span class="number">1</span> </span><br><span class="line">) / ( <span class="keyword">SELECT</span> <span class="keyword">count</span>( <span class="keyword">DISTINCT</span> player_id ) total_cnt <span class="keyword">FROM</span> activity ),<span class="comment">-- 总用户数</span></span><br><span class="line"><span class="number">2</span> </span><br><span class="line">) fraction</span><br></pre></td></tr></table></figure><h2 id="569-员工薪水中位数"><a href="#569-员工薪水中位数" class="headerlink" title="569 员工薪水中位数"></a>569 员工薪水中位数</h2><h3 id="描述-14"><a href="#描述-14" class="headerlink" title="描述"></a>描述</h3><p>有一张员工表Employees，字段为Id，Name，Salary，其中Id为员工Id，Name为公司名称，Salary为员工工资。如下面数据所示：</p><table><thead><tr><th align="center">Id</th><th align="center">Company</th><th align="center">Salary</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">A</td><td align="center">2341</td></tr><tr><td align="center">2</td><td align="center">A</td><td align="center">341</td></tr><tr><td align="center">3</td><td align="center">A</td><td align="center">15</td></tr><tr><td align="center">4</td><td align="center">A</td><td align="center">15314</td></tr><tr><td align="center">5</td><td align="center">A</td><td align="center">451</td></tr><tr><td align="center">6</td><td align="center">A</td><td align="center">513</td></tr><tr><td align="center">7</td><td align="center">B</td><td align="center">15</td></tr><tr><td align="center">8</td><td align="center">B</td><td align="center">13</td></tr><tr><td align="center">9</td><td align="center">B</td><td align="center">1154</td></tr><tr><td align="center">10</td><td align="center">B</td><td align="center">1345</td></tr><tr><td align="center">11</td><td align="center">B</td><td align="center">1221</td></tr><tr><td align="center">12</td><td align="center">B</td><td align="center">234</td></tr><tr><td align="center">13</td><td align="center">C</td><td align="center">2345</td></tr><tr><td align="center">14</td><td align="center">C</td><td align="center">2645</td></tr><tr><td align="center">15</td><td align="center">C</td><td align="center">2645</td></tr><tr><td align="center">16</td><td align="center">C</td><td align="center">2652</td></tr><tr><td align="center">17</td><td align="center">C</td><td align="center">65</td></tr></tbody></table><p>请编写SQL查询来查找每个公司的薪水中位数。结果如下：</p><table><thead><tr><th align="center">Id</th><th align="center">Company</th><th align="center">Salary</th></tr></thead><tbody><tr><td align="center">5</td><td align="center">A</td><td align="center">451</td></tr><tr><td align="center">6</td><td align="center">A</td><td align="center">513</td></tr><tr><td align="center">12</td><td align="center">B</td><td align="center">234</td></tr><tr><td align="center">9</td><td align="center">B</td><td align="center">1154</td></tr><tr><td align="center">14</td><td align="center">C</td><td align="center">2645</td></tr></tbody></table><h3 id="数据准备-15"><a href="#数据准备-15" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span>  <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> employees;</span><br><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> employees(<span class="keyword">Id</span> <span class="built_in">int</span>,Company <span class="built_in">varchar</span>(<span class="number">2</span>),salary <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'A'</span>,<span class="number">2341</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">'A'</span>,<span class="number">341</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">3</span>,<span class="string">'A'</span>,<span class="number">15</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">4</span>,<span class="string">'A'</span>,<span class="number">15314</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">5</span>,<span class="string">'A'</span>,<span class="number">451</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">6</span>,<span class="string">'A'</span>,<span class="number">513</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">7</span>,<span class="string">'B'</span>,<span class="number">15</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">8</span>,<span class="string">'B'</span>,<span class="number">13</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">9</span>,<span class="string">'B'</span>,<span class="number">1154</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">10</span>,<span class="string">'B'</span>,<span class="number">1345</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">11</span>,<span class="string">'B'</span>,<span class="number">1221</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">12</span>,<span class="string">'B'</span>,<span class="number">234</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">13</span>,<span class="string">'C'</span>,<span class="number">2345</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">14</span>,<span class="string">'C'</span>,<span class="number">2645</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">15</span>,<span class="string">'C'</span>,<span class="number">2645</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">16</span>,<span class="string">'C'</span>,<span class="number">2652</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees <span class="keyword">values</span>(<span class="number">17</span>,<span class="string">'C'</span>,<span class="number">65</span>);</span><br></pre></td></tr></table></figure><h3 id="SQL语句-15"><a href="#SQL语句-15" class="headerlink" title="SQL语句"></a>SQL语句</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">     t1.id,</span><br><span class="line">     t1.company,</span><br><span class="line">     t1.salary</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line"></span><br><span class="line">(</span><br><span class="line"><span class="comment">-- 查询每个公司员工薪水排名</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">     <span class="keyword">id</span>,</span><br><span class="line">     company,</span><br><span class="line">     salary,</span><br><span class="line">     @<span class="keyword">num</span> := <span class="keyword">if</span>( @company =company  ,@<span class="keyword">num</span> + <span class="number">1</span>,<span class="number">1</span>) <span class="keyword">as</span> <span class="keyword">rank</span>,</span><br><span class="line">     @company := company</span><br><span class="line"><span class="keyword">from</span> employees a ,(<span class="keyword">select</span> @<span class="keyword">num</span> := <span class="number">0</span>,@company:=<span class="string">""</span>) b</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> company,salary) t1 </span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">(</span><br><span class="line"><span class="comment">-- 查询每个公司有多少个员工</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">     company,</span><br><span class="line">     <span class="keyword">count</span>(*) <span class="keyword">as</span> cnt</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    employees</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> company</span><br><span class="line"></span><br><span class="line">) t2 <span class="keyword">on</span> t1.company= t2.company <span class="keyword">and</span> t1.rank = (t2.cnt + <span class="number">1</span>) <span class="keyword">div</span> <span class="number">2</span> <span class="comment">-- （员工总数+1）/2 为中位数</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> LeeCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeeCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>电商业务常用指标分析之SQL实现</title>
      <link href="/2019/12/05/%E7%94%B5%E5%95%86%E4%B8%9A%E5%8A%A1%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E5%88%86%E6%9E%90%E4%B9%8BSQL%E5%AE%9E%E7%8E%B0/"/>
      <url>/2019/12/05/%E7%94%B5%E5%95%86%E4%B8%9A%E5%8A%A1%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E5%88%86%E6%9E%90%E4%B9%8BSQL%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<p>当构建好电商业务数仓之后，需要对业务需要的指标进行计算，从而进一步进行报表的展示，那么，电商的业务知识大概涉及哪些？关于电商业务的常用指标计算都有哪些？这些常用的指标该如何通过Hive数仓进行分析？本文将进行一一梳理.</p><a id="more"></a><h2 id="电商业务领域知识梳理"><a href="#电商业务领域知识梳理" class="headerlink" title="电商业务领域知识梳理"></a>电商业务领域知识梳理</h2><ul><li><strong>用户</strong></li></ul><p>用户以设备为判断标准，在移动统计中，每个独立设备认为是一个独立用户。Android系统根据IMEI号，IOS系统根据OpenUDID来标识一个独立用户，每部手机一个用户。</p><ul><li><strong>新增用户</strong></li></ul><p>首次联网使用应用的用户。如果一个用户首次打开某APP，那这个用户定义为新增用户；卸载再安装的设备，不会被算作一次新增。新增用户包括日新增用户、周新增用户、月新增用户。</p><ul><li><strong>活跃用户</strong></li></ul><p>打开应用的用户即为活跃用户，不考虑用户的使用情况。每天一台设备打开多次会被计为一个活跃用户。</p><ul><li><strong>周（月）活跃用户</strong></li></ul><p>某个自然周（月）内启动过应用的用户，该周（月）内的多次启动只记一个活跃用户。</p><ul><li><strong>月活跃率</strong></li></ul><p>月活跃用户与截止到该月累计的用户总和之间的比例。</p><ul><li><strong>沉默用户</strong></li></ul><p>用户仅在安装当天（次日）启动一次，后续时间无再启动行为。该指标可以反映新增用户质量和用户与APP的匹配程度。</p><ul><li><strong>版本分布</strong></li></ul><p>不同版本的周内各天新增用户数，活跃用户数和启动次数。利于判断APP各个版本之间的优劣和用户行为习惯。</p><ul><li><strong>本周回流用户</strong></li></ul><p>上周未启动过应用，本周启动了应用的用户。</p><ul><li><strong>连续N周活跃用户</strong></li></ul><p>连续n周，每周至少启动一次。</p><ul><li><strong>忠诚用户</strong></li></ul><p>连续活跃5周以上的用户</p><ul><li><strong>连续活跃用户</strong></li></ul><p>连续2周及以上活跃的用户</p><ul><li><strong>近期流失用户</strong></li></ul><p>连续n(2&lt;= n &lt;= 4)周没有启动应用的用户。（第n+1周没有启动过）</p><ul><li><strong>留存用户</strong></li></ul><p>某段时间内的新增用户，经过一段时间后，仍然使用应用的被认作是留存用户；这部分用户占当时新增用户的比例即是留存率。<br>例如，5月份新增用户200，这200人在6月份启动过应用的有100人，7月份启动过应用的有80人，8月份启动过应用的有50人；则5月份新增用户一个月后的留存率是50%，二个月后的留存率是40%，三个月后的留存率是25%。</p><ul><li><strong>用户新鲜度</strong></li></ul><p>每天启动应用的新老用户比例，即新增用户数占活跃用户数的比例。</p><ul><li><strong>单次使用时长</strong></li></ul><p>每次启动使用的时间长度。</p><ul><li><strong>日使用时长</strong></li></ul><p>累计一天内的使用时间长度。</p><ul><li><strong>启动次数计算标准</strong></li></ul><p>IOS平台应用退到后台就算一次独立的启动；Android平台我们规定，两次启动之间的间隔小于30秒，被计算一次启动。用户在使用过程中，若因收发短信或接电话等退出应用30秒又再次返回应用中，那这两次行为应该是延续而非独立的，所以可以被算作一次使用行为，即一次启动。业内大多使用30秒这个标准，但用户还是可以自定义此时间间隔。</p><h2 id="常用的日期函数处理"><a href="#常用的日期函数处理" class="headerlink" title="常用的日期函数处理"></a>常用的日期函数处理</h2><ul><li><strong>date_format函数（根据格式整理日期）</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">select date_format(<span class="string">'2019-12-05'</span>,<span class="string">'yyyy-MM'</span>);</span><br></pre></td></tr></table></figure><p>输出：2019-12</p><ul><li><strong>date_add函数（加减日期）</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">select date_add(<span class="string">'2019-12-05'</span>,-1);</span><br></pre></td></tr></table></figure><p>输出：2019-12-04</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">select date_add(<span class="string">'2019-12-05'</span>,1);</span><br></pre></td></tr></table></figure><p>输出：2019-12-06</p><ul><li><strong>next_day函数(返回当前时间的下一个星期X所对应的日期)</strong></li></ul><p>1）取当前天的下一个周一</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">select next_day(<span class="string">'2019-12-05'</span>,<span class="string">'MO'</span>)</span><br></pre></td></tr></table></figure><p>输出：2019-12-09</p><p>说明：星期一到星期日的英文（Monday，Tuesday、Wednesday、Thursday、Friday、Saturday、Sunday）</p><p>2）取当前周的周一</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">select date_add(next_day(<span class="string">'2019-12-05'</span>,<span class="string">'MO'</span>),-7);</span><br></pre></td></tr></table></figure><p>输出：2019-12-02</p><ul><li><strong>last_day函数（返回这个月的最后一天的日期）</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">select last_day(<span class="string">'2019-12-05'</span>);</span><br></pre></td></tr></table></figure><p>输出：2019-12-31</p><h2 id="业务指标分析"><a href="#业务指标分析" class="headerlink" title="业务指标分析"></a>业务指标分析</h2><h3 id="用户活跃相关指标分析"><a href="#用户活跃相关指标分析" class="headerlink" title="用户活跃相关指标分析"></a>用户活跃相关指标分析</h3><p>数仓的DWS层会建立好每日的活跃用户表明细、每周的活跃用户表明细以及每月的活跃用户明细表。</p><ul><li><strong>每日活跃用户明细表结构</strong></li></ul><p>每天一个分区，存储当天的日活明细，该表根据mid_id进行去重。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table dws_uv_detail_day</span><br><span class="line">(</span><br><span class="line">    `mid_id` string COMMENT <span class="string">'设备唯一标识'</span>,</span><br><span class="line">    `user_id` string COMMENT <span class="string">'用户标识'</span>, </span><br><span class="line">    `version_code` string COMMENT <span class="string">'程序版本号'</span>, </span><br><span class="line">    `version_name` string COMMENT <span class="string">'程序版本名'</span>, </span><br><span class="line">    `lang` string COMMENT <span class="string">'系统语言'</span>, </span><br><span class="line">    `<span class="built_in">source</span>` string COMMENT <span class="string">'渠道号'</span>, </span><br><span class="line">    `os` string COMMENT <span class="string">'安卓系统版本'</span>, </span><br><span class="line">    `area` string COMMENT <span class="string">'区域'</span>, </span><br><span class="line">    `model` string COMMENT <span class="string">'手机型号'</span>, </span><br><span class="line">    `brand` string COMMENT <span class="string">'手机品牌'</span>, </span><br><span class="line">    `sdk_version` string COMMENT <span class="string">'sdkVersion'</span>, </span><br><span class="line">    `gmail` string COMMENT <span class="string">'gmail'</span>, </span><br><span class="line">    `height_width` string COMMENT <span class="string">'屏幕宽高'</span>,</span><br><span class="line">    `app_time` string COMMENT <span class="string">'客户端日志产生时的时间'</span>,</span><br><span class="line">    `network` string COMMENT <span class="string">'网络模式'</span>,</span><br><span class="line">    `lng` string COMMENT <span class="string">'经度'</span>,</span><br><span class="line">    `lat` string COMMENT <span class="string">'纬度'</span></span><br><span class="line">)</span><br><span class="line">partitioned by(dt string)</span><br><span class="line">stored as parquet</span><br><span class="line">location <span class="string">'/warehouse/gmall/dws/dws_uv_detail_day'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><ul><li><strong>每周活跃用户明细表</strong></li></ul><p>根据日用户访问明细，获得周用户访问明细,周明细表按周一日期和周末日期拼接字段进行分区。即每个分区存储的是本周内的活跃用户明细，该表按mid_id进行去重，即一周内获取多次，只记录一条记录。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table dws_uv_detail_wk( </span><br><span class="line">    `mid_id` string COMMENT <span class="string">'设备唯一标识'</span>,</span><br><span class="line">    `user_id` string COMMENT <span class="string">'用户标识'</span>, </span><br><span class="line">    `version_code` string COMMENT <span class="string">'程序版本号'</span>, </span><br><span class="line">    `version_name` string COMMENT <span class="string">'程序版本名'</span>, </span><br><span class="line">    `lang` string COMMENT <span class="string">'系统语言'</span>, </span><br><span class="line">    `<span class="built_in">source</span>` string COMMENT <span class="string">'渠道号'</span>, </span><br><span class="line">    `os` string COMMENT <span class="string">'安卓系统版本'</span>, </span><br><span class="line">    `area` string COMMENT <span class="string">'区域'</span>, </span><br><span class="line">    `model` string COMMENT <span class="string">'手机型号'</span>, </span><br><span class="line">    `brand` string COMMENT <span class="string">'手机品牌'</span>, </span><br><span class="line">    `sdk_version` string COMMENT <span class="string">'sdkVersion'</span>, </span><br><span class="line">    `gmail` string COMMENT <span class="string">'gmail'</span>, </span><br><span class="line">    `height_width` string COMMENT <span class="string">'屏幕宽高'</span>,</span><br><span class="line">    `app_time` string COMMENT <span class="string">'客户端日志产生时的时间'</span>,</span><br><span class="line">    `network` string COMMENT <span class="string">'网络模式'</span>,</span><br><span class="line">    `lng` string COMMENT <span class="string">'经度'</span>,</span><br><span class="line">    `lat` string COMMENT <span class="string">'纬度'</span>,</span><br><span class="line">    `monday_date` string COMMENT <span class="string">'周一日期'</span>,</span><br><span class="line">    `sunday_date` string COMMENT  <span class="string">'周日日期'</span> </span><br><span class="line">) COMMENT <span class="string">'活跃用户按周明细'</span></span><br><span class="line">PARTITIONED BY (`wk_dt` string)</span><br><span class="line">stored as parquet</span><br><span class="line">location <span class="string">'/warehouse/gmall/dws/dws_uv_detail_wk/'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><ul><li><strong>每月活跃用户明细</strong></li></ul><p>该表按月进行分区，并按mid_id去重，数据来源与日活明细表</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table dws_uv_detail_mn( </span><br><span class="line">    `mid_id` string COMMENT <span class="string">'设备唯一标识'</span>,</span><br><span class="line">    `user_id` string COMMENT <span class="string">'用户标识'</span>, </span><br><span class="line">    `version_code` string COMMENT <span class="string">'程序版本号'</span>, </span><br><span class="line">    `version_name` string COMMENT <span class="string">'程序版本名'</span>, </span><br><span class="line">    `lang` string COMMENT <span class="string">'系统语言'</span>, </span><br><span class="line">    `<span class="built_in">source</span>` string COMMENT <span class="string">'渠道号'</span>, </span><br><span class="line">    `os` string COMMENT <span class="string">'安卓系统版本'</span>, </span><br><span class="line">    `area` string COMMENT <span class="string">'区域'</span>, </span><br><span class="line">    `model` string COMMENT <span class="string">'手机型号'</span>, </span><br><span class="line">    `brand` string COMMENT <span class="string">'手机品牌'</span>, </span><br><span class="line">    `sdk_version` string COMMENT <span class="string">'sdkVersion'</span>, </span><br><span class="line">    `gmail` string COMMENT <span class="string">'gmail'</span>, </span><br><span class="line">    `height_width` string COMMENT <span class="string">'屏幕宽高'</span>,</span><br><span class="line">    `app_time` string COMMENT <span class="string">'客户端日志产生时的时间'</span>,</span><br><span class="line">    `network` string COMMENT <span class="string">'网络模式'</span>,</span><br><span class="line">    `lng` string COMMENT <span class="string">'经度'</span>,</span><br><span class="line">    `lat` string COMMENT <span class="string">'纬度'</span></span><br><span class="line">) COMMENT <span class="string">'活跃用户按月明细'</span></span><br><span class="line">PARTITIONED BY (`mn` string)</span><br><span class="line">stored as parquet</span><br><span class="line">location <span class="string">'/warehouse/gmall/dws/dws_uv_detail_mn/'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><ul><li><strong>建立ADS层的活跃用户指标表</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table ads_uv_count( </span><br><span class="line">    `dt` string COMMENT <span class="string">'统计日期'</span>,</span><br><span class="line">    `day_count` bigint COMMENT <span class="string">'当日用户数量'</span>,</span><br><span class="line">    `wk_count`  bigint COMMENT <span class="string">'当周用户数量'</span>,</span><br><span class="line">    `mn_count`  bigint COMMENT <span class="string">'当月用户数量'</span>,</span><br><span class="line">    `is_weekend` string COMMENT <span class="string">'Y,N是否是周末,用于得到本周最终结果'</span>,</span><br><span class="line">    `is_monthend` string COMMENT <span class="string">'Y,N是否是月末,用于得到本月最终结果'</span> </span><br><span class="line">) COMMENT <span class="string">'活跃设备数'</span></span><br><span class="line">row format delimited fields terminated by <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/warehouse/gmall/ads/ads_uv_count/'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><p><strong>SQL具体实现：</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">insert into table ads_uv_count </span><br><span class="line">select  </span><br><span class="line">  <span class="string">'2019-02-10'</span> dt,</span><br><span class="line">   daycount.ct,</span><br><span class="line">   wkcount.ct,</span><br><span class="line">   mncount.ct,</span><br><span class="line">   <span class="keyword">if</span>(date_add(next_day(<span class="string">'2019-02-10'</span>,<span class="string">'MO'</span>),-1)=<span class="string">'2019-02-10'</span>,<span class="string">'Y'</span>,<span class="string">'N'</span>) , -- 判断跑任务的当天是否是周末</span><br><span class="line">   <span class="keyword">if</span>(last_day(<span class="string">'2019-02-10'</span>)=<span class="string">'2019-02-10'</span>,<span class="string">'Y'</span>,<span class="string">'N'</span>)  -- 判断跑任务的当天是否是月末</span><br><span class="line">from </span><br><span class="line">(</span><br><span class="line">-- 计算当天的日活</span><br><span class="line">   select  </span><br><span class="line">      <span class="string">'2019-02-10'</span> dt,</span><br><span class="line">       count(*) ct</span><br><span class="line">   from dws_uv_detail_day</span><br><span class="line">   <span class="built_in">where</span> dt=<span class="string">'2019-02-10'</span>  </span><br><span class="line">)daycount join </span><br><span class="line">( </span><br><span class="line">-- 计算当天所属周的周活</span><br><span class="line">   select  </span><br><span class="line">     <span class="string">'2019-02-10'</span> dt,</span><br><span class="line">     count (*) ct</span><br><span class="line">   from dws_uv_detail_wk</span><br><span class="line">   <span class="built_in">where</span> wk_dt=concat(date_add(next_day(<span class="string">'2019-02-10'</span>,<span class="string">'MO'</span>),-7),<span class="string">'_'</span> ,date_add(next_day(<span class="string">'2019-02-10'</span>,<span class="string">'MO'</span>),-1) )</span><br><span class="line">) wkcount on daycount.dt=wkcount.dt</span><br><span class="line">join </span><br><span class="line">( </span><br><span class="line">-- 计算当天所属月的月活</span><br><span class="line">   select  </span><br><span class="line">     <span class="string">'2019-02-10'</span> dt,</span><br><span class="line">     count (*) ct</span><br><span class="line">   from dws_uv_detail_mn</span><br><span class="line">   <span class="built_in">where</span> mn=date_format(<span class="string">'2019-02-10'</span>,<span class="string">'yyyy-MM'</span>)  </span><br><span class="line">)mncount on daycount.dt=mncount.dt</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h3 id="新增用户指标分析"><a href="#新增用户指标分析" class="headerlink" title="新增用户指标分析"></a>新增用户指标分析</h3><p>首次联网使用应用的用户。如果一个用户首次打开某APP，那这个用户定义为新增用户；卸载再安装的设备，不会被算作一次新增。新增用户包括日新增用户、周新增用户、月新增用户。</p><ul><li><strong>每日新增用户明细表</strong></li></ul><p>每日新增用户明细表来源于每天的日活表，使用每天的日活表去LEFT JOIN 每天新增用户明细表，关联的条件是mid_id,筛选条件为，每日新增设备表中为空</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table dws_new_mid_day</span><br><span class="line">(</span><br><span class="line">    `mid_id` string COMMENT <span class="string">'设备唯一标识'</span>,</span><br><span class="line">    `user_id` string COMMENT <span class="string">'用户标识'</span>, </span><br><span class="line">    `version_code` string COMMENT <span class="string">'程序版本号'</span>, </span><br><span class="line">    `version_name` string COMMENT <span class="string">'程序版本名'</span>, </span><br><span class="line">    `lang` string COMMENT <span class="string">'系统语言'</span>, </span><br><span class="line">    `<span class="built_in">source</span>` string COMMENT <span class="string">'渠道号'</span>, </span><br><span class="line">    `os` string COMMENT <span class="string">'安卓系统版本'</span>, </span><br><span class="line">    `area` string COMMENT <span class="string">'区域'</span>, </span><br><span class="line">    `model` string COMMENT <span class="string">'手机型号'</span>, </span><br><span class="line">    `brand` string COMMENT <span class="string">'手机品牌'</span>, </span><br><span class="line">    `sdk_version` string COMMENT <span class="string">'sdkVersion'</span>, </span><br><span class="line">    `gmail` string COMMENT <span class="string">'gmail'</span>, </span><br><span class="line">    `height_width` string COMMENT <span class="string">'屏幕宽高'</span>,</span><br><span class="line">    `app_time` string COMMENT <span class="string">'客户端日志产生时的时间'</span>,</span><br><span class="line">    `network` string COMMENT <span class="string">'网络模式'</span>,</span><br><span class="line">    `lng` string COMMENT <span class="string">'经度'</span>,</span><br><span class="line">    `lat` string COMMENT <span class="string">'纬度'</span>,</span><br><span class="line">    `create_date`  string  comment <span class="string">'创建时间'</span> </span><br><span class="line">)  COMMENT <span class="string">'每日新增设备信息'</span></span><br><span class="line">stored as parquet</span><br><span class="line">location <span class="string">'/warehouse/gmall/dws/dws_new_mid_day/'</span>;</span><br></pre></td></tr></table></figure><ul><li><strong>每日新增用户表</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table ads_new_mid_count</span><br><span class="line">(</span><br><span class="line">    `create_date`     string comment <span class="string">'创建时间'</span> ,</span><br><span class="line">    `new_mid_count`   BIGINT comment <span class="string">'新增设备数量'</span> </span><br><span class="line">)  COMMENT <span class="string">'每日新增设备信息数量'</span></span><br><span class="line">row format delimited fields terminated by <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/warehouse/gmall/ads/ads_new_mid_count/'</span>;</span><br></pre></td></tr></table></figure><p><strong>每日新增用户表装载SQL实现</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">insert into table ads_new_mid_count </span><br><span class="line">select</span><br><span class="line">create_date,</span><br><span class="line">count(*)</span><br><span class="line">from dws_new_mid_day</span><br><span class="line"><span class="built_in">where</span> create_date=<span class="string">'2019-02-10'</span></span><br><span class="line">group by create_date;</span><br></pre></td></tr></table></figure><h3 id="用户留存指标分析"><a href="#用户留存指标分析" class="headerlink" title="用户留存指标分析"></a>用户留存指标分析</h3><p><img src="//jiamaoxiang.top/2019/12/05/电商业务常用指标分析之SQL实现/%E7%94%A8%E6%88%B7%E7%95%99%E5%AD%98.png" alt></p><ul><li><strong>每日用户留存明细</strong></li></ul><p>该表以天作为分区，每天计算前1天的新用户访问留存明细，</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table dws_user_retention_day </span><br><span class="line">(</span><br><span class="line">    `mid_id` string COMMENT <span class="string">'设备唯一标识'</span>,</span><br><span class="line">    `user_id` string COMMENT <span class="string">'用户标识'</span>, </span><br><span class="line">    `version_code` string COMMENT <span class="string">'程序版本号'</span>, </span><br><span class="line">    `version_name` string COMMENT <span class="string">'程序版本名'</span>, </span><br><span class="line">`lang` string COMMENT <span class="string">'系统语言'</span>, </span><br><span class="line">`<span class="built_in">source</span>` string COMMENT <span class="string">'渠道号'</span>, </span><br><span class="line">`os` string COMMENT <span class="string">'安卓系统版本'</span>, </span><br><span class="line">`area` string COMMENT <span class="string">'区域'</span>, </span><br><span class="line">`model` string COMMENT <span class="string">'手机型号'</span>, </span><br><span class="line">`brand` string COMMENT <span class="string">'手机品牌'</span>, </span><br><span class="line">`sdk_version` string COMMENT <span class="string">'sdkVersion'</span>, </span><br><span class="line">`gmail` string COMMENT <span class="string">'gmail'</span>, </span><br><span class="line">`height_width` string COMMENT <span class="string">'屏幕宽高'</span>,</span><br><span class="line">`app_time` string COMMENT <span class="string">'客户端日志产生时的时间'</span>,</span><br><span class="line">`network` string COMMENT <span class="string">'网络模式'</span>,</span><br><span class="line">`lng` string COMMENT <span class="string">'经度'</span>,</span><br><span class="line">`lat` string COMMENT <span class="string">'纬度'</span>,</span><br><span class="line">   `create_date`    string  comment <span class="string">'设备新增时间'</span>,</span><br><span class="line">   `retention_day`  int comment <span class="string">'截止当前日期留存天数'</span></span><br><span class="line">)  COMMENT <span class="string">'每日用户留存情况'</span></span><br><span class="line">PARTITIONED BY (`dt` string)</span><br><span class="line">stored as parquet</span><br><span class="line">location <span class="string">'/warehouse/gmall/dws/dws_user_retention_day/'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>每日用户留存明细装载语句</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">insert overwrite table dws_user_retention_day</span><br><span class="line">partition(dt=<span class="string">"2019-02-11"</span>)</span><br><span class="line">select  </span><br><span class="line">    nm.mid_id,</span><br><span class="line">    nm.user_id , </span><br><span class="line">    nm.version_code , </span><br><span class="line">    nm.version_name , </span><br><span class="line">    nm.lang , </span><br><span class="line">    nm.source, </span><br><span class="line">    nm.os, </span><br><span class="line">    nm.area, </span><br><span class="line">    nm.model, </span><br><span class="line">    nm.brand, </span><br><span class="line">    nm.sdk_version, </span><br><span class="line">    nm.gmail, </span><br><span class="line">    nm.height_width,</span><br><span class="line">    nm.app_time,</span><br><span class="line">    nm.network,</span><br><span class="line">    nm.lng,</span><br><span class="line">    nm.lat,</span><br><span class="line">nm.create_date,</span><br><span class="line">1 retention_day </span><br><span class="line">from dws_uv_detail_day ud join dws_new_mid_day nm   on ud.mid_id =nm.mid_id </span><br><span class="line"><span class="built_in">where</span> ud.dt=<span class="string">'2019-02-11'</span> and nm.create_date=date_add(<span class="string">'2019-02-11'</span>,-1);</span><br></pre></td></tr></table></figure><ul><li><strong>留存用户数</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table ads_user_retention_day_count </span><br><span class="line">(</span><br><span class="line">   `create_date`       string  comment <span class="string">'设备新增日期'</span>,</span><br><span class="line">   `retention_day`     int comment <span class="string">'截止当前日期留存天数'</span>,</span><br><span class="line">   `retention_count`    bigint comment  <span class="string">'留存数量'</span></span><br><span class="line">)  COMMENT <span class="string">'每日用户留存情况'</span></span><br><span class="line">row format delimited fields terminated by <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/warehouse/gmall/ads/ads_user_retention_day_count/'</span>;</span><br></pre></td></tr></table></figure><p>留存用户数装载SQL</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">insert into table ads_user_retention_day_count </span><br><span class="line">select</span><br><span class="line">    create_date,</span><br><span class="line">    retention_day,</span><br><span class="line">    count(*) retention_count</span><br><span class="line">from dws_user_retention_day</span><br><span class="line"><span class="built_in">where</span> dt=<span class="string">'2019-02-11'</span> </span><br><span class="line">group by create_date,retention_day;</span><br></pre></td></tr></table></figure><h3 id="流失用户数分析"><a href="#流失用户数分析" class="headerlink" title="流失用户数分析"></a>流失用户数分析</h3><p>流失用户：最近7天未登录我们称之为流失用户</p><ul><li><strong>流失用户数表</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table ads_wastage_count( </span><br><span class="line">    `dt` string COMMENT <span class="string">'统计日期'</span>,</span><br><span class="line">    `wastage_count` bigint COMMENT <span class="string">'流失设备数'</span></span><br><span class="line">) </span><br><span class="line">row format delimited fields terminated by <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/warehouse/gmall/ads/ads_wastage_count'</span>;</span><br></pre></td></tr></table></figure><p>装载SQL,如果统计日期为2019-02-20，则7天未登陆的用户数的计算逻辑为：</p><p>查询日活表，并按mid_id进行分组，并且设备的最近访问时间小于等于当前时间的一周前，即活跃的最大日期(最近一次访问日期)小于等于2019-02-20</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">insert into table ads_wastage_count</span><br><span class="line">select</span><br><span class="line">     <span class="string">'2019-02-20'</span>,</span><br><span class="line">     count(*)</span><br><span class="line">from </span><br><span class="line">(</span><br><span class="line">    select mid_id</span><br><span class="line">from dws_uv_detail_day</span><br><span class="line">    group by mid_id</span><br><span class="line">    having max(dt)&lt;=date_add(<span class="string">'2019-02-20'</span>,-7)</span><br><span class="line">)t1;</span><br></pre></td></tr></table></figure><h3 id="最近七天内连续三天活跃用户数指标分析"><a href="#最近七天内连续三天活跃用户数指标分析" class="headerlink" title="最近七天内连续三天活跃用户数指标分析"></a>最近七天内连续三天活跃用户数指标分析</h3><ul><li><strong>最近七天内连续三天活跃用户数表</strong></li></ul><p>需要使用日活表，来获取最近7天内连续3天活跃用户数</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">create external table ads_continuity_uv_count( </span><br><span class="line">    `dt` string COMMENT <span class="string">'统计日期'</span>,</span><br><span class="line">    `wk_dt` string COMMENT <span class="string">'最近7天日期'</span>,</span><br><span class="line">    `continuity_count` bigint</span><br><span class="line">) COMMENT <span class="string">'连续活跃设备数'</span></span><br><span class="line">row format delimited fields terminated by <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/warehouse/gmall/ads/ads_continuity_uv_count'</span>;</span><br></pre></td></tr></table></figure><p>装载语句为：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert into table ads_continuity_uv_count</span><br><span class="line">select</span><br><span class="line">    <span class="string">'2019-02-12'</span>,</span><br><span class="line">    concat(date_add(<span class="string">'2019-02-12'</span>,-6),<span class="string">'_'</span>,<span class="string">'2019-02-12'</span>),</span><br><span class="line">    count(*)</span><br><span class="line">from</span><br><span class="line"></span><br><span class="line">(</span><br><span class="line">select</span><br><span class="line">mid_id</span><br><span class="line">from </span><br><span class="line">( -- 筛选出连续3的活跃用户，可能存在重复</span><br><span class="line">select</span><br><span class="line">mid_id</span><br><span class="line"></span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">-- 计算活跃用户的活跃日期与其排名的差值</span><br><span class="line">select </span><br><span class="line"></span><br><span class="line">mid_id,</span><br><span class="line">date_sub(dt,rank) date_dif</span><br><span class="line"></span><br><span class="line">from </span><br><span class="line">(-- 查询出最近7天的活跃用户，并对活跃日期进行排名</span><br><span class="line">select</span><br><span class="line">mid_id,</span><br><span class="line">dt,</span><br><span class="line">rank() over (partition by mid_id order by dt) rank</span><br><span class="line">from dws_uv_detail_day</span><br><span class="line"><span class="built_in">where</span> dt &gt;= date_add(<span class="string">'2019-02-12'</span>,-6) and dt &lt;= <span class="string">'2109-02-12'</span></span><br><span class="line">) t1</span><br><span class="line">) t2</span><br><span class="line">group by mid_id,date_dif -- 对用户设备id和差值进行分组</span><br><span class="line">having count(*) &gt;=3   -- 统计大于等于3的差值数据筛选出来</span><br><span class="line"></span><br><span class="line">) t3</span><br><span class="line">group by mid_id -- 对mid_id进行去重</span><br><span class="line">) t4</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CDH集群之YARN性能调优</title>
      <link href="/2019/12/03/CDH%E9%9B%86%E7%BE%A4%E4%B9%8BYARN%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
      <url>/2019/12/03/CDH%E9%9B%86%E7%BE%A4%E4%B9%8BYARN%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<p>本文主要讨论CDH集群的YARN调优配置，关于YARN的调优配置，主要关注CPU和内存的调优，其中CPU是指物理CPU个数乘以CPU核数，即Vcores = CPU数量*CPU核数。YARN是以container容器的形式封装资源的，task在container内部执行。</p><a id="more"></a><h2 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h2><p>集群的配置主要包括三步，第一是先规划集群的工作主机以及每台主机的配置，第二是规划每台主机的安装的组件及其资源分配，第三是规划集群的规模大小。</p><h3 id="工作主机的配置"><a href="#工作主机的配置" class="headerlink" title="工作主机的配置"></a>工作主机的配置</h3><p>如下表所示：主机的内存为256G，4个6核CPU，CPU支持超线程，网络带宽为2G</p><table>    <tr>        <td bgcolor="#6495ED">主机组件</td>        <td bgcolor="#6495ED">数量</td>         <td bgcolor="#6495ED">大小</td>        <td bgcolor="#6495ED">总计</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>RAM</td>        <td>256G</td>         <td></td>        <td>256G</td>          <td>内存大小</td>     </tr>    <tr>        <td>CPU</td>        <td>4</td>         <td>6</td>        <td>48</td>         <td>总CPU核数</td>      </tr>    <tr>        <td>HyperThreading CPU</td>        <td>YES</td>         <td></td>        <td></td>         <td>超线程CPU，使操作系统认为处理器的核心数是实际核心数的2倍，因此如果有24个核心的处理器，操作系统会认为处理器有48个核心</td>      </tr>    <tr>        <td>网络</td>        <td>2</td>         <td>1G</td>        <td>2G</td>         <td>网络带宽</td>      </tr>  </table><h3 id="工作主机安装组件配置"><a href="#工作主机安装组件配置" class="headerlink" title="工作主机安装组件配置"></a>工作主机安装组件配置</h3><p>第一步已经明确每台主机的内存和CPU配置，下面为每台节点的服务分配资源，主要分配CPU和内存</p><table>    <tr>        <td bgcolor="#6495ED">服务</td>        <td bgcolor="#6495ED">类别</td>         <td bgcolor="#6495ED">CPU核数</td>        <td bgcolor="#6495ED">内存(MB)</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>操作系统</td>        <td>Overhead</td>         <td>1</td>        <td>8192</td>          <td>为操作系统分配1核8G内存，一般4~8G</td>     </tr>    <tr>        <td>其它服务</td>        <td>Overhead</td>         <td>0</td>        <td>0</td>         <td>非CDH集群、非操作系统占用的资源</td>      </tr>    <tr>        <td>Cloudera Manager agent</td>        <td>Overhead</td>         <td>1</td>        <td>1024</td>         <td>分配1核1G</td>      </tr>    <tr>        <td>HDFS DataNode</td>        <td>CDH</td>         <td>1</td>        <td>1024</td>         <td>默认1核1G</td>      </tr>       <tr>        <td>YARN NodeManager</td>        <td>CDH</td>         <td>1</td>        <td>1024</td>         <td>默认1核1G</td>      </tr>           <tr>        <td>Impala daemon</td>        <td>CDH</td>         <td>0</td>        <td>0</td>         <td>可选的服务，建议至少为impala demon分配16G内存</td>      </tr>           <tr>        <td>Hbase RegionServer</td>        <td>CDH</td>         <td>0</td>        <td>0</td>         <td>可选的服务，建议12~16G内存</td>      </tr>           <tr>        <td>Solr Server</td>        <td>CDH</td>         <td>0</td>        <td>0</td>         <td>可选的服务，最低1G内存</td>      </tr>           <tr>        <td>Kudu Server</td>        <td>CDH</td>         <td>0</td>        <td>0</td>         <td>可选的服务，kudu Tablet server最低1G内存</td>      </tr>           <tr>        <td>Available Container  Resources</td>        <td></td>         <td>44</td>        <td>250880</td>         <td>剩余分配给yarn的container</td>      </tr></table><p>container资源分配<br>Physical Cores to Vcores Multiplier：每个container的cpu core的并发线程数，本文设置为1</p><p>YARN Available Vcores：YARN可用的CPU核数=Available Container  Resources * Physical Cores to Vcores Multiplier，即为44</p><p>YARN Available Memory：250880</p><h3 id="集群大小"><a href="#集群大小" class="headerlink" title="集群大小"></a>集群大小</h3><p>集群的工作节点个数：10</p><h2 id="YARN配置"><a href="#YARN配置" class="headerlink" title="YARN配置"></a>YARN配置</h2><h3 id="YARN-NodeManager配置属性"><a href="#YARN-NodeManager配置属性" class="headerlink" title="YARN NodeManager配置属性"></a>YARN NodeManager配置属性</h3><table>     <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>yarn.nodemanager.resource.cpu-vcores</td>        <td>44</td>          <td>yarn 的nodemanager分配44核，每台节点剩余的CPU</td>     </tr>    <tr>        <td>yarn.nodemanager.resource.memory-mb</td>        <td>250800</td>         <td>分配的内存大小，每台节点剩余的内存</td>      </tr></table><h3 id="验证YARN的配置"><a href="#验证YARN的配置" class="headerlink" title="验证YARN的配置"></a>验证YARN的配置</h3><p>登录YARN的resourcemanager的WEBUI：http://<resourcemanagerip>:8088/，验证’Memory Total’与’Vcores Total’，如果节点都正常，那么Vcores Total应该为440，Memory应该为2450G，即250800/1024*10</resourcemanagerip></p><h3 id="YARN的container配置"><a href="#YARN的container配置" class="headerlink" title="YARN的container配置"></a>YARN的container配置</h3><p>YARN的container的Vcore配置</p><table>    <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>yarn.scheduler.minimum-allocation-vcores</td>        <td>1</td>          <td>分配给container的最小vcore个数</td>     </tr>    <tr>        <td>yarn.scheduler.maximum-allocation-vcores</td>        <td>44</td>         <td>分配给container的最大vcore数</td>      </tr>    <tr>        <td>yarn.scheduler.increment-allocation-vcores</td>        <td>1</td>         <td>容器虚拟CPU内核增量</td>      </tr></table><p>YARN的container内存配置</p><table>    <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>yarn.scheduler.minimum-allocation-mb</td>        <td>1024</td>          <td>分配给container的最小内存大小，为1G</td>     </tr>    <tr>        <td>yarn.scheduler.maximum-allocation-mb</td>        <td>250880</td>         <td>分配给container的最大内存，等于245G，即为每台节点剩余的最大内存</td>      </tr>    <tr>        <td>yarn.scheduler.increment-allocation-mb</td>        <td>512</td>         <td>容器内存增量，默认512M</td>      </tr></table><h3 id="集群资源分配估计"><a href="#集群资源分配估计" class="headerlink" title="集群资源分配估计"></a>集群资源分配估计</h3><table>    <tr>        <td bgcolor="#6495ED">描述</td>        <td bgcolor="#6495ED">最小值</td>         <td bgcolor="#6495ED">最大值</td>      </tr>    <tr>        <td>根据每个container的最小内存分配，集群最大的container数量为</td>        <td></td>          <td>2450</td>     </tr>    <tr>        <td>根据每个container的最小Vcore分配，集群最大的container数量为</td>        <td></td>         <td>440</td>      </tr>    <tr>        <td>根据每个container的最大内存分配，集群的最小container数为</td>        <td>10</td>         <td></td>      </tr>    <tr>        <td>根据每个container的最大Vcores分配，集群的最小container数为</td>        <td>10</td>         <td></td>      </tr></table><h3 id="container合理配置检查"><a href="#container合理配置检查" class="headerlink" title="container合理配置检查"></a>container合理配置检查</h3><table>    <tr>        <td bgcolor="#6495ED">配置约束</td>        <td bgcolor="#6495ED">描述</td>     </tr>    <tr>        <td>最大的Vcore数量必须大于等于分配的最小Vcore数</td>        <td>yarn.scheduler.maximum-allocation-vcores >= yarn.scheduler.minimum-allocation-vcores</td>      </tr>    <tr>        <td>分配的最大内存数必须大于等于分配的最小内存数</td>        <td>yarn.scheduler.maximum-allocation-mb >= yarn.scheduler.minimum-allocation-mb</td>     </tr>    <tr>        <td>分配的最小核数必须大于等于0</td>        <td>yarn.scheduler.minimum-allocation-vcores >= 0</td>      </tr>    <tr>        <td>分配的最大Vcore数必须大于等于1</td>        <td>yarn.scheduler.maximum-allocation-vcores >= 1</td>      </tr>    <tr>        <td>每台主机分配给nodemanaer的vcore总数必须大于分配的最小vcore数</td>        <td> yarn.nodemanager.resource.cpu-vcores >= yarn.scheduler.minimum-allocation-vcores</td>      </tr>    <tr>        <td>每台主机分配给nodemanaer的vcore总数必须大于分配的最大vcore数</td>        <td>yarn.nodemanager.resource.cpu-vcores >= yarn.scheduler.maximum-allocation-vcores </td>      </tr>    <tr>        <td>每台主机分配给nodemanaer的内存必须大于调度分配的最小内存</td>        <td>yarn.nodemanager.resource.memory-mb >= yarn.scheduler.maximum-allocation-mb</td>      </tr>    <tr>        <td>每台主机分配给nodemanaer的内存必须大于调度分配的最大内存</td>        <td>yarn.nodemanager.resource.memory-mb >= yarn.scheduler.minimum-allocation-mb</td>      </tr>    <tr>        <td>container最小配置</td>        <td>如果yarn.scheduler.minimum-allocation-mb小于1GB，container可能会被YARN杀死，因为会出现OutOfMemory内存溢出的现象</td>      </tr></table><h2 id="MapReduce配置"><a href="#MapReduce配置" class="headerlink" title="MapReduce配置"></a>MapReduce配置</h2><h3 id="ApplicationMaster配置"><a href="#ApplicationMaster配置" class="headerlink" title="ApplicationMaster配置"></a>ApplicationMaster配置</h3><table>     <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>yarn.app.mapreduce.am.resource.cpu-vcores</td>        <td>1</td>          <td>ApplicationMaster 的虚拟CPU内核数</td>     </tr>    <tr>        <td>yarn.app.mapreduce.am.resource.mb</td>        <td>1024</td>         <td>ApplicationMaster的物理内存要求(MiB)</td>      </tr>    <tr>        <td>yarn.app.mapreduce.am.command-opts</td>        <td>800</td>         <td>传递到 MapReduce ApplicationMaster 的 Java 命令行参数，AM Java heap 大小，为800M</td>      </tr></table><h3 id="堆与容器大小之比"><a href="#堆与容器大小之比" class="headerlink" title="堆与容器大小之比"></a>堆与容器大小之比</h3><table>    <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>task自动堆大小</td>        <td>yes</td>          <td></td>     </tr>    <tr>        <td>mapreduce.job.heap.memory-mb.ratio</td>        <td>0.8</td>         <td>Map 和 Reduce 任务的堆大小与容器大小之比。堆大小应小于容器大小，以允许 JVM 的某些开销，默认为0.8</td>      </tr></table><h3 id="map-task配置"><a href="#map-task配置" class="headerlink" title="map task配置"></a>map task配置</h3><table>    <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>mapreduce.map.cpu.vcores</td>        <td>1</td>          <td>分配给map task的vcore数</td>     </tr>    <tr>        <td>mapreduce.map.memory.mb</td>        <td>1024</td>         <td>分配给map task的内存数，1G</td>      </tr>    <tr>        <td>mapreduce.task.io.sort.mb</td>        <td>400</td>         <td>I/O 排序内存缓冲 (MiB),默认256M，一般不用修改</td>      </tr></table><h3 id="reduce-task配置"><a href="#reduce-task配置" class="headerlink" title="reduce task配置"></a>reduce task配置</h3><table>    <tr>        <td bgcolor="#6495ED">配置参数</td>        <td bgcolor="#6495ED">值</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>mapreduce.reduce.cpu.vcores</td>        <td>1</td>          <td>分配给reduce task的vcore数</td>     </tr>    <tr>        <td>mapreduce.reduce.memory.mb</td>        <td>1024</td>         <td>分配给reduce task的内存数，1G</td>      </tr></table><h3 id="MapReduce配置合理性检查"><a href="#MapReduce配置合理性检查" class="headerlink" title="MapReduce配置合理性检查"></a>MapReduce配置合理性检查</h3><ul><li>Application Master配置的合理性检查</li></ul><p>yarn.scheduler.minimum-allocation-vcores &lt;=  <strong>yarn.app.mapreduce.am.resource.cpu-vcores</strong>&lt;= yarn-scheduler.maximum-allocation-vcores</p><p>yarn.scheduler.minimum-allocation-mb &lt;= <strong>yarn.app.mapreduce.am.resource.cpu-vcores</strong> &lt;= yarn.scheduler.maximum-allocation-mb  </p><p>Java Heap大小是container大小的75%~90%: 太低会造成资源浪费, 太高会造成OOM<br>Map Task配置的合理性检查</p><ul><li>Reduce Task配置的合理性检查</li></ul><p>yarn.scheduler.minimum-allocation-vcores &lt;= <strong>mapreduce.map.cpu.vcores</strong> &lt;= yarn-scheduler.maximum-allocation-vcores</p><p>yarn.scheduler.minimum-allocation-mb &lt;= <strong>mapreduce.map.memory.mb</strong> &lt;= yarn.scheduler.maximum-allocation-mb</p><p>Spill/Sort内存为每个task堆内存的40%~60%</p><ul><li>Reduce Task配置的合理性检查</li></ul><p>yarn.scheduler.minimum-allocation-vcores &lt;= <strong>mapreduce.reduce.cpu.vcores</strong> &lt;= yarn-scheduler.maximum-allocation-vcores   </p><p>yarn.scheduler.minimum-allocation-mb &lt;= <strong>mapreduce.reduce.memory.mb</strong> &lt;= yarn.scheduler.maximum-allocation-mb</p><h2 id="YARN和MapReduce配置参数总结"><a href="#YARN和MapReduce配置参数总结" class="headerlink" title="YARN和MapReduce配置参数总结"></a>YARN和MapReduce配置参数总结</h2><table>    <tr>        <td bgcolor="#6495ED">YARN/MapReduce参数配置</td>         <td bgcolor="#6495ED">描述</td>      </tr>    <tr>        <td>yarn.nodemanager.resource.cpu-vcores</td>        <td>分配给container的虚拟cpu数</td>     </tr>    <tr>        <td>yarn.nodemanager.resource.memory-mb</td>        <td>分配给container的内存大小</td>     </tr>    <tr>        <td>yarn.scheduler.minimum-allocation-vcores</td>        <td>分配给container的最小虚拟cpu数</td>     </tr>    <tr>        <td>    yarn.scheduler.maximum-allocation-vcores</td>        <td>分配给container的最大虚拟cpu数</td>     </tr>    <tr>        <td>yarn.scheduler.increment-allocation-vcores</td>        <td>分配给container的递增虚拟cpu数</td>     </tr>    <tr>        <td>yarn.scheduler.minimum-allocation-mb</td>        <td>分配给container的最小内存大小</td>     </tr>    <tr>        <td>yarn.scheduler.maximum-allocation-mb</td>        <td>分配给container的最大内存</td>     </tr>    <tr>        <td>yarn.scheduler.increment-allocation-mb</td>        <td>分配给container的递增内存大小</td>     </tr>    <tr>        <td>yarn.app.mapreduce.am.resource.cpu-vcores</td>        <td>ApplicationMaste的虚拟cpu数</td>     </tr>    <tr>        <td>yarn.app.mapreduce.am.resource.mb</td>        <td>ApplicationMaste的内存大小</td>     </tr>    <tr>        <td>mapreduce.map.cpu.vcores</td>        <td>map task的虚拟CPU数</td>     </tr>    <tr>        <td>mapreduce.map.memory.mb</td>        <td>map task的内存大小</td>     </tr>    <tr>        <td>mapreduce.reduce.cpu.vcores</td>        <td>reduce task的虚拟cpu数</td>     </tr>    <tr>        <td>mapreduce.reduce.memory.mb</td>        <td>reduce task的内存大小</td>     </tr>    <tr>        <td>mapreduce.task.io.sort.mb</td>        <td>I/O排序内存大小</td>     </tr></table><p>note：在CDH5.5或者更高版本中，参数<strong>mapreduce.map.java.opts</strong>, <strong>mapreduce.reduce.java.opts</strong>, <strong>yarn.app.mapreduce.am.command-opts</strong>会基于container堆内存的比例进行自动配置 </p>]]></content>
      
      
      <categories>
          
          <category> CDH,YARN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH,YARN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>历史拉链表实战</title>
      <link href="/2019/11/08/%E5%8E%86%E5%8F%B2%E6%8B%89%E9%93%BE%E8%A1%A8%E5%AE%9E%E6%88%98/"/>
      <url>/2019/11/08/%E5%8E%86%E5%8F%B2%E6%8B%89%E9%93%BE%E8%A1%A8%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[<p>历史拉链表是一种数据模型，主要是针对数据仓库设计中表存储数据的方式而定义的。所谓历史拉链表，就是指记录一个事物从开始一直到当前状态的所有变化信息。拉所有记录链表可以避免按每一天存储造成的海量存储问题，同时也是处理缓慢变化数据的一种常见方式。</p><a id="more"></a><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>现假设有如下场景：一个企业拥有5000万会员信息，每天有20万会员资料变更，需要在数仓中记录会员表的历史变化以备分析使用，即每天都要保留一个快照供查询，反映历史数据的情况。在此场景中，需要反映5000万会员的历史变化，如果保留快照，存储两年就需要2X365X5000W条数据存储空间，数据量为365亿，如果存储更长时间，则无法估计需要的存储空间。而利用拉链算法存储，每日只向历史表中添加新增和变化的数据，每日不过20万条，存储4年也只需要3亿存储空间。</p><h2 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h2><p>在拉链表中，每一条数据都有一个生效日期(effective_date)和失效日期(expire_date)。假设在一个用户表中，在2019年11月8日新增了两个用户，如下表所示，则这两条记录的生效时间为当天，由于到2019年11月8日为止,这两条就还没有被修改过，所以失效时间为一个给定的比较大的值，比如：3000-12-31  </p><table>    <tr>        <td> member_id</td>         <td>phoneno</td>        <td>create_time</td>        <td>update_time</td>     </tr>    <tr>        <td>10001</td>        <td>13300000001</td>          <td>2019-11-08</td>         <td>3000-12-31</td>     </tr>    <tr>        <td>10002</td>        <td>13500000002</td>         <td>2019-11-08</td>        <td>3000-12-31</td>       </tr></table><p>第二天(2019-11-09)，用户10001被删除了，用户10002的电话号码被修改成13600000002.为了保留历史状态，用户10001的失效时间被修改为2019-11-09，用户10002则变成了两条记录，如下表所示：  </p><table>    <tr>        <td> member_id</td>         <td>phoneno</td>        <td>create_time</td>        <td>update_time</td>     </tr>    <tr>        <td>10001</td>        <td>13300000001</td>          <td>2019-11-08</td>         <td>2019-11-09</td>     </tr>    <tr>        <td>10002</td>        <td>13500000002</td>         <td>2019-11-08</td>        <td>2019-11-09</td>       </tr>    <tr>        <td>10002</td>        <td>13600000002</td>         <td>2019-11-09</td>        <td>3000-12-31</td>       </tr></table><p>第三天(2019-11-10),又新增了用户10003，则用户表数据如小表所示：  </p><table>    <tr>        <td> member_id</td>         <td>phoneno</td>        <td>create_time</td>        <td>update_time</td>     </tr>    <tr>        <td>10001</td>        <td>13300000001</td>          <td>2019-11-08</td>         <td>2019-11-09</td>     </tr>    <tr>        <td>10002</td>        <td>13500000002</td>         <td>2019-11-08</td>        <td>2019-11-09</td>       </tr>    <tr>        <td>10002</td>        <td>13600000002</td>         <td>2019-11-09</td>        <td>3000-12-31</td>       </tr>    <tr>        <td>10003</td>        <td>13300000006</td>         <td>2019-11-10</td>        <td>3000-12-31</td>       </tr></table><p>如果要查询最新的数据，那么只要查询失效时间为3000-12-31的数据即可，如果要查11月8号的历史数据，则筛选生效时间&lt;= 2019-11-08并且失效时间&gt;2019-11-08的数据即可。如果查询11月9号的数据，那么筛选条件则是生效时间&lt;=2019-11-09并且失效时间&gt;2019-11-09</p><h2 id="表结构"><a href="#表结构" class="headerlink" title="表结构"></a>表结构</h2><ul><li>MySQL源member表</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE member(</span><br><span class="line">             member_id VARCHAR ( 64 ),</span><br><span class="line">             phoneno VARCHAR ( 20 ), </span><br><span class="line">             create_time datetime,</span><br><span class="line">             update_time datetime );</span><br></pre></td></tr></table></figure><ul><li>ODS层增量表member_delta,每天一个分区</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">CREATE TABLE member_delta</span><br><span class="line">            (member_id string,</span><br><span class="line">             phoneno string,</span><br><span class="line">             create_time string,</span><br><span class="line">             update_time string) </span><br><span class="line">PARTITIONED BY (DAY string);</span><br></pre></td></tr></table></figure><ul><li>临时表</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">CREATE TABLE member_his_tmp</span><br><span class="line">            (member_id string,</span><br><span class="line">             phoneno string,</span><br><span class="line">             effective_date date,</span><br><span class="line">             expire_date date</span><br><span class="line">             );</span><br></pre></td></tr></table></figure><ul><li>DW层历史拉链表</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">CREATE TABLE member_his</span><br><span class="line">            (member_id string,</span><br><span class="line">             phoneno string,</span><br><span class="line">             effective_date date,</span><br><span class="line">             expire_date date);</span><br></pre></td></tr></table></figure><h2 id="Demo数据准备"><a href="#Demo数据准备" class="headerlink" title="Demo数据准备"></a>Demo数据准备</h2><p>2019-11-08的数据为：  </p><table>    <tr>        <td> member_id</td>         <td>phoneno</td>        <td>create_time</td>        <td>update_time</td>     </tr>    <tr>        <td>10001</td>        <td>13500000001</td>          <td>2019-11-08 14:47:55</td>         <td>2019-11-08 14:47:55</td>     </tr>    <tr>        <td>10002</td>        <td>13500000002</td>         <td>2019-11-08 14:48:33</td>        <td>2019-11-08 14:48:33</td>       </tr>    <tr>        <td>10003</td>        <td>13500000003</td>         <td>2019-11-08 14:48:53</td>        <td>2019-11-08 14:48:53</td>       </tr>    <tr>        <td>10004</td>        <td>13500000004</td>         <td>2019-11-08 14:49:02</td>        <td>2019-11-08 14:49:02</td>       </tr></table><p>2019-11-09的数据为：其中蓝色代表新增数据，红色代表修改的数据</p><table>    <tr>        <td> member_id</td>         <td>phoneno</td>        <td>create_time</td>        <td>update_time</td>     </tr>    <tr>        <td>10001</td>        <td>13500000001</td>          <td>2019-11-08 14:47:55</td>         <td>2019-11-08 14:47:55</td>     </tr>    <tr>        <td bgcolor="#DC143C">10002</td>        <td bgcolor="#DC143C">13600000002</td>         <td bgcolor="#DC143C">2019-11-08 14:48:33</td>        <td bgcolor="#DC143C">2019-11-09 14:48:33</td>       </tr>    <tr>        <td>10003</td>        <td>13500000003</td>         <td>2019-11-08 14:48:53</td>        <td>2019-11-08 14:48:53</td>       </tr>    <tr>        <td>10004</td>        <td>13500000004</td>         <td>2019-11-08 14:49:02</td>        <td>2019-11-08 14:49:02</td>       </tr>    <tr>        <td bgcolor="#6495ED">10005</td>         <td bgcolor="#6495ED">13500000005</td>        <td bgcolor="#6495ED">2019-11-09 08:54:03 </td>        <td bgcolor="#6495ED">2019-11-09 08:54:03</td>    </tr>    <tr>        <td bgcolor="#6495ED">10006</td>         <td bgcolor="#6495ED">13500000006</td>        <td bgcolor="#6495ED">2019-11-09 09:54:25 </td>        <td bgcolor="#6495ED">2019-11-09 09:54:25</td>    </tr></table><p>2019-11-10的数据：其中蓝色代表新增数据，红色代表修改的数据  </p><table>    <tr>        <td> member_id</td>         <td>phoneno</td>        <td>create_time</td>        <td>update_time</td>     </tr>    <tr>        <td>10001</td>        <td>13500000001</td>          <td>2019-11-08 14:47:55</td>         <td>2019-11-08 14:47:55</td>     </tr>    <tr>        <td>10002</td>        <td>13600000002</td>         <td>2019-11-08 14:48:33</td>        <td>2019-11-09 14:48:33</td>       </tr>    <tr>        <td>10003</td>        <td>13500000003</td>         <td>2019-11-08 14:48:53</td>        <td>2019-11-08 14:48:53</td>       </tr>    <tr>        <td bgcolor="#DC143C">10004</td>        <td bgcolor="#DC143C">13600000004</td>         <td bgcolor="#DC143C">2019-11-08 14:49:02</td>        <td bgcolor="#DC143C">2019-11-10 14:49:02</td>       </tr>    <tr>        <td>10005</td>         <td>13500000005</td>        <td>2019-11-09 08:54:03 </td>        <td>2019-11-09 08:54:03</td>    </tr>    <tr>        <td>10006</td>         <td>13500000006</td>        <td>2019-11-09 09:54:25 </td>        <td>2019-11-09 09:54:25</td>    </tr>    <tr>        <td bgcolor="#6495ED">10007</td>         <td bgcolor="#6495ED">13500000007</td>        <td bgcolor="#6495ED">2019-11-10 17:41:49 </td>        <td bgcolor="#6495ED">2019-11-10 17:41:49</td>    </tr></table><h2 id="全量初始装载"><a href="#全量初始装载" class="headerlink" title="全量初始装载"></a>全量初始装载</h2><p>在启用拉链表时，先对其进行初始装载，比如以2019-11-08为开始时间，<br>那么将MySQL源表全量抽取到ODS层member_delta表的2018-11-08的分区中，<br>然后初始装载DW层的拉链表member_his</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">INSERT overwrite TABLE member_his </span><br><span class="line">SELECT</span><br><span class="line">   member_id,</span><br><span class="line">   phoneno,</span><br><span class="line">   to_date ( create_time ) AS effective_date,</span><br><span class="line">  <span class="string">'3000-12-31'</span> </span><br><span class="line">FROM</span><br><span class="line">member_delta </span><br><span class="line">WHERE</span><br><span class="line">DAY = <span class="string">'2019-11-08'</span></span><br></pre></td></tr></table></figure><p>查询初始的历史拉链表数据</p><p><img src="//jiamaoxiang.top/2019/11/08/历史拉链表实战/1.png" alt></p><h2 id="增量抽取数据"><a href="#增量抽取数据" class="headerlink" title="增量抽取数据"></a>增量抽取数据</h2><p>每天，从源系统member表中，将前一天的增量数据抽取到ODS层的增量数据表member_delta对应的分区中。这里的增量需要通过member表中的创建时间和修改时间来确定，或者使用sqoop job监控update时间来进行增联抽取。<br>比如，本案例中2019-11-09和2019-11-10为两个分区，分别存储了2019-11-09和2019-11-10日的增量数据。<br>2019-11-09分区的数据为:</p><p><img src="//jiamaoxiang.top/2019/11/08/历史拉链表实战/2.png" alt></p><p>2019-11-10分区的数据为：</p><p><img src="//jiamaoxiang.top/2019/11/08/历史拉链表实战/3.png" alt></p><h2 id="增量刷新历史拉链数据"><a href="#增量刷新历史拉链数据" class="headerlink" title="增量刷新历史拉链数据"></a>增量刷新历史拉链数据</h2><ul><li>2019-11-09增量刷新历史拉链表<br>将数据放进临时表</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">INSERT overwrite TABLE member_his_tmp</span><br><span class="line">SELECT *</span><br><span class="line">FROM</span><br><span class="line">  (</span><br><span class="line">-- 2019-11-09增量数据，代表最新的状态，该数据的生效时间是2019-11-09，过期时间为3000-12-31</span><br><span class="line">-- 这些增量的数据需要被全部加载到历史拉链表中</span><br><span class="line">SELECT member_id,</span><br><span class="line">       phoneno,</span><br><span class="line">       <span class="string">'2019-11-09'</span> effective_date,</span><br><span class="line">                    <span class="string">'3000-12-31'</span> expire_date</span><br><span class="line">   FROM member_delta</span><br><span class="line">   WHERE DAY=<span class="string">'2019-11-09'</span></span><br><span class="line">   UNION ALL </span><br><span class="line">-- 用当前为生效状态的拉链数据，去left join 增量数据，</span><br><span class="line">-- 如果匹配得上，则表示该数据已发生了更新，</span><br><span class="line">-- 此时，需要将发生更新的数据的过期时间更改为当前时间.</span><br><span class="line">-- 如果匹配不上，则表明该数据没有发生更新，此时过期时间不变</span><br><span class="line">SELECT a.member_id,</span><br><span class="line">       a.phoneno,</span><br><span class="line">       a.effective_date,</span><br><span class="line">       <span class="keyword">if</span>(b.member_id IS NULL, to_date(a.expire_date), to_date(b.day)) expire_date</span><br><span class="line">   FROM</span><br><span class="line">     (SELECT *</span><br><span class="line">      FROM member_his</span><br><span class="line">    ) a</span><br><span class="line">   LEFT JOIN</span><br><span class="line">     (SELECT *</span><br><span class="line">      FROM member_delta</span><br><span class="line">      WHERE DAY=<span class="string">'2019-11-09'</span>) b ON a.member_id=b.member_id)his</span><br></pre></td></tr></table></figure><p>将数据覆盖到历史拉链表</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">INSERT overwrite TABLE member_his</span><br><span class="line">SELECT *</span><br><span class="line">FROM member_his_tmp</span><br></pre></td></tr></table></figure><p>查看历史拉链表</p><p><img src="//jiamaoxiang.top/2019/11/08/历史拉链表实战/4.png" alt></p><ul><li>2019-11-10增量刷新历史拉链表</li></ul><p>将数据放进临时表</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">INSERT overwrite TABLE member_his_tmp</span><br><span class="line">SELECT *</span><br><span class="line">FROM</span><br><span class="line">  (</span><br><span class="line">-- 2019-11-10增量数据，代表最新的状态，该数据的生效时间是2019-11-10，过期时间为3000-12-31</span><br><span class="line">-- 这些增量的数据需要被全部加载到历史拉链表中</span><br><span class="line">SELECT member_id,</span><br><span class="line">       phoneno,</span><br><span class="line">       <span class="string">'2019-11-10'</span> effective_date,</span><br><span class="line">                    <span class="string">'3000-12-31'</span> expire_date</span><br><span class="line">   FROM member_delta</span><br><span class="line">   WHERE DAY=<span class="string">'2019-11-10'</span></span><br><span class="line">   UNION ALL </span><br><span class="line">-- 用当前为生效状态的拉链数据，去left join 增量数据，</span><br><span class="line">-- 如果匹配得上，则表示该数据已发生了更新，</span><br><span class="line">-- 此时，需要将发生更新的数据的过期时间更改为当前时间.</span><br><span class="line">-- 如果匹配不上，则表明该数据没有发生更新，此时过期时间不变</span><br><span class="line">SELECT a.member_id,</span><br><span class="line">       a.phoneno,</span><br><span class="line">       a.effective_date,</span><br><span class="line">       <span class="keyword">if</span>(b.member_id IS NULL, to_date(a.expire_date), to_date(b.day)) expire_date</span><br><span class="line">   FROM</span><br><span class="line">     (SELECT *</span><br><span class="line">      FROM member_his</span><br><span class="line">    ) a</span><br><span class="line">   LEFT JOIN</span><br><span class="line">     (SELECT *</span><br><span class="line">      FROM member_delta</span><br><span class="line">      WHERE DAY=<span class="string">'2019-11-10'</span>) b ON a.member_id=b.member_id)his</span><br></pre></td></tr></table></figure><p>查看历史拉链表</p><p><img src="//jiamaoxiang.top/2019/11/08/历史拉链表实战/5.png" alt></p><p>将以上脚本封装成shell调度的脚本</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天</span></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$1</span>"</span> ] ;<span class="keyword">then</span></span><br><span class="line">do_date=<span class="variable">$1</span></span><br><span class="line"><span class="keyword">else</span> </span><br><span class="line">do_date=`date -d <span class="string">"-1 day"</span> +%F`  </span><br><span class="line"><span class="keyword">fi</span> </span><br><span class="line"></span><br><span class="line">sql=<span class="string">"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">INSERT overwrite TABLE member_his_tmp</span></span><br><span class="line"><span class="string">SELECT *</span></span><br><span class="line"><span class="string">FROM</span></span><br><span class="line"><span class="string">  (</span></span><br><span class="line"><span class="string">-- 2019-11-10增量数据，代表最新的状态，该数据的生效时间是2019-11-10，过期时间为3000-12-31</span></span><br><span class="line"><span class="string">-- 这些增量的数据需要被全部加载到历史拉链表中</span></span><br><span class="line"><span class="string">SELECT member_id,</span></span><br><span class="line"><span class="string">       phoneno,</span></span><br><span class="line"><span class="string">       '<span class="variable">$do_date</span>' effective_date,</span></span><br><span class="line"><span class="string">       '3000-12-31' expire_date</span></span><br><span class="line"><span class="string">   FROM member_delta</span></span><br><span class="line"><span class="string">   WHERE DAY='<span class="variable">$do_date</span>'</span></span><br><span class="line"><span class="string">   UNION ALL </span></span><br><span class="line"><span class="string">-- 用当前为生效状态的拉链数据，去left join 增量数据，</span></span><br><span class="line"><span class="string">-- 如果匹配得上，则表示该数据已发生了更新，</span></span><br><span class="line"><span class="string">-- 此时，需要将发生更新的数据的过期时间更改为当前时间.</span></span><br><span class="line"><span class="string">-- 如果匹配不上，则表明该数据没有发生更新，此时过期时间不变</span></span><br><span class="line"><span class="string">SELECT a.member_id,</span></span><br><span class="line"><span class="string">       a.phoneno,</span></span><br><span class="line"><span class="string">       a.effective_date,</span></span><br><span class="line"><span class="string">       if(b.member_id IS NULL, to_date(a.expire_date), to_date(b.day)) expire_date</span></span><br><span class="line"><span class="string">   FROM</span></span><br><span class="line"><span class="string">     (SELECT *</span></span><br><span class="line"><span class="string">      FROM member_his</span></span><br><span class="line"><span class="string"> ) a</span></span><br><span class="line"><span class="string">   LEFT JOIN</span></span><br><span class="line"><span class="string">     (SELECT *</span></span><br><span class="line"><span class="string">      FROM member_delta</span></span><br><span class="line"><span class="string">      WHERE DAY='<span class="variable">$do_date</span>') b ON a.member_id=b.member_id)his;</span></span><br><span class="line"><span class="string">"</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$hive</span> -e <span class="string">"<span class="variable">$sql</span>"</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数据仓库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink运行架构剖析</title>
      <link href="/2019/10/23/Flink%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84%E5%89%96%E6%9E%90/"/>
      <url>/2019/10/23/Flink%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84%E5%89%96%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>本文主要介绍 Flink Runtime 的作业执行的核心机制。首先介绍 Flink Runtime 的整体架构以及 Job 的基本执行流程，然后介绍Flink 的Standalone运行架构，最后对Flink on YARN的两种模式进行了详细剖析。</p><a id="more"></a><h2 id="Flink-Runtime作业执行流程分析"><a href="#Flink-Runtime作业执行流程分析" class="headerlink" title="Flink Runtime作业执行流程分析"></a>Flink Runtime作业执行流程分析</h2><h3 id="整体架构图"><a href="#整体架构图" class="headerlink" title="整体架构图"></a>整体架构图</h3><p>Flink Runtime 层的主要架构如下图所示，它展示了一个 Flink 集群的基本结构。整体来说，它采用了标准 master-slave 的结构，master负责管理整个集群中的资源和作业；TaskExecutor 则是 Slave，负责提供具体的资源并实际执行作业。  </p><p><img src="//jiamaoxiang.top/2019/10/23/Flink运行架构剖析/Runtime.png" alt></p><h3 id="执行流程分析"><a href="#执行流程分析" class="headerlink" title="执行流程分析"></a>执行流程分析</h3><h4 id="组件介绍"><a href="#组件介绍" class="headerlink" title="组件介绍"></a>组件介绍</h4><p>Application Master 部分包含了三个组件，即 Dispatcher、ResourceManager 和 JobManager。其中，Dispatcher 负责接收用户提供的作业，并且负责为这个新提交的作业拉起一个新的 JobManager 组件。ResourceManager 负责资源的管理，在整个 Flink 集群中只有一个 ResourceManager。JobManager 负责管理作业的执行，在一个 Flink 集群中可能有多个作业同时执行，每个作业都有自己的 JobManager 组件。这三个组件都包含在 AppMaster 进程。  </p><p>TaskManager主要负责执行具体的task任务，<a href="https://jiamaoxiang.top/2019/08/23/Flink%E7%9A%84%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF-State-Backends/">StateBackend</a> 主要应用于状态的checkpoint。  </p><p>Cluster Manager是集群管理器，比如Standalone、YARN、K8s等。  </p><h4 id="流程分析"><a href="#流程分析" class="headerlink" title="流程分析"></a>流程分析</h4><p>1.当用户提交作业的时候，提交脚本会首先启动一个 Client进程负责作业的编译与提交。它首先将用户编写的代码编译为一个 JobGraph，在这个过程，它还会进行一些检查或优化等工作，例如判断哪些 Operator 可以 Chain 到同一个 Task 中。然后，Client 将产生的 JobGraph 提交到集群中执行。此时有两种情况，一种是类似于 Standalone 这种 Session 模式，AM 会预先启动，此时 Client 直接与 Dispatcher 建立连接并提交作业即可。另一种是 Per-Job 模式，AM 不会预先启动，此时 Client 将首先向资源管理系统 （如Yarn、K8S）申请资源来启动 AM，然后再向 AM 中的 Dispatcher 提交作业。  </p><p>2.当作业到 Dispatcher 后，Dispatcher 会首先启动一个 JobManager 组件，然后 JobManager 会向 ResourceManager 申请资源来启动作业中具体的任务。如果是Session模式，则TaskManager已经启动了，就可以直接分配资源。如果是per-Job模式，ResourceManager 也需要首先向外部资源管理系统申请资源来启动 TaskExecutor，然后等待 TaskExecutor 注册相应资源后再继续选择空闲资源进程分配，JobManager 收到 TaskExecutor 注册上来的 Slot 后，就可以实际提交 Task 了。  </p><p>3.TaskExecutor 收到 JobManager 提交的 Task 之后，会启动一个新的线程来执行该 Task。Task 启动后就会开始进行预先指定的计算，并通过数据 Shuffle 模块互相交换数据。</p><h2 id="Flink-Standalone运行架构"><a href="#Flink-Standalone运行架构" class="headerlink" title="Flink Standalone运行架构"></a>Flink Standalone运行架构</h2><p>Flink Standalone运行架构如下图所示：</p><p><img src="//jiamaoxiang.top/2019/10/23/Flink运行架构剖析/standalone.png" alt><br>Standalone模式需要先启动Jobmanager和TaskManager进程，每一个作业都是自己的JobManager。<br>Client：任务提交，生成JobGraph  </p><p>JobManager：调度Job，协调Task，通信，申请资源  </p><p>TaskManager：具体任务执行，请求资源</p><h2 id="Flink-On-YARN运行架构"><a href="#Flink-On-YARN运行架构" class="headerlink" title="Flink On YARN运行架构"></a>Flink On YARN运行架构</h2><p>关于YARN的基本架构原理，详见另一篇我的博客<a href="https://blog.csdn.net/jmx_bigdata/article/details/84320188" target="_blank" rel="noopener">YARN架构原理</a></p><h3 id="Per-Job模式"><a href="#Per-Job模式" class="headerlink" title="Per-Job模式"></a>Per-Job模式</h3><p>Per-job 模式下整个 Flink 集群只执行单个作业，即每个作业会独享 Dispatcher 和 ResourceManager 组件。此外，Per-job 模式下 AppMaster 和 TaskExecutor 都是按需申请的。因此，Per-job 模式更适合运行执行时间较长的大作业，这些作业对稳定性要求较高，并且对申请资源的时间不敏感。<br>1.独享Dispatcher与ResourceManager  </p><p>2.按需申请资源(TaskExecutor)  </p><p>3.适合执行时间较长的大作业  </p><p><img src="//jiamaoxiang.top/2019/10/23/Flink运行架构剖析/perjob.png" alt></p><h3 id="Session模式"><a href="#Session模式" class="headerlink" title="Session模式"></a>Session模式</h3><p>在 Session 模式下，Flink 预先启动 AppMaster 以及一组 TaskExecutor，然后在整个集群的生命周期中会执行多个作业。可以看出，Session 模式更适合规模小，执行时间短的作业。<br>1.共享Dispatcher与ResourceManager  </p><p>2.共享资源  </p><p>3.适合小规模，执行时间较短的作业  </p><p><img src="//jiamaoxiang.top/2019/10/23/Flink运行架构剖析/session.png" alt></p><p>Reference:<br>[1]<a href="https://ververica.cn/developers/advanced-tutorial-1-analysis-of-the-core-mechanism-of-runtime/" target="_blank" rel="noopener">https://ververica.cn/developers/advanced-tutorial-1-analysis-of-the-core-mechanism-of-runtime/</a><br>[2]<a href="https://ververica.cn/developers/flink-training-course2/" target="_blank" rel="noopener">https://ververica.cn/developers/flink-training-course2/</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>经典Hive-SQL面试题</title>
      <link href="/2019/10/15/%E7%BB%8F%E5%85%B8Hive-SQL%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
      <url>/2019/10/15/%E7%BB%8F%E5%85%B8Hive-SQL%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>HQL练习</p><a id="more"></a><h2 id="第一题"><a href="#第一题" class="headerlink" title="第一题"></a>第一题</h2><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">我们有如下的用户访问数据</span><br><span class="line">userId  visitDate   visitCount</span><br><span class="line">u01 2017/1/21   5</span><br><span class="line">u02 2017/1/23   6</span><br><span class="line">u03 2017/1/22   8</span><br><span class="line">u04 2017/1/20   3</span><br><span class="line">u01 2017/1/23   6</span><br><span class="line">u01 2017/2/21   8</span><br><span class="line">U02 2017/1/23   6</span><br><span class="line">U01 2017/2/22   4</span><br><span class="line">要求使用SQL统计出每个用户的累积访问次数，如下表所示：</span><br><span class="line">用户id    月份  小计  累积</span><br><span class="line">u01 2017-01 11  11</span><br><span class="line">u01 2017-02 12  23</span><br><span class="line">u02 2017-01 12  12</span><br><span class="line">u03 2017-01 8   8</span><br><span class="line">u04 2017-01 3   3</span><br></pre></td></tr></table></figure><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test1 ( </span><br><span class="line">userId string, </span><br><span class="line">visitDate string,</span><br><span class="line">visitCount INT )</span><br><span class="line">ROW format delimited FIELDS TERMINATED BY <span class="string">"\t"</span>;</span><br><span class="line">INSERT INTO TABLE test_sql.test1</span><br><span class="line">VALUES</span><br><span class="line">( <span class="string">'u01'</span>, <span class="string">'2017/1/21'</span>, 5 ),</span><br><span class="line">( <span class="string">'u02'</span>, <span class="string">'2017/1/23'</span>, 6 ),</span><br><span class="line">( <span class="string">'u03'</span>, <span class="string">'2017/1/22'</span>, 8 ),</span><br><span class="line">( <span class="string">'u04'</span>, <span class="string">'2017/1/20'</span>, 3 ),</span><br><span class="line">( <span class="string">'u01'</span>, <span class="string">'2017/1/23'</span>, 6 ),</span><br><span class="line">( <span class="string">'u01'</span>, <span class="string">'2017/2/21'</span>, 8 ),</span><br><span class="line">( <span class="string">'u02'</span>, <span class="string">'2017/1/23'</span>, 6 ),</span><br><span class="line">( <span class="string">'u01'</span>, <span class="string">'2017/2/22'</span>, 4 );</span><br></pre></td></tr></table></figure><h4 id="查询SQL"><a href="#查询SQL" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT t2.userid,</span><br><span class="line">   t2.visitmonth,</span><br><span class="line">   subtotal_visit_cnt,</span><br><span class="line">   sum(subtotal_visit_cnt) over (partition BY userid</span><br><span class="line"> ORDER BY visitmonth) AS total_visit_cnt</span><br><span class="line">FROM</span><br><span class="line">  (SELECT userid,</span><br><span class="line">  visitmonth,</span><br><span class="line">  sum(visitcount) AS subtotal_visit_cnt</span><br><span class="line">   FROM</span><br><span class="line"> (SELECT userid,</span><br><span class="line"> date_format(regexp_replace(visitdate,<span class="string">'/'</span>,<span class="string">'-'</span>),<span class="string">'yyyy-MM'</span>) AS visitmonth,</span><br><span class="line"> visitcount</span><br><span class="line">  FROM test_sql.test1) t1</span><br><span class="line">   GROUP BY userid,</span><br><span class="line">visitmonth)t2</span><br><span class="line">ORDER BY t2.userid,</span><br><span class="line"> t2.visitmonth</span><br></pre></td></tr></table></figure><h2 id="第二题"><a href="#第二题" class="headerlink" title="第二题"></a>第二题</h2><h3 id="需求-1"><a href="#需求-1" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">有50W个京东店铺，每个顾客访客访问任何一个店铺的任何一个商品时都会产生一条访问日志，</span><br><span class="line">访问日志存储的表名为Visit，访客的用户id为user_id，被访问的店铺名称为shop，数据如下：</span><br><span class="line"></span><br><span class="line">u1a</span><br><span class="line">u2b</span><br><span class="line">u1b</span><br><span class="line">u1a</span><br><span class="line">u3c</span><br><span class="line">u4b</span><br><span class="line">u1a</span><br><span class="line">u2c</span><br><span class="line">u5b</span><br><span class="line">u4b</span><br><span class="line">u6c</span><br><span class="line">u2c</span><br><span class="line">u1b</span><br><span class="line">u2a</span><br><span class="line">u2a</span><br><span class="line">u3a</span><br><span class="line">u5a</span><br><span class="line">u5a</span><br><span class="line">u5a</span><br><span class="line">请统计：</span><br><span class="line">(1)每个店铺的UV（访客数）</span><br><span class="line">(2)每个店铺访问次数top3的访客信息。输出店铺名称、访客id、访问次数</span><br></pre></td></tr></table></figure><h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-1"><a href="#数据准备-1" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test2 ( </span><br><span class="line"> user_id string, </span><br><span class="line"> shop string )</span><br><span class="line">ROW format delimited FIELDS TERMINATED BY <span class="string">'\t'</span>;</span><br><span class="line">INSERT INTO TABLE test_sql.test2 VALUES</span><br><span class="line">( <span class="string">'u1'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u2'</span>, <span class="string">'b'</span> ),</span><br><span class="line">( <span class="string">'u1'</span>, <span class="string">'b'</span> ),</span><br><span class="line">( <span class="string">'u1'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u3'</span>, <span class="string">'c'</span> ),</span><br><span class="line">( <span class="string">'u4'</span>, <span class="string">'b'</span> ),</span><br><span class="line">( <span class="string">'u1'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u2'</span>, <span class="string">'c'</span> ),</span><br><span class="line">( <span class="string">'u5'</span>, <span class="string">'b'</span> ),</span><br><span class="line">( <span class="string">'u4'</span>, <span class="string">'b'</span> ),</span><br><span class="line">( <span class="string">'u6'</span>, <span class="string">'c'</span> ),</span><br><span class="line">( <span class="string">'u2'</span>, <span class="string">'c'</span> ),</span><br><span class="line">( <span class="string">'u1'</span>, <span class="string">'b'</span> ),</span><br><span class="line">( <span class="string">'u2'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u2'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u3'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u5'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u5'</span>, <span class="string">'a'</span> ),</span><br><span class="line">( <span class="string">'u5'</span>, <span class="string">'a'</span> );</span><br></pre></td></tr></table></figure><h4 id="查询SQL实现"><a href="#查询SQL实现" class="headerlink" title="查询SQL实现"></a>查询SQL实现</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(1)方式1：</span><br><span class="line">SELECT shop,</span><br><span class="line">   count(DISTINCT user_id)</span><br><span class="line">FROM test_sql.test2</span><br><span class="line">GROUP BY shop</span><br><span class="line">方式2：</span><br><span class="line">SELECT t.shop,</span><br><span class="line">   count(*)</span><br><span class="line">FROM</span><br><span class="line">  (SELECT user_id,</span><br><span class="line">  shop</span><br><span class="line">   FROM test_sql.test2</span><br><span class="line">   GROUP BY user_id,</span><br><span class="line">shop) t</span><br><span class="line">GROUP BY t.shop</span><br><span class="line">(2)</span><br><span class="line">SELECT t2.shop,</span><br><span class="line">   t2.user_id,</span><br><span class="line">   t2.cnt</span><br><span class="line">FROM</span><br><span class="line">  (SELECT t1.*,</span><br><span class="line">  row_number() over(partition BY t1.shop</span><br><span class="line">ORDER BY t1.cnt DESC) rank</span><br><span class="line">   FROM</span><br><span class="line"> (SELECT user_id,</span><br><span class="line"> shop,</span><br><span class="line"> count(*) AS cnt</span><br><span class="line">  FROM test_sql.test2</span><br><span class="line">  GROUP BY user_id,</span><br><span class="line">   shop) t1)t2</span><br><span class="line">WHERE rank &lt;= 3</span><br></pre></td></tr></table></figure><h2 id="第三题"><a href="#第三题" class="headerlink" title="第三题"></a>第三题</h2><h3 id="需求-2"><a href="#需求-2" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">已知一个表STG.ORDER，有如下字段:Date，Order_id，User_id，amount。</span><br><span class="line">数据样例:2017-01-01,10029028,1000003251,33.57。</span><br><span class="line">请给出sql进行统计:</span><br><span class="line">(1)给出 2017年每个月的订单数、用户数、总成交金额。</span><br><span class="line">(2)给出2017年11月的新客数(指在11月才有第一笔订单)</span><br></pre></td></tr></table></figure><h3 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-2"><a href="#数据准备-2" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test3 ( </span><br><span class="line">dt string,</span><br><span class="line">order_id string, </span><br><span class="line">user_id string, </span><br><span class="line">amount DECIMAL ( 10, 2 ) )</span><br><span class="line">ROW format delimited FIELDS TERMINATED BY <span class="string">'\t'</span>;</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-01-01'</span>,<span class="string">'10029028'</span>,<span class="string">'1000003251'</span>,33.57);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-01-01'</span>,<span class="string">'10029029'</span>,<span class="string">'1000003251'</span>,33.57);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-01-01'</span>,<span class="string">'100290288'</span>,<span class="string">'1000003252'</span>,33.57);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-02-02'</span>,<span class="string">'10029088'</span>,<span class="string">'1000003251'</span>,33.57);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-02-02'</span>,<span class="string">'100290281'</span>,<span class="string">'1000003251'</span>,33.57);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-02-02'</span>,<span class="string">'100290282'</span>,<span class="string">'1000003253'</span>,33.57);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2017-11-02'</span>,<span class="string">'10290282'</span>,<span class="string">'100003253'</span>,234);</span><br><span class="line">INSERT INTO TABLE test_sql.test3 VALUES (<span class="string">'2018-11-02'</span>,<span class="string">'10290284'</span>,<span class="string">'100003243'</span>,234);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-1"><a href="#查询SQL-1" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(1)</span><br><span class="line">SELECT t1.mon,</span><br><span class="line">   count(t1.order_id) AS order_cnt,</span><br><span class="line">   count(DISTINCT t1.user_id) AS user_cnt,</span><br><span class="line">   sum(amount) AS total_amount</span><br><span class="line">FROM</span><br><span class="line">  (SELECT order_id,</span><br><span class="line">  user_id,</span><br><span class="line">  amount,</span><br><span class="line">  date_format(dt,<span class="string">'yyyy-MM'</span>) mon</span><br><span class="line">   FROM test_sql.test3</span><br><span class="line">   WHERE date_format(dt,<span class="string">'yyyy'</span>) = <span class="string">'2017'</span>) t1</span><br><span class="line">GROUP BY t1.mon</span><br><span class="line">(2)</span><br><span class="line">SELECT count(user_id)</span><br><span class="line">FROM test_sql.test3</span><br><span class="line">GROUP BY user_id</span><br><span class="line">HAVING date_format(min(dt),<span class="string">'yyyy-MM'</span>)=<span class="string">'2017-11'</span>;</span><br></pre></td></tr></table></figure><h2 id="第四题"><a href="#第四题" class="headerlink" title="第四题"></a>第四题</h2><h3 id="需求-3"><a href="#需求-3" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">有一个5000万的用户文件(user_id，name，age)，一个2亿记录的用户看电影的记录文件(user_id，url)，根据年龄段观看电影的次数进行排序？</span><br></pre></td></tr></table></figure><h3 id="实现-3"><a href="#实现-3" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-3"><a href="#数据准备-3" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test4user</span><br><span class="line">   (user_id string,</span><br><span class="line">name string,</span><br><span class="line">age int);</span><br><span class="line"></span><br><span class="line">CREATE TABLE test_sql.test4log</span><br><span class="line">(user_id string,</span><br><span class="line">url string);</span><br><span class="line"></span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'001'</span>,<span class="string">'u1'</span>,10);</span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'002'</span>,<span class="string">'u2'</span>,15);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'003'</span>,<span class="string">'u3'</span>,15);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'004'</span>,<span class="string">'u4'</span>,20);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'005'</span>,<span class="string">'u5'</span>,25);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'006'</span>,<span class="string">'u6'</span>,35);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'007'</span>,<span class="string">'u7'</span>,40);</span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'008'</span>,<span class="string">'u8'</span>,45);  </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'009'</span>,<span class="string">'u9'</span>,50);  </span><br><span class="line">INSERT INTO TABLE test_sql.test4user VALUES(<span class="string">'0010'</span>,<span class="string">'u10'</span>,65);  </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'001'</span>,<span class="string">'url1'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'002'</span>,<span class="string">'url1'</span>);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'003'</span>,<span class="string">'url2'</span>);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'004'</span>,<span class="string">'url3'</span>);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'005'</span>,<span class="string">'url3'</span>);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'006'</span>,<span class="string">'url1'</span>);   </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'007'</span>,<span class="string">'url5'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'008'</span>,<span class="string">'url7'</span>);  </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'009'</span>,<span class="string">'url5'</span>);  </span><br><span class="line">INSERT INTO TABLE test_sql.test4log VALUES(<span class="string">'0010'</span>,<span class="string">'url1'</span>);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-2"><a href="#查询SQL-2" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT </span><br><span class="line">t2.age_phase,</span><br><span class="line">sum(t1.cnt) as view_cnt</span><br><span class="line">FROM</span><br><span class="line"></span><br><span class="line">(SELECT user_id,</span><br><span class="line">  count(*) cnt</span><br><span class="line">FROM test_sql.test4log</span><br><span class="line">GROUP BY user_id) t1</span><br><span class="line">JOIN</span><br><span class="line">(SELECT user_id,</span><br><span class="line">  CASE WHEN age &lt;= 10 AND age &gt; 0 THEN <span class="string">'0-10'</span> </span><br><span class="line">  WHEN age &lt;= 20 AND age &gt; 10 THEN <span class="string">'10-20'</span></span><br><span class="line">  WHEN age &gt;20 AND age &lt;=30 THEN <span class="string">'20-30'</span></span><br><span class="line">  WHEN age &gt;30 AND age &lt;=40 THEN <span class="string">'30-40'</span></span><br><span class="line">  WHEN age &gt;40 AND age &lt;=50 THEN <span class="string">'40-50'</span></span><br><span class="line">  WHEN age &gt;50 AND age &lt;=60 THEN <span class="string">'50-60'</span></span><br><span class="line">  WHEN age &gt;60 AND age &lt;=70 THEN <span class="string">'60-70'</span></span><br><span class="line">  ELSE <span class="string">'70以上'</span> END as age_phase</span><br><span class="line">FROM test_sql.test4user) t2 ON t1.user_id = t2.user_id </span><br><span class="line">GROUP BY t2.age_phase</span><br></pre></td></tr></table></figure><h2 id="第五题"><a href="#第五题" class="headerlink" title="第五题"></a>第五题</h2><h3 id="需求-4"><a href="#需求-4" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">有日志如下，请写出代码求得所有用户和活跃用户的总数及平均年龄。（活跃用户指连续两天都有访问记录的用户）</span><br><span class="line">日期 用户 年龄</span><br><span class="line">2019-02-11,test_1,23</span><br><span class="line">2019-02-11,test_2,19</span><br><span class="line">2019-02-11,test_3,39</span><br><span class="line">2019-02-11,test_1,23</span><br><span class="line">2019-02-11,test_3,39</span><br><span class="line">2019-02-11,test_1,23</span><br><span class="line">2019-02-12,test_2,19</span><br><span class="line">2019-02-13,test_1,23</span><br><span class="line">2019-02-15,test_2,19</span><br><span class="line">2019-02-16,test_2,19</span><br></pre></td></tr></table></figure><h3 id="实现-4"><a href="#实现-4" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-4"><a href="#数据准备-4" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test5(</span><br><span class="line">dt string,</span><br><span class="line">user_id string,</span><br><span class="line">age int)</span><br><span class="line">ROW format delimited fields terminated BY <span class="string">','</span>;</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-11'</span>,<span class="string">'test_1'</span>,23);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-11'</span>,<span class="string">'test_2'</span>,19);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-11'</span>,<span class="string">'test_3'</span>,39);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-11'</span>,<span class="string">'test_1'</span>,23);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-11'</span>,<span class="string">'test_3'</span>,39);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-11'</span>,<span class="string">'test_1'</span>,23);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-12'</span>,<span class="string">'test_2'</span>,19);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-13'</span>,<span class="string">'test_1'</span>,23);</span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-15'</span>,<span class="string">'test_2'</span>,19);                                        </span><br><span class="line">INSERT INTO TABLE test_sql.test5 VALUES (<span class="string">'2019-02-16'</span>,<span class="string">'test_2'</span>,19);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-3"><a href="#查询SQL-3" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT sum(total_user_cnt) total_user_cnt,</span><br><span class="line">   sum(total_user_avg_age) total_user_avg_age,</span><br><span class="line">   sum(two_days_cnt) two_days_cnt,</span><br><span class="line">   sum(avg_age) avg_age</span><br><span class="line">FROM</span><br><span class="line">  (SELECT 0 total_user_cnt,</span><br><span class="line">  0 total_user_avg_age,</span><br><span class="line">  count(*) AS two_days_cnt,</span><br><span class="line">  cast(sum(age) / count(*) AS decimal(5,2)) AS avg_age</span><br><span class="line">   FROM</span><br><span class="line"> (SELECT user_id,</span><br><span class="line"> max(age) age</span><br><span class="line">  FROM</span><br><span class="line">(SELECT user_id,</span><br><span class="line">max(age) age</span><br><span class="line"> FROM</span><br><span class="line">   (SELECT user_id,</span><br><span class="line">   age,</span><br><span class="line">   date_sub(dt,rank) flag</span><br><span class="line">FROM</span><br><span class="line">  (SELECT dt,</span><br><span class="line">  user_id,</span><br><span class="line">  max(age) age,</span><br><span class="line">  row_number() over(PARTITION BY user_id</span><br><span class="line">ORDER BY dt) rank</span><br><span class="line">   FROM test_sql.test5</span><br><span class="line">   GROUP BY dt,</span><br><span class="line">user_id) t1) t2</span><br><span class="line"> GROUP BY user_id,</span><br><span class="line">  flag</span><br><span class="line"> HAVING count(*) &gt;=2) t3</span><br><span class="line">  GROUP BY user_id) t4</span><br><span class="line">   UNION ALL SELECT count(*) total_user_cnt,</span><br><span class="line">cast(sum(age) /count(*) AS decimal(5,2)) total_user_avg_age,</span><br><span class="line">0 two_days_cnt,</span><br><span class="line">0 avg_age</span><br><span class="line">   FROM</span><br><span class="line"> (SELECT user_id,</span><br><span class="line"> max(age) age</span><br><span class="line">  FROM test_sql.test5</span><br><span class="line">  GROUP BY user_id) t5) t6</span><br></pre></td></tr></table></figure><h2 id="第六题"><a href="#第六题" class="headerlink" title="第六题"></a>第六题</h2><h3 id="需求-5"><a href="#需求-5" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">请用sql写出所有用户中在今年10月份第一次购买商品的金额，</span><br><span class="line">表ordertable字段:</span><br><span class="line">(购买用户：userid，金额：money，购买时间：paymenttime(格式：2017-10-01)，订单id：orderid</span><br></pre></td></tr></table></figure><h3 id="实现-5"><a href="#实现-5" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-5"><a href="#数据准备-5" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test6 (</span><br><span class="line">userid string,</span><br><span class="line">money decimal(10,2),</span><br><span class="line">paymenttime string,</span><br><span class="line">orderid string);</span><br><span class="line"></span><br><span class="line">INSERT INTO TABLE test_sql.test6 VALUES(<span class="string">'001'</span>,100,<span class="string">'2017-10-01'</span>,<span class="string">'123'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test6 VALUES(<span class="string">'001'</span>,200,<span class="string">'2017-10-02'</span>,<span class="string">'124'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test6 VALUES(<span class="string">'002'</span>,500,<span class="string">'2017-10-01'</span>,<span class="string">'125'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test6 VALUES(<span class="string">'001'</span>,100,<span class="string">'2017-11-01'</span>,<span class="string">'126'</span>);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-4"><a href="#查询SQL-4" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT</span><br><span class="line">userid,</span><br><span class="line">paymenttime,</span><br><span class="line">money,</span><br><span class="line">orderid</span><br><span class="line">from</span><br><span class="line">(SELECT userid,</span><br><span class="line">   money,</span><br><span class="line">   paymenttime,</span><br><span class="line">   orderid,</span><br><span class="line">   row_number() over (PARTITION BY userid</span><br><span class="line">  ORDER BY paymenttime) rank</span><br><span class="line">FROM test_sql.test6</span><br><span class="line">WHERE date_format(paymenttime,<span class="string">'yyyy-MM'</span>) = <span class="string">'2017-10'</span>) t</span><br><span class="line">WHERE rank = 1</span><br></pre></td></tr></table></figure><h2 id="第七题"><a href="#第七题" class="headerlink" title="第七题"></a>第七题</h2><h3 id="需求-6"><a href="#需求-6" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">现有图书管理数据库的三个数据模型如下：</span><br><span class="line">图书（数据表名：BOOK）</span><br><span class="line">序号  字段名称    字段描述    字段类型</span><br><span class="line">1   BOOK_ID 总编号 文本</span><br><span class="line">2   SORT    分类号 文本</span><br><span class="line">3   BOOK_NAME   书名  文本</span><br><span class="line">4   WRITER  作者  文本</span><br><span class="line">5   OUTPUT  出版单位    文本</span><br><span class="line">6   PRICE   单价  数值（保留小数点后2位）</span><br><span class="line">读者（数据表名：READER）</span><br><span class="line">序号  字段名称    字段描述    字段类型</span><br><span class="line">1   READER_ID   借书证号    文本</span><br><span class="line">2   COMPANY 单位  文本</span><br><span class="line">3   NAME    姓名  文本</span><br><span class="line">4   SEX 性别  文本</span><br><span class="line">5   GRADE   职称  文本</span><br><span class="line">6   ADDR    地址  文本</span><br><span class="line">借阅记录（数据表名：BORROW LOG）</span><br><span class="line">序号  字段名称    字段描述    字段类型</span><br><span class="line">1   READER_ID   借书证号    文本</span><br><span class="line">2   BOOK_ID  总编号 文本</span><br><span class="line">3   BORROW_DATE  借书日期    日期</span><br><span class="line">（1）创建图书管理库的图书、读者和借阅三个基本表的表结构。请写出建表语句。</span><br><span class="line">（2）找出姓李的读者姓名（NAME）和所在单位（COMPANY）。</span><br><span class="line">（3）查找“高等教育出版社”的所有图书名称（BOOK_NAME）及单价（PRICE），结果按单价降序排序。</span><br><span class="line">（4）查找价格介于10元和20元之间的图书种类(SORT）出版单位（OUTPUT）和单价（PRICE），结果按出版单位（OUTPUT）和单价（PRICE）升序排序。</span><br><span class="line">（5）查找所有借了书的读者的姓名（NAME）及所在单位（COMPANY）。</span><br><span class="line">（6）求”科学出版社”图书的最高单价、最低单价、平均单价。</span><br><span class="line">（7）找出当前至少借阅了2本图书（大于等于2本）的读者姓名及其所在单位。</span><br><span class="line">（8）考虑到数据安全的需要，需定时将“借阅记录”中数据进行备份，请使用一条SQL语句，在备份用户bak下创建与“借阅记录”表结构完全一致的数据表BORROW_LOG_BAK.井且将“借阅记录”中现有数据全部复制到BORROW_L0G_ BAK中。</span><br><span class="line">（9）现在需要将原Oracle数据库中数据迁移至Hive仓库，请写出“图书”在Hive中的建表语句（Hive实现，提示：列分隔符|；数据表数据需要外部导入：分区分别以month＿part、day＿part 命名）</span><br><span class="line">（10）Hive中有表A，现在需要将表A的月分区　201505　中　user＿id为20000的user＿dinner字段更新为bonc8920，其他用户user＿dinner字段数据不变，请列出更新的方法步骤。（Hive实现，提示：Hlive中无update语法，请通过其他办法进行数据更新）</span><br></pre></td></tr></table></figure><h3 id="实现-6"><a href="#实现-6" class="headerlink" title="实现"></a>实现</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(1)</span><br><span class="line"></span><br><span class="line">-- 创建图书表book</span><br><span class="line"></span><br><span class="line">CREATE TABLE test_sql.book(book_id string,</span><br><span class="line">   `SORT` string,</span><br><span class="line">   book_name string,</span><br><span class="line">   writer string,</span><br><span class="line">   OUTPUT string,</span><br><span class="line">   price decimal(10,2));</span><br><span class="line">INSERT INTO TABLE test_sql.book VALUES (<span class="string">'001'</span>,<span class="string">'TP391'</span>,<span class="string">'信息处理'</span>,<span class="string">'author1'</span>,<span class="string">'机械工业出版社'</span>,<span class="string">'20'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.book VALUES (<span class="string">'002'</span>,<span class="string">'TP392'</span>,<span class="string">'数据库'</span>,<span class="string">'author12'</span>,<span class="string">'科学出版社'</span>,<span class="string">'15'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.book VALUES (<span class="string">'003'</span>,<span class="string">'TP393'</span>,<span class="string">'计算机网络'</span>,<span class="string">'author3'</span>,<span class="string">'机械工业出版社'</span>,<span class="string">'29'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.book VALUES (<span class="string">'004'</span>,<span class="string">'TP399'</span>,<span class="string">'微机原理'</span>,<span class="string">'author4'</span>,<span class="string">'科学出版社'</span>,<span class="string">'39'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.book VALUES (<span class="string">'005'</span>,<span class="string">'C931'</span>,<span class="string">'管理信息系统'</span>,<span class="string">'author5'</span>,<span class="string">'机械工业出版社'</span>,<span class="string">'40'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.book VALUES (<span class="string">'006'</span>,<span class="string">'C932'</span>,<span class="string">'运筹学'</span>,<span class="string">'author6'</span>,<span class="string">'科学出版社'</span>,<span class="string">'55'</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-- 创建读者表reader</span><br><span class="line"></span><br><span class="line">CREATE TABLE test_sql.reader (reader_id string,</span><br><span class="line">  company string,</span><br><span class="line">  name string,</span><br><span class="line">  sex string,</span><br><span class="line">  grade string,</span><br><span class="line">  addr string);</span><br><span class="line">INSERT INTO TABLE test_sql.reader VALUES (<span class="string">'0001'</span>,<span class="string">'阿里巴巴'</span>,<span class="string">'jack'</span>,<span class="string">'男'</span>,<span class="string">'vp'</span>,<span class="string">'addr1'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.reader VALUES (<span class="string">'0002'</span>,<span class="string">'百度'</span>,<span class="string">'robin'</span>,<span class="string">'男'</span>,<span class="string">'vp'</span>,<span class="string">'addr2'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.reader VALUES (<span class="string">'0003'</span>,<span class="string">'腾讯'</span>,<span class="string">'tony'</span>,<span class="string">'男'</span>,<span class="string">'vp'</span>,<span class="string">'addr3'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.reader VALUES (<span class="string">'0004'</span>,<span class="string">'京东'</span>,<span class="string">'jasper'</span>,<span class="string">'男'</span>,<span class="string">'cfo'</span>,<span class="string">'addr4'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.reader VALUES (<span class="string">'0005'</span>,<span class="string">'网易'</span>,<span class="string">'zhangsan'</span>,<span class="string">'女'</span>,<span class="string">'ceo'</span>,<span class="string">'addr5'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.reader VALUES (<span class="string">'0006'</span>,<span class="string">'搜狐'</span>,<span class="string">'lisi'</span>,<span class="string">'女'</span>,<span class="string">'ceo'</span>,<span class="string">'addr6'</span>);</span><br><span class="line"></span><br><span class="line">-- 创建借阅记录表borrow_log</span><br><span class="line"></span><br><span class="line">CREATE TABLE test_sql.borrow_log(reader_id string,</span><br><span class="line"> book_id string,</span><br><span class="line"> borrow_date string);</span><br><span class="line"> </span><br><span class="line">INSERT INTO TABLE test_sql.borrow_log VALUES (<span class="string">'0001'</span>,<span class="string">'002'</span>,<span class="string">'2019-10-14'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.borrow_log VALUES (<span class="string">'0002'</span>,<span class="string">'001'</span>,<span class="string">'2019-10-13'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.borrow_log VALUES (<span class="string">'0003'</span>,<span class="string">'005'</span>,<span class="string">'2019-09-14'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.borrow_log VALUES (<span class="string">'0004'</span>,<span class="string">'006'</span>,<span class="string">'2019-08-15'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.borrow_log VALUES (<span class="string">'0005'</span>,<span class="string">'003'</span>,<span class="string">'2019-10-10'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.borrow_log VALUES (<span class="string">'0006'</span>,<span class="string">'004'</span>,<span class="string">'2019-17-13'</span>);</span><br><span class="line"></span><br><span class="line">(2)</span><br><span class="line">SELECT name,</span><br><span class="line">   company</span><br><span class="line">FROM test_sql.reader</span><br><span class="line">WHERE name LIKE <span class="string">'李%'</span>;</span><br><span class="line">(3)</span><br><span class="line">SELECT book_name,</span><br><span class="line">   price</span><br><span class="line">FROM test_sql.book</span><br><span class="line">WHERE OUTPUT = <span class="string">"高等教育出版社"</span></span><br><span class="line">ORDER BY price DESC;</span><br><span class="line">(4)</span><br><span class="line">SELECT sort,</span><br><span class="line">   output,</span><br><span class="line">   price</span><br><span class="line">FROM test_sql.book</span><br><span class="line">WHERE price &gt;= 10 and price &lt;= 20</span><br><span class="line">ORDER BY output,price ;</span><br><span class="line">(5)</span><br><span class="line">SELECT b.name,</span><br><span class="line">   b.company</span><br><span class="line">FROM test_sql.borrow_log a</span><br><span class="line">JOIN test_sql.reader b ON a.reader_id = b.reader_id;</span><br><span class="line">(6)</span><br><span class="line">SELECT max(price),</span><br><span class="line">   min(price),</span><br><span class="line">   avg(price)</span><br><span class="line">FROM test_sql.book</span><br><span class="line">WHERE OUTPUT = <span class="string">'科学出版社'</span>;</span><br><span class="line">(7)</span><br><span class="line">SELECT b.name,</span><br><span class="line">   b.company</span><br><span class="line">FROM</span><br><span class="line">  (SELECT reader_id</span><br><span class="line">   FROM test_sql.borrow_log</span><br><span class="line">   GROUP BY reader_id</span><br><span class="line">   HAVING count(*) &gt;= 2) a</span><br><span class="line">JOIN test_sql.reader b ON a.reader_id = b.reader_id;</span><br><span class="line"></span><br><span class="line">(8)</span><br><span class="line">CREATE TABLE test_sql.borrow_log_bak AS</span><br><span class="line">SELECT *</span><br><span class="line">FROM test_sql.borrow_log;</span><br><span class="line">(9)</span><br><span class="line">CREATE TABLE book_hive ( </span><br><span class="line">book_id string,</span><br><span class="line">SORT string, </span><br><span class="line">book_name string,</span><br><span class="line">writer string, </span><br><span class="line">OUTPUT string, </span><br><span class="line">price DECIMAL ( 10, 2 ) )</span><br><span class="line">partitioned BY ( month_part string, day_part string )</span><br><span class="line">ROW format delimited FIELDS TERMINATED BY <span class="string">'\\|'</span> stored AS textfile;</span><br><span class="line">(10)</span><br><span class="line">方式1：配置hive支持事务操作，分桶表，orc存储格式</span><br><span class="line">方式2：第一步找到要更新的数据，将要更改的字段替换为新的值，第二步找到不需要更新的数据，第三步将上两步的数据插入一张新表中。</span><br></pre></td></tr></table></figure><h2 id="第八题"><a href="#第八题" class="headerlink" title="第八题"></a>第八题</h2><h3 id="需求-7"><a href="#需求-7" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">有一个线上服务器访问日志格式如下（用sql答题）</span><br><span class="line">时间                    接口                         ip地址</span><br><span class="line">2016-11-09 14:22:05/api/user/login110.23.5.33</span><br><span class="line">2016-11-09 14:23:10/api/user/detail57.3.2.16</span><br><span class="line">2016-11-09 15:59:40/api/user/login200.6.5.166</span><br><span class="line">… …</span><br><span class="line">求11月9号下午14点（14-15点），访问/api/user/login接口的top10的ip地址</span><br></pre></td></tr></table></figure><h3 id="实现-7"><a href="#实现-7" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-6"><a href="#数据准备-6" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test8(`date` string,</span><br><span class="line">interface string,</span><br><span class="line">ip string);</span><br><span class="line"></span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES (<span class="string">'2016-11-09 11:22:05'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'110.23.5.23'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES (<span class="string">'2016-11-09 11:23:10'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'57.3.2.16'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES (<span class="string">'2016-11-09 23:59:40'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'200.6.5.166'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 11:14:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'136.79.47.70'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 11:15:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'94.144.143.141'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 11:16:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'197.161.8.206'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 12:14:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'240.227.107.145'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 13:14:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'79.130.122.205'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:14:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'65.228.251.189'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:15:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'245.23.122.44'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:17:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'22.74.142.137'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:19:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'54.93.212.87'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:20:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'218.15.167.248'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:24:23'</span>,<span class="string">'/api/user/detail'</span>,<span class="string">'20.117.19.75'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 15:14:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'183.162.66.97'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 16:14:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'108.181.245.147'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:17:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'22.74.142.137'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test8 VALUES(<span class="string">'2016-11-09 14:19:23'</span>,<span class="string">'/api/user/login'</span>,<span class="string">'22.74.142.137'</span>);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-5"><a href="#查询SQL-5" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT ip,</span><br><span class="line">   count(*) AS cnt</span><br><span class="line">FROM test_sql.test8</span><br><span class="line">WHERE date_format(date,<span class="string">'yyyy-MM-dd HH'</span>) &gt;= <span class="string">'2016-11-09 14'</span></span><br><span class="line">  AND date_format(date,<span class="string">'yyyy-MM-dd HH'</span>) &lt; <span class="string">'2016-11-09 15'</span></span><br><span class="line">  AND interface=<span class="string">'/api/user/login'</span></span><br><span class="line">GROUP BY ip</span><br><span class="line">ORDER BY cnt desc</span><br><span class="line">LIMIT 10;</span><br></pre></td></tr></table></figure><h2 id="第九题"><a href="#第九题" class="headerlink" title="第九题"></a>第九题</h2><h3 id="需求-8"><a href="#需求-8" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">有一个充值日志表credit_log，字段如下：</span><br><span class="line"></span><br><span class="line">`dist_id` int  <span class="string">'区组id'</span>,</span><br><span class="line">`account` string  <span class="string">'账号'</span>,</span><br><span class="line">`money` int   <span class="string">'充值金额'</span>,</span><br><span class="line">`create_time` string  <span class="string">'订单时间'</span></span><br><span class="line"></span><br><span class="line">请写出SQL语句，查询充值日志表2019年01月02号每个区组下充值额最大的账号，要求结果：</span><br><span class="line">区组id，账号，金额，充值时间</span><br></pre></td></tr></table></figure><h3 id="实现-8"><a href="#实现-8" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-7"><a href="#数据准备-7" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test9(</span><br><span class="line">dist_id string COMMENT <span class="string">'区组id'</span>,</span><br><span class="line">account string COMMENT <span class="string">'账号'</span>,</span><br><span class="line">   `money` decimal(10,2) COMMENT <span class="string">'充值金额'</span>,</span><br><span class="line">create_time string COMMENT <span class="string">'订单时间'</span>);</span><br><span class="line"></span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'11'</span>,100006,<span class="string">'2019-01-02 13:00:01'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'22'</span>,110000,<span class="string">'2019-01-02 13:00:02'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'33'</span>,102000,<span class="string">'2019-01-02 13:00:03'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'44'</span>,100300,<span class="string">'2019-01-02 13:00:04'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'55'</span>,100040,<span class="string">'2019-01-02 13:00:05'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'66'</span>,100005,<span class="string">'2019-01-02 13:00:06'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'77'</span>,180000,<span class="string">'2019-01-03 13:00:07'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'88'</span>,106000,<span class="string">'2019-01-02 13:00:08'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'99'</span>,100400,<span class="string">'2019-01-02 13:00:09'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'12'</span>,100030,<span class="string">'2019-01-02 13:00:10'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'13'</span>,100003,<span class="string">'2019-01-02 13:00:20'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'14'</span>,100020,<span class="string">'2019-01-02 13:00:30'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'15'</span>,100500,<span class="string">'2019-01-02 13:00:40'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'16'</span>,106000,<span class="string">'2019-01-02 13:00:50'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'1'</span>,<span class="string">'17'</span>,100800,<span class="string">'2019-01-02 13:00:59'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'2'</span>,<span class="string">'18'</span>,100800,<span class="string">'2019-01-02 13:00:11'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'2'</span>,<span class="string">'19'</span>,100030,<span class="string">'2019-01-02 13:00:12'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'2'</span>,<span class="string">'10'</span>,100000,<span class="string">'2019-01-02 13:00:13'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'2'</span>,<span class="string">'45'</span>,100010,<span class="string">'2019-01-02 13:00:14'</span>);</span><br><span class="line">INSERT INTO TABLE test_sql.test9 VALUES (<span class="string">'2'</span>,<span class="string">'78'</span>,100070,<span class="string">'2019-01-02 13:00:15'</span>);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-6"><a href="#查询SQL-6" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">WITH TEMP AS</span><br><span class="line">  (SELECT dist_id,</span><br><span class="line">  account,</span><br><span class="line">  sum(`money`) sum_money</span><br><span class="line">   FROM test_sql.test9</span><br><span class="line">   WHERE date_format(create_time,<span class="string">'yyyy-MM-dd'</span>) = <span class="string">'2019-01-02'</span></span><br><span class="line">   GROUP BY dist_id,</span><br><span class="line">account)</span><br><span class="line">SELECT t1.dist_id,</span><br><span class="line">   t1.account,</span><br><span class="line">   t1.sum_money</span><br><span class="line">FROM</span><br><span class="line">  (SELECT temp.dist_id,</span><br><span class="line">  temp.account,</span><br><span class="line">  temp.sum_money,</span><br><span class="line">  rank() over(partition BY temp.dist_id</span><br><span class="line">  ORDER BY temp.sum_money DESC) ranks</span><br><span class="line">   FROM TEMP) t1</span><br><span class="line">WHERE ranks = 1</span><br></pre></td></tr></table></figure><h2 id="第十题"><a href="#第十题" class="headerlink" title="第十题"></a>第十题</h2><h3 id="需求-9"><a href="#需求-9" class="headerlink" title="需求"></a>需求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">有一个账号表如下，请写出SQL语句，查询各自区组的money排名前十的账号（分组取前10）</span><br><span class="line">dist_id string  <span class="string">'区组id'</span>,</span><br><span class="line">account string  <span class="string">'账号'</span>,</span><br><span class="line">gold     int    <span class="string">'金币'</span></span><br></pre></td></tr></table></figure><h3 id="实现-9"><a href="#实现-9" class="headerlink" title="实现"></a>实现</h3><h4 id="数据准备-8"><a href="#数据准备-8" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE TABLE test_sql.test10(</span><br><span class="line">`dist_id` string COMMENT <span class="string">'区组id'</span>,</span><br><span class="line">`account` string COMMENT <span class="string">'账号'</span>,</span><br><span class="line">`gold` int COMMENT <span class="string">'金币'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'77'</span>,18);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'88'</span>,106);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'99'</span>,10);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'12'</span>,13);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'13'</span>,14);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'14'</span>,25);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'15'</span>,36);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'16'</span>,12);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'1'</span>,<span class="string">'17'</span>,158);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'2'</span>,<span class="string">'18'</span>,12);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'2'</span>,<span class="string">'19'</span>,44);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'2'</span>,<span class="string">'10'</span>,66);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'2'</span>,<span class="string">'45'</span>,80);</span><br><span class="line">INSERT INTO TABLE test_sql.test10 VALUES (<span class="string">'2'</span>,<span class="string">'78'</span>,98);</span><br></pre></td></tr></table></figure><h4 id="查询SQL-7"><a href="#查询SQL-7" class="headerlink" title="查询SQL"></a>查询SQL</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SELECT dist_id,</span><br><span class="line">   account,</span><br><span class="line">   gold</span><br><span class="line">FROM</span><br><span class="line">(SELECT dist_id,</span><br><span class="line">  account,</span><br><span class="line">  gold,</span><br><span class="line">  row_number () over (PARTITION BY dist_id</span><br><span class="line">  ORDER BY gold DESC) rank</span><br><span class="line">FROM test_sql.test10) t</span><br><span class="line">WHERE rank &lt;= 10</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Impala使用的端口</title>
      <link href="/2019/08/29/Impala%E4%BD%BF%E7%94%A8%E7%9A%84%E7%AB%AF%E5%8F%A3/"/>
      <url>/2019/08/29/Impala%E4%BD%BF%E7%94%A8%E7%9A%84%E7%AB%AF%E5%8F%A3/</url>
      
        <content type="html"><![CDATA[<p>本文主要介绍了Impala所使用的端口号，在部署Impala的时候，确保下面列出的端口是开启的。</p><a id="more"></a><table><thead><tr><th align="center">组件</th><th>服务</th><th>端口</th><th align="center"><span style="white-space:nowrap;">访问需求&emsp;&emsp;</span></th><th>备注</th></tr></thead><tbody><tr><td align="center">Impala Daemon</td><td>Impala Daemon Frontend Port</td><td>21000</td><td align="center">外部</td><td>被 impala-shell, Beeswax, Cloudera ODBC 1.2 驱动 用于传递命令和接收结果</td></tr><tr><td align="center">Impala Daemon</td><td>Impala Daemon Frontend Port</td><td>21050</td><td align="center">外部</td><td>被使用 JDBC 或 Cloudera ODBC 2.0 及以上驱动的诸如 BI 工具之类的应用用来传递命令和接收结果</td></tr><tr><td align="center">Impala Daemon</td><td>Impala Daemon Backend Port</td><td>22000</td><td align="center">内部</td><td>仅内部使用。Impala</td></tr><tr><td align="center">Impala Daemon</td><td>StateStoreSubscriber Service Port</td><td>23000</td><td align="center">内部</td><td>仅内部使用。Impala 守护进程监听该端口接收来源于 state store 的更新</td></tr><tr><td align="center">Catalog Daemon</td><td>StateStoreSubscriber Service Port</td><td>23020</td><td align="center">内部</td><td>仅内部使用，catalog daemon监听该端口接收来源于 state store 的更新</td></tr><tr><td align="center">Impala Daemon</td><td>Impala Daemon HTTP Server Port</td><td>25000</td><td align="center">外部</td><td>Impala Daemon的Web端口，用于管理员监控和线上故障排查</td></tr><tr><td align="center">Impala StateStore Daemon</td><td>StateStore HTTP Server Port</td><td>25010</td><td align="center">外部</td><td>StateStore的Web端口，用于管理员监控和线上故障排查</td></tr><tr><td align="center">Impala Catalog Daemon</td><td>Catalog HTTP Server Port</td><td>25020</td><td align="center">外部</td><td>Catalog的Web端口，用于管理员监控和线上故障排查，从Impala1.2开始加入</td></tr><tr><td align="center">Impala StateStore Daemon</td><td>StateStore Service Port</td><td>24000</td><td align="center">内部</td><td>仅内部使用，statestore daemon监听的端口，用于registration/unregistration请求</td></tr><tr><td align="center">Impala Catalog Daemon</td><td>Catalog Service Port</td><td>26000</td><td align="center">内部</td><td>仅内部使用，catalog服务使用此端口与Impala Daemon进行通信，从Impala1.2开始加入</td></tr><tr><td align="center">Impala Daemon</td><td>KRPC Port</td><td>27000</td><td align="center">内部</td><td>仅内部使用，Impala daemon使用此端口进行基于krpc的相互通信。</td></tr></tbody></table><hr><p>Refrence:<a href="https://www.cloudera.com/documentation/enterprise/6/latest/topics/impala_ports.html#ports" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/6/latest/topics/impala_ports.html#ports</a></p>]]></content>
      
      
      <categories>
          
          <category> Impala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Impala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban安装部署</title>
      <link href="/2019/08/28/Azkaban%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
      <url>/2019/08/28/Azkaban%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<p>Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。</p><a id="more"></a><h1 id="安装前准备"><a href="#安装前准备" class="headerlink" title="安装前准备"></a>安装前准备</h1><p>(1)将Azkaban Web服务器、Azkaban执行服务器要安装机器的/opt/software目录下：  </p><ul><li>azkaban-web-server-2.5.0.tar.gz  </li><li>azkaban-executor-server-2.5.0.tar.gz  </li><li>azkaban-sql-script-2.5.0.tar.gz  </li><li>mysql-libs.zip  </li></ul><p>(2)目前azkaban只支持 mysql作为元数据库，需安装mysql，本文档中默认已安装好mysql服务器</p><h1 id="安装Azkaban"><a href="#安装Azkaban" class="headerlink" title="安装Azkaban"></a>安装Azkaban</h1><p>(1)在/opt/module/目录下创建azkaban目录<br>(2)解压azkaban-web-server-2.5.0.tar.gz、azkaban-executor-server-2.5.0.tar.gz、azkaban-sql-script-2.5.0.tar.gz到/opt/module/azkaban目录下<br>解压完成后的文件夹如下图所示：<br><img src="//jiamaoxiang.top/2019/08/28/Azkaban安装部署/1.png" alt><br>(3)初始化Azkaban的元数据库<br>登录mysql，创建azkaban的数据库，并执行脚本create-all-sql-2.5.0.sql，如下所示：  </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; create database azkaban;  </span><br><span class="line">Query OK, 1 row affected (0.00 sec)  </span><br><span class="line">mysql&gt; use azkaban;  </span><br><span class="line">Database changed  </span><br><span class="line">mysql&gt; <span class="built_in">source</span> /opt/module/azkaban/azkaban-2.5.0/create-all-sql-2.5.0.sql</span><br></pre></td></tr></table></figure><h2 id="创建SSL配置"><a href="#创建SSL配置" class="headerlink" title="创建SSL配置"></a>创建SSL配置</h2><p>(1)生成 keystore的密码及相应信息  </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban]$ keytool -keystore keystore -<span class="built_in">alias</span> jetty -genkey -keyalg RSA  </span><br><span class="line">输入keystore密码：   </span><br><span class="line">再次输入新密码:    </span><br><span class="line">您的名字与姓氏是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您的组织单位名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您的组织名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您所在的城市或区域名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">您所在的州或省份名称是什么？    </span><br><span class="line">[Unknown]：     </span><br><span class="line">该单位的两字母国家代码是什么    </span><br><span class="line">[Unknown]：  CN    </span><br><span class="line">CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN 正确吗？  </span><br><span class="line">[否]：  y    </span><br><span class="line"> </span><br><span class="line">输入&lt;jetty&gt;的主密码    </span><br><span class="line">（如果和 keystore 密码相同，按回车）  ：</span><br></pre></td></tr></table></figure><p>(2)将keystore 考贝到 azkaban web服务器根目录中</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban]<span class="comment"># mv keystore  azkaban-web-2.5.0/</span></span><br></pre></td></tr></table></figure><h2 id="Web服务器配置"><a href="#Web服务器配置" class="headerlink" title="Web服务器配置"></a>Web服务器配置</h2><p>(1)进入azkaban web服务器安装目录 conf目录，修改azkaban.properties文件<br>(2)按照如下配置修改azkaban.properties文件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Azkaban Personalization Settings      </span></span><br><span class="line"><span class="comment">#服务器UI名称,用于服务器上方显示的名字      </span></span><br><span class="line">azkaban.name=Test      </span><br><span class="line"><span class="comment">#描述                            </span></span><br><span class="line">azkaban.label=My Local Azkaban     </span><br><span class="line"><span class="comment">#UI颜色                         </span></span><br><span class="line">azkaban.color=<span class="comment">#FF3601                                         </span></span><br><span class="line">azkaban.default.servlet.path=/index  </span><br><span class="line"><span class="comment">#根web目录,配置绝对路径，即web的目录  </span></span><br><span class="line">web.resource.dir=/opt/module/azkaban/azkaban-web-2.5.0/web   </span><br><span class="line"><span class="comment">#默认时区,已改为亚洲/上海 默认为美国                                            </span></span><br><span class="line">default.timezone.id=Asia/Shanghai                           </span><br><span class="line"><span class="comment">#用户权限管理默认类  </span></span><br><span class="line">user.manager.class=azkaban.user.XmlUserManager    </span><br><span class="line"><span class="comment">#用户配置,配置绝对路径，即azkaban-users.xml的路径     </span></span><br><span class="line">user.manager.xml.file=/opt/module/azkaban/azkaban-web-2.5.0/conf/azkaban-users.xml             </span><br><span class="line"><span class="comment">#Loader for projects .global配置文件所在位置,即global.properties绝对路径    </span></span><br><span class="line">executor.global.properties=/opt/module/azkaban/azkaban-executor-2.5.0/conf/global.properties    </span><br><span class="line">azkaban.project.dir=projects                                                 </span><br><span class="line"><span class="comment">#数据库类型  </span></span><br><span class="line">database.type=mysql                                                            </span><br><span class="line">mysql.port=3306                                                                  </span><br><span class="line">mysql.host=cdh01                                                    </span><br><span class="line">mysql.database=azkaban                                                      </span><br><span class="line">mysql.user=root  </span><br><span class="line"><span class="comment">#数据库密码                                                               </span></span><br><span class="line">mysql.password=123qwe                                                     </span><br><span class="line">mysql.numconnections=100                                                </span><br><span class="line"><span class="comment"># Velocity dev mode   </span></span><br><span class="line">velocity.dev.mode=<span class="literal">false</span>  </span><br><span class="line"><span class="comment"># Jetty服务器属性.  </span></span><br><span class="line"><span class="comment">#最大线程数     </span></span><br><span class="line">jetty.maxThreads=25   </span><br><span class="line"><span class="comment">#Jetty SSL端口                                                                 </span></span><br><span class="line">jetty.ssl.port=8443  </span><br><span class="line"><span class="comment">#Jetty端口                                                                      </span></span><br><span class="line">jetty.port=8081    </span><br><span class="line"><span class="comment">#SSL文件名,即keystore绝对路径                                                                              </span></span><br><span class="line">jetty.keystore=/opt/module/azkaban/azkaban-web-2.5.0/keystore    </span><br><span class="line"><span class="comment">#SSL文件密码,本配置与keystore密码相同                                                           </span></span><br><span class="line">jetty.password=123qwe  </span><br><span class="line"><span class="comment">#Jetty主密码 与 keystore文件相同                                                          </span></span><br><span class="line">jetty.keypassword=123qwe  </span><br><span class="line"><span class="comment">#SSL文件名,即keystore绝对路径                                                            </span></span><br><span class="line">jetty.truststore=/opt/module/azkaban/azkaban-web-2.5.0/keystore    </span><br><span class="line"><span class="comment"># SSL文件密码                                                             </span></span><br><span class="line">jetty.trustpassword=123qwe                                                    </span><br><span class="line"><span class="comment"># 执行服务器属性, 执行服务器端口  </span></span><br><span class="line">executor.port=12321                                                                </span><br><span class="line"><span class="comment"># 邮件设置,发送邮箱    </span></span><br><span class="line">mail.sender=xxxxxxxx@163.com    </span><br><span class="line"><span class="comment">#发送邮箱smtp地址                                           </span></span><br><span class="line">mail.host=smtp.163.com     </span><br><span class="line"><span class="comment">#发送邮件时显示的名称                                                            </span></span><br><span class="line">mail.user=xxxxxxxx  </span><br><span class="line"><span class="comment">#邮箱密码                                            </span></span><br><span class="line">mail.password=**********   </span><br><span class="line"><span class="comment">#任务失败时发送邮件的地址                                                        </span></span><br><span class="line">job.failure.email=xxxxxxxx@163.com   </span><br><span class="line"><span class="comment">#任务成功时发送邮件的地址                                 </span></span><br><span class="line">job.success.email=xxxxxxxx@163.com                            </span><br><span class="line">lockdown.create.projects=<span class="literal">false</span>    </span><br><span class="line"><span class="comment">#缓存目录                                            </span></span><br><span class="line">cache.directory=cache</span><br></pre></td></tr></table></figure><p>(3)web服务器用户配置<br>在azkaban web服务器安装目录 conf目录，按照如下配置修改azkaban-users.xml 文件，增加管理员用户。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;azkaban-users&gt;  </span><br><span class="line">       &lt;user username=<span class="string">"azkaban"</span> password=<span class="string">"azkaban"</span> roles=<span class="string">"admin"</span> groups=<span class="string">"azkaban"</span> /&gt;    </span><br><span class="line">       &lt;user username=<span class="string">"metrics"</span> password=<span class="string">"metrics"</span> roles=<span class="string">"metrics"</span>/&gt;  </span><br><span class="line">       &lt;user username=<span class="string">"admin"</span> password=<span class="string">"admin"</span> roles=<span class="string">"admin,metrics"</span> /&gt;  </span><br><span class="line">       &lt;role name=<span class="string">"admin"</span> permissions=<span class="string">"ADMIN"</span> /&gt;  </span><br><span class="line">       &lt;role name=<span class="string">"metrics"</span> permissions=<span class="string">"METRICS"</span>/&gt;    </span><br><span class="line">&lt;/azkaban-users&gt;</span><br></pre></td></tr></table></figure><h2 id="executor服务器配置"><a href="#executor服务器配置" class="headerlink" title="executor服务器配置"></a>executor服务器配置</h2><p>(1)进入executor安装目录，修改azkaban.properties</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Azkaban  </span></span><br><span class="line">default.timezone.id=Asia/Shanghai  </span><br><span class="line"><span class="comment"># Azkaban JobTypes Plugins  </span></span><br><span class="line">azkaban.jobtype.plugin.dir=./../plugins/jobtypes  </span><br><span class="line"><span class="comment">#Loader for projects  </span></span><br><span class="line">executor.global.properties=/opt/module/azkaban/azkaban-executor-2.5.0/conf/global.properties  </span><br><span class="line">azkaban.project.dir=projects  </span><br><span class="line">database.type=mysql  </span><br><span class="line">mysql.port=3306  </span><br><span class="line">mysql.host=cdh01  </span><br><span class="line">mysql.database=azkaban  </span><br><span class="line">mysql.user=root  </span><br><span class="line">mysql.password=123qwe    </span><br><span class="line">mysql.numconnections=100    </span><br><span class="line"><span class="comment"># Azkaban Executor settings  </span></span><br><span class="line">executor.maxThreads=50  </span><br><span class="line">executor.port=12321  </span><br><span class="line">executor.flow.threads=30</span><br></pre></td></tr></table></figure><h1 id="启动web服务器"><a href="#启动web服务器" class="headerlink" title="启动web服务器"></a>启动web服务器</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban-web-2.5.0]<span class="comment"># bin/azkaban-web-start.sh  &amp;</span></span><br></pre></td></tr></table></figure><h1 id="启动executor服务器"><a href="#启动executor服务器" class="headerlink" title="启动executor服务器"></a>启动executor服务器</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@cdh02 azkaban-executor-2.5.0]<span class="comment"># bin/azkaban-executor-start.sh  &amp;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的数据类型</title>
      <link href="/2019/08/27/Flink%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
      <url>/2019/08/27/Flink%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>Flink使用type information来代表数据类型，Flink还具有一个类型提取系统，该系统分析函数的输入和返回类型，以自动获取类型信息(type information)，从而获得序列化程序和反序列化程序。但是，在某些情况下，例如lambda函数或泛型类型，需要显式地提供类型信息((type information)),从而提高其性能。本文主要讨论包括：(1)Flink支持的数据类型,(2)如何为数据类型创建type information，（3）如果无法自动推断函数的返回类型，如何使用提示(hints)来帮助Flink的类型系统识别类型信息。</p><a id="more"></a><h2 id="支持的数据类型"><a href="#支持的数据类型" class="headerlink" title="支持的数据类型"></a>支持的数据类型</h2><p>Flink支持Java和Scala中所有常见的数据类型，使用比较广泛的类型主要包括以下五种：</p><ul><li>原始类型  </li><li>Java和Scala的tuple类型  </li><li>Scala样例类  </li><li>POJO类型  </li><li>一些特殊的类型  </li></ul><p><strong>NOTE：</strong>不能被处理的类型将会被视为普通的数据类型，通过Kyro序列化框架进行序列化。</p><h3 id="原始类型"><a href="#原始类型" class="headerlink" title="原始类型"></a>原始类型</h3><p>Flink支持所有Java和Scala的原始类型，比如Int(Java中的Integer)，String、Double等。下面的例子是处理一个Long类型的数据流，处理每个元素+1  </p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> numbers: <span class="type">DataStream</span>[<span class="type">Long</span>] = env.fromElements(<span class="number">1</span>L, <span class="number">2</span>L,<span class="number">3</span>L, <span class="number">4</span>L)  </span><br><span class="line">numbers.map( n =&gt; n + <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="Java和Scala的tuple类型"><a href="#Java和Scala的tuple类型" class="headerlink" title="Java和Scala的tuple类型"></a>Java和Scala的tuple类型</h3><p>基于Scala的DataStream API使用的Scala的tuple。下面的例子是过滤一个具有两个字段的tuple类型的数据流.  </p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// DataStream of Tuple2[String, Integer] for Person(name,age)  </span></span><br><span class="line"><span class="keyword">val</span> persons: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">Integer</span>)] = env.fromElements((<span class="string">"Adam"</span>, <span class="number">17</span>),(<span class="string">"Sarah"</span>, <span class="number">23</span>))  </span><br><span class="line"><span class="comment">// filter for persons of age &gt; 18  </span></span><br><span class="line">persons.filter(p =&gt; p._2 &gt; <span class="number">18</span>)</span><br></pre></td></tr></table></figure><p>Flink提供了有效的Java tuple实现，Flink的Java tuple最多包括25个字段，分别为tuple1，tuple2，直到tuple25，tuple类型是强类型的。使用Java DataStream API重写上面的例子:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// DataStream of Tuple2&lt;String, Integer&gt; for Person(name,age)  </span></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; persons =env.fromElements(Tuple2.of(<span class="string">"Adam"</span>, <span class="number">17</span>),Tuple2.of(<span class="string">"Sarah"</span>,<span class="number">23</span>));  </span><br><span class="line"><span class="comment">// filter for persons of age &gt; 18  </span></span><br><span class="line">persons.filter(p -&gt; p.f1 &gt; <span class="number">18</span>);</span><br></pre></td></tr></table></figure><p>Tuple字段可以通过使用f0，f1，f2的形式访问，也可以通过getField(int pos)方法访问，参数的索引起始值为0，比如:</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Tuple2&lt;String, Integer&gt; personTuple = Tuple2.of(<span class="string">"Alex"</span>,<span class="string">"42"</span>);  </span><br><span class="line">Integer age = personTuple.getField(<span class="number">1</span>); <span class="comment">// age = 42</span></span><br></pre></td></tr></table></figure><p>与Scala相比，Flink的Java tuple是可变的，所以tuple的元素值是可以被重新复制的。Function可以重用Java tuple,从而减小垃圾回收的压力。下面的例子展示了如何更新一个tuple字段值</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">personTuple.f1 = <span class="number">42</span>; <span class="comment">// set the 2nd field to 42     </span></span><br><span class="line">personTuple.setField(<span class="number">43</span>, <span class="number">1</span>); <span class="comment">// set the 2nd field to 43</span></span><br></pre></td></tr></table></figure><h3 id="Scala的样例类"><a href="#Scala的样例类" class="headerlink" title="Scala的样例类"></a>Scala的样例类</h3><p>Flink支持Scala的样例类，可以通过字段名称来访问样例类的字段，下面的例子定义了一个<code>Person</code>样例类，该样例类有两个字段：<code>name</code>和<code>age</code>,按<code>age</code>过滤DataStream，如下所示</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)  </span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">persons</span></span>: <span class="type">DataStream</span>[<span class="type">Person</span>] = env.fromElements(<span class="type">Person</span>(<span class="string">"Adam"</span>, <span class="number">17</span>),<span class="type">Person</span>(<span class="string">"Sarah"</span>, <span class="number">23</span>))  </span><br><span class="line"><span class="comment">// filter for persons with age &gt; 18  </span></span><br><span class="line">persons.filter(p =&gt; p.age &gt; <span class="number">18</span>)</span><br></pre></td></tr></table></figure><h3 id="POJO"><a href="#POJO" class="headerlink" title="POJO"></a>POJO</h3><p>Flink接受的POJO类型需满足以下条件：</p><ul><li>public 类  </li><li>无参的共有构造方法  </li><li>所有字段都是public的，可以通过getter和setter方法访问  </li><li>所有字段类型必须是Flink能够支持的<br>下面的例子定义一个<code>Person</code>POJO</li></ul><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;  </span><br><span class="line"><span class="comment">// both fields are public  </span></span><br><span class="line"><span class="keyword">public</span> String name;  </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">int</span> age;  </span><br><span class="line"><span class="comment">// default constructor is present  </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">()</span> </span>&#123;&#125;  </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(String name, <span class="keyword">int</span> age)</span> </span>&#123;  </span><br><span class="line"><span class="keyword">this</span>.name = name;  </span><br><span class="line"><span class="keyword">this</span>.age = age;  </span><br><span class="line">&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">DataStream&lt;Person&gt; persons = env.fromElements(   </span><br><span class="line"><span class="keyword">new</span> Person(<span class="string">"Alex"</span>, <span class="number">42</span>),  </span><br><span class="line"><span class="keyword">new</span> Person(<span class="string">"Wendy"</span>, <span class="number">23</span>));</span><br></pre></td></tr></table></figure><h3 id="一些特殊的类型"><a href="#一些特殊的类型" class="headerlink" title="一些特殊的类型"></a>一些特殊的类型</h3><p>Flink支持一些有特殊作用的数据类型，比如Array，Java中的ArrayList、HashMap和Enum等，也支持Hadoop的Writable类型。  </p><h2 id="为数据类型创建类型信息-type-information"><a href="#为数据类型创建类型信息-type-information" class="headerlink" title="为数据类型创建类型信息(type information)"></a>为数据类型创建类型信息(type information)</h2><h2 id="显示地指定类型信息-type-information"><a href="#显示地指定类型信息-type-information" class="headerlink" title="显示地指定类型信息(type information)"></a>显示地指定类型信息(type information)</h2>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于SparkStreaming的日志分析项目</title>
      <link href="/2019/08/26/%E5%9F%BA%E4%BA%8ESparkStreaming%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E9%A1%B9%E7%9B%AE/"/>
      <url>/2019/08/26/%E5%9F%BA%E4%BA%8ESparkStreaming%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E9%A1%B9%E7%9B%AE/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp;基于SparkStreaming实现实时的日志分析，首先基于discuz搭建一个论坛平台，然后将该论坛的日志写入到指定文件，最后通过SparkStreaming实时对日志进行分析。</p><a id="more"></a><h1 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h1><ul><li>统计指定时间段的热门文章</li></ul><ul><li>统计指定时间段内的最受欢迎的用户（以 ip 为单位）</li></ul><ul><li>统计指定时间段内的不同模块的访问量  </li></ul><h1 id="项目架构"><a href="#项目架构" class="headerlink" title="项目架构"></a>项目架构</h1><p><img src="//jiamaoxiang.top/2019/08/26/基于SparkStreaming的日志分析项目/%E9%A1%B9%E7%9B%AE%E6%9E%B6%E6%9E%84.png" alt></p><h1 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h1><p>resources<br>&emsp;&emsp;&emsp;&emsp;access_log.txt:日志样例<br>&emsp;&emsp;&emsp;&emsp;log_sta.conf ：配置文件<br>scala.com.jmx.analysis<br>&emsp;&emsp;&emsp;&emsp;AccessLogParser.scala :日志解析<br>&emsp;&emsp;&emsp;&emsp;logAnalysis：日志分析<br>scala.com.jmx.util<br>&emsp;&emsp;&emsp;&emsp;Utility.scala:工具类<br>scala<br>&emsp;&emsp;&emsp;&emsp;Run：驱动程序(main)<br>具体代码详见<a href="https://github.com/jiamx/log_analysis" target="_blank" rel="noopener">github</a></p><h1 id="搭建discuz论坛"><a href="#搭建discuz论坛" class="headerlink" title="搭建discuz论坛"></a>搭建discuz论坛</h1><h2 id="安装XAMPP"><a href="#安装XAMPP" class="headerlink" title="安装XAMPP"></a>安装XAMPP</h2><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><p><code>wget https://www.apachefriends.org/xampp-files/5.6.33/xampp-linux-x64-5.6.33-0-installer.run</code></p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><code># 赋予文件执行权限</code><br><code>chmod u+x xampp-linux-x64-5.6.33-0-installer.run</code><br><code># 运行安装文件</code><br>`./xampp-linux-x64-5.6.33-0-installer.run``</p><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><p>将以下内容加入到 ~/.bash_profile<br><code>export XAMPP=/opt/lampp/</code><br><code>export PATH=$PATH:$XAMPP:$XAMPP/bin</code>   </p><h3 id="刷新环境变量"><a href="#刷新环境变量" class="headerlink" title="刷新环境变量"></a>刷新环境变量</h3><p><code>source ~/.bash_profile</code></p><h3 id="启动XAMPP"><a href="#启动XAMPP" class="headerlink" title="启动XAMPP"></a>启动XAMPP</h3><p><code>xampp restart</code></p><h2 id="root用户密码和权限修改"><a href="#root用户密码和权限修改" class="headerlink" title="root用户密码和权限修改"></a>root用户密码和权限修改</h2><p><code>#修改root用户密码为123</code><br><code>update mysql.user set password=PASSWORD(&#39;123&#39;) where user=&#39;root&#39;;</code><br><code>flush privileges;</code><br><code>#赋予root用户远程登录权限</code><br><code>grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;123&#39; with grant option;</code><br><code>flush privileges;</code>  </p><h2 id="安装Discuz"><a href="#安装Discuz" class="headerlink" title="安装Discuz"></a>安装Discuz</h2><h3 id="下载discuz"><a href="#下载discuz" class="headerlink" title="下载discuz"></a>下载discuz</h3><p><code>wget http://download.comsenz.com/DiscuzX/3.2/Discuz_X3.2_SC_UTF8.zip</code>  </p><h3 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h3><p><code>#删除原有的web应用</code><br><code>rm -rf /opt/lampp/htdocs/*</code><br><code>unzip Discuz_X3.2_SC_UTF8.zip –d /opt/lampp/htdocs/</code><br><code>cd /opt/lampp/htdocs/</code><br><code>mv upload/*</code><br><code>#修改目录权限</code><br><code>chmod 777 -R /opt/lampp/htdocs/config/</code><br><code>chmod 777 -R /opt/lampp/htdocs/data/</code><br><code>chmod 777 -R /opt/lampp/htdocs/uc_client/</code><br><code>chmod 777 -R /opt/lampp/htdocs/uc_server/</code>  </p><h2 id="Discuz基本操作"><a href="#Discuz基本操作" class="headerlink" title="Discuz基本操作"></a>Discuz基本操作</h2><h3 id="自定义版块"><a href="#自定义版块" class="headerlink" title="自定义版块"></a>自定义版块</h3><ul><li>进入discuz后台：<a href="http://slave1/admin.php" target="_blank" rel="noopener">http://slave1/admin.php</a>  </li><li>点击顶部的“论坛”菜单  </li><li>按照页面提示创建所需版本，可以创建父子版块  </li></ul><h3 id="查看访问日志"><a href="#查看访问日志" class="headerlink" title="查看访问日志"></a>查看访问日志</h3><p>日志默认地址<br><code>/opt/lampp/logs/access_log</code><br>实时查看日志命令<br><code>tail –f /opt/lampp/logs/access_log</code>  </p><h2 id="Discuz帖子-版块存储简介"><a href="#Discuz帖子-版块存储简介" class="headerlink" title="Discuz帖子/版块存储简介"></a>Discuz帖子/版块存储简介</h2><p><code>mysql -uroot -p123 ultrax # 登录ultrax数据库</code><br><code>查看包含帖子id及标题对应关系的表</code><br><code>#tid, subject（文章id、标题）</code><br><code>select tid, subject from pre_forum_post limit 10;</code><br><code>#fid, name（版块id、标题）</code><br><code>select fid, name from pre_forum_forum limit 40;</code>  </p><h2 id="修改日志格式"><a href="#修改日志格式" class="headerlink" title="修改日志格式"></a>修改日志格式</h2><h3 id="找到Apache配置文件"><a href="#找到Apache配置文件" class="headerlink" title="找到Apache配置文件"></a>找到Apache配置文件</h3><p>Apache配置文件名称为httpd.conf，所在目录为 /opt/lampp/etc/ ，完整路径为 /opt/lampp/etc/httpd.conf</p><h3 id="修改日志格式-1"><a href="#修改日志格式-1" class="headerlink" title="修改日志格式"></a>修改日志格式</h3><p>关闭通用日志文件的使用<br><code>CustomLog &quot;logs/access_log&quot; common</code><br>启用组合日志文件<br><code>CustomLog &quot;logs/access_log&quot; combined</code><br>重新加载配置文件<br><code>xampp reload</code><br>检查访问日志<br><code>tail -f /opt/lampp/logs/access_log</code>  </p><h3 id="Flume与Kafka配置"><a href="#Flume与Kafka配置" class="headerlink" title="Flume与Kafka配置"></a>Flume与Kafka配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#agent的名称为a1  </span><br><span class="line">a1.sources = source1  </span><br><span class="line">a1.channels = channel1  </span><br><span class="line">a1.sinks = sink1</span><br><span class="line">#set source</span><br><span class="line">a1.sources.source1.type = TAILDIR  </span><br><span class="line">a1.sources.source1.filegroups = f1  </span><br><span class="line">a1.sources.source1.filegroups.f1 = /opt/lampp/logs/access_log  </span><br><span class="line">a1sources.source1.fileHeader = flase  </span><br><span class="line">#set sink</span><br><span class="line">a1.sinks.sink1.type = org.apache.flume.sink.kafka.KafkaSink  </span><br><span class="line">a1.sinks.sink1.brokerList=kms-2.apache.com:9092,kms-3.apache.com:9092,kms-4.apache.com:9092    </span><br><span class="line">​a1.sinks.sink1.topic= discuzlog  </span><br><span class="line">​a1.sinks.sink1.kafka.flumeBatchSize = 20  </span><br><span class="line">​a1.sinks.sink1.kafka.producer.acks = 1  </span><br><span class="line">​a1.sinks.sink1.kafka.producer.linger.ms = 1  </span><br><span class="line">​a1.sinks.sink1.kafka.producer.compression.type = snappy  </span><br><span class="line">#set channel</span><br><span class="line">​a1.channels.channel1.type = file  </span><br><span class="line">​a1.channels.channel1.checkpointDir = /home/kms/data/flume_data/checkpoint  </span><br><span class="line">​a1.channels.channel1.dataDirs= /home/kms/data/flume_data/data  </span><br><span class="line">#bind</span><br><span class="line">​a1.sources.source1.channels = channel1  </span><br><span class="line">​a1.sinks.sink1.channel = channel1</span><br></pre></td></tr></table></figure><h2 id="创建MySQL数据库和所需要的表"><a href="#创建MySQL数据库和所需要的表" class="headerlink" title="创建MySQL数据库和所需要的表"></a>创建MySQL数据库和所需要的表</h2><p><strong>创建数据库</strong>  </p><p><code>CREATE DATABASE</code>statistics<code>CHARACTER SET &#39;utf8&#39; COLLATE &#39;utf8_general_ci&#39;;</code>  </p><p><strong>创建表:</strong><br>&emsp;&emsp;特定时间段内不同ip的访问次数：client_ip_access<br>CREATE TABLE client_ip_access (<br>&emsp;&emsp;&emsp;&emsp;client_ip text COMMENT ‘客户端ip’,<br>&emsp;&emsp;&emsp;&emsp;sum BIGINT ( 20 ) NOT NULL COMMENT ‘访问次数’,<br>&emsp;&emsp;&emsp;&emsp;time text NOT NULL COMMENT ‘统计时间’<br>) ENGINE = INNODB DEFAULT CHARSET = utf8;  </p><p>&emsp;&emsp;特定时间段内不同文章的访问次数：hot_article<br>CREATE TABLE hot_article (<br>&emsp;&emsp;&emsp;&emsp;article_id text COMMENT ‘文章id’,<br>&emsp;&emsp;&emsp;&emsp;subject text NOT NULL COMMENT ‘文章标题’,<br>&emsp;&emsp;&emsp;&emsp;sum BIGINT ( 20 ) NOT NULL COMMENT ‘访问次数’,<br>&emsp;&emsp;&emsp;&emsp;time text NOT NULL COMMENT ‘统计时间’<br>) ENGINE = INNODB DEFAULT CHARSET = utf8;  </p><p>&emsp;&emsp;特定时间段内不同版块的访问次数：hot_section<br>CREATE TABLE hot_section (<br>&emsp;&emsp;&emsp;&emsp;section_id text COMMENT ‘版块id’,<br>&emsp;&emsp;&emsp;&emsp;name text NOT NULL COMMENT ‘版块标题’,<br>&emsp;&emsp;&emsp;&emsp;sum BIGINT ( 20 ) NOT NULL COMMENT ‘访问次数’,<br>&emsp;&emsp;&emsp;&emsp;time text NOT NULL COMMENT ‘统计时间’<br>) ENGINE = INNODB DEFAULT CHARSET = utf8; </p><h2 id="打包部署存在的问题"><a href="#打包部署存在的问题" class="headerlink" title="打包部署存在的问题"></a>打包部署存在的问题</h2><p><strong>问题1</strong> </p><pre><code>Exception in thread &quot;main&quot; java.lang.SecurityException: Invalid signature file digest for Manifest main attributes</code></pre><p><strong>解决方式</strong>  </p><p>原因:使用sbt打包的时候导致某些包的重复引用，所以打包之后的META-INF的目录下多出了一些<em>.SF,</em>.DSA,*.RSA文件  </p><p>解决办法：删除掉多于的<em>.SF,</em>.DSA,*.RSA文件  </p><p><code>zip -d log_analysis-1.0-SNAPSHOT.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF</code>  </p><p><strong>问题2</strong>    </p><pre><code>Exception in thread &quot;main&quot; java.io.FileNotFoundException: File file:/data/spark_data/history/event-log does not exist</code></pre><p><strong>解决方式</strong>  </p><p>原因:由于spark的spark-defaults.conf配置文件中配置 eventLog 时指定的路径在本机不存在。  </p><p>解决办法：创建对应的文件夹，并赋予对应权限<br><code>sudo mkdir -p /data/spark_data/history/spark-events</code><br><code>sudo mkdir -p /data/spark_data/history/event-log</code><br><code>sudo chmod 777 -R /data</code>  </p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>首先，基于discuz搭建了论坛，针对论坛产生的日志，对其进行分析。主要的处理流程为log—&gt;flume—&gt;kafka—&gt;sparkstreaming—&gt;MySQL,最后将处理的结果写入MySQL共报表查询。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的状态后端(State Backends)</title>
      <link href="/2019/08/23/Flink%E7%9A%84%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF-State-Backends/"/>
      <url>/2019/08/23/Flink%E7%9A%84%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF-State-Backends/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp;当使用checkpoint时，状态(state)会被持久化到checkpoint上，以防止数据的丢失并确保发生故障时能够完全恢复。状态是通过什么方式在哪里持久化，取决于使用的状态后端。</p><a id="more"></a><h2 id="可用的状态后端"><a href="#可用的状态后端" class="headerlink" title="可用的状态后端"></a>可用的状态后端</h2><p><strong>MemoryStateBackend</strong><br><strong>FsStateBackend</strong><br><strong>FsStateBackend</strong>  </p><p>注意：如果什么都不配置，系统默认的是MemoryStateBackend</p><h2 id="MemoryStateBackend"><a href="#MemoryStateBackend" class="headerlink" title="MemoryStateBackend"></a>MemoryStateBackend</h2><p><img src="//jiamaoxiang.top/2019/08/23/Flink的状态后端-State-Backends/memorystatebackend.png" alt><br>&emsp;&emsp;<code>MemoryStateBackend</code> 是将状态维护在 Java 堆上的一个内部状态后端。键值状态和窗口算子使用哈希表来存储数据（values）和定时器（timers）。当应用程序 checkpoint 时，此后端会在将状态发给 JobManager 之前快照下状态，JobManager 也将状态存储在 Java 堆上。默认情况下，<code>MemoryStateBackend</code> 配置成支持异步快照。异步快照可以避免阻塞数据流的处理，从而避免反压的发生。当然，使用 <code>new MemoryStateBackend(MAX_MEM_STATE_SIZE, false)</code>也可以禁用该特点。</p><p><strong>缺点</strong>：</p><ul><li>默认情况下，每一个状态的大小限制为 5 MB。可以通过 <code>MemoryStateBackend</code> 的构造函数增加这个大小。状态大小受到 akka 帧大小的限制(maxStateSize &lt;= akka.framesize 默认 10 M)，所以无论怎么调整状态大小配置，都不能大于 akka 的帧大小。也可以通过 akka.framesize 调整 akka 帧大小。</li><li>状态的总大小不能超过 JobManager 的内存。</li></ul><p><strong>推荐使用的场景</strong>：</p><ul><li>本地测试、几乎无状态的作业，比如 ETL、JobManager 不容易挂，或挂掉影响不大的情况。</li><li>不推荐在生产场景使用。</li></ul><h2 id="FsStateBackend"><a href="#FsStateBackend" class="headerlink" title="FsStateBackend"></a>FsStateBackend</h2><p><img src="//jiamaoxiang.top/2019/08/23/Flink的状态后端-State-Backends/fsstatebackend.png" alt><br>&emsp;&emsp;<code>FsStateBackend</code>需要配置的主要是文件系统，如 URL（类型，地址，路径）。比如可以是：<br><code>“hdfs://namenode:40010/flink/checkpoints”</code> 或<code>“s3://flink/checkpoints”</code></p><p>&emsp;&emsp;当选择使用 <code>FsStateBackend</code>时，正在进行的数据会被存在TaskManager的内存中。在checkpoint时，此后端会将状态快照写入配置的文件系统和目录的文件中，同时会在JobManager的内存中（在高可用场景下会存在 Zookeeper 中）存储极少的元数据。容量限制上，单 TaskManager 上 State 总量不超过它的内存，总大小不超过配置的文件系统容量。</p><p>&emsp;&emsp;默认情况下，<code>FsStateBackend</code> 配置成提供异步快照，以避免在状态 checkpoint 时阻塞数据流的处理。该特性可以实例化 <code>FsStateBackend</code> 时传入false的布尔标志来禁用掉，例如：<code>new FsStateBackend(path, false)</code></p><p><strong>推荐使用的场景</strong>：</p><ul><li>处理大状态，长窗口，或大键值状态的有状态处理任务， 例如分钟级窗口聚合或 join。</li><li>适合用于高可用方案（需要开启HA的作业）。</li><li>可以在生产环境中使用</li></ul><h2 id="RocksDBStateBackend"><a href="#RocksDBStateBackend" class="headerlink" title="RocksDBStateBackend"></a>RocksDBStateBackend</h2><p><img src="//jiamaoxiang.top/2019/08/23/Flink的状态后端-State-Backends/rocksdbstatebackend.png" alt><br>&emsp;&emsp;<code>RocksDBStateBackend</code> 的配置也需要一个文件系统（类型，地址，路径），如下所示：<br>“hdfs://namenode:40010/flink/checkpoints” 或“s3://flink/checkpoints”<br>RocksDB 是一种嵌入式的本地数据库。RocksDBStateBackend 将处理中的数据使用 <a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a> 存储在本地磁盘上。在 checkpoint 时，整个 RocksDB 数据库会被存储到配置的文件系统中，或者在超大状态作业时可以将增量的数据存储到配置的文件系统中。同时 Flink 会将极少的元数据存储在 JobManager 的内存中，或者在 Zookeeper 中（对于高可用的情况）。<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a> 默认也是配置成异步快照的模式。</p><p>&emsp;&emsp;<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>是一个 key/value 的内存存储系统，和其他的 key/value 一样，先将状态放到内存中，如果内存快满时，则写入到磁盘中，但需要注意<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>不支持同步的 Checkpoint，构造方法中没有同步快照这个选项。不过<a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>支持增量的 Checkpoint，也是目前唯一增量 Checkpoint 的 Backend，意味着并不需要把所有 sst 文件上传到 Checkpoint 目录，仅需要上传新生成的 sst 文件即可。它的 Checkpoint 存储在外部文件系统（本地或HDFS），其容量限制只要单个 TaskManager 上 State 总量不超过它的内存+磁盘，单Key最大2G，总大小不超过配置的文件系统容量即可。</p><p><strong>缺点</strong>：</p><ul><li><a href="https://rocksdb.org/" target="_blank" rel="noopener">RocksDB</a>支持的单key和单value的大小最大为每个 2^31 字节。这是因为 RocksDB 的 JNI API 是基于byte[]的。<br></li><li>对于使用具有合并操作的状态的应用程序，例如 ListState，随着时间可能会累积到超过 2^31 字节大小，这将会导致在接下来的查询中失败。</li></ul><p><strong>推荐使用的场景</strong>：</p><ul><li>最适合用于处理大状态，长窗口，或大键值状态的有状态处理任务。</li><li>非常适合用于高可用方案。</li><li>最好是对状态读写性能要求不高的作业</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&emsp;&emsp;那如何选择状态的类型和存储方式？结合前面的内容，可以看到，首先是要分析清楚业务场景；比如想要做什么，状态到底大不大。比较各个方案的利弊，选择根据需求合适的状态类型和存储方式即可。</p><hr><p><strong>Reference</strong></p><p>[1]<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/state/state_backends.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/state/state_backends.html</a><br>[2]<a href="https://ververica.cn/developers/state-management/" target="_blank" rel="noopener">https://ververica.cn/developers/state-management/</a><br>[3]<a href="https://www.ververica.com/blog/stateful-stream-processing-apache-flink-state-backends" target="_blank" rel="noopener">https://www.ververica.com/blog/stateful-stream-processing-apache-flink-state-backends</a></p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浅析数据库缓冲池与SQL查询成本</title>
      <link href="/2019/08/14/%E6%B5%85%E6%9E%90%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%86%B2%E6%B1%A0%E4%B8%8ESQL%E6%9F%A5%E8%AF%A2%E6%88%90%E6%9C%AC/"/>
      <url>/2019/08/14/%E6%B5%85%E6%9E%90%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%86%B2%E6%B1%A0%E4%B8%8ESQL%E6%9F%A5%E8%AF%A2%E6%88%90%E6%9C%AC/</url>
      
        <content type="html"><![CDATA[<p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/background.jpg" alt><br>&emsp;&emsp;如果我们想要查找多行记录，查询时间是否会成倍地提升呢？其实数据库会采用缓冲池的方式提升页(page)的查找效率。数据库的缓冲池在数据库中起到了怎样的作用？如何查看一条 SQL 语句需要在缓冲池中进行加载的页的数量呢？</p><hr><h2 id="数据库缓冲池"><a href="#数据库缓冲池" class="headerlink" title="数据库缓冲池"></a>数据库缓冲池</h2><p>​        &emsp;&emsp;磁盘 I/O 需要消耗的时间很多，而在内存中进行操作，效率则会高很多，为了能让数据表或者索引中的数据随时被我们所用，DBMS 会申请占用内存来作为数据缓冲池，这样做的好处是可以让磁盘活动最小化，从而减少与磁盘直接进行 I/O 的时间。要知道，这种策略对提升 SQL 语句的查询性能来说至关重要。如果索引的数据在缓冲池里，那么访问的成本就会降低很多。<br>​       &emsp;&emsp;那么缓冲池如何读取数据呢？<br>​        &emsp;&emsp;缓冲池管理器会尽量将经常使用的数据保存起来，在数据库进行页面读操作的时候，首先会判断该页面是否在缓冲池中，如果存在就直接读取，如果不存在，就会通过内存或磁盘将页面存放到缓冲池中再进行读取。</p><h2 id="查看缓冲池大小"><a href="#查看缓冲池大小" class="headerlink" title="查看缓冲池大小"></a>查看缓冲池大小</h2><p>​         &emsp;&emsp;如果使用的是 MyISAM 存储引擎(只缓存索引，不缓存数据)，对应的键缓存参数为 key_buffer_size，可以用它进行查看。<br>​        &emsp;&emsp;如果使用的是 InnoDB 存储引擎，可以通过查看 innodb_buffer_pool_size 变量来查看缓冲池的大小，命令如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show variables like <span class="string">'innodb_buffer_pool_size'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/query_innodb_buffer_size.png" alt><br>​        &emsp;&emsp;此时 InnoDB 的缓冲池大小只有 8388608/1024/1024=8MB，我们可以修改缓冲池大小为 128MB，方法如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; <span class="built_in">set</span> global innodb_buffer_pool_size = 1073741824;</span><br></pre></td></tr></table></figure><p>​      &emsp;&emsp; 在 InnoDB 存储引擎中，可以同时开启多个缓冲池，查看缓冲池的个数，使用命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show variables like <span class="string">'innodb_buffer_pool_instances'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/innodb_buffer_pool_instance.png" alt><br>​        &emsp;&emsp;只有一个缓冲池。实际上innodb_buffer_pool_instances默认情况下为 8，为什么只显示只有一个呢？这里需要说明的是，如果想要开启多个缓冲池，你首先需要将innodb_buffer_pool_size参数设置为大于等于 1GB，这时innodb_buffer_pool_instances才会大于 1。你可以在 MySQL 的配置文件中对innodb_buffer_pool_size进行设置，大于等于 1GB，然后再针对innodb_buffer_pool_instances参数进行修改。</p><h2 id="查看SQL语句的查询成本"><a href="#查看SQL语句的查询成本" class="headerlink" title="查看SQL语句的查询成本"></a>查看SQL语句的查询成本</h2><p>​        &emsp;&emsp; 一条 SQL 查询语句在执行前需要确定查询计划，如果存在多种查询计划的话，MySQL 会计算每个查询计划所需要的成本，从中选择成本最小的一个作为最终执行的查询计划。</p><p>​          &emsp;&emsp;如果查看某条 SQL 语句的查询成本，可以在执行完这条 SQL 语句之后，通过查看当前会话中的 last_query_cost 变量值来得到当前查询的成本。这个查询成本对应的是 SQL 语句所需要读取的页(page)的数量。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show status like <span class="string">'last_query_cost'</span></span><br></pre></td></tr></table></figure><p><strong>example</strong>  </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; select userid,rating from movierating <span class="built_in">where</span> userid = 4169;</span><br></pre></td></tr></table></figure><p>结果：2313 rows in set (0.05 sec) </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show status like <span class="string">'last_query_cost'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/test1.png" alt></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; select userid,rating from movierating <span class="built_in">where</span> userid between 4168 and 4175;</span><br></pre></td></tr></table></figure><p>结果：2643 rows in set (0.01 sec) </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; show status like <span class="string">'last_query_cost'</span>;</span><br></pre></td></tr></table></figure><p><img src="//jiamaoxiang.top/2019/08/14/浅析数据库缓冲池与SQL查询成本/test2.png" alt></p><p>&emsp;&emsp;你能看到页的数量是刚才的 1.4 倍，但是查询的效率并没有明显的变化，实际上这两个 SQL 查询的时间基本上一样，就是因为采用了顺序读取的方式将页面一次性加载到缓冲池中，然后再进行查找。虽然页数量（last_query_cost）增加了不少，但是通过缓冲池的机制，并没有增加多少查询时间。</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink自学系列教程</title>
      <link href="/2019/08/13/Flink%E8%87%AA%E5%AD%A6%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B/"/>
      <url>/2019/08/13/Flink%E8%87%AA%E5%AD%A6%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p><img src="//jiamaoxiang.top/2019/08/13/Flink自学系列教程/logo.png" alt><br>&emsp;&emsp;Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</p><hr><h4 id="1-Flink的几个重要概念"><a href="#1-Flink的几个重要概念" class="headerlink" title="1.Flink的几个重要概念"></a><a href="https://jiamaoxiang.top/2019/08/19/Flink%E7%9A%84%E5%87%A0%E4%B8%AA%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5/">1.Flink的几个重要概念</a></h4><h4 id="2-DataFrame-API"><a href="#2-DataFrame-API" class="headerlink" title="2. DataFrame API"></a><a href="https://jiamaoxiang.top/2019/08/19/DataFrame-API/">2. DataFrame API</a></h4><h4 id="3-基于时间的算子"><a href="#3-基于时间的算子" class="headerlink" title="3. 基于时间的算子"></a><a href="https://jiamaoxiang.top/2019/08/19/%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E7%9A%84%E7%AE%97%E5%AD%90/">3. 基于时间的算子</a></h4><h4 id="4-状态与容错"><a href="#4-状态与容错" class="headerlink" title="4. 状态与容错"></a><a href="https://jiamaoxiang.top/2019/08/19/%E7%8A%B6%E6%80%81%E4%B8%8E%E5%AE%B9%E9%94%99/">4. 状态与容错</a></h4><h4 id="5-source-sink-Connectors"><a href="#5-source-sink-Connectors" class="headerlink" title="5. source/sink Connectors"></a><a href="https://jiamaoxiang.top/2019/08/19/source-sink-Connectors/">5. source/sink Connectors</a></h4><h4 id="6-集群与部署"><a href="#6-集群与部署" class="headerlink" title="6. 集群与部署"></a><a href="https://jiamaoxiang.top/2019/08/19/%E9%9B%86%E7%BE%A4%E4%B8%8E%E9%83%A8%E7%BD%B2/">6. 集群与部署</a></h4><h4 id="7-运维与监控"><a href="#7-运维与监控" class="headerlink" title="7.运维与监控"></a><a href="https://jiamaoxiang.top/2019/08/19/%E8%BF%90%E7%BB%B4%E4%B8%8E%E7%9B%91%E6%8E%A7/">7.运维与监控</a></h4>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/08/12/hello-world/"/>
      <url>/2019/08/12/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p><p><img src="//jiamaoxiang.top/2019/08/12/hello-world/logo.png" alt></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
